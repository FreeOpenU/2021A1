Linear Algebra, Theory And Applications Kenneth Kuttler January 29, 2012 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationContents 1 Preliminaries 11 1.1 Sets And Set Notation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11 1.2 Functions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12 1.3 The Number Line And Algebra Of The Real Numbers .
.
.
.
.
.
.
.
.
.
.
.
.
12 1.4 Ordered ﬁelds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14 1.5 The Complex Numbers.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15 1.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19 1.7 Completeness of R .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20 1.8 Well Ordering And Archimedean Property .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21 1.9 Division And Numbers .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23 1.10 Systems Of Equations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26 1.11 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31 1.12 Fn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32 1.13 Algebra in Fn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32 1.14 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33 1.15 The Inner Product In Fn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33 1.16 What Is Linear Algebra?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36 1.17 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36 2 Matrices And Linear Transformations 37 2.1 Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37 2.1.1 The ijth Entry Of A Product .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41 2.1.2 Digraphs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43 2.1.3 Properties Of Matrix Multiplication .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45 2.1.4 Finding The Inverse Of A Matrix .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
48 2.2 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51 2.3 Linear Transformations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
53 2.4 Subspaces And Spans .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
56 2.5 An Application To Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
61 2.6 Matrices And Calculus .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62 2.6.1 The Coriolis Acceleration .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63 2.6.2 The Coriolis Acceleration On The Rotating Earth .
.
.
.
.
.
.
.
.
.
.
66 2.7 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71 3 Determinants 77 3.1 Basic Techniques And Properties .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
77 3.2 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
81 3.3 The Mathematical Theory Of Determinants .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
83 3.3.1 The Function sgn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
84 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4 CONTENTS 3.3.2 The Deﬁnition Of The Determinant .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
86 3.3.3 A Symmetric Deﬁnition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
87 3.3.4 Basic Properties Of The Determinant .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
88 3.3.5 Expansion Using Cofactors .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
90 3.3.6 A Formula For The Inverse .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
92 3.3.7 Rank Of A Matrix .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
94 3.3.8 Summary Of Determinants .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
96 3.4 The Cayley Hamilton Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
97 3.5 Block Multiplication Of Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
98 3.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
102 4 Row Operations 105 4.1 Elementary Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
105 4.2 The Rank Of A Matrix .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
110 4.3 The Row Reduced Echelon Form .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
112 4.4 Rank And Existence Of Solutions To Linear Systems .
.
.
.
.
.
.
.
.
.
.
.
.
.
116 4.5 Fredholm Alternative.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
117 4.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
118 5 Some Factorizations 123 5.1 LU Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
123 5.2 Finding An LU Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
123 5.3 Solving Linear Systems Using An LU Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
125 5.4 The PLU Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
126 5.5 Justiﬁcation For The Multiplier Method .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
127 5.6 Existence For The PLU Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
128 5.7 The QR Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
130 5.8 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133 6 Linear Programming 135 6.1 Simple Geometric Considerations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
135 6.2 The Simplex Tableau .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
136 6.3 The Simplex Algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
140 6.3.1 Maximums .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
140 6.3.2 Minimums.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
143 6.4 Finding A Basic Feasible Solution.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
150 6.5 Duality .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
152 6.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
156 7 Spectral Theory 157 7.1 Eigenvalues And Eigenvectors Of A Matrix .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
157 7.2 Some Applications Of Eigenvalues And Eigenvectors .
.
.
.
.
.
.
.
.
.
.
.
.
.
164 7.3 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
167 7.4 Schur’s Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
173 7.5 Trace And Determinant .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
180 7.6 Quadratic Forms .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
181 7.7 Second Derivative Test .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
182 7.8 The Estimation Of Eigenvalues .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
186 7.9 Advanced Theorems .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
187 7.10 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
190 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationCONTENTS 5 8 Vector Spaces And Fields 199 8.1 Vector Space Axioms .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
199 8.2 Subspaces And Bases .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
200 8.2.1 Basic Deﬁnitions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
200 8.2.2 A Fundamental Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
201 8.2.3 The Basis Of A Subspace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
205 8.3 Lots Of Fields.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
205 8.3.1 Irreducible Polynomials .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
205 8.3.2 Polynomials And Fields .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
210 8.3.3 The Algebraic Numbers .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
215 8.3.4 The Lindemannn Weierstrass Theorem And Vector Spaces.
.
.
.
.
.
.
219 8.4 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
219 9 Linear Transformations 225 9.1 Matrix Multiplication As A Linear Transformation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
225 9.2 L(V,W) As A Vector Space .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
225 9.3 The Matrix Of A Linear Transformation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
227 9.3.1 Some Geometrically Deﬁned Linear Transformations .
.
.
.
.
.
.
.
.
.
234 9.3.2 Rotations About A Given Vector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
237 9.3.3 The Euler Angles .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
238 9.4 Eigenvalues And Eigenvectors Of Linear Transformations .
.
.
.
.
.
.
.
.
.
.
240 9.5 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
242 10 Linear Transformations Canonical Forms 245 10.1 A Theorem Of Sylvester, Direct Sums .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
245 10.2 Direct Sums, Block Diagonal Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
248 10.3 Cyclic Sets .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
251 10.4 Nilpotent Transformations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
255 10.5 The Jordan Canonical Form .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
257 10.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
262 10.7 The Rational Canonical Form .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
266 10.8 Uniqueness .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
269 10.9 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
273 11 Markov Chains And Migration Processes 275 11.1 Regular Markov Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
275 11.2 Migration Matrices .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
279 11.3 Markov Chains .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
279 11.4 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
284 12 Inner Product Spaces 287 12.1 General Theory .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
287 12.2 The Gram Schmidt Process .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
289 12.3 Riesz Representation Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
292 12.4 The Tensor Product Of Two Vectors .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
295 12.5 Least Squares .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
296 12.6 Fredholm Alternative Again .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
298 12.7 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
298 12.8 The Determinant And Volume .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
303 12.9 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
306 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6 CONTENTS 13 Self Adjoint Operators 307 13.1 Simultaneous Diagonalization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
307 13.2 Schur’s Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
310 13.3 Spectral Theory Of Self Adjoint Operators.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
312 13.4 Positive And Negative Linear Transformations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
317 13.5 Fractional Powers .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
319 13.6 Polar Decompositions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
322 13.7 An Application To Statistics .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
325 13.8 The Singular Value Decomposition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
327 13.9 Approximation In The Frobenius Norm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
329 13.10Least Squares And Singular Value Decomposition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
331 13.11The Moore Penrose Inverse .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
331 13.12Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
334 14 Norms For Finite Dimensional Vector Spaces 337 14.1 The p Norms .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
343 14.2 The Condition Number .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
345 14.3 The Spectral Radius .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
348 14.4 Series And Sequences Of Linear Operators .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
350 14.5 Iterative Methods For Linear Systems .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
354 14.6 Theory Of Convergence .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
360 14.7 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
363 15 Numerical Methods For Finding Eigenvalues 371 15.1 The Power Method For Eigenvalues.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
371 15.1.1 The Shifted Inverse Power Method .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
375 15.1.2 The Explicit Description Of The Method .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
376 15.1.3 Complex Eigenvalues .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
381 15.1.4 Rayleigh Quotients And Estimates for Eigenvalues .
.
.
.
.
.
.
.
.
.
.
383 15.2 The QR Algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
386 15.2.1 Basic Properties And Deﬁnition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
386 15.2.2 The Case Of Real Eigenvalues .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
390 15.2.3 The QR Algorithm In The General Case.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
394 15.3 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
401 A Positive Matrices 403 B Functions Of Matrices 411 C Applications To Diﬀerential Equations 417 C.1 Theory Of Ordinary Diﬀerential Equations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
417 C.2 Linear Systems .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
418 C.3 Local Solutions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
419 C.4 First Order Linear Systems .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
421 C.5 Geometric Theory Of Autonomous Systems .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
428 C.6 General Geometric Theory.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
432 C.7 The Stable Manifold .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
434 D Compactness And Completeness 439 D.0.1 The Nested Interval Lemma .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
439 D.0.2 Convergent Sequences, Sequential Compactness .
.
.
.
.
.
.
.
.
.
.
.
.
440 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationCONTENTS 7 E The Fundamental Theorem Of Algebra 443 F Fields And Field Extensions 445 F.1 The Symmetric Polynomial Theorem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
445 F.2 The Fundamental Theorem Of Algebra.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
447 F.3 Transcendental Numbers .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
451 F.4 More On Algebraic Field Extensions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
459 F.5 The Galois Group .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
464 F.6 Normal Subgroups .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
469 F.7 Normal Extensions And Normal Subgroups .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
470 F.8 Conditions For Separability .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
471 F.9 Permutations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
475 F.10 Solvable Groups .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
479 F.11 Solvability By Radicals.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
482 G Answers To Selected Exercises 487 G.1 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
487 G.2 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
487 G.3 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
487 G.4 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
487 G.5 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
487 G.6 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
488 G.7 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
489 G.8 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
489 G.9 Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
490 G.10Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
491 G.11Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
492 G.12Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
492 G.13Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
493 G.14Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
494 G.15Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
494 G.16Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
494 G.17Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
495 G.18Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
495 G.19Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
495 G.20Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
496 G.21Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
496 G.22Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
496 G.23Exercises .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
496 Copyright ⃝c 2012, Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8 CONTENTS Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationPreface This is a book on linear algebra and matrix theory.
While it is self contained, it will work bestforthosewhohavealreadyhadsomeexposuretolinearalgebra.
Itisalsoassumedthat the reader has had calculus.
Some optional topics require more analysis than this, however.
I think that the subject of linear algebra is likely the most signiﬁcant topic discussed in undergraduate mathematics courses.
Part of the reason for this is its usefulness in unifying so many diﬀerent topics.
Linear algebra is essential in analysis, applied math, and even in theoretical mathematics.
This is the point of view of this book, more than a presentation of linear algebra for its own sake.
This is why there are numerous applications, some fairly unusual.
This book features an ugly, elementary, and complete treatment of determinants early in the book.
Thus it might be considered as Linear algebra done wrong.
I have done this because of the usefulness of determinants.
However, all major topics are also presented in an alternative manner which is independent of determinants.
The book has an introduction to various numerical methods used in linear algebra.
This is done because of the interesting nature of these methods.
The presentation here emphasizes the reasons why they work.
It does not discuss many important numerical considerations necessary to use the methods eﬀectively.
These considerations are found in numerical analysis texts.
In the exercises, you may occasionally see ↑ at the beginning.
This means you ought to have a look at the exercise above it.
Some exercises develop a topic sequentially.
There are also a few exercises whichappear more than once in the book.
I havedone this deliberately becauseIthinkthattheseillustrateexceptionallyimportanttopicsandbecausesomepeople don’treadthewholebookfromstarttoﬁnishbutinsteadjumpintothemiddlesomewhere.
ThereisoneonatheoremofSylvesterwhichappearsnofewerthan3times.
Thenitisalso proved in the text.
There are multiple proofs of the Cayley Hamilton theorem, some in the exercises.
Someexercisesalsoareincludedforthesakeofemphasizingsomethingwhichhas been done in the preceding chapter.
9 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10 CONTENTS Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationPreliminaries 1.1 Sets And Set Notation A set is just a collection of things called elements.
For example {1,2,3,8} would be a set consisting of the elements 1,2,3, and 8.
To indicate that 3 is an element of {1,2,3,8}, it is customary to write 3∈{1,2,3,8}.
9∈/ {1,2,3,8} means 9 is not an element of {1,2,3,8}.
Sometimes a rule speciﬁes a set.
For example you could specify a set as all integers larger than 2.
This would be written as S = {x∈Z:x>2}.
This notation says: the set of all integers, x, such that x>2.
IfAandBaresetswiththepropertythateveryelementofAisanelementofB,thenAis asubsetofB.Forexample,{1,2,3,8}isasubsetof{1,2,3,4,5,8},insymbols,{1,2,3,8}⊆ {1,2,3,4,5,8}.
It is sometimes said that “A is contained in B” or even “B contains A”.
The same statement about the two sets may also be written as {1,2,3,4,5,8}⊇{1,2,3,8}.
The union of two sets is the set consisting of everything which is an element of at least one of the sets, A or B.
As an example of the union of two sets {1,2,3,8}∪{3,4,7,8} = {1,2,3,4,7,8}becausethese numbersarethosewhichareinatleastone ofthetwosets.
In general A∪B ≡{x:x∈A or x∈B}.
Be sure you understand that something which is in both A and B is in the union.
It is not an exclusive or.
The intersection of two sets, A and B consists of everything which is in both of the sets.
Thus {1,2,3,8}∩{3,4,7,8}={3,8} because 3 and 8 are those elements the two sets have in common.
In general, A∩B ≡{x:x∈A and x∈B}.
The symbol [a,b] where a and b are real numbers, denotes the set of real numbers x, such that a ≤ x ≤ b and [a,b) denotes the set of real numbers such that a ≤ x < b.
(a,b) consists of the set of real numbers x such that a < x < b and (a,b] indicates the set of numbers x such that a<x≤b.
[a,∞) means the set of all numbers x such that x≥a and (−∞,a] means the set of all real numbers which are less than or equal to a.
These sorts of sets of real numbers are called intervals.
The two points a and b are called endpoints of the interval.
Otherintervalssuchas(−∞,b)aredeﬁnedbyanalogytowhatwasjustexplained.
In general, the curved parenthesis indicates the end point it sits next to is not included while the square parenthesis indicates this end point is included.
The reason that there will always be a curved parenthesis next to ∞ or −∞ is that these are not real numbers.
Therefore, they cannot be included in any set of real numbers.
A special set which needs to be given a name is the empty set also called the null set, denoted by ∅.
Thus ∅ is deﬁned as the set which has no elements in it.
Mathematicians like to say the empty set is a subset of every set.
The reason they say this is that if it were not 11 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12 PRELIMINARIES so, there would have to exist a set A, such that ∅ has something in it which is not in A.
However, ∅ has nothing in it and so the least intellectual discomfort is achieved by saying ∅⊆A.
If A and B are two sets, A\B denotes the set of things which are in A but not in B.
Thus A\B ≡{x∈A:x∈/ B}.
Set notation is used whenever convenient.
1.2 Functions Theconceptofafunctionisthatofsomethingwhichgivesauniqueoutputforagiveninput.
Deﬁnition 1.2.1 Consider two sets, D and R along with a rule which assigns a unique element of R to every element of D. This rule is called a function and it is denoted by a letter such as f. Given x∈D, f(x) is the name of the thing in R which results from doing f to x.
Then D is called the domain of f. In order to specify that D pertains to f, the notation D(f) may be used.
The set R is sometimes called the range of f. These days it is referred to as the codomain.
The set of all elements of R which are of the form f(x) for some x ∈ D is therefore, a subset of R. This is sometimes referred to as the image of f. When this set equals R, the function f is said to be onto, also surjective.
If whenever x ̸= y it follows f(x) ̸= f(y), the function is called one to one.
, also injective It is common notation to write f :D 7→R to denote the situation just described in this deﬁnition where f is a function deﬁned on a domain D which has values in a codomain R. Sometimes you may also see something like D 7→f R to denote the same thing.
1.3 The Number Line And Algebra Of The Real Num- bers Next, consider the real numbers, denoted by R, as a line extending inﬁnitely far in both directions.
In this book, the notation, ≡ indicates something is being deﬁned.
Thus the integers are deﬁned as Z≡{···−1,0,1,···}, the natural numbers, N≡{1,2,···} and the rational numbers, deﬁned as the numbers which are the quotient of two integers.
{ } m Q≡ such that m,n∈Z,n̸=0 n are each subsets of R as indicated in the following picture.
−4 −3 −2 −1 0 1 2 3 4 (cid:27) - 1/2 As shown in the picture, 1 is half way between the number 0 and the number, 1.
By 2 analogy,youcanseewheretoplacealltheotherrationalnumbers.
ItisassumedthatRhas Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.3.
THE NUMBER LINE AND ALGEBRA OF THE REAL NUMBERS 13 thefollowingalgebraproperties,listedhereasacollectionofassertionscalledaxioms.
These properties will not be proved which is why they are called axioms rather than theorems.
In general, axioms are statements which are regarded as true.
Often these are things which are “self evident” either from experience or from some sort of intuition but this does not have to be the case.
Axiom 1.3.1 x+y =y+x, (commutative law for addition) Axiom 1.3.2 x+0=x, (additive identity).
Axiom 1.3.3 For each x ∈ R, there exists −x ∈ R such that x+(−x) = 0, (existence of additive inverse).
Axiom 1.3.4 (x+y)+z =x+(y+z),(associative law for addition).
Axiom 1.3.5 xy =yx,(commutative law for multiplication).
Axiom 1.3.6 (xy)z =x(yz),(associative law for multiplication).
Axiom 1.3.7 1x=x,(multiplicative identity).
Axiom 1.3.8 For each x̸=0, there exists x−1 such that xx−1 =1.
(existence of multiplica- tive inverse).
Axiom 1.3.9 x(y+z)=xy+xz.
(distributive law).
These axioms are known as the ﬁeld axioms and any set (there are many others besides R)whichhastwosuchoperationssatisfyingtheaboveaxiomsiscalledaﬁel(d. D)ivisionand subtraction are deﬁned in the usual way by x−y ≡x+(−y) and x/y ≡x y−1 .
Here is a little proposition which derives some familiar facts.
Proposition 1.3.10 0 and 1 are unique.
Also −x is unique and x−1 is unique.
Further- more, 0x=x0=0 and −x=(−1)x.
Proof: Suppose 0′ is another additive identity.
Then ′ ′ 0 =0 +0=0.
Thus 0 is unique.
Say 1′ is another multiplicative identity.
Then ′ ′ 1=11=1.
Now suppose y acts like the additive inverse of x.
Then −x=(−x)+0=(−x)+(x+y)=(−x+x)+y =y Finally, 0x=(0+0)x=0x+0x and so 0=−(0x)+0x=−(0x)+(0x+0x)=(−(0x)+0x)+0x=0x Finally x+(−1)x=(1+(−1))x=0x=0 and so by uniqueness of the additive inverse, (−1)x=−x.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14 PRELIMINARIES 1.4 Ordered ﬁelds The real numbers R are an example of an ordered ﬁeld.
More generally, here is a deﬁnition.
Deﬁnition 1.4.1 Let F be a ﬁeld.
It is an ordered ﬁeld if there exists an order, < which satisﬁes 1.
For any x̸=y, either x<y or y <x.
2.
If x<y and either z <w or z =w, then, x+z <y+w.
3.
If 0<x,0<y, then xy >0.
With this deﬁnition, the familiar properties of order can be proved.
The following proposition lists many of these familiar properties.
The relation ‘a > b’ has the same meaning as ‘b<a’.
Proposition 1.4.2 The following are obtained.
1.
If x<y and y <z, then x<z.
2.
If x>0 and y >0, then x+y >0.
3.
If x>0, then −x<0.
4.
If x̸=0, either x or −x is >0.
5.
If x<y, then −x>−y.
6.
If x̸=0, then x2 >0.
7.
If 0<x<y then x−1 >y−1.
Proof: First consider 1, called the transitive law.
Suppose that x<y and y <z.
Then from the axioms, x+y <y+z and so, adding −y to both sides, it follows x<z Next consider 2.
Suppose x>0 and y >0.
Then from 2, 0=0+0<x+y.
Next consider 3.
It is assumed x>0 so 0=−x+x>0+(−x)=−x Now consider 4.
If x<0, then 0=x+(−x)<0+(−x)=−x.
Consider the 5.
Since x<y, it follows from 2 0=x+(−x)<y+(−x) and so by 4 and Proposition 1.3.10, (−1)(y+(−x))<0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.5.
THE COMPLEX NUMBERS 15 Also from Proposition 1.3.10 (−1)(−x)=−(−x)=x and so −y+x<0.
Hence −y <−x.
Consider 6.
If x > 0, there is nothing to show.
It follows from the deﬁnition.
If x < 0, then by 4, −x>0 and so by Proposition 1.3.10 and the deﬁnition of the order, (−x)2 =(−1)(−1)x2 >0 By this proposition again, (−1)(−1) = −(−1) = 1 and so x2 > 0 as claimed.
Note that 1>0 because it equals 12.
Finally, consider 7.
First, if x>0 then if x−1 <0, it would follow (−1)x−1 >0 and so x(−1)x−1 =(−1)1=−1>0.
However, this would require 0>1=12 >0 fromwhatwasjustshown.
Therefore, x−1 >0.
Nowtheassumptionimpliesy+(−1)x>0 and so multiplying by x−1, yx−1+(−1)xx−1 =yx−1+(−1)>0 Now multiply by y−1, which by the above satisﬁes y−1 >0, to obtain x−1+(−1)y−1 >0 and so x−1 >y−1.
(cid:4) In an ordered ﬁeld the symbols ≤ and ≥ have the usual meanings.
Thus a ≤ b means a<b or else a=b, etc.
1.5 The Complex Numbers Just as a real number should be considered as a point on the line, a complex number is consideredapointintheplanewhichcanbeidentiﬁedintheusualwayusingtheCartesian coordinates of the point.
Thus (a,b) identiﬁes a point whose x coordinate is a and whose y coordinate is b.
In dealing with complex numbers, such a point is written as a+ib and multiplication and addition are deﬁned in the most obvious way subject to the convention that i2 =−1.
Thus, (a+ib)+(c+id)=(a+c)+i(b+d) and (a+ib)(c+id) = ac+iad+ibc+i2bd = (ac−bd)+i(bc+ad).
Everynonzerocomplexnumber,a+ib,witha2+b2 ̸=0,hasauniquemultiplicativeinverse.
1 a−ib a b = = −i .
a+ib a2+b2 a2+b2 a2+b2 You should prove the following theorem.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation16 PRELIMINARIES Theorem 1.5.1 The complex numbers with multiplication and addition deﬁned as above form a ﬁeld satisfying all the ﬁeld axioms listed on Page 13.
Note that if x+iy is a complex number, it can be written as ( ) √ x y x+iy = x2+y2 √ +i√ x2+y2 x2+y2 ( ) Now √ x ,√ y isapointontheunitcircleandsothereexistsauniqueθ ∈[0,2π) x2+y2 x2+y2 √ such that this ordered pair equals (cosθ,sinθ).
Letting r = x2+y2, it follows that the complex number can be written in the form x+iy =r(cosθ+isinθ) This is called the polar form of the complex number.
The ﬁeld of complex numbers is denoted as C. An important construction regarding complex numbers is the complex conjugate denoted by a horizontal line above the number.
It is deﬁned as follows.
a+ib≡a−ib.
Whatitdoesisreﬂectagivencomplexnumberacrossthexaxis.
Algebraically,thefollowing formula is easy to obtain.
( ) a+ib (a+ib)=a2+b2.
Deﬁnition 1.5.2 Deﬁne the absolute value of a complex number as follows.
√ |a+ib|≡ a2+b2.
Thus, denoting by z the complex number, z =a+ib, |z|=(zz)1/2.
With this deﬁnition, it is important to note the following.
Be sure to verify this.
It is not too hard but you need to do it.
√ Remark 1.5.3 : Let z =a+ib and w =c+id.
Then |z−w|= (a−c)2+(b−d)2.
Thus the distance between the point in the plane determined by the ordered pair, (a,b) and the ordered pair (c,d) equals |z−w| where z and w are as just described.
For example, con√sider the distance between (2,5) and (1,8).
From the distance formula √ this distance equals (2−1)2+(5−8)2 = 10.
On the other hand, letting z =2+i5 and √ w =1+i8,z−w =1−i3andso(z−w)(z−w)=(1−i3)(1+i3)=10so|z−w|= 10, the same thing obtained with the distance formula.
Complex numbers, are often written in the so called polar form which is described next.
Suppose x+iy is a complex number.
Then ( ) √ x y x+iy = x2+y2 √ +i√ .
x2+y2 x2+y2 Now note that ( ) ( ) 2 2 x y √ + √ =1 x2+y2 x2+y2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.5.
THE COMPLEX NUMBERS 17 and so ( ) x y √ ,√ x2+y2 x2+y2 is a point on the unit circle.
Therefore, there exists a unique angle, θ ∈[0,2π) such that x y cosθ = √ ,sinθ = √ .
x2+y2 x2+y2 The polar form of the complex number is then r(cosθ+isinθ) √ where θ is this angle just described and r = x2+y2.
A fundamental identity is the formula of De Moivre which follows.
Theorem 1.5.4 Let r >0 be given.
Then if n is a positive integer, [r(cost+isint)]n =rn(cosnt+isinnt).
Proof: It is clear the formula holds if n=1.
Suppose it is true for n. [r(cost+isint)]n+1 =[r(cost+isint)]n[r(cost+isint)] which by induction equals =rn+1(cosnt+isinnt)(cost+isint) =rn+1((cosntcost−sinntsint)+i(sinntcost+cosntsint)) =rn+1(cos(n+1)t+isin(n+1)t) by the formulas for the cosine and sine of the sum of two angles.
(cid:4) Corollary 1.5.5 Let z be a non zero complex number.
Then there are always exactly k kth roots of z in C. Proof: Let z = x+iy and let z = |z|(cost+isint) be the polar form of the complex number.
By De Moivre’s theorem, a complex number, r(cosα+isinα), is a kth root of z if and only if rk(coskα+isinkα)=|z|(cost+isint).
This requires rk =|z| and so r =|z|1/k and also both cos(kα)=cost and sin(kα)=sint.
This can only happen if kα=t+2lπ for l an integer.
Thus t+2lπ α= ,l∈Z k and so the kth roots of z are of the form ( ( ) ( )) t+2lπ t+2lπ |z|1/k cos +isin , l∈Z.
k k Since the cosine and sine are periodic of period 2π, there are exactly k distinct numbers which result from this formula.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation18 PRELIMINARIES Example 1.5.6 Find the three cube roots of i.
( ( ) ( )) First note that i = 1 cos π +isin π .
Using the formula in the proof of the above 2 2 corollary, the cube roots of i are ( ( ) ( )) (π/2)+2lπ (π/2)+2lπ 1 cos +isin 3 3 where l=0,1,2.
Therefore, the roots are ( ) ( ) ( ) ( ) π π 5 5 cos +isin ,cos π +isin π , 6 6 6 6 and ( ) ( ) 3 3 cos π +isin π .
2 2 √ ( ) √ ( ) Thus the cube roots of i are 3 +i 1 ,− 3 +i 1 , and −i.
2 2 2 2 The ability to ﬁnd kth roots can also be used to factor some polynomials.
Example 1.5.7 Factor the polynomial x3−27.
First ﬁnd the cube r(oots of 27.
)By the(above proc)edure using De Moivre’s theorem, √ √ these cube roots are 3,3 −1 +i 3 , and 3 −1 −i 3 .
Therefore, x3+27= 2 2 2 2 ( ( √ ))( ( √ )) −1 3 −1 3 (x−3) x−3 +i x−3 −i .
2 2 2 2 ( ( ))( ( )) √ √ Note also x−3 −1 +i 3 x−3 −1 −i 3 =x2+3x+9 and so 2 2 2 2 ( ) x3−27=(x−3) x2+3x+9 where the quadratic polynomial, x2 +3x+9 cannot be factored without using complex numbers.
TherealandcomplexnumbersbothareﬁeldssatisfyingtheaxiomsonPage13anditis usually one of these two ﬁelds which is used in linear algebra.
The numbers are often called scalars.
However, it turns out that all algebraic notions work for any ﬁeld and there are many others.
For this reason, I will often refer to the ﬁeld of scalars as F although F will usually be either the real or complex numbers.
If there is any doubt, assume it is the ﬁeld of complex numbers which is meant.
The reason the complex numbers are so signiﬁcant in l∑inear algebra is that they are algebraically complete.
This means that every polynomial n a zk, n≥1,a ̸=0, having coeﬃcients a in C has a root in in C. k=0 k n k Laterinthebook,proofsofthefundamentaltheoremofalgebraaregiven.
However,here is a simple explanation of why you should believe this theorem.
The issue is whether there exists z ∈C such that p(z)=0 for p(z) a polynomial having coeﬃcients in C. Dividing by the leading coeﬃcient, we can assume that p(z) is of the form p(z)=zn+an−1zn−1+···+a1z+a0, a0 ̸=0.
Ifa =0,thereisnothingtoprove.
DenotebyC thecircleofradiusr inthecomplexplane 0 r which is centered at 0.
Then if r is suﬃciently large and |z| = r, the term zn is far larger than the rest of the polynomial.
Thus, for r large enough, A = {p(z):z ∈C } describes r r a closed curve which misses the inside of some circle having 0 as its center.
Now shrink r. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.6.
EXERCISES 19 Eventually, for r small enough, the non constant terms are negligible and so A is a curve r which is contained in some circle centered at a which has 0 in its outside.
0 A r a 0 A r large r 0 r small Thus it is reasonable to believe that for some r during this shrinking process, the set A must hit 0.
It follows that p(z) = 0 for some z.
This is one of those arguments which r seems all right until you think about it too much.
Nevertheless, it will suﬃce to see that the fundamental theorem of algebra is at least very plausible.
A complete proof is in an appendix.
1.6 Exercises 1.
Let z =5+i9.
Find z−1.
2.
Let z =2+i7 and let w =3−i8.
Find zw,z+w,z2, and w/z.
3.
Give the complete solution to x4+16=0.
4.
Graph the complex cube roots of 8 in the complex plane.
Do the same for the four fourth roots of 16.
5.
If z is a complex number, show there exists ω a complex number with |ω| = 1 and ωz =|z|.
6.
De Moivre’s theorem says [r(cost+isint)]n = rn(cosnt+isinnt) for n a positive integer.
Does this formula continue to hold for all integers, n, even negative integers?
Explain.
7.
Youalreadyknowformulasforcos(x+y)andsin(x+y)andthesewereusedtoprove De Moivre’s theorem.
Now using De Moivre’s theorem, derive a formula for sin(5x) and one for cos(5x).
Hint: Use the binomial theorem.
8.
Ifz andw aretwocomplexnumbersandthepolarformofz involvestheangleθ while the polar form of w involves the angle ϕ, show that in the polar form for zw the angle involved is θ+ϕ.
Also, show that in the polar form of a complex number, z, r =|z|.
9.
Factor x3+8 as a product of linear factors.
( ) 10.
Write x3+27 in the form (x+3) x2+ax+b where x2+ax+b cannot be factored any more using only real numbers.
11.
Completely factor x4+16 as a product of linear factors.
12.
Factor x4+16 as the product of two quadratic polynomials each of which cannot be factored further without using complex numbers.
13.
Ifz,warecomplexnumbers∑provezw =∑zwandthenshowbyinductionthatz1···zm = z ···z .
Also verify that m z = m z .
In words this says the conjugate of a 1 m k=1 k k=1 k product equals the product of the conjugates and the conjugate of a sum equals the sum of the conjugates.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation20 PRELIMINARIES 14.
Suppose p(x)=anxn+an−1xn−1+···+a1x+a0 where all the ak are real numbers.
Suppose also that p(z)=0 for some z ∈C.
Show it follows that p(z)=0 also.
15.
I claim that 1=−1.
Here is why.
√ √ √ √ −1=i2 = −1 −1= (−1)2 = 1=1.
This is clearly a remarkable result but is there something wrong with it?
If so, what is wrong?
16.
DeMoivre’stheoremisreallyagrandthing.
Iplantouseitnowforrationalexponents, not just integers.
1=1(1/4) =(cos2π+isin2π)1/4 =cos(π/2)+isin(π/2)=i.
Therefore, squaring both sides it follows 1 = −1 as in the previous problem.
What does this tell you about De Moivre’s theorem?
Is there a profound diﬀerence between raising numbers to integer powers and raising numbers to non integer powers?
17.
Show that C cannot be considered an ordered ﬁeld.
Hint: Consider i2 = −1.
Recall that 1>0 by Proposition 1.4.2.
18.
Say a+ib < x+iy if a < x or if a = x, then b < y.
This is called the lexicographic order.
Showthatanytwodiﬀerentcomplexnumberscanbecomparedwiththisorder.
What goes wrong in terms of the other requirements for an ordered ﬁeld.
19.
With the order of Problem 18, consider for n ∈ N the complex number 1− 1.
Show n that with the lexicographic order just described, each of 1−in is an upper bound to all these numbers.
Therefore, this is a set which is “bounded above” but has no least upper bound with respect to the lexicographic order on C. 1.7 Completeness of R Recall the following important deﬁnition from calculus, completeness of R. Deﬁnition 1.7.1 A non empty set, S ⊆ R is bounded above (below) if there exists x ∈ R such that x ≥ (≤)s for all s ∈ S. If S is a nonempty set in R which is bounded above, then a number, l which has the property that l is an upper bound and that every other upper bound is no smaller than l is called a least upper bound, l.u.b.
(S) or often sup(S).
If S is a nonempty set bounded below, deﬁne the greatest lower bound, g.l.b.
(S) or inf(S) similarly.
Thus g is the g.l.b.
(S) means g is a lower bound for S and it is the largest of all lower bounds.
If S is a nonempty subset of R which is not bounded above, this information is expressed by saying sup(S)=+∞ and if S is not bounded below, inf(S)=−∞.
Every existence theorem in calculus depends on some form of the completeness axiom.
Axiom 1.7.2 (completeness) Every nonempty set of real numbers which is bounded above has a least upper bound and every nonempty set of real numbers which is bounded below has a greatest lower bound.
It is this axiom which distinguishes Calculus from Algebra.
A fundamental result about sup and inf is the following.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.8.
WELL ORDERING AND ARCHIMEDEAN PROPERTY 21 Proposition 1.7.3 Let S be a nonempty set and suppose sup(S) exists.
Then for every δ >0, S∩(sup(S)−δ,sup(S)]̸=∅.
If inf(S) exists, then for every δ >0, S∩[inf(S),inf(S)+δ)̸=∅.
Proof: Consider the ﬁrst claim.
If the indicated set equals ∅, then sup(S)−δ is an upper bound for S which is smaller than sup(S), contrary to the deﬁnition of sup(S) as the least upper bound.
In the second claim, if the indicated set equals ∅, then inf(S)+δ would be a lower bound which is larger than inf(S) contrary to the deﬁnition of inf(S).
(cid:4) 1.8 Well Ordering And Archimedean Property Deﬁnition 1.8.1 A set is well ordered if every nonempty subset S, contains a smallest element z having the property that z ≤x for all x∈S.
Axiom 1.8.2 Any set of integers larger than a given number is well ordered.
In particular, the natural numbers deﬁned as N≡{1,2,···} is well ordered.
The above axiom implies the principle of mathematical induction.
Theorem 1.8.3 (Mathematical induction) A set S ⊆ Z, having the property that a ∈ S and n+1∈S whenever n∈S contains all integers x∈Z such that x≥a.
Proof: Let T ≡ ([a,∞)∩Z)\S.
Thus T consists of all integers larger than or equal to a which are not in S. The theorem will be proved if T = ∅.
If T ̸= ∅ then by the well ordering principle, there would have to exist a smallest element of T, denoted as b.
It must be the case that b>a since by deﬁnition, a∈/ T. Then the integer, b−1 ≥a and b−1∈/ S because if b−1 ∈ S, then b−1+1 = b ∈ S by the assumed property of S. Therefore, b−1∈([a,∞)∩Z)\S =T which contradicts the choice of b as the smallest element of T. (b−1 is smaller.)
Since a contradiction is obtained by assuming T ̸=∅, it must be the case that T =∅ and this says that everything in [a,∞)∩Z is also in S. (cid:4) Example 1.8.4 Show that for all n∈N, 1 · 3···2n−1 < √ 1 .
2 4 2n 2n+1 If n = 1 this reduces to the statement that 1 < √1 which is obviously true.
Suppose 2 3 then that the inequality holds for n. Then 1 3 2n−1 2n+1 1 2n+1 · ··· · < √ 2 4 2n 2n+2 2n+12n+2 √ 2n+1 = .
2n+2 The theorem will be proved if this last expression is less than √ 1 .
This happens if and 2n+3 only if ( ) 1 2 1 2n+1 √ = > 2n+3 2n+3 (2n+2)2 which occurs if and only if (2n+2)2 >(2n+3)(2n+1) and this is clearly true which may be seen from expanding both sides.
This proves the inequality.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation22 PRELIMINARIES Deﬁnition 1.8.5 The Archimedean property states that whenever x∈R, and a>0, there exists n∈N such that na>x.
Proposition 1.8.6 R has the Archimedean property.
Proof: Suppose it is not true.
Then there exists x ∈ R and a > 0 such that na ≤ x for all n ∈ N. Let S = {na:n∈N}.
By assumption, this is bounded above by x.
By completeness, it has a least upper bound y.
By Proposition 1.7.3 there exists n ∈ N such that y−a<na≤y.
Then y =y−a+a<na+a=(n+1)a≤y, a contradiction.
(cid:4) Theorem 1.8.7 Suppose x < y and y−x > 1.
Then there exists an integer l ∈ Z, such that x<l<y.
If x is an integer, there is no integer y satisfying x<y <x+1.
Proof: Let x be the smallest positive integer.
Not surprisingly, x = 1 but this can be proved.
If x < 1 then x2 < x contradicting the assertion that x is the smallest natural number.
Therefore, 1 is the smallest natural number.
This shows there is no integer, y, satisfying x<y <x+1 since otherwise, you could subtract x and conclude 0<y−x<1 for some integer y−x.
Now suppose y−x>1 and let S ≡{w ∈N:w ≥y}.
The set S is nonempty by the Archimedean property.
Let k be the smallest element of S. Therefore, k−1<y.
Either k−1≤x or k−1>x.
If k−1≤x, then z≤}|0{ y−x≤y−(k−1)=y−k+1≤1 contrary to the assumption that y−x>1.
Therefore, x<k−1<y.
Let l=k−1.
(cid:4) It is the next theorem which gives the density of the rational numbers.
This means that for any real number, there exists a rational number arbitrarily close to it.
Theorem 1.8.8 If x<y then there exists a rational number r such that x<r <y.
Proof: Let n∈N be large enough that n(y−x)>1.
Thus (y−x) added to itself n times is larger than 1.
Therefore, n(y−x)=ny+n(−x)=ny−nx>1.
It follows from Theorem 1.8.7 there exists m∈Z such that nx<m<ny and so take r =m/n.
(cid:4) Deﬁnition 1.8.9 A set, S ⊆R is dense in R if whenever a<b, S∩(a,b)̸=∅.
Thus the above theorem says Q is “dense” in R. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.9.
DIVISION AND NUMBERS 23 Theorem 1.8.10 Suppose 0 < a and let b ≥ 0.
Then there exists a unique integer p and real number r such that 0≤r <a and b=pa+r.
Proof: Let S ≡{n∈N:an>b}.
By the Archimedean property this set is nonempty.
Let p+1 be the smallest element of S. Then pa ≤ b because p+1 is the smallest in S. Therefore, r ≡b−pa≥0.
If r ≥ a then b−pa ≥ a and so b ≥ (p+1)a contradicting p+1 ∈ S. Therefore, r < a as desired.
Toverifyuniquenessofpandr,supposep andr ,i=1,2,bothworkandr >r .Then i i 2 1 a little algebra shows r −r p −p = 2 1 ∈(0,1).
1 2 a Thus p −p is an integer between 0 and 1, contradicting Theorem 1.8.7.
The case that 1 2 r >r cannot occur either by similar reasoning.
Thus r =r and it follows that p =p .
1 2 1 2 1 2 (cid:4) This theorem is called the Euclidean algorithm when a and b are integers.
1.9 Division And Numbers First recall Theorem 1.8.10, the Euclidean algorithm.
Theorem 1.9.1 Suppose 0<a and let b≥0.
Then there exists a unique integer p and real number r such that 0≤r <a and b=pa+r.
The following deﬁnition describes what is meant by a prime number and also what is meant by the word “divides”.
Deﬁnition 1.9.2 The number, a divides the number, b if in Theorem 1.8.10, r = 0.
That is there is zero remainder.
The notation for this is a|b, read a divides b and a is called a factor of b.
A prime number is one which has the property that the only numbers which divide it are itself and 1.
The greatest common divisor of two positive integers, m,n is that number, p which has the property that p divides both m and n and also if q divides both m and n, then q divides p. Two integers are relatively prime if their greatest common divisor is one.
The greatest common divisor of m and n is denoted as (m,n).
There is a phenomenal and amazing theorem which relates the greatest common divisor tothesmallestnumberinacertainset.
Supposem,naretwopositiveintegers.
Thenifx,y are integers, so is xm+yn.
Consider all integers which are of this form.
Some are positive such as 1m+1n and some are not.
The set S in the following theorem consists of exactly those integers of this form which are positive.
Then the greatest common divisor of m and n will be the smallest number in S. This is what the following theorem says.
Theorem 1.9.3 Let m,n be two positive integers and deﬁne S ≡{xm+yn∈N:x,y ∈Z }.
Then the smallest number in S is the greatest common divisor, denoted by (m,n).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation24 PRELIMINARIES Proof: FirstnotethatbothmandnareinS soitisanonemptysetofpositiveintegers.
By well ordering, there is a smallest element of S, called p=x m+y n. Either p divides m 0 0 or it does not.
If p does not divide m, then by Theorem 1.8.10, m=pq+r where 0<r <p.
Thus m=(x m+y n)q+r and so, solving for r, 0 0 r =m(1−x )+(−y q)n∈S.
0 0 However,thisisacontradictionbecausepwasthesmallestelementofS.Thusp|m.Similarly p|n.
Now suppose q divides both m and n. Then m = qx and n = qy for integers, x and y.
Therefore, p=mx +ny =x qx+y qy =q(x x+y y) 0 0 0 0 0 0 showing q|p.
Therefore, p=(m,n).
(cid:4) There is a relatively simple algorithm for ﬁnding (m,n) which will be discussed now.
Suppose 0 < m < n where m,n are integers.
Also suppose the greatest common divisor is (m,n)=d.
Then by the Euclidean algorithm, there exist integers q,r such that n=qm+r, r <m (1.1) Now d divides n and m so there are numbers k,l such that dk =m,dl=n.
From the above equation, r =n−qm=dl−qdk =d(l−qk) Thus d divides both m and r. If k divides both m and r, then from the equation of (1.1) it follows k also divides n. Therefore, k divides d by the deﬁnition of the greatest common divisor.
Thus d is the greatest common divisor of m and r but m+r <m+n.
This yields another pair of positive integers for which d is still the greatest common divisor but the sum of these integers is strictly smaller than the sum of the ﬁrst two.
Now you can do the samethingtotheseintegers.
Eventuallytheprocessmustendbecausethesumgetsstrictly smaller each time it is done.
It ends when there are not two positive integers produced.
That is, one is a multiple of the other.
At this point, the greatest common divisor is the smaller of the two numbers.
Procedure 1.9.4 To ﬁnd the greatest common divisor of m,n where 0 < m < n, replace the pair {m,n} with {m,r} where n = qm+r for r < m. This new pair of numbers has the same greatest common divisor.
Do the process to this pair and continue doing this till you obtain a pair of numbers where one is a multiple of the other.
Then the smaller is the sought for greatest common divisor.
Example 1.9.5 Find the greatest common divisor of 165 and 385.
Use the Euclidean algorithm to write 385=2(165)+55 Thus the next two numbers are 55 and 165.
Then 165=3×55 and so the greatest common divisor of the ﬁrst two numbers is 55.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.9.
DIVISION AND NUMBERS 25 Example 1.9.6 Find the greatest common divisor of 1237 and 4322.
Use the Euclidean algorithm 4322=3(1237)+611 Now the two new numbers are 1237,611.
Then 1237=2(611)+15 The two new numbers are 15,611.
Then 611=40(15)+11 The two new numbers are 15,11.
Then 15=1(11)+4 The two new numbers are 11,4 2(4)+3 The two new numbers are 4,3.
Then 4=1(3)+1 The two new numbers are 3,1.
Then 3=3×1 and so 1 is the greatest common divisor.
Of course you could see this right away when the twonewnumberswere15and11.
Recalltheprocessdeliversnumberswhichhavethesame greatest common divisor.
This amazing theorem will now be used to prove a fundamental property of prime num- bers which leads to the fundamental theorem of arithmetic, the major theorem which says every integer can be factored as a product of primes.
Theorem 1.9.7 If p is a prime and p|ab then either p|a or p|b.
Proof: Suppose p does not divide a.
Then since p is prime, the only factors of p are 1 and p so follows (p,a)=1 and therefore, there exists integers, x and y such that 1=ax+yp.
Multiplying this equation by b yields b=abx+ybp.
Since p|ab, ab=pz for some integer z.
Therefore, b=abx+ybp=pzx+ybp=p(xz+yb) and this shows p divides b.
(cid:4) ∏ Theorem 1.9.8 (Fundamental theorem of arithmetic) Let a ∈ N\{1}.
Then a = n p i=1 i where p are all prime numbers.
Furthermore, this prime factorization is unique except for i the order of the factors.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation26 PRELIMINARIES Proof: If a equals a prime number, the prime factorization clearly exists.
In particular the prime factorization exists for the prime number 2.
Assume this theorem is true for all a≤n−1.
If n is a prime, then it has a prime factorization.
On the other hand, if n is not a prime, then there exist two integers k and m such that n = km where each of k and m are less than n. Therefore, each of these is no larger than n−1 and consequently, each has a prime factorization.
Thus so does n. It remains to argue the prime factorization is unique except for order of the factors.
Suppose ∏n ∏m p = q i j i=1 j=1 where the p and q are all prime, there is no way to reorder the q such that m = n and i j k p = q for all i, and n+m is the smallest positive integer such that this happens.
Then i i by Theorem 1.9.7, p |q for some j.
Since these are prime numbers this requires p = q .
1 j 1 j Reorderingifnecessaryitcanbeassumedthatq =q .Thendividingbothsidesbyp =q , j 1 1 1 n∏−1 m∏−1 p = q .
i+1 j+1 i=1 j=1 Since n+m was as small as possible for the theorem to fail, it follows that n−1=m−1 and the prime numbers, q ,··· ,q can be reordered in such a way that p = q for all 2 m k k k = 2,··· ,n. Hence p = q for all i because it was already argued that p = q , and this i i 1 1 results in a contradiction.
(cid:4) 1.10 Systems Of Equations Sometimes it is necessary to solve systems of equations.
For example the problem could be to ﬁnd x and y such that x+y =7 and 2x−y =8.
(1.2) The set of ordered pairs, (x,y) which solve both equations is called the solution set.
For example, you can see that (5,2) = (x,y) is a solution to the above system.
To solve this, notethatthesolutionsetdoesnotchangeifanyequationisreplacedbyanonzeromultiple of itself.
It also does not change if one equation is replaced by itself added to a multiple of the other equation.
For example, x and y solve the above system if and only if x and y solve the system z −3y}=|−6 { x+y =7,2x−y+(−2)(x+y)=8+(−2)(7).
(1.3) Thesecondequationwasreplacedby−2timestheﬁrstequationaddedtothesecond.
Thus the solution is y = 2, from −3y = −6 and now, knowing y = 2, it follows from the other equation that x+2=7 and so x=5.
Why exactly does the replacement of one equation with a multiple of another added to it not change the solution set?
The two equations of (1.2) are of the form E =f ,E =f (1.4) 1 1 2 2 whereE andE areexpressionsinvolvingthevariables.
Theclaimisthatifaisanumber, 1 2 then (1.4) has the same solution set as E =f , E +aE =f +af .
(1.5) 1 1 2 1 2 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.10.
SYSTEMS OF EQUATIONS 27 Why is this?
If (x,y) solves (1.4) then it solves the ﬁrst equation in (1.5).
Also, it satisﬁes aE =af 1 1 and so, since it also solves E = f it must solve the second equation in (1.5).
If (x,y) 2 2 solves (1.5) then it solves the ﬁrst equation of (1.4).
Also aE = af and it is given that 1 1 thesecondequationof(1.5)isveriﬁed.
Therefore, E =f anditfollows(x,y)isasolution 2 2 of the second equation in (1.4).
This shows the solutions to (1.4) and (1.5) are exactly the same which means they have the same solution set.
Of course the same reasoning applies with no change if there are many more variables than two and many more equations than two.
It is still the case that when one equation is replaced with a multiple of another one added to itself, the solution set of the whole system does not change.
Theotherthingwhichdoesnotchangethesolutionsetofasystemofequationsconsists of listing the equations in a diﬀerent order.
Here is another example.
Example 1.10.1 Find the solutions to the system, x+3y+6z =25 2x+7y+14z =58 (1.6) 2y+5z =19 To solve this system replace the second equation by (−2) times the ﬁrst equation added to the second.
This yields.
the system x+3y+6z =25 y+2z =8 (1.7) 2y+5z =19 Now take (−2) times the second and add to the third.
More precisely, replace the third equation with (−2) times the second added to the third.
This yields the system x+3y+6z =25 y+2z =8 (1.8) z =3 At this point, you can tell what the solution is.
This system has the same solution as the original system and in the above, z = 3.
Then using this in the second equation, it follows y+6 = 8 and so y = 2.
Now using this in the top equation yields x+6+18 = 25 and so x=1.
This process is not really much diﬀerent from what you have always done in solving a single equation.
For example, suppose you wanted to solve 2x+5 = 3x−6.
You did the same thing to both sides of the equation thus preserving the solution set until you obtained an equation which was simple enough to give the answer.
In this case, you would add −2x to both sides and then add 6 to both sides.
This yields x=11.
In (1.8) you could have continued as follows.
Add (−2) times the bottom equation to the middle and then add (−6) times the bottom to the top.
This yields x+3y =19 y =6 z =3 Now add (−3) times the second to the top.
This yields x=1 y =6 , z =3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation28 PRELIMINARIES a system which has the same solution set as the original system.
It is foolish to write the variables every time you do these operations.
It is easier to write the system (1.6) as the following “augmented matrix”   1 3 6 25   2 7 14 58 .
0 2 5 19 Ithasexactlythesameinformationastheoriginalsystembuthereitisunderstoodthereis 1 3 6       an xcolumn, 2 , a y column, 7 anda z column, 14 .
Therowscorrespond 0 2 5 to the equations in the system.
Thus the top row in the augmented matrix corresponds to the equation, x+3y+6z =25.
Now when you replace an equation with a multiple of another equation added to itself, you are just taking a row of this augmented matrix and replacing it with a multiple of another row added to it.
Thus the ﬁrst step in solving (1.6) would be to take (−2) times the ﬁrst row of the augmented matrix above and add it to the second row,   1 3 6 25   0 1 2 8 .
0 2 5 19 Note how this corresponds to (1.7).
Next take (−2) times the second row and add to the third,   1 3 6 25   0 1 2 8 0 0 1 3 which is the same as (1.8).
You get the idea I hope.
Write the system as an augmented matrix and follow the procedure of either switching rows, multiplying a row by a non zero number,orreplacingarowbyamultipleofanotherrowaddedtoit.
Eachoftheseoperations leaves the solution set unchanged.
These operations are called row operations.
Deﬁnition 1.10.2 The row operations consist of the following 1.
Switch two rows.
2.
Multiply a row by a nonzero number.
3.
Replace a row by a multiple of another row added to it.
It is important to observe that any row operation can be “undone” by another inverse row operation.
For example, if r ,r are two rows, and r is replaced with r′ = αr +r 1 2 2 2 1 2 usingrowoperation3,thenyoucouldgetbacktowhereyoustartedbyreplacingtherowr′ 2 with −α times r and adding to r′.
In the case of operation 2, you would simply multiply 1 2 the row that was changed by the inverse of the scalar which multiplied it in the ﬁrst place, and in the case of row operation 1, you would just make the same switch again and you would be back to where you started.
In each case, the row operation which undoes what was done is called the inverse row operation.
Example 1.10.3 Givethecompletesolutiontothesystemofequations,5x+10y−7z =−2, 2x+4y−3z =−1, and 3x+6y+5z =9.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.10.
SYSTEMS OF EQUATIONS 29 The augmented matrix for this system is   2 4 −3 −1  5 10 −7 −2  3 6 5 9 Multiplythesecondrowby2,theﬁrstrowby5, andthentake(−1)timestheﬁrstrowand add to the second.
Then multiply the ﬁrst row by 1/5.
This yields   2 4 −3 −1   0 0 1 1 3 6 5 9 Now, combining some row operations, take (−3) times the ﬁrst row and add this to 2 times the last row and replace the last row with this.
This yields.
  2 4 −3 −1   0 0 1 1 .
0 0 1 21 Putting in the variables, the last two rows say z = 1 and z = 21.
This is impossible so the last system of equations determined by the above augmented matrix has no solution.
However,ithasthesamesolutionsetastheﬁrstsystemofequations.
Thisshowsthereisno solutiontothe threegivenequations.
When thishappens, thesystem iscalledinconsistent.
Thisshouldnotbesurprisingthatsomethinglikethiscantakeplace.
Itcanevenhappen foroneequationinonevariable.
Considerforexample,x=x+1.Thereisclearlynosolution to this.
Example 1.10.4 Give the complete solution to the system of equations, 3x−y−5z = 9, y−10z =0, and −2x+y =−6.
The augmented matrix of this system is   3 −1 −5 9  0 1 −10 0  −2 1 0 −6 Replace the last row with 2 times the top row added to 3 times the bottom row.
This gives   3 −1 −5 9  0 1 −10 0  0 1 −10 0 Next take −1 times the middle row and add to the bottom.
  3 −1 −5 9  0 1 −10 0  0 0 0 0 Take the middle row and add to the top and then divide the top row which results by 3.
  1 0 −5 3  0 1 −10 0 .
0 0 0 0 This says y = 10z and x = 3+5z.
Apparently z can equal any number.
Therefore, the solution set of this system is x=3+5t,y =10t, and z =t where t is completely arbitrary.
The system has an inﬁnite set of solutions and this is a good description of the solutions.
This is what it is all about, ﬁnding the solutions to the system.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation30 PRELIMINARIES Deﬁnition 1.10.5 Sincez =twheretisarbitrary,thevariablez iscalledafree variable.
The phenomenon of an inﬁnite solution set occurs in equations having only one variable also.
For example, consider the equation x=x.
It doesn’t matter what x equals.
Deﬁnition 1.10.6 A system of linear equations is a list of equations, ∑n a x =f , i=1,2,3,··· ,m ij j j j=1 where a are numbers, f is a number, and it is desired to ﬁnd (x ,··· ,x ) solving each of ij j 1 n the equations listed.
As illustrated above, such a system of linear equations may have a unique solution, no solution, or inﬁnitely many solutions.
It turns out these are the only three cases which can occur for linear systems.
Furthermore, you do exactly the same things to solve any linear system.
You write the augmented matrix and do row operations until you get a simpler system in which it is possible to see the solution.
All is based on the observation that the rowoperationsdonotchangethesolutionset.
Youcanhavemoreequationsthanvariables, fewer equations than variables, etc.
It doesn’t matter.
You always set up the augmented matrix and go to work on it.
These things are all the same.
Example 1.10.7 Give the complete solution to the system of equations,−41x+15y =168, 109x−40y =−447, −3x+y =12, and 2x+z =−1.
The augmented matrix is   −41 15 0 168  109 −40 0 −447   −3 1 0 12 .
2 0 1 −1 To solve this multiply the top row by 109, the second row by 41, add the top row to the second row, and multiply the top row by 1/109.
Note how this process combined several row operations.
This yields   −41 15 0 168  0 −5 0 −15   −3 1 0 12 .
2 0 1 −1 Next take 2 times the third row and replace the fourth row by this added to 3 times the fourth row.
Then take (−41) times the third row and replace the ﬁrst row by this added to 3 times the ﬁrst row.
Then switch the third and the ﬁrst rows.
This yields   123 −41 0 −492  0 −5 0 −15   .
0 4 0 12 0 2 3 21 Take −1/2 times the third row and add to the bottom row.
Then take 5 times the third row and add to four times the second.
Finally take 41 times the third row and add to 4 times the top row.
This yields   492 0 0 −1476    0 0 0 0    0 4 0 12 0 0 3 15 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.11.
EXERCISES 31 It follows x= −1476 =−3,y =3 and z =5.
492 You should practice solving systems of equations.
Here are some exercises.
1.11 Exercises 1.
Give the complete solution to the system of equations, 3x−y+4z = 6, y+8z = 0, and −2x+y =−4.
2.
Givethecompletesolutiontothesystemofequations,x+3y+3z =3,3x+2y+z =9, and −4x+z =−9.
3.
Consider the system −5x+2y−z = 0 and −5x−2y−z = 0.
Both equations equal zero and so −5x+2y−z = −5x−2y−z which is equivalent to y = 0.
Thus x and z can equal anything.
But when x = 1, z = −4, and y = 0 are plugged in to the equations, it doesn’t work.
Why?
4.
Givethecompletesolutiontothesystemofequations,x+2y+6z =5,3x+2y+6z =7 ,−4x+5y+15z =−7.
5.
Give the complete solution to the system of equations x+2y+3z = 5,3x+2y+z =7, −4x+5y+z = −7,x+3z =5.
6.
Give the complete solution of the system of equations, x+2y+3z = 5, 3x+2y+2z =7 −4x+5y+5z = −7, x=5 7.
Give the complete solution of the system of equations x+y+3z = 2, 3x−y+5z =6 −4x+9y+z = −8, x+5y+7z =2 8.
Determine a such that there are inﬁnitely many solutions and then ﬁnd them.
Next determine a such that there are no solutions.
Finally determine which values of a correspond to a unique solution.
The system of equations for the unknown variables x,y,z is 3za2−3a+(x+y+)1=0 3x−a−y+z a2+4 −5=0 za2−a−4x+9y+9=0 9.
Find the solutions to the following system of equations for x,y,z,w.
y+z =2,z+w =0,y−4z−5w =2,2y+z−w =4 10.
Find all solutions to the following equations.
x+y+z = 2, z+w =0, 2x+2y+z−w = 4, x+y−4z−5z =2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation32 PRELIMINARIES 1.12 Fn The notation, Cn refers to the collection of ordered lists of n complex numbers.
Since every real number is also a complex number, this simply generalizes the usual notion of Rn, the collection of all ordered lists of n real numbers.
In order to avoid worrying about whether it is real or complex numbers which are being referred to, the symbol F will be used.
If it is not clear, always pick C. More generally, Fn refers to the ordered lists of n elements of Fn.
Deﬁnition 1.12.1 Deﬁne Fn ≡ {(x ,··· ,x ):x ∈F for j =1,··· ,n}.
(x ,··· ,x ) = 1 n j 1 n (y ,··· ,y ) if and only if for all j = 1,··· ,n, x = y .
When (x ,··· ,x ) ∈ Fn, it is 1 n j j 1 n conventional to denote (x ,··· ,x ) by the single bold face letter x.
The numbers x are 1 n j called the coordinates.
The set {(0,··· ,0,t,0,··· ,0):t∈F} for t in the ith slot is called the ith coordinate axis.
The point 0 ≡ (0,··· ,0) is called the origin.
Thus (1,2,4i)∈F3 and (2,1,4i)∈F3 but (1,2,4i)̸=(2,1,4i) because, even though the same numbers are involved, they don’t match up.
In particular, the ﬁrst entries are not equal.
1.13 Algebra in Fn There are twoalgebraic operations done with elementsof Fn.
One is addition and the other is multiplication by numbers, called scalars.
In the case of Cn the scalars are complex numberswhileinthecaseofRn theonlyallowedscalarsarerealnumbers.
Thus,thescalars always come from F in either case.
Deﬁnition 1.13.1 If x∈Fn and a∈F, also called a scalar, then ax∈Fn is deﬁned by ax=a(x ,··· ,x )≡(ax ,··· ,ax ).
(1.9) 1 n 1 n This is known as scalar multiplication.
If x,y∈Fn then x+y∈Fn and is deﬁned by x+y=(x ,··· ,x )+(y ,··· ,y ) 1 n 1 n ≡(x +y ,··· ,x +y ) (1.10) 1 1 n n With this deﬁnition, the algebraic properties satisfy the conclusions of the following theorem.
Theorem 1.13.2 For v,w∈Fn and α,β scalars, (real numbers), the following hold.
v+w=w+v, (1.11) the commutative law of addition, (v+w)+z=v+(w+z), (1.12) the associative law for addition, v+0=v, (1.13) the existence of an additive identity, v+(−v)=0, (1.14) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.14.
EXERCISES 33 the existence of an additive inverse, Also α(v+w)=αv+αw, (1.15) (α+β)v=αv+βv, (1.16) α(βv)=αβ(v), (1.17) 1v=v.
(1.18) In the above 0=(0,··· ,0).
You should verify that these properties all hold.
As usual subtraction is deﬁned as x−y≡x+(−y).Theconclusionsoftheabovetheoremarecalledthevectorspaceaxioms.
1.14 Exercises 1.
Verify all the properties (1.11)-(1.18).
2.
Compute 5(1,2+3i,3,−2)+6(2−i,1,−2,7).
3.
Draw a picture of the points in R2 which are determined by the following ordered pairs.
(a) (1,2) (b) (−2,−2) (c) (−2,3) (d) (2,−5) 4.
Does it make sense to write (1,2)+(2,3,1)?
Explain.
5.
Draw a picture of the points in R3 which are determined by the following ordered triples.
If you have trouble drawing this, describe it in words.
(a) (1,2,0) (b) (−2,−2,1) (c) (−2,3,−2) 1.15 The Inner Product In Fn When F=R or C, there is something called an inner product.
In case of R it is also called the dot product.
This is also often referred to as the scalar product.
Deﬁnition 1.15.1 Let a,b∈Fn deﬁne a·b as ∑n a·b≡ a b .
k k k=1 Withthisdeﬁnition,thereareseveralimportantpropertiessatisﬁedbytheinnerproduct.
In the statement of these properties, α and β will denote scalars and a,b,c will denote vectors or in other words, points in Fn.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation34 PRELIMINARIES Proposition 1.15.2 The inner product satisﬁes the following properties.
a·b=b·a (1.19) a·a≥0 and equals zero if and only if a=0 (1.20) (αa+βb)·c=α(a·c)+β(b·c) (1.21) c·(αa+βb)=α(c·a)+β(c·b) (1.22) |a|2 =a·a (1.23) Youshouldverifytheseproperties.
Alsobesureyouunderstandthat(1.22)followsfrom the ﬁrst three and is therefore redundant.
It is listed here for the sake of convenience.
Example 1.15.3 Find (1,2,0,−1)·(0,i,2,3).
This equals 0+2(−i)+0+−3=−3−2i The Cauchy Schwarz inequality takes the following form in terms of the inner product.
I will prove it using only the above axioms for the inner product.
Theorem 1.15.4 The inner product satisﬁes the inequality |a·b|≤|a||b|.
(1.24) Furthermoreequalityisobtainedif andonlyifone ofa or bisascalarmultipleoftheother.
Proof: First deﬁne θ ∈C such that θ(a·b)=|a·b|,|θ|=1, and deﬁne a function of t∈R f(t)=(a+tθb)·(a+tθb).
Then by (1.20), f(t)≥0 for all t∈R.
Also from (1.21),(1.22),(1.19), and (1.23) f(t)=a·(a+tθb)+tθb·(a+tθb) =a·a+tθ(a·b)+tθ(b·a)+t2|θ|2b·b =|a|2+2tReθ(a·b)+|b|2t2 =|a|2+2t|a·b|+|b|2t2 Now if |b|2 = 0 it must be the case that a·b = 0 because otherwise, you could pick large negative values of t and violate f(t) ≥ 0.
Therefore, in this case, the Cauchy Schwarz inequality holds.
In the case that |b| ̸= 0, y = f(t) is a polynomial which opens up and therefore,ifitisalwaysnonnegative,itsgraphislikethatillustratedinthefollowingpicture t t Then the quadratic formula requires that z Thedis}cr|iminant { 4|a·b|2−4|a|2|b|2 ≤0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation1.15.
THE INNER PRODUCT IN FN 35 since otherwise the function, f(t) would have two real zeros and would necessarily have a graph which dips below the t axis.
This proves (1.24).
It is clear from the axioms of the inner product that equality holds in (1.24) whenever one of the vectors is a scalar multiple of the other.
It only remains to verify this is the only way equality can occur.
If either vector equals zero, then equality is obtained in (1.24) so it can be assumed both vectors are non zero.
Then if equality is achieved, it follows f(t) has exactly one real zero because the discriminant vanishes.
Therefore, for some value of t,a+tθb=0 showing that a is a multiple of b.
(cid:4) You should note that the entire argument was based only on the properties of the in- ner product listed in (1.19) - (1.23).
This means that whenever something satisﬁes these properties, the Cauchy Schwartz inequality holds.
There are many other instances of these propertiesbesidesvectorsinFn.
Alsonotethat(1.24)holdsif(1.20)issimpliﬁedtoa·a≥0.
The Cauchy Schwartz inequality allows a proof of the triangle inequality for distances in Fn in much the same way as the triangle inequality for the absolute value.
Theorem 1.15.5 (Triangle inequality) For a,b∈Fn |a+b|≤|a|+|b| (1.25) and equality holds if and only if one of the vectors is a nonnegative scalar multiple of the other.
Also ||a|−|b||≤|a−b| (1.26) Proof: By properties of the inner product and the Cauchy Schwartz inequality, |a+b|2 =(a+b)·(a+b)=(a·a)+(a·b)+(b·a)+(b·b) =|a|2+2Re(a·b)+|b|2 ≤|a|2+2|a·b|+|b|2 ≤|a|2+2|a||b|+|b|2 =(|a|+|b|)2.
Taking square roots of both sides you obtain (1.25).
It remains to consider when equality occurs.
If either vector equals zero, then that vector equals zero times the other vector and the claim about when equality occurs is veriﬁed.
Therefore, it can be assumed both vectors are nonzero.
To get equality in the second inequality above, Theorem 1.15.4 implies one of the vectors must be a multiple of the other.
Say b = αa.
Also, to get equality in the ﬁrst inequality, (a·b) must be a nonnegative real number.
Thus 0≤(a·b)=(a·αa)=α|a|2.
Therefore, α must be a real number which is nonnegative.
To get the other form of the triangle inequality, a=a−b+b so |a|=|a−b+b|≤|a−b|+|b|.
Therefore, |a|−|b|≤|a−b| (1.27) Similarly, |b|−|a|≤|b−a|=|a−b|.
(1.28) It follows from (1.27) and (1.28) that (1.26) holds.
This is because ||a|−|b|| equals the left side of either (1.27) or (1.28) and either way, ||a|−|b||≤|a−b|.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation36 PRELIMINARIES 1.16 What Is Linear Algebra?
The above preliminary considerations form the necessary scaﬀolding upon which linear al- gebra is built.
Linear algebra is the study of a certain algebraic structure called a vector spacedescribedinaspecialcaseinTheorem1.13.2andinmoregeneralitybelowalongwith special functions known as linear transformations.
These linear transformations preserve certain algebraic properties.
A good argument could be made that linear algebra is the most useful subject in all of mathematics and that it exceeds even courses like calculus in its signiﬁcance.
It is used extensively in applied mathematics and engineering.
Continuum mechanics, for example, makes use of topics from linear algebra in deﬁning things like the strain and in determining appropriate constitutive laws.
It is fundamental in the study of statistics.
For example, principal component analysis is really based on the singular value decomposition discussed inthisbook.
Itisalsofundamentalinpuremathematicsareaslikenumbertheory,functional analysis, geometric measure theory, and diﬀerential geometry.
Even calculus cannot be correctlyunderstoodwithoutit.
Forexample,thederivativeofafunctionofmanyvariables is an example of a linear transformation, and this is the way it must be understood as soon as you consider functions of more than one variable.
1.17 Exercises [ ] 1.
Show that (a·b)= 1 |a+b|2−|a−b|2 .
4 2.
Prove from the axioms of the inner product the parallelogram identity, |a+b|2 + |a−b|2 =2|a|2+2|b|2.
∑ 3.
Fora,b∈Rn,deﬁnea·b≡ n β a b whereβ >0foreachk.Showthissatisﬁes k=1 k k k k the axioms of the inner product.
What does the Cauchy Schwarz inequality say in this case.
4.
In Problem 3 above, suppose you only know β ≥ 0.
Does the Cauchy Schwarz in- k equality still hold?
If so, prove it.
5.
Let f,g be continuous functions and deﬁne ∫ 1 f ·g ≡ f(t)g(t)dt 0 show this satisﬁes the axioms of a inner product if you think of continuous functions in the place of a vector in Fn.
What does the Cauchy Schwarz inequality say in this case?
6.
Show that if f is a real valued continuous function, ( ) ∫ 2 ∫ b b f(t)dt ≤(b−a) f(t)2dt.
a a Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationMatrices And Linear Transformations 2.1 Matrices Youhavenowsolvedsystemsofequationsbywritingthemintermsofanaugmentedmatrix andthendoingrowoperationsonthisaugmentedmatrix.
Itturnsoutthatsuchrectangular arrays of numbers are important from many other diﬀerent points of view.
Numbers are also called scalars.
In general, scalars are just elements of some ﬁeld.
However, in the ﬁrst partofthisbook,theﬁeldwilltypicallybeeithertherealnumbersorthecomplexnumbers.
A matrix is a rectangular array of numbers.
Several of them are referred to as matrices.
For example, here is a matrix.
  1 2 3 4   5 2 8 7 6 −9 1 2 This matrix is a 3×4 matrix because there are three rows and four columns.
The ﬁrst 1   row is (1234), the second row is (5287) and so forth.
The ﬁrst column is 5 .
The 6 convention in dealing with matrices is to always list the rows ﬁrst and then the columns.
Also, you can remember the columns are like columns in a Greek temple.
They stand up rightwhiletherowsjustlaytherelikerowsmadebyatractorinaplowedﬁeld.
Elementsof the matrix are identiﬁed according to position in the matrix.
For example, 8 is in position 2,3 because it is in the second row and the third column.
You might remember that you alwayslisttherowsbeforethecolumnsbyusingthephraseRowmanCatholic.
Thesymbol, (a ) refers to a matrix in which the i denotes the row and the j denotes the column.
Using ij this notation on the above matrix, a =8,a =−9,a =2, etc.
23 32 12 Therearevariousoperationswhicharedoneonmatrices.
Theycansometimesbeadded, multipliedbyascalarandsometimesmultiplied.
Toillustratescalarmultiplication,consider the following example.
    1 2 3 4 3 6 9 12     3 5 2 8 7 = 15 6 24 21 .
6 −9 1 2 18 −27 3 6 The new matrix is obtained by multiplying every entry of the original matrix by the given scalar.
If A is an m×n matrix −A is deﬁned to equal (−1)A. Twomatriceswhicharethesamesizecanbeadded.
Whenthisisdone, theresultisthe 37 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation38 MATRICES AND LINEAR TRANSFORMATIONS matrix which is obtained by adding corresponding entries.
Thus       1 2 −1 4 0 6       3 4 + 2 8 = 5 12 .
5 2 6 −4 11 −2 Two matrices are equal exactly when they are the same size and the corresponding entries are identical.
Thus   ( ) 0 0  0 0 ̸= 0 0 0 0 0 0 because they are diﬀerent sizes.
As noted above, you write (c ) for the matrix C whose ij ijth entry is c .
In doing arithmetic with matrices you must deﬁne what happens in terms ij of the c sometimes called the entries of the matrix or the components of the matrix.
ij The above discussion stated for general matrices is given in the following deﬁnition.
Deﬁnition 2.1.1 Let A = (a ) and B = (b ) be two m×n matrices.
Then A+B = C ij ij where C =(c ) ij for c =a +b .
Also if x is a scalar, ij ij ij xA=(c ) ij where c =xa .
The number A will typically refer to the ijth entry of the matrix A.
The ij ij ij zero matrix, denoted by 0 will be the matrix consisting of all zeros.
Do not be upset by the use of the subscripts, ij.
The expression c = a +b is just ij ij ij saying that you add corresponding entries to get the result of summing two matrices as discussed above.
Note that there are 2×3 zero matrices, 3×4 zero matrices, etc.
In fact for every size there is a zero matrix.
With this deﬁnition, the following properties are all obvious but you should verify all of these properties are valid for A, B, and C, m×n matrices and 0 an m×n zero matrix, A+B =B+A, (2.1) the commutative law of addition, (A+B)+C =A+(B+C), (2.2) the associative law for addition, A+0=A, (2.3) the existence of an additive identity, A+(−A)=0, (2.4) the existence of an additive inverse.
Also, for α,β scalars, the following also hold.
α(A+B)=αA+αB, (2.5) (α+β)A=αA+βA, (2.6) α(βA)=αβ(A), (2.7) 1A=A.
(2.8) The above properties, (2.1) - (2.8) are known as the vector space axioms and the fact that the m×n matrices satisfy these axioms is what is meant by saying this set of matrices with addition and scalar multiplication as deﬁned above forms a vector space.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 39 Deﬁnition 2.1.2 Matrices which are n×1 or 1×n are especially called vectors and are often denoted by a bold letter.
Thus   x 1  .
 x= .
 .
x n is an n×1 matrix also called a column vector while a 1×n matrix of the form (x ···x ) 1 n is referred to as a row vector.
All the above is ﬁne, but the real reason for considering matrices is that they can be multiplied.
This is where things quit being banal.
First consider the problem of multiplying an m×n matrix by an n×1 column vector.
Consider the following example   ( ) 7 1 2 3   8 =?
4 5 6 9 It equals ( ) ( ) ( ) 1 2 3 7 +8 +9 4 5 6 Thus it is what is called a linear combination of the columns.
These will be discussed more later.
Motivated by this example, here is the deﬁnition of how to multiply an m×n matrix by an n×1 matrix.
(vector) Deﬁnition 2.1.3 Let A=A be an m×n matrix and let v be an n×1 matrix, ij   v 1 v= .. , A=(a ,··· ,a ) .
1 n v n where a is an m×1 vector.
Then Av, written as i   v ( a ··· a ) ..1 , 1 n .
v n is the m×1 column vector which equals the following linear combination of the columns.
∑n v a +v a +···+v a ≡ v a (2.9) 1 1 2 2 n n j j j=1 If the jth column of A is   A 1j    A2j   .
  .
 .
A mj then (2.9) takes the form       A A A 11 12 1n        A21   A22   A2n  v1 .. +v2 .. +···+vn ..  .
.
.
A A A m1 m2 mn Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation40 MATRICES AND LINEAR TRANSFORMATIONS ∑ Thus the ith entry of Av is n A v .
Note that multiplication by an m×n matrix takes j=1 ij j an n×1 matrix, and produces an m×1 matrix (vector).
Here is another example.
Example 2.1.4 Compute     1 1 2 1 3    0 2 1 −2  2 .
0 2 1 4 1 1 First of all, this is of the form (3×4)(4×1) and so the result should be a (3×1).
Note how the inside numbers cancel.
To get the entry in the second row and ﬁrst and only column, compute ∑4 a v = a v +a v +a v +a v 2k k 21 1 22 2 23 3 24 4 k=1 = 0×1+2×2+1×0+(−2)×1=2.
You should do the rest of the problem and verify       1 1 2 1 3   8  0 2 1 −2  2 = 2 .
0 2 1 4 1 5 1 With this done, the next task is to multiply an m×n matrix times an n×p matrix.
Before doing so, the following may be helpful.
thesemustmatch (m× n[)(n×p )=m×p If the two middle numbers don’t match, you can’t multiply the matrices!
Deﬁnition 2.1.5 Let A be an m×n matrix and let B be an n×p matrix.
Then B is of the form B =(b ,··· ,b ) 1 p where b is an n×1 matrix.
Then an m×p matrix AB is deﬁned as follows: k AB ≡(Ab ,··· ,Ab ) (2.10) 1 p where Ab is an m×1 matrix.
Hence AB as just deﬁned is an m×p matrix.
For example, k Example 2.1.6 Multiply the following.
  ( ) 1 2 0 1 2 1   0 3 1 0 2 1 −2 1 1 The ﬁrst thing you need to check before doing anything else is whether it is possible to dothemultiplication.
Theﬁrstmatrixisa2×3andthesecondmatrixisa3×3.
Therefore, Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 41 is it possible to multiply these matrices.
According to the above discussion it should be a 2×3 matrix of the form   z First}c|olumn { z Second}|column { z Third}c|olumn {        ( ) 1 ( ) 2 ( ) 0   1 2 1  0 , 1 2 1  3 , 1 2 1  1   0 2 1 0 2 1 0 2 1   −2 1 1  You know how to multiply a matrix times a vector and so you do so to obtain each of the three columns.
Thus   ( ) ( ) 1 2 0 1 2 1   −1 9 3 0 3 1 = .
0 2 1 −2 7 3 −2 1 1 Here is another example.
Example 2.1.7 Multiply the following.
  ( ) 1 2 0   1 2 1 0 3 1 0 2 1 −2 1 1 Firstcheckifitispossible.
Thisisoftheform(3×3)(2×3).Theinsidenumbersdonot match and so you can’t do this multiplication.
This means that anything you write will be absolute nonsense because it is impossible to multiply these matrices in this order.
Aren’t they the same two matrices considered in the previous example?
Yes they are.
It is just that here they are in a diﬀerent order.
This shows something you must always remember about matrix multiplication.
Order Matters!
Matrix multiplication is not commutative.
This is very diﬀerent than multiplication of numbers!
2.1.1 The ijth Entry Of A Product It is important to describe matrix multiplication in terms of entries of the matrices.
What is the ijth entry of AB?
It would be the ith entry of the jth column of AB.
Thus it would be the ith entry of Abj.
Now   B 1j  .
 b = .
 j .
B nj and from the above deﬁnition, the ith entry is ∑n A B .
(2.11) ik kj k=1 In terms of pictures of the matrix, you are doing    A A ··· A B B ··· B 11 12 1n 11 12 1p  A21 A22 ··· A2n  B21 B22 ··· B2p   .
.
.
 .
.
.
  .
.
.
 .
.
.
 .
.
.
.
.
.
A A ··· A B B ··· B m1 m2 mn n1 n2 np Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation42 MATRICES AND LINEAR TRANSFORMATIONS Then as explained above, the jth column is of the form    A A ··· A B 11 12 1n 1j  A21 A22 ··· A2n  B2j   .
.
.
 .
  .
.
.
 .
 .
.
.
.
A A ··· A B m1 m2 mn nj which is a m×1 matrix or column vector which equals       A A A 11 12 1n        A21   A22   A2n   .. B1j + .. B2j +···+ .. Bnj.
.
.
.
A A A m1 m2 mn The ith entry of this m×1 matrix is ∑m A B +A B +···+A B = A B .
i1 1j i2 2j in nj ik kj k=1 This shows the following deﬁnition for matrix multiplication in terms of the ijth entries of the product harmonizes with Deﬁnition 2.1.3.
This motivates the deﬁnition for matrix multiplication which identiﬁes the ijth entries of the product.
Deﬁnition 2.1.8 Let A=(A ) be an m×n matrix and let B =(B ) be an n×p matrix.
ij ij Then AB is an m×p matrix and ∑n (AB) = A B .
(2.12) ij ik kj k=1 Two matrices, A and B are said to be conformable in a particular order if they can be multiplied in that order.
Thus if A is an r×s matrix and B is a s×p then A and B are conformable in the order AB.
The above formula for (AB) says that it equals the ith row ij of A times the jth column of B.
  ( ) 1 2   2 3 1 Example 2.1.9 Multiply if possible 3 1 .
7 6 2 2 6 First check to see if this is possible.
It is of the form (3×2)(2×3) and since the inside numbers match, it must be possible to do this and the result should be a 3×3 matrix.
The answer is of the form        ( ) ( ) ( ) 1 2 1 2 1 2   2   3   1  3 1 , 3 1 , 3 1 7 6 2 2 6 2 6 2 6 where the commas separate the columns in the resulting product.
Thus the above product equals   16 15 5   13 15 5 , 46 42 14 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 43 a 3×3 matrix as desired.
In terms of the ijth entries and the above deﬁnition, the entry in the third row and second column of the product should equal ∑ a b =a b +a b =2×3+6×6=42.
3k k2 31 12 32 22 j You should try a few more such examples to verify the above deﬁnition in terms of the ijth entries works for other entries.
   1 2 2 3 1    Example 2.1.10 Multiply if possible 3 1 7 6 2 .
2 6 0 0 0 This is not possible because it is of the form (3×2)(3×3) and the middle numbers don’t match.
   2 3 1 1 2    Example 2.1.11 Multiply if possible 7 6 2 3 1 .
0 0 0 2 6 This is possible because in this case it is of the form (3×3)(3×2) and the middle numbers do match.
When the multiplication is done it equals   13 13   29 32 .
0 0 Check this and be sure you come up with the same answer.
  1 ( )   Example 2.1.12 Multiply if possible 2 1 2 1 0 .
1 In this case you are trying to do (3×1)(1×4).
The inside numbers match so you can do it.
Verify     1 ( ) 1 2 1 0     2 1 2 1 0 = 2 4 2 0 1 1 2 1 0 2.1.2 Digraphs Consider the following graph illustrated in the picture.
1 2 3 There are three locations in this graph, labelled 1,2, and 3.
The directed lines represent a way of going from one location to another.
Thus there is one way to go from location 1 to location 1.
There is one way to go from location 1 to location 3.
It is not possible to go Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation44 MATRICES AND LINEAR TRANSFORMATIONS fromlocation2tolocation3althoughitispossibletogofromlocation3tolocation2.
Lets refer to moving along one of these directed lines as a step.
The following 3×3 matrix is a numerical way of writing the above graph.
This is sometimes called a digraph, short for directed graph.
  1 1 1   1 0 0 1 1 0 Thusa , the entryin the ith rowand jth columnrepresentsthe number of waysto go from ij location i to location j in one step.
Problem: Find the number of ways to go from i to j using exactly k steps.
Denote the answer to the above problem by ak.
We don’t know what it is right now ij unless k = 1 when it equals a described above.
However, if we did know what it was, we ij could ﬁnd ak+1 as follows.
ij ∑ ak+1 = aka ij ir rj r This is because if you go from i to j in k+1 steps, you ﬁrst go from i to r in k steps and then for each of these ways there are a ways to go from there to j.
Thus aka gives rj ir rj the number of ways to go from i to j in k+1 steps such that the kth step leaves you at location r. Adding these gives the above sum.
Now you recognize this as the ijth entry of the product of two matrices.
Thus ∑ ∑ a2 = a a , a3 = a2 a ij ir rj ij ir rj r r and so forth.
From the above deﬁnition of matrix multiplication, this shows that if A is the matrix associated with the directed graph as above, then ak is just the ijth entry of Ak ij where Ak is just what you would think it should be, A multiplied by itself k times.
Thusintheaboveexample, toﬁndthenumberofwaysofgoingfrom1to3intwosteps youwouldtakethatmatrixandmultiplyitbyitselfandthentaketheentryintheﬁrstrow and third column.
Thus     2 1 1 1 3 2 1     1 0 0 = 1 1 1 1 1 0 2 1 1 and you see there is exactly one way to go from 1 to 3 in two steps.
You can easily see this is true from looking at the graph also.
Note there are three ways to go from 1 to 1 in 2 steps.
Can you ﬁnd them from the graph?
What would you do if you wanted to consider 5 steps?
    5 1 1 1 28 19 13     1 0 0 = 13 9 6 1 1 0 19 13 9 There are 19 ways to go from 1 to 2 in ﬁve steps.
Do you think you could list them all by looking at the graph?
I don’t think you could do it without wasting a lot of time.
Of course there is nothing sacred about having only three locations.
Everything works just as well with any number of locations.
In general if you have n locations, you would need to use a n×n matrix.
Example 2.1.13 Consider the following directed graph.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 45 1 2 3 4 Writethematrixwhichisassociatedwiththisdirectedgraphandﬁndthenumberofways to go from 2 to 4 in three steps.
Here you need to use a 4×4 matrix.
The one you need is   0 1 1 0    1 0 0 0    1 1 0 1 0 1 0 1 Thentoﬁndtheanswer,youjustneedtomultiplythismatrixbyitselfthreetimesandlook at the entry in the second row and fourth column.
    3 0 1 1 0 1 3 2 1      1 0 0 0   2 1 0 1    =  1 1 0 1 3 3 1 2 0 1 0 1 1 2 1 1 There is exactly one way to go from 2 to 4 in three steps.
How many ways would there be of going from 2 to 4 in ﬁve steps?
    5 0 1 1 0 5 9 5 4      1 0 0 0   5 4 1 3    =  1 1 0 1 9 10 4 6 0 1 0 1 4 6 3 3 There are three ways.
Note there are 10 ways to go from 3 to 2 in ﬁve steps.
Thisisaninterestingapplicationoftheconceptoftheijth entryoftheproductmatrices.
2.1.3 Properties Of Matrix Multiplication As pointed out above, sometimes it is possible to multiply matrices in one order but not in the other order.
What if it makes sense to multiply them in either order?
Will they be equal then?
( )( ) ( )( ) 1 2 0 1 0 1 1 2 Example 2.1.14 Compare and .
3 4 1 0 1 0 3 4 The ﬁrst product is ( )( ) ( ) 1 2 0 1 2 1 = , 3 4 1 0 4 3 the second product is ( )( ) ( ) 0 1 1 2 3 4 = , 1 0 3 4 1 2 and you see these are not equal.
Therefore, you cannot conclude that AB =BA for matrix multiplication.
However, there are some properties which do hold.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation46 MATRICES AND LINEAR TRANSFORMATIONS Proposition 2.1.15 If all multiplications and additions make sense, the following hold for matrices, A,B,C and a,b scalars.
A(aB+bC)=a(AB)+b(AC) (2.13) (B+C)A=BA+CA (2.14) A(BC)=(AB)C (2.15) Proof: Using the above deﬁnition of matrix multiplication, ∑ (A(aB+bC)) = A (aB+bC) ij ik kj ∑k = A (aB +bC ) ik kj kj k∑ ∑ = a A B +b A C ik kj ik kj k k = a(AB) +b(AC) ij ij = (a(AB)+b(AC)) ij showing that A(B+C)=AB+AC as claimed.
Formula (2.14) is entirely similar.
Consider (2.15), the associative law of multiplication.
Before reading this, review the deﬁnition of matrix multiplication in terms of entries of the matrices.
∑ (A(BC)) = A (BC) ij ik kj ∑k ∑ = A B C ik kl lj ∑k l = (AB) C il lj l = ((AB)C) .
(cid:4) ij Another important operation on matrices is that of taking the transpose.
The following example shows what is meant by this operation, denoted by placing a T as an exponent on the matrix.
  T ( ) 1 1+2i   1 3 2 3 1 = 1+2i 1 6 2 6 What happened?
The ﬁrst column became the ﬁrst row and the second column became the second row.
Thus the 3×2 matrix became a 2×3 matrix.
The number 3 was in the second row and the ﬁrst column and it ended up in the ﬁrst row and second column.
This motivates the following deﬁnition of the transpose of a matrix.
Deﬁnition 2.1.16 Let A be an m×n matrix.
Then AT denotes the n×m matrix which is deﬁned as follows.
( ) AT =A ij ji The transpose of a matrix has the following important property.
Lemma 2.1.17 Let A be an m×n matrix and let B be a n×p matrix.
Then (AB)T =BTAT (2.16) and if α and β are scalars, (αA+βB)T =αAT +βBT (2.17) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 47 Proof: From the deﬁnition, ( ) (AB)T = (AB) ji ij ∑ = A B jk ki ∑k ( ) ( ) = BT AT ik kj (k ) = BTAT ij (2.17) is left as an exercise.
(cid:4) Deﬁnition 2.1.18 An n×n matrix A is said to be symmetric if A = AT.
It is said to be skew symmetric if AT =−A.
Example 2.1.19 Let   2 1 3 A= 1 5 −3 .
3 −3 7 Then A is symmetric.
Example 2.1.20 Let   0 1 3 A= −1 0 2  −3 −2 0 Then A is skew symmetric.
There is a special matrix called I and deﬁned by I =δ ij ij where δ is the Kronecker symbol deﬁned by ij { 1 if i=j δ = ij 0 if i̸=j It is called the identity matrix because it is a multiplicative identity in the following sense.
Lemma 2.1.21 Suppose A is an m×n matrix and I is the n×n identity matrix.
Then n AI =A.
If I is the m×m identity matrix, it also follows that I A=A.
n m m Proof: ∑ (AI ) = A δ n ij ik kj k = A ij and so AI =A.
The other case is left as an exercise for you.
n Deﬁnition 2.1.22 An n×n matrix A has an inverse A−1 if and only if there exists a matrix, denoted as A−1 such that AA−1 =A−1A=I where I =(δ ) for ij { 1 if i=j δ ≡ ij 0 if i̸=j Such a matrix is called invertible.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation48 MATRICES AND LINEAR TRANSFORMATIONS If it acts like an inverse, then it is the inverse.
This is the message of the following proposition.
Proposition 2.1.23 Suppose AB =BA=I.
Then B =A−1.
Proof: From the deﬁnition B is an inverse for A.
Could there be another one B′?
′ ′ ′ ′ B =B I =B (AB)=(B A)B =IB =B.
Thus, the inverse, if it exists, is unique.
(cid:4) 2.1.4 Finding The Inverse Of A Matrix A little later a formula is given for the inverse of a matrix.
However, it is not a good way toﬁndtheinverseforamatrix.
Thereisamucheasierwayanditisthiswhichispresented here.
It is also important to note that not all matrices have inverses.
( ) 1 1 Example 2.1.24 Let A= .
Does A have an inverse?
1 1 One might think A would have an inverse because it does not equal zero.
However, ( )( ) ( ) 1 1 −1 0 = 1 1 1 0 and if A−1 existed, this could not happen because you could multiply on the left by the inverse A and conclude the vector (−1,1)T = (0,0)T. Thus the answer is that A does not have an inverse.
Suppose you want to ﬁnd B such that AB =I.
Let ( ) B = b ··· b 1 n Also the ith column of I is ( ) e = 0 ··· 0 1 0 ··· 0 T i Thus,ifAB =I,b ,theithcolumnofBmustsatisfytheequationAb =e .Theaugmented i i i matrix for ﬁnding b is (A|e ).
Thus, by doing row operations till A becomes I, you end up i i with (I|b ) where b is the solution to Ab =e .
Now the same sequence of row operations i i i i worksregardlessoftherightsideoftheagumentedmatrix(A|e )andsoyoucansavetrouble i by simply doing the following.
(A|I)rowop→erations (I|B) and the ith column of B is b , the solution to Ab =e .
Thus AB =I.
i i i This is the reason for the following simple procedure for ﬁnding the inverse of a matrix.
This procedure is called the Gauss Jordan procedure.
It produces the inverse if the matrix has one.
Actually, it produces the right inverse.
Procedure 2.1.25 Suppose A is an n × n matrix.
To ﬁnd A−1 if it exists, form the augmented n×2n matrix, (A|I) and then do row operations until you obtain an n×2n matrix of the form (I|B) (2.18) if possible.
When this has been done, B =A−1.
The matrix A has an inverse exactly when it is possible to do row operations and end up with one like (2.18).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.1.
MATRICES 49 As described above, the following is a description of what you have just done.
ARqRq→−1···R1 I I RqRq→−1···R1 B where those R sympolize row operations.
It follows that you could undo what you did by i doing the inverse of these row operations in the opposite order.
Thus I R1−1···R→q−−11Rq−1 A B R1−1···R→q−−11Rq−1 I Here R−1 is the row operation which undoes the row operation R. Therefore, if you form (B|I) and do the inverse of the row operations which produced I from A in the reverse order, you would obtain (I|A).
By the same reasoning above, it follows that A is a right inverse of B and so BA = I also.
It follows from Proposition 2.1.23 that B = A−1.
Thus the procedure produces the inverse whenever it works.
If it is possible to do row operations and end up with A rowop→erations I, then the above argument shows that A has an inverse.
Conversely, if A has an inverse, can it be found by the above procedure?
In this case there exists a unique solution x to the equation Ax=y.
In fact it is just x = Ix = A−1y.
Thus in terms of augmented matrices, you would expect to obtain ( ) (A|y)→ I|A−1y That is, you would expect to be able to do row operations to A and end up with I. Thedetailswillbeexplainedfullywhenamorecarefuldiscussionisgivenwhichisbased onmorefundamentalconsiderations.
Fornow,itsuﬃcestoobservethatwhenevertheabove procedure works, it ﬁnds the inverse.
  1 0 1 Example 2.1.26 Let A= 1 −1 1 .
Find A−1.
1 1 −1 Form the augmented matrix   1 0 1 1 0 0  1 −1 1 0 1 0 .
1 1 −1 0 0 1 Nowdorowoperationsuntilthen×nmatrixontheleftbecomestheidentitymatrix.
This yields after some computations,   1 0 0 0 1 1  0 1 0 1 −21 02  0 0 1 1 −1 −1 2 2 and so the inverse of A is the matrix on the right,   0 1 1  1 −21 02 .
1 −1 −1 2 2 Checking the answer is easy.
Just multiply the matrices and see if it works.
     1 0 1 0 1 1 1 0 0  1 −1 1  1 −21 02 = 0 1 0 .
1 1 −1 1 −1 −1 0 0 1 2 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation50 MATRICES AND LINEAR TRANSFORMATIONS Always check your answer because if you are like some of us, you will usually have made a mistake.
  1 2 2 Example 2.1.27 Let A= 1 0 2 .
Find A−1.
3 1 −1 Set up the augmented matrix (A|I)   1 2 2 1 0 0   1 0 2 0 1 0 3 1 −1 0 0 1 Next take (−1) times the ﬁrst row and add to the second followed by (−3) times the ﬁrst row added to the last.
This yields   1 2 2 1 0 0  0 −2 0 −1 1 0 .
0 −5 −7 −3 0 1 Then take 5 times the second row and add to −2 times the last row.
  1 2 2 1 0 0  0 −10 0 −5 5 0  0 0 14 1 5 −2 Next take the last row and add to (−7) times the top row.
This yields   −7 −14 0 −6 5 −2  0 −10 0 −5 5 0 .
0 0 14 1 5 −2 Now take (−7/5) times the second row and add to the top.
  −7 0 0 1 −2 −2  0 −10 0 −5 5 0 .
0 0 14 1 5 −2 Finally divide the top row by −7, the second row by -10 and the bottom row by 14 which yields   1 0 0 −1 2 2  0 1 0 17 −71 07 .
2 2 0 0 1 1 5 −1 14 14 7 Therefore, the inverse is   −1 2 2  17 −71 07  2 2 1 5 −1 14 14 7   1 2 2 Example 2.1.28 Let A= 1 0 2 .
Find A−1.
2 2 4 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.2.
EXERCISES 51 Write the augmented matrix (A|I)   1 2 2 1 0 0   1 0 2 0 1 0 2 2 4 0 0 1 ( ) and proceed to do row operations attempting to obtain I|A−1 .
Take (−1) times the top row and add to the second.
Then take (−2) times the top row and add to the bottom.
  1 2 2 1 0 0  0 −2 0 −1 1 0  0 −2 0 −2 0 1 Next add (−1) times the second row to the bottom row.
  1 2 2 1 0 0  0 −2 0 −1 1 0  0 0 0 −1 −1 1 At this point, you can see there will be no inverse because you have obtained a row of zeros in the left half of the augmented matrix (A|I).
Thus there will be no way to obtain I on the left.
In other words, the three systems of equations you must solve to ﬁnd the inverse have no solution.
In particular, there is no solution for the ﬁrst column of A−1 which must solve     x 1     A y = 0 z 0 because a sequence of row operations leads to the impossible equation, 0x+0y+0z =−1.
2.2 Exercises 1.
In (2.1) - (2.8) describe −A and 0.
2.
LetAbeann×nmatrix.
ShowAequalsthesumofasymmetricandaskewsymmetric matrix.
3.
Show every skew symmetric matrix has all zeros down the main diagonal.
The main diagonal consists of every entry of the matrix which is of the form a .
It runs from ii the upper left down to the lower right.
4.
Using only the properties (2.1) - (2.8) show −A is unique.
5.
Using only the properties (2.1) - (2.8) show 0 is unique.
6.
Usingonlytheproperties(2.1)-(2.8)show0A=0.Herethe0ontheleftisthescalar 0 and the 0 on the right is the zero for m×n matrices.
7.
Using only the properties (2.1) - (2.8) and previous problems show (−1)A=−A.
8.
Prove (2.17).
9.
Prove that I A=A where A is an m×n matrix.
m 10.
(Let A a)nd be a real m×n matrix and let x ∈ Rn and y ∈ Rm.
Show (Ax,y)Rm = x,ATy where (·,·) denotes the dot product in Rk.
Rn Rk Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation52 MATRICES AND LINEAR TRANSFORMATIONS 11.
Use the result of Problem 10 to verify directly that (AB)T =BTAT without making any reference to subscripts.
12.
Let x=(−1,−1,1) and y=(0,1,2).
Find xTy and xyT if possible.
13.
Give an example of matrices, A,B,C such that B ̸=C, A̸=0, and yet AB =AC.
    1 1 ( ) 1 1 −3 14.
Let A =  −2 −1 , B = 1 −1 −2 , and C =  −1 2 0 .
Find 2 1 −2 1 2 −3 −1 0 if possible the following products.
AB,BA,AC,CA,CB,BC.
15.
Consider the following digraph.
1 2 4 3 Writethematrixassociatedwiththisdigraphandﬁndthenumberofwaystogofrom 3 to 4 in three steps.
16.
Show that if A−1 exists for an n×n matrix, then it is unique.
That is, if BA=I and AB =I, then B =A−1.
17.
Show (AB)−1 =B−1A−1.
( ) ( ) 18.
Show that if A is an invertible n×n matrix, then so is AT and AT −1 = A−1 T .
19.
Show that if A is an n×n invertible matrix and x is a n×1 matrix such that Ax=b for b an n×1 matrix, then x=A−1b.
20.
Give an example of a matrix A such that A2 =I and yet A̸=I and A̸=−I.
21.
Give an example of matrices, A,B such that neither A nor B equals zero and yet AB =0.
    x −x +2x x 1 2 3 1     22.
Write 2x3+x1  intheform A x2  where Aisanappropriatematrix.
3x x 3 3 3x +3x +x x 4 2 1 4 23.
Give another example other than the one given in this section of two square matrices, A and B such that AB ̸=BA.
24.
Suppose A and B are square matrices of the same size.
Which of the following are correct?
(a) (A−B)2 =A2−2AB+B2 (b) (AB)2 =A2B2 (c) (A+B)2 =A2+2AB+B2 (d) (A+B)2 =A2+AB+BA+B2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.3.
LINEAR TRANSFORMATIONS 53 (e) A2B2 =A(AB)B (f) (A+B)3 =A3+3A2B+3AB2+B3 (g) (A+B)(A−B)=A2−B2 (h) None of the above.
They are all wrong.
(i) All of the above.
They are all right.
( ) −1 −1 25.
Let A= .
Find all 2×2 matrices, B such that AB =0.
3 3 26.
Prove that if A−1 exists and Ax=0 then x=0.
27.
Let   1 2 3   A= 2 1 4 .
1 0 2 Find A−1 if possible.
If A−1 does not exist, determine why.
28.
Let   1 0 3   A= 2 3 4 .
1 0 2 Find A−1 if possible.
If A−1 does not exist, determine why.
29.
Let   1 2 3   A= 2 1 4 .
4 5 10 Find A−1 if possible.
If A−1 does not exist, determine why.
30.
Let   1 2 0 2    1 1 2 0  A= 2 1 −3 2  1 2 1 2 Find A−1 if possible.
If A−1 does not exist, determine why.
2.3 Linear Transformations By (2.13), if A is an m×n matrix, then for v,u vectors in Fn and a,b scalars,   ∈Fn z }| { Aau+bv=aAu+bAv∈Fm (2.19) Deﬁnition 2.3.1 A function, A : Fn → Fm is called a linear transformation if for all u,v∈Fn and a,b scalars, (2.19) holds.
From (2.19), matrix multiplication deﬁnes a linear transformation as just deﬁned.
It turns out this is the only type of linear transformation available.
Thus if A is a linear transformation from Fn to Fm, there is always a matrix which produces A.
Before showing this, here is a simple deﬁnition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation54 MATRICES AND LINEAR TRANSFORMATIONS Deﬁnition 2.3.2 A vector, e ∈Fn is deﬁned as follows: i   0  .
  .
  .
 ei ≡ 1 ,  .
  .
 .
0 where the 1 is in the ith position and there are zeros everywhere else.
Thus e =(0,··· ,0,1,0,··· ,0)T .
i Of course the e for a particular value of i in Fn would be diﬀerent than the e for that i i same value of i in Fm for m̸=n.
One of them is longer than the other.
However, which one is meant will be determined by the context in which they occur.
These vectors have a signiﬁcant property.
Lemma 2.3.3 Letv∈Fn.Thusv isalistofnumbersarrangedvertically, v ,··· ,v .Then 1 n eTv=v .
(2.20) i i Also, if A is an m×n matrix, then letting e ∈Fm and e ∈Fn, i j eTAe =A (2.21) i j ij Proof: First note that eT is a 1×n matrix and v is an n×1 matrix so the above i multiplication in (2.20) makes perfect sense.
It equals   v 1  .
  .
  .
 (0,··· ,1,···0) vi =vi  .
  .
 .
v n as claimed.
Consider (2.21).
From the deﬁnition of matrix multiplication, and noting that (e ) = j k δkj  ∑    A (e ) A  k 1.k j k   .1j   .
  .
  ∑ .
  .
 eTi Aej =eTi  kAi.k(ej)k =eTi  A.ij =Aij  .
  .
 ∑ .
.
A (e ) A k mk j k mj by the ﬁrst part of the lemma.
(cid:4) Theorem 2.3.4 Let L : Fn → Fm be a linear transformation.
Then there exists a unique m×n matrix A such that Ax=Lx for all x∈Fn.
The ikth entry of this matrix is given by eTLe (2.22) i k Stated in another way, the kth column of A equals Le .
k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.3.
LINEAR TRANSFORMATIONS 55 Proof: By the lemma, ( ) (Lx) =eTLx=eTx Le = eTLe x .
i i i k k i k k Let A =eTLe , to prove the existence part of the theorem.
ik i k To verify uniqueness, suppose Bx=Ax=Lx for all x∈Fn.
Then in particular, this is true for x=e and then multiply on the left by eT to obtain j i B =eTBe =eTAe =A ij i j i j ij showing A=B.
(cid:4) Corollary 2.3.5 A linear transformation, L : Fn → Fm is completely determined by the vectors {Le ,··· ,Le }.
1 n Proof: This follows immediately from the above theorem.
The unique matrix deter- mining the linear transformation which is given in (2.22) depends only on these vectors.
(cid:4) This theorem shows that any linear transformation deﬁned on Fn can always be con- sidered as a matrix.
Therefore, the terms “linear transformation” and “matrix” are often used interchangeably.
For example, to say that a matrix is one to one, means the linear transformation determined by the matrix is one to one.
Examp(le 2).3.6 Find the(linea)r transformation, L : R2 → R2 which has the property that 2 1 Le = and Le = .
From the above theorem and corollary, this linear trans- 1 1 2 3 formation is that determined by matrix multiplication by the matrix ( ) 2 1 .
1 3 Deﬁnition 2.3.7 Let L : Fn → Fm be a linear transformation and let its matrix be the m×n matrix A.
Then ker(L) ≡ {x∈Fn :Lx=0}.
Sometimes people also write this as N(A), the null space of A.
Then there is a fundamental result in the case where m<n.
In this case, the matrix A of the linear transformation looks like the following.
Theorem 2.3.8 Let A be an m×n matrix where m < n. Then N(A) contains nonzero vectors.
Proof: First consider the case where A is a 1×n matrix for n>1.
Say ( ) A= a ··· a 1 n If a =0, consider the vector x=e .
If a ̸=0, let 1 1 1   b    1  x= .
  .
 .
1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation56 MATRICES AND LINEAR TRANSFORMATIONS where b is chosen to satisfy the equation ∑n a b+ a =0 1 k k=2 Suppose now that the theorem is true for any m×n matrix with n > m and consider an (m×1)×n matrix A where n > m+1.
If the ﬁrst column of A is 0, then you could let x=e as above.
If the ﬁrst column is not the zero vector, then by doing row operations, 1 the equation Ax=0 can be reduced to the equivalent system A x=0 1 where A1 is of the form ( ) 1 aT A = 1 0 B where B is an m×(n−1) matrix.
Since n > m+1, it follows that (n−1) > m and so by induction, there exists a nonzero vector y ∈Fn−1 such that By=0.
Then consider the vector ( ) b x= y   bT 1  .
 A x has for its top entry the expression b+aTy.
Letting B =  .
, the ith entry of 1 .
bT m A xfori>1isoftheformbTy=0.
Thusifbischosentosatisfytheequationb+aTy=0, 1 i then A x=0.
(cid:4) 1 2.4 Subspaces And Spans Deﬁnition 2.4.1 Let{x ,··· ,x }bevectorsinFn.Alinearcombinationisanyexpression 1 p of the form ∑p c x i i i=1 where the c are scalars.
The set of all linear combinations of these vectors is called i span(x ,··· ,x ).
If V ⊆ Fn, then V is called a subspace if whenever α,β are scalars 1 n and u and v are vectors of V, it follows αu+βv ∈ V. That is, it is “closed under the algebraic operations of vector addition and scalar multiplication”.
A linear combination of vectors is said to be trivial if all the scalars in the linear combination equal zero.
A set of vectors is said to be linearly independent if the only linear combination of these vectors which equals the zero vector is the trivial linear combination.
Thus {x ,··· ,x } is called 1 n linearly independent if whenever ∑p c x =0 k k k=1 it follows that all the scalars c equal zero.
A set of vectors, {x ,··· ,x }, is called linearly k 1 p dependent if it is not linearly independent.
Thus the set∑of vectors is linearly dependent if there exist scalars c ,i=1,··· ,n, not all zero such that p c x =0.
i k=1 k k Proposition 2.4.2 Let V ⊆ Fn.
Then V is a subspace if and only if it is a vector space itself with respect to the same operations of scalar multiplication and vector addition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.4.
SUBSPACES AND SPANS 57 Proof: Suppose ﬁrst that V is a subspace.
All algebraic properties involving scalar multiplicationandvectoradditionholdforV becausethesethingsholdforFn.
Is0∈V?Yes it is.
This is because 0v∈V and 0v=0.
By assumption, for α a scalar and v∈V,αv∈V.
Therefore, −v = (−1)v ∈ V. Thus V has the additive identity and additive inverse.
By assumption, V is closed with respect to the two operations.
Thus V is a vector space.
If V ⊆ Fn is a vector space, then by deﬁnition, if α,β are scalars and u,v vectors in V, it follows that αv+βu∈V.
(cid:4) Thus,fromtheabove,subspacesofFn arejustsubsetsofFn whicharethemselvesvector spaces.
Lemma 2.4.3 A set of vectors {x ,··· ,x } is linearly independent if and only if none of 1 p the vectors can be obtained as a linear combination of the others.
∑ Proof: Supposeﬁrstthat{x ,··· ,x }islinearlyindependent.
Ifx = c x ,then 1 p k j̸=k j j ∑ 0=1x + (−c )x , k j j j̸=k anontriviallinearcombination,contrarytoassumption.
Thisshowsthatifthesetislinearly independent, then none of the vectors is a linear combination of the others.
Now suppose no vector is a linear combination of the others.
Is {x ,··· ,x } linearly 1 p independent?
If it is not, there exist scalars c , not all zero such that i ∑p c x =0.
i i i=1 Say c ̸=0.
Then you can solve for x as k k ∑ x = (−c )/c x k j k j j̸=k contrary to assumption.
(cid:4) The following is called the exchange theorem.
Theorem 2.4.4 (ExchangeTheorem)Let{x ,··· ,x }bealinearlyindependentsetofvec- 1 r tors such that each x is in span(y ,··· ,y ).
Then r ≤s.
i 1 s Proof 1: Suppose not.
Then r >s.
By assumption, there exist scalars a such that ji ∑s x = a y i ji j j=1 The matrix whose jith entry is a has more columns than rows.
Therefore, by Theorem ji 2.3.8 there exists a nonzero vector b∈Fr such that Ab=0.
Thus ∑r 0= a b , each j. ji i i=1 Then ( ) ∑r ∑r ∑s ∑s ∑r b x = b a y = a b y =0 i i i ji j ji i j i=1 i=1 j=1 j=1 i=1 contradicting the assumption that {x ,··· ,x } is linearly independent.
1 r Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation58 MATRICES AND LINEAR TRANSFORMATIONS Proof 2: Deﬁne span{y ,··· ,y } ≡ V, it follows there exist scalars c ,··· ,c such 1 s 1 s that ∑s x = c y .
(2.23) 1 i i i=1 Not all of these scalars can equal zero because if this were the case, it would follow that x∑1 = 0 and so {x1,··· ,xr} would not be linearly independent.
Indeed, if x1 = 0, 1x1 + r 0x = x = 0 and so there would exist a nontrivial linear combination of the vectors i=2 i 1 {x ,··· ,x } which equals zero.
1 r Say c ̸=0.
Then solve ((2.23)) for y and obtain k k   z s-1vec}to|rshere { yk ∈spanx1,y1,··· ,yk−1,yk+1,··· ,ys.
Deﬁne {z1,··· ,zs−1} by {z1,··· ,zs−1}≡{y1,··· ,yk−1,yk+1,··· ,ys} Therefore, span{x1,z1,··· ,zs−1} = V because if v ∈ V, there exist constants c1,··· ,cs such that ∑s−1 v= c z +c y .
i i s k i=1 Nowreplacetheyk intheabovewithalinearcombinationofthevectors,{x1,z1,··· ,zs−1} toobtainv∈span{x1,z1,··· ,zs−1}.Thevectoryk,inthelist{y1,··· ,ys},hasnowbeen replaced with the vector x and the resulting modiﬁed list of vectors has the same span as 1 the original list of vectors, {y ,··· ,y }.
1 s Now suppose that r > s and that span{x ,··· ,x ,z ,··· ,z } = V where the vectors, 1 l 1 p z ,··· ,z are each taken from the set, {y ,··· ,y } and l+p=s.
This has now been done 1 p 1 s for l=1 above.
Then since r >s, it follows that l≤s<r and so l+1≤r.
Therefore, x l+1 is a vector not in the list, {x ,··· ,x } and since span{x ,··· ,x ,z ,··· ,z } = V, there 1 l 1 l 1 p exist scalars c and d such that i j ∑l ∑p x = c x + d z .
(2.24) l+1 i i j j i=1 j=1 Now not all the d can equal zero because if this were so, it would follow that {x ,··· ,x } j 1 r wouldbealinearlydependentsetbecauseoneofthevectorswouldequalalinearcombination of the others.
Therefore, ((2.24)) can be solved for one of the z , say z , in terms of x i k l+1 and the other z and just as in the above argument, replace that z with x to obtain i i l+1    z p-1vec}t|orshere { spanx1,···xl,xl+1,z1,···zk−1,zk+1,··· ,zp=V.
Continue this way, eventually obtaining span{x ,··· ,x }=V.
1 s But then x ∈ span{x ,··· ,x } contrary to the assumption that {x ,··· ,x } is linearly r 1 s 1 r independent.
Therefore, r ≤s as claimed.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.4.
SUBSPACES AND SPANS 59 Proof 3: Suppose r >s.
Let z denote a vector of {y ,··· ,y }.
Thus there exists j as k 1 s small as possible such that span(y ,··· ,y )=span(x ,··· ,x ,z ,··· ,z ) 1 s 1 m 1 j where m+j = s. It is given that m = 0, corresponding to no vectors of {x ,··· ,x } and 1 m j =s,correspondingtoallthey resultsintheaboveequationholding.
Ifj >0thenm<s k and so ∑m ∑j x = a x + b z m+1 k k i i k=1 i=1 Notalltheb canequal0andsoyoucansolveforoneofthemintermsofx ,x ,··· ,x , i m+1 m 1 and the other z .
Therefore, there exists k {z1,··· ,zj−1}⊆{y1,··· ,ys} such that span(y1,··· ,ys)=span(x1,··· ,xm+1,z1,··· ,zj−1) contradicting the choice of j.
Hence j =0 and span(y ,··· ,y )=span(x ,··· ,x ) 1 s 1 s It follows that x ∈span(x ,··· ,x ) s+1 1 s contrarytotheassumptionthex arelinearlyindependent.
Therefore, r ≤sasclaimed.
(cid:4) k Deﬁnition 2.4.5 Aﬁnitesetofvectors,{x ,··· ,x }isabasisforFn ifspan(x ,··· ,x )= 1 r 1 r Fn and {x ,··· ,x } is linearly independent.
1 r Corollary 2.4.6 Let {x ,··· ,x } and {y ,··· ,y } be two bases1 of Fn.
Then r =s=n.
1 r 1 s Proof: From the exchange theorem, r ≤s and s≤r.
Now note the vectors, z 1isint}he|ith slot { e =(0,··· ,0,1,0··· ,0) i for i=1,2,··· ,n are a basis for Fn.
(cid:4) Lemma 2.4.7 Let {v ,··· ,v } be a set of vectors.
Then V ≡ span(v ,··· ,v ) is a sub- 1 r 1 r space.
∑ ∑ Proof: Supposeα,β aretwoscalarsandlet r c v and r d v aretwoelements k=1 k k k=1 k k of V. What about ∑r ∑r α c v +β d v ?
k k k k k=1 k=1 Is it also in V?
∑r ∑r ∑r α c v +β d v = (αc +βd )v ∈V k k k k k k k k=1 k=1 k=1 so the answer is yes.
(cid:4) 1This is the plural form of basis.
We could say basiss but it would involve an inordinate amount of hissingasin“Thesixthshiek’ssixthsheepissick”.
Thisisthereasonthatbasesisusedinsteadofbasiss.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation60 MATRICES AND LINEAR TRANSFORMATIONS Deﬁnition 2.4.8 A ﬁnite set of vectors, {x ,··· ,x } is a basis for a subspace V of Fn if 1 r span(x ,··· ,x )=V and {x ,··· ,x } is linearly independent.
1 r 1 r Corollary 2.4.9 Let {x ,··· ,x } and {y ,··· ,y } be two bases for V. Then r =s.
1 r 1 s Proof: From the exchange theorem, r ≤s and s≤r.
(cid:4) Deﬁnition 2.4.10 Let V be a subspace of Fn.
Then dim(V) read as the dimension of V is the number of vectors in a basis.
Of course you should wonder right now whether an arbitrary subspace even has a basis.
In fact it does and this is in the next theorem.
First, here is an interesting lemma.
Lemma 2.4.11 Suppose v ∈/ span(u ,··· ,u ) and {u ,··· ,u } is linearly independent.
1 k 1 k Then {u ,··· ,u ,v} is also linearly independent.
1 k ∑ Proof: Suppose k c u + dv = 0.
It is required to verify that each c = 0 and i=1 i i i that d = 0.
But if d ̸= 0, then you can solve for v as a linear combination of the vectors, {u ,··· ,u }, 1 k ∑k ( ) c v=− i u d i i=1 ∑ contrary to assumption.
Therefore, d=0.
But then k c u =0 and the linear indepen- i=1 i i dence of {u ,··· ,u } implies each c =0 also.
(cid:4) 1 k i Theorem 2.4.12 Let V be a nonzero subspace of Fn.
Then V has a basis.
Proof: Let v ∈ V where v ̸= 0.
If span{v } = V, stop.
{v } is a basis for V. 1 1 1 1 Otherwise, there exists v ∈ V which is not in span{v }.
By Lemma 2.4.11 {v ,v } is a 2 1 1 2 linearly independent set of vectors.
If span{v ,v } = V stop, {v ,v } is a basis for V. If 1 2 1 2 span{v ,v }̸= V, then there exists v ∈/ span{v ,v } and {v ,v ,v } is a larger linearly 1 2 3 1 2 1 2 3 independent set of vectors.
Continuing this way, the process must stop before n+1 steps because if not, it would be possible to obtain n+1 linearly independent vectors contrary to the exchange theorem.
(cid:4) In words the following corollary states that any linearly independent set of vectors can be enlarged to form a basis.
Corollary 2.4.13 Let V be a subspace of Fn and let {v ,··· ,v } be a linearly independent 1 r set of vectors in V. Then either it is a basis for V or there exist vectors, v ,··· ,v such r+1 s that {v ,··· ,v ,v ,··· ,v } is a basis for V. 1 r r+1 s Proof: This follows immediately from the proof of Theorem 2.4.12.
You do exactly the same argument except you start with {v ,··· ,v } rather than {v }.
(cid:4) 1 r 1 It is also true that any spanning set of vectors can be restricted to obtain a basis.
Theorem 2.4.14 Let V be a subspace of Fn and suppose span(u ··· ,u ) = V where 1 p the u are nonzero vectors.
Then there exist vectors {v ··· ,v } such that {v ··· ,v } ⊆ i 1 r 1 r {u ··· ,u } and {v ··· ,v } is a basis for V. 1 p 1 r Proof: Let r be the smallest positive integer with the property that for some set {v ··· ,v }⊆{u ··· ,u }, 1 r 1 p span(v ··· ,v )=V.
1 r Then r ≤ p and it must be the case that {v ··· ,v } is linearly independent because if it 1 r were not so, one of the vectors, say v would be a linear combination of the others.
But k then you could delete this vector from {v ··· ,v } and the resulting list of r−1 vectors 1 r would still span V contrary to the deﬁnition of r. (cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.5.
AN APPLICATION TO MATRICES 61 2.5 An Application To Matrices The following is a theorem of major signiﬁcance.
Theorem 2.5.1 Suppose A is an n×n matrix.
Then A is one to one (injective) if and only if A is onto (surjective).
Also, if B is an n×n matrix and AB = I, then it follows BA=I.
Proof: First suppose A is one to one.
Consider the vectors, {Ae ,··· ,Ae } where e 1 n k is the column vector which is all zeros except for a 1 in the kth position.
This set of vectors is linearly independent because if ∑n c Ae =0, k k k=1 then since A is linear, ( ) ∑n A c e =0 k k k=1 and since A is one to one, it follows ∑n c e =0 k k k=1 which implies each c =0 because the e are clearly linearly independent.
k k Therefore, {Ae ,··· ,Ae } must be a basis for Fn because if not there would exist a 1 n vector, y ∈/ span(Ae ,··· ,Ae ) and then by Lemma 2.4.11, {Ae ,··· ,Ae ,y} would be 1 n 1 n an independent set of vectors having n+1 vectors in it, contrary to the exchange theorem.
It follows that for y∈Fn there exist constants, c such that i ( ) ∑n ∑n y= c Ae =A c e k k k k k=1 k=1 showing that, since y was arbitrary, A is onto.
Next suppose A is onto.
This means the span of the columns of A equals Fn.
If these columns are not linearly independent, then by Lemma2.4.3 on Page 57, one of the columns is a linear combination of the others and so the span of the columns of A equals the span of then−1othercolumns.
Thisviolatestheexchangetheorembecause{e ,··· ,e }wouldbe 1 n a linearly independent set of vectors contained in the span of only n−1 vectors.
Therefore, the columns of A must be independent and this is equivalent to saying that Ax=0 if and only if x=0.
This implies A is one to one because if Ax=Ay, then A(x−y)=0 and so x−y=0.
Now suppose AB = I.
Why is BA = I?
Since AB = I it follows B is one to one since otherwise, there would exist, x̸=0 such that Bx=0 and then ABx = A0=0 ̸= Ix.
Therefore, from what was just shown, B is also onto.
In addition to this, A must be one to one because if Ay=0, then y = Bx for some x and then x = ABx = Ay=0 showing y=0.
Now from what is given to be so, it follows (AB)A=A and so using the associative law for matrix multiplication, A(BA)−A=A(BA−I)=0.
But this means (BA−I)x=0 for all x since otherwise, A would not be one to one.
Hence BA=I as claimed.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation62 MATRICES AND LINEAR TRANSFORMATIONS This theorem shows that if an n×n matrix B acts like an inverse when multiplied on one side of A, it follows that B =A−1and it will act like an inverse on both sides of A.
The conclusion of this theorem pertains to square matrices only.
For example, let   ( ) 1 0   1 0 0 A= 0 1 , B = (2.25) 1 1 −1 1 0 Then ( ) 1 0 BA= 0 1 but   1 0 0 AB = 1 1 −1 .
1 0 0 2.6 Matrices And Calculus Thestudyofmovingcoordinatesystemsgivesanontrivialexampleoftheusefulnessofthe ideas involving linear transformations and matrices.
To begin with, here is the concept of the product rule extended to matrix multiplication.
Deﬁnition 2.6.1 Let A(t) be an m×n matrix.
Say A(t) = (Ai(j(t)).
S)uppose also that A (t) is a diﬀerentiable function for all i,j.
Then deﬁne A′(t)≡ A′ (t) .
That is, A′(t) ij ij is the matrix which consists of replacing each entry by its derivative.
Such an m×n matrix in which the entries are diﬀerentiable functions is called a diﬀerentiable matrix.
The next lemma is just a version of the product rule.
Lemma 2.6.2 Let A(t) be an m × n matrix and let B(t) be an n × p matrix with the property that all the entries of these matrices are diﬀerentiable functions.
Then ′ ′ ′ (A(t)B(t)) =A (t)B(t)+A(t)B (t).
Proof: This is like the usual proof.
1 (A(t+h)B(t+h)−A(t)B(t))= h 1 1 (A(t+h)B(t+h)−A(t+h)B(t))+ (A(t+h)B(t)−A(t)B(t)) h h B(t+h)−B(t) A(t+h)−A(t) =A(t+h) + B(t) h h and now, using the fact that the entries of the matrices are all diﬀerentiable, one can pass to a limit in both sides as h→0 and conclude that (A(t)B(t))′ =A′(t)B(t)+A(t)B′(t)(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.6.
MATRICES AND CALCULUS 63 2.6.1 The Coriolis Acceleration Imagine a point on the surface of the earth.
Now consider unit vectors, one pointing South, one pointing East and one pointing directly away from the center of the earth.
(cid:27)k j j i(cid:15) Denotetheﬁrstasi,thesecondasj,andthethirdask.
Ifyouarestandingontheearth youwillconsiderthesevectorsasﬁxed, butofcoursetheyarenot.
Astheearthturns, they change direction and so each is in reality a function of t. Nevertheless, it is with respect to these apparently ﬁxed vectors that you wish to understand acceleration, velocities, and displacements.
In general, let i∗,j∗,k∗ be the usual ﬁxed vectors in space and let i(t),j(t),k(t) be an orthonormal basis of vectors for each t, like the vectors described in the ﬁrst paragraph.
It is assumed these vectors are C1 functions of t. Letting the positive x axis extend in the direction of i(t), the positive y axis extend in the direction of j(t), and the positive z axis extend in the direction of k(t), yields a moving coordinate system.
Now let u be a vector and let t be some reference time.
For example you could let t = 0.
Then deﬁne the 0 0 components of u with respect to these vectors, i,j,k at time t as 0 u≡u1i(t )+u2j(t )+u3k(t ).
0 0 0 Let u(t) be deﬁned as the vector which has the same components with respect to i,j,k but at time t. Thus u(t)≡u1i(t)+u2j(t)+u3k(t).
and the vector has changed although the components have not.
Thisisexactlythesituationinthecaseoftheapparentlyﬁxedbasisvectorsontheearth if u is a position vector from the given spot on the earth’s surface to a point regarded as ﬁxed with the earth due to its keeping the same coordinates relative to the coordinate axes which are ﬁxed with the earth.
Now deﬁne a linear transformation Q(t) mapping R3 to R3 by Q(t)u≡u1i(t)+u2j(t)+u3k(t) where u≡u1i(t )+u2j(t )+u3k(t ) 0 0 0 Thus letting v be a vector deﬁned in the same manner as u and α,β, scalars, ( ) ( ) ( ) Q(t)(αu+βv)≡ αu1+βv1 i(t)+ αu2+βv2 j(t)+ αu3+βv3 k(t) ( ) ( ) = αu1i(t)+αu2j(t)+αu3k(t) + βv1i(t)+βv2j(t)+βv3k(t) ( ) ( ) = α u1i(t)+u2j(t)+u3k(t) +β v1i(t)+v2j(t)+v3k(t) ≡ αQ(t)u+βQ(t)v Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation64 MATRICES AND LINEAR TRANSFORMATIONS showing that Q(t) is a linear transformation.
Also, Q(t) preserves all distances because, since the vectors, i(t),j(t),k(t) form an orthonormal set, ( ) ∑3 ( ) 1/2 |Q(t)u|= ui 2 =|u|.
i=1 Lemma 2.6.3 SupposeQ(t)isareal, diﬀerentiablen×nmatrixwhichpreservesdistances.
ThenQ(t)Q(t)T =Q(t)T Q(t)=I.Also,ifu(t)≡Q(t)u,thenthereexistsavector,Ω(t) such that u′(t)=Ω(t)×u(t).
The symbol × refers to the cross product.
( ) Proof: Recall that (z·w)= 1 |z+w|2−|z−w|2 .
Therefore, 4 ( ) 1 (Q(t)u·Q(t)w) = |Q(t)(u+w)|2−|Q(t)(u−w)|2 4 ( ) 1 = |u+w|2−|u−w|2 4 = (u·w).
This implies ( ) Q(t)T Q(t)u·w =(u·w) forallu,w.Therefore,Q(t)T Q(t)u=uandsoQ(t)T Q(t)=Q(t)Q(t)T =I.Thisproves the ﬁrst part of the lemma.
It follows from the product rule, Lemma 2.6.2 that Q′(t)Q(t)T +Q(t)Q′(t)T =0 and so ( ) T Q′(t)Q(t)T =− Q′(t)Q(t)T .
(2.26) From the deﬁnition, Q(t)u=u(t), z =}|u { u′(t)=Q′(t)u=Q′(t)Q(t)T u(t).
Then writing the matrix of Q′(t)Q(t)T with respect to ﬁxed in space orthonormal basis vectors, i∗,j∗,k∗, where these are the usual basis vectors for R3, it follows from (2.26) that the matrix of Q′(t)Q(t)T is of the form   0 −ω (t) ω (t) 3 2  ω (t) 0 −ω (t)  3 1 −ω (t) ω (t) 0 2 1 for some time dependent scalars ω .
Therefore, i      u1 ′ 0 −ω (t) ω (t) u1 3 2  u2  (t)= ω (t) 0 −ω (t)  u2 (t) 3 1 u3 −ω (t) ω (t) 0 u3 2 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.6.
MATRICES AND CALCULUS 65 where the ui are the components of the vector u(t) in terms of the ﬁxed vectors i∗,j∗,k∗.
Therefore, u′(t)=Ω(t)×u(t)=Q′(t)Q(t)T u(t) (2.27) where ∗ ∗ ∗ Ω(t)=ω (t)i +ω (t)j +ω (t)k .
1 2 3 because (cid:12) (cid:12) (cid:12)(cid:12) i∗ j∗ k∗ (cid:12)(cid:12) Ω(t)×u(t)≡(cid:12)(cid:12) w1 w2 w3 (cid:12)(cid:12)≡ (cid:12) (cid:12) u1 u2 u3 ( ) ( ) ( ) i∗ w u3−w u2 +j∗ w u1−w3 +k∗ w u2−w u1 .
2 3 3 1 1 2 This proves the lemma and yields the existence part of the following theorem.
(cid:4) Theorem 2.6.4 Leti(t),j(t),k(t)beasdescribed.
ThenthereexistsauniquevectorΩ(t) such that if u(t) is a vector whose components are constant with respect to i(t),j(t),k(t), then u′(t)=Ω(t)×u(t).
Proof:Itonlyremainstoproveuniqueness.
SupposeΩ alsoworks.
Thenu(t)=Q(t)u 1 and so u′(t)=Q′(t)u and Q′(t)u=Ω×Q(t)u=Ω ×Q(t)u 1 for all u.
Therefore, (Ω−Ω )×Q(t)u=0 1 for all u and since Q(t) is one to one and onto, this implies (Ω−Ω )×w=0 for all w and 1 thus Ω−Ω =0.
(cid:4) 1 Now let R(t) be a position vector and let r(t)=R(t)+r (t) B where r (t)≡x(t)i(t)+y(t)j(t)+z(t)k(t).
B r (t) B (cid:14) R (cid:18) R(t) r(t) In the example of the earth, R(t) is the position vector of a point p(t) on the earth’s surface and r (t) is the position vector of another point from p(t), thus regarding p(t) B as the origin.
r (t) is the position vector of a point as perceived by the observer on the B earth with respect to the vectors he thinks of as ﬁxed.
Similarly, v (t) and a (t) will be B B the velocity and acceleration relative to i(t),j(t),k(t), and so v = x′i + y′j + z′k and B a =x′′i + y′′j + z′′k.
Then B v≡r′ =R′+x′i+y′j+z′k+xi′+yj′+zk′.
By,(2.27),ife∈{i,j,k},e′ =Ω×ebecausethecomponentsofthesevectorswithrespect to i,j,k are constant.
Therefore, xi′+yj′+zk′ = xΩ×i+yΩ×j+zΩ×k = Ω×(xi+yj+zk) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation66 MATRICES AND LINEAR TRANSFORMATIONS and consequently, v=R′+x′i+y′j+z′k+Ω×r =R′+x′i+y′j+z′k+Ω×(xi+yj+zk).
B Now consider the acceleration.
Quantities which are relative to the moving coordinate system and quantities which are relative to a ﬁxed coordinate system are distinguished by using the subscript B on those relative to the moving coordinate system.
z Ω×}|vB { a=v′ =R′′+x′′i+y′′j+z′′k+x′i′+y′j′+z′k′+ Ω′×r B   z v}B| { z Ω×}r|B(t) {   +Ω×x′i+y′j+z′k+xi′+yj′+zk′ =R′′+a +Ω′×r +2Ω×v +Ω×(Ω×r ).
B B B B The acceleration a is that perceived by an observer who is moving with the moving coor- B dinatesystemandforwhomthemovingcoordinatesystemisﬁxed.
ThetermΩ×(Ω×r ) B is called the centripetal acceleration.
Solving for a , B a =a−R′′−Ω′×r −2Ω×v −Ω×(Ω×r ).
(2.28) B B B B Heretheterm−(Ω×(Ω×r ))iscalledthecentrifugalacceleration,itbeinganacceleration B feltbytheobserverrelativetothemovingcoordinatesystemwhichheregardsasﬁxed, and the term −2Ω×v is called the Coriolis acceleration, an acceleration experienced by the B observeras he moves relativeto the moving coordinate system.
The mass multipliedbythe Coriolis acceleration deﬁnes the Coriolis force.
There is a ride found in some amusement parks in which the victims stand next to a circular wall covered with a carpet or some rough material.
Then the whole circular room begins to revolve faster and faster.
At some point, the bottom drops out and the victims are held in place by friction.
The force they feel is called centrifugal force and it causescentrifugalacceleration.
Itisnotnecessarytomoverelativetocoordinatesﬁxedwith the revolving wall in order to feel this force and it is pretty predictable.
However, if the nauseated victim moves relative to the rotating wall, he will feel the eﬀects of the Coriolis forceandthisforceisreallystrange.
ThediﬀerencebetweentheseforcesisthattheCoriolis force is caused by movement relative to the moving coordinate system and the centrifugal force is not.
2.6.2 The Coriolis Acceleration On The Rotating Earth Now consider the earth.
Let i∗,j∗,k∗, be the usual basis vectors ﬁxed in space with k∗ pointing in the direction of the north pole from the center of the earth and let i,j,k be the unit vectors described earlier with i pointing South, j pointing East, and k pointing away fromthecenteroftheearthatsomepointoftherotatingearth’ssurfacep.
LettingR(t)be the position vector of the point p, from the center of the earth, observe the coordinates of R(t) are constant with respect to i(t),j(t),k(t).
Also, since the earth rotates from West to East and the speed of a point on the surface of the earth relative to an observer ﬁxed in spaceisω|R|sinϕwhereω istheangularspeedoftheearthaboutanaxisthroughthepoles andϕisthepolaranglemeasuredfromthepositivez axisdownasinsphericalcoordinates.
It follows from the geometric deﬁnition of the cross product that R′ =ωk∗×R Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.6.
MATRICES AND CALCULUS 67 Therefore, the vector of Theorem 2.6.4 is Ω= ωk∗ and so z =}|0 { R′′ = Ω′×R+ Ω×R′ =Ω×(Ω×R) since Ω does not depend on t. Formula (2.28) implies a =a−Ω×(Ω×R)−2Ω×v −Ω×(Ω×r ).
(2.29) B B B In this formula, you can totally ignore the term Ω×(Ω×r ) because it is so small when- B ever you are considering motion near some point on the earth’s surface.
To see this, note sezconds}|inad{ay ω (24)(3600) = 2π, and so ω = 7.2722×10−5 in radians per second.
If you are using secondstomeasuretimeandfeettomeasuredistance, thistermistherefore, nolargerthan ( ) 7.2722×10−5 2|r |.
B Clearlythisisnotworthconsideringinthepresenceoftheaccelerationduetogravitywhich is approximately 32 feet per second squared near the surface of the earth.
If the acceleration a is due to gravity, then a =a−Ω×(Ω×R)−2Ω×v = B B z ≡}|g { GM(R+r ) − B −Ω×(Ω×R)−2Ω×v ≡g−2Ω×v .
|R+r |3 B B B Note that Ω×(Ω×R)=(Ω·R)Ω−|Ω|2R and so g, the acceleration relative to the moving coordinate system on the earth is not directed exactly toward the center of the earth except at the poles and at the equator, although the components of acceleration which are in other directions are very small when compared with the acceleration due to the force of gravity and are often neglected.
There- fore, if the only force acting on an object is due to gravity, the following formula describes the acceleration relative to a coordinate system moving with the earth’s surface.
a =g−2(Ω×v ) B B WhilethevectorΩisquitesmall,iftherelativevelocity,v islarge,theCoriolisacceleration B could be signiﬁcant.
This is described in terms of the vectors i(t),j(t),k(t) next.
Letting (ρ,θ,ϕ) be the usual spherical coordinates of the point p(t) on the surface taken with respect to i∗,j∗,k∗ the usual way with ϕ the polar angle, it follows the i∗,j∗,k∗ coordinates of this point are   ρsin(ϕ)cos(θ)   ρsin(ϕ)sin(θ) .
ρcos(ϕ) It follows, i=cos(ϕ)cos(θ)i∗+cos(ϕ)sin(θ)j∗−sin(ϕ)k∗ j=−sin(θ)i∗+cos(θ)j∗+0k∗ and ∗ ∗ ∗ k=sin(ϕ)cos(θ)i +sin(ϕ)sin(θ)j +cos(ϕ)k .
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation68 MATRICES AND LINEAR TRANSFORMATIONS It is necessary to obtain k∗ in terms of the vectors, i,j,k.
Thus the following equation needs to be solved for a,b,c to ﬁnd k∗ =ai+bj+ck z }k|∗ {      0 cos(ϕ)cos(θ) −sin(θ) sin(ϕ)cos(θ) a      0 = cos(ϕ)sin(θ) cos(θ) sin(ϕ)sin(θ) b (2.30) 1 −sin(ϕ) 0 cos(ϕ) c The ﬁrst column is i, the second is j and the third is k in the above matrix.
The solution is a=−sin(ϕ),b=0, and c=cos(ϕ).
Now the Coriolis acceleration on the earth equals   z }k|∗ {   2(Ω×v )=2ω−sin(ϕ)i+0j+cos(ϕ)k×(x′i+y′j+z′k).
B This equals 2ω[(−y′cosϕ)i+(x′cosϕ+z′sinϕ)j−(y′sinϕ)k].
(2.31) Rememberϕisﬁxedandpertainstotheﬁxedpoint,p(t)ontheearth’ssurface.
Therefore, if the acceleration a is due to gravity, a =g−2ω[(−y′cosϕ)i+(x′cosϕ+z′sinϕ)j−(y′sinϕ)k] B where g=−GM(R+rB)−Ω×(Ω×R) as explained above.
The term Ω×(Ω×R) is pretty |R+rB|3 small and so it will be neglected.
However, the Coriolis force will not be neglected.
Example 2.6.5 Suppose a rock is dropped from a tall building.
Where will it strike?
Assume a=−gk and the j component of a is approximately B −2ω(x′cosϕ+z′sinϕ).
The dominant term in this expression is clearly the second one because x′ will be small.
Also, the i and k contributions will be very small.
Therefore, the following equation is descriptive of the situation.
a =−gk−2z′ωsinϕj.
B z′ =−gt approximately.
Therefore, considering the j component, this is 2gtωsinϕ.
( ) Two integrations give ωgt3/3 sinϕ for the j component of the relative displacement at time t. This shows the rock does not fall directly towards the center of the earth as expected but slightly to the east.
Example 2.6.6 In 1851 Foucault set a pendulum vibrating and observed the earth rotate out from under it.
It was a very long pendulum with a heavy weight at the end so that it would vibrate for a long time without stopping2.
This is what allowed him to observe the earth rotate out from under it.
Clearly such a pendulum will take 24 hours for the plane of vibration to appear to make one complete revolution at the north pole.
It is also reasonable to expect that no such observed rotation would take place on the equator.
Is it possible to predict what will take place at various latitudes?
2ThereissuchapendulumintheEyringbuildingatBYUandtokeeppeoplefromtouchingit,thereis alittlesignwhichsaysWarning!
1000ohms.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.6.
MATRICES AND CALCULUS 69 Using (2.31), in (2.29), a =a−Ω×(Ω×R) B −2ω[(−y′cosϕ)i+(x′cosϕ+z′sinϕ)j−(y′sinϕ)k].
Neglecting the small term, Ω×(Ω×R), this becomes =−gk+T/m−2ω[(−y′cosϕ)i+(x′cosϕ+z′sinϕ)j−(y′sinϕ)k] where T, the tension in the string of the pendulum, is directed towards the point at which thependulumissupported, andmisthemassofthependulumbob.
Thependulumcanbe thoughtofasthepositionvectorfrom(0,0,l)tothesurfaceofthespherex2+y2+(z−l)2 = l2.
Therefore, x y l−z T=−T i−T j+T k l l l and consequently, the diﬀerential equations of relative motion are x x′′ =−T +2ωy′cosϕ ml y y′′ =−T −2ω(x′cosϕ+z′sinϕ) ml and l−z z′′ =T −g+2ωy′sinϕ.
ml If the vibrations of the pendulum are small so that for practical purposes, z′′ = z = 0, the last equation may be solved for T to get gm−2ωy′sin(ϕ)m=T.
Therefore, the ﬁrst two equations become x x′′ =−(gm−2ωmy′sinϕ) +2ωy′cosϕ ml and y y′′ =−(gm−2ωmy′sinϕ) −2ω(x′cosϕ+z′sinϕ).
ml All terms of the form xy′ or y′y can be neglected because it is assumed x and y remain small.
Also, the pendulum is assumed to be long with a heavy weight so that x′ and y′ are also small.
With these simplifying assumptions, the equations of motion become x ′′ ′ x +g =2ωy cosϕ l and y y′′+g =−2ωx′cosϕ.
l These equations are of the form x′′+a2x=by′, y′′+a2y =−bx′ (2.32) where a2 = g and b=2ωcosϕ.
Then it is fairly tedious but routine to verify that for each l constant, c, ( ) (√ ) ( ) (√ ) bt b2+4a2 bt b2+4a2 x=csin sin t , y =ccos sin t (2.33) 2 2 2 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation70 MATRICES AND LINEAR TRANSFORMATIONS yields a solution to (2.32) along with the initial conditions, √ c b2+4a2 ′ ′ x(0)=0,y(0)=0,x (0)=0,y (0)= .
(2.34) 2 It is clear from experiments with the pendulum that the earth does indeed rotate out from under it causing the plane of vibration of the pendulum to appear to rotate.
The purpose of this discussion is not to establish these self evident facts but to predict how long it takes for the plane of vibration to make one revolution.
Therefore, there will be some instant in time at which the pendulum will be vibrating in a plane determined by k and j.
(Recall k points away from the center of the earth and j points East. )
At this instant in time, deﬁned as t = 0, the conditions of (2.34) will hold for some value of c and so the solution to (2.32) having these initial conditions will be those of (2.33) by uniqueness of the initial value problem.
Writing these solutions diﬀerently, ( ) ( ( ) ) (√ ) x(t) sin(bt) b2+4a2 =c 2 sin t y(t) cos bt 2 2 ( ( ) ) This is very interesting!
The vector, c sin(b2t) always has magnitude equal to |c| cos bt 2 but its direction changes very slowly because b is very(small.
Th)e plane of vibration is √ determinedbythisvectorandthevectork.Thetermsin b2+4a2t changesrelativelyfast 2 and takes values between −1 and 1.
This is what describes the actual observed vibrations ofthependulum.
Thustheplaneofvibrationwillhavemadeonecompleterevolutionwhen t=T for bT ≡2π.
2 Therefore, the time it takes for the earth to turn out from under the pendulum is 4π 2π T = = secϕ.
2ωcosϕ ω Since ω is the angular speed of the rotating earth, it follows ω = 2π = π in radians per 24 12 hour.
Therefore, the above formula implies T =24secϕ.
Ithinkthisisreallyamazing.
Youcouldactuallydeterminelatitude,notbytakingreadings with instruments using the North Star but by doing an experiment with a big pendulum.
You would set it vibrating, observe T in hours, and then solve the above equation for ϕ.
Also note the pendulum would not appear to change its plane of vibration at the equator because limϕ→π/2secϕ=∞.
The Coriolis acceleration is also responsible for the phenomenon of the next example.
Example 2.6.7 It is known that low pressure areas rotate counterclockwise as seen from above in the Northern hemisphere but clockwise in the Southern hemisphere.
Why?
Neglect accelerations other than the Coriolis acceleration and the following acceleration which comes from an assumption that the point p(t) is the location of the lowest pressure.
a=−a(r )r B B wherer =r willdenotethedistancefromtheﬁxedpointp(t)ontheearth’ssurfacewhich B is also the lowest pressure point.
Of course the situation could be more complicated but Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.7.
EXERCISES 71 this will suﬃce to explain the above question.
Then the acceleration observed by a person on the earth relative to the apparently ﬁxed vectors, i,k,j, is a =−a(r )(xi+yj+zk)−2ω[−y′cos(ϕ)i+(x′cos(ϕ)+z′sin(ϕ))j−(y′sin(ϕ)k)] B B Therefore, one obtains some diﬀerential equations from a =x′′i+y′′j+z′′k by matching B the components.
These are ′′ ′ x +a(r )x = 2ωy cosϕ B y′′+a(r )y = −2ωx′cosϕ−2ωz′sin(ϕ) B ′′ ′ z +a(r )z = 2ωy sinϕ B Nowremember,thevectors,i,j,kareﬁxedrelativetotheearthandsoareconstantvectors.
Therefore, from the properties of the determinant and the above diﬀerential equations, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)′ (cid:12) (cid:12) (cid:12) i j k (cid:12) (cid:12) i j k (cid:12) (r′B ×rB)′ =(cid:12)(cid:12)(cid:12) x′ y′ z′ (cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12) x′′ y′′ z′′ (cid:12)(cid:12)(cid:12) x y z x y z (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i j k (cid:12) =(cid:12)(cid:12) −a(rB)x+2ωy′cosϕ −a(rB)y−2ωx′cosϕ−2ωz′sin(ϕ) −a(rB)z+2ωy′sinϕ (cid:12)(cid:12) (cid:12) (cid:12) x y z Then the kth component of this cross product equals ( ) ωcos(ϕ) y2+x2 ′+2ωxz′sin(ϕ).
The ﬁrst term will be negative because it is assumed p(t) is the location of low pressure causingy2+x2 tobeadecreasingfunction.
Ifitisassumedthereisnotasubstantialmotion in the k direction, so that z is fairly constant and the las(t ter)m can be neglected, th(en th)e kth component of (r′ ×r )′ is negative provided ϕ ∈ 0,π and positive if ϕ ∈ π,π .
B B 2 2 Beginningwithapointatrest,thisimpliesr′ ×r =0initiallyandthentheaboveimplies B B its kth component is negative in the upper hemisphere when ϕ < π/2 and positive in the lower hemisphere when ϕ > π/2.
Using the right hand and the geometric deﬁnition of the cross product, this shows clockwise rotation in the lower hemisphere and counter clockwise rotation in the upper hemisphere.
Note also that as ϕ gets close to π/2 near the equator, the above reasoning tends to break down because cos(ϕ) becomes close to zero.
Therefore, the motion towards the low pressure has to be more pronounced in comparison with the motion in the k direction in order to draw this conclusion.
2.7 Exercises 1.
Show the map T :Rn →Rm deﬁned by T (x)=Ax where A is an m×n matrix and x is an m×1 column vector is a linear transformation.
2.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/3.
3.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/4.
4.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of −π/3.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation72 MATRICES AND LINEAR TRANSFORMATIONS 5.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of 2π/3.
6.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/12.
Hint: Note that π/12=π/3−π/4.
7.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of 2π/3 and then reﬂects across the x axis.
8.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/3 and then reﬂects across the x axis.
9.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/4 and then reﬂects across the x axis.
10.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/6 and then reﬂects across the x axis followed by a reﬂection across the y axis.
11.
Find the matrix for the linear transformation which reﬂects every vector in R2 across the x axis and then rotates every vector through an angle of π/4.
12.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of π/4 and next reﬂects every vector across the x axis.
Compare with the above problem.
13.
Find the matrix for the linear transformation which reﬂects every vector in R2 across the x axis and then rotates every vector through an angle of π/6.
14.
Find the matrix for the linear transformation which reﬂects every vector in R2 across the y axis and then rotates every vector through an angle of π/6.
15.
FindthematrixforthelineartransformationwhichrotateseveryvectorinR2 through an angle of 5π/12.
Hint: Note that 5π/12=2π/3−π/4.
16.
Find the matrix for proj (v) where u=(1,−2,3)T .
u 17.
Find the matrix for proj (v) where u=(1,5,3)T .
u 18.
Find the matrix for proj (v) where u=(1,0,3)T .
u 19.
Give an example of a 2×2 matrix A which has all its entries nonzero and satisﬁes A2 =A.
A matrix which satisﬁes A2 =A is called idempotent.
20.
Let A be an m×n matrix and let B be an n×m matrix where n < m. Show that AB cannot have an inverse.
21.
Find ker(A) for   1 2 3 2 1    0 2 1 1 2  A= .
1 4 4 3 3 0 2 1 1 2 Recall ker(A) is just the set of solutions to Ax=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.7.
EXERCISES 73 22.
If A is a linear transformation, and Ax =b, show that the general solution to the p equationAx=bisoftheformx +y wherey∈ker(A).
BythisImeantoshowthat p whenever Az=b there exists y ∈ker(A) such that x +y=z.
For the deﬁnition of p ker(A) see Problem 21.
23.
Using Problem 21, ﬁnd the general solution to the following linear system.
      x 1 2 3 2 1  1  11  0 2 1 1 2  x2   7   1 4 4 3 3  x3 = 18  x 0 2 1 1 2 4 7 x 5 24.
Using Problem 21, ﬁnd the general solution to the following linear system.
      x 1 2 3 2 1  1  6  0 2 1 1 2  x2   7   1 4 4 3 3  x3 = 13  x 0 2 1 1 2 4 7 x 5 25.
Show that the function T deﬁned by T (v)≡v−proj (v) is also a linear transfor- u u u mation.
26.
If u=(1,2,3)T, as in Example 9.3.22 and T is given in the above problem, ﬁnd the u matrix A which satisﬁes A x=T (x).
u u u 27.
Suppose V is a subspace of Fn and T : V → Fp is a nonzero linear transformation.
Show that there exists a basis for Im(T)≡T(V) {Tv ,··· ,Tv } 1 m and that in this situation, {v ,··· ,v } 1 m is linearly independent.
28.
↑In the situation of Problem 27 where V is a subspace of Fn, show that there exists {z ,··· ,z } a basis for ker(T).
(Recall Theorem 2.4.12.
Since ker(T) is a subspace, 1 r it has a basis.)
Now for an arbitrary Tv∈T (V), explain why Tv=a Tv +···+a Tv 1 1 m m and why this implies v−(a v +···+a v )∈ker(T).
1 1 m m Then explain why V =span(v ,··· ,v ,z ,··· ,z ).
1 m 1 r 29.
↑In the situation of the above problem, show {v ,··· ,v ,z ,··· ,z } is a basis for V 1 m 1 r and therefore, dim(V)=dim(ker(T))+dim(T (V)).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation74 MATRICES AND LINEAR TRANSFORMATIONS 30.
↑Let A be a linear transformation from V to W and let B be a linear transformation from W to U where V,W,U are all subspaces of some Fp.
Explain why A(ker(BA))⊆ker(B), ker(A)⊆ker(BA).
ker(BA) ker(B) ker(A) A - A(ker(BA)) 31.
↑Let{x ,··· ,x }beabasisofker(A)andlet{Ay ,··· ,Ay }beabasisofA(ker(BA)).
1 n 1 m Let z∈ker(BA).
Explain why Az ∈span{Ay ,··· ,Ay } 1 m and why there exist scalars a such that i A(z−(a y +···+a y ))=0 1 1 m m and why it follows z−(a y +···+a y )∈span{x ,··· ,x }.
Now explain why 1 1 m m 1 n ker(BA)⊆span{x ,··· ,x ,y ,··· ,y } 1 n 1 m and so dim(ker(BA))≤dim(ker(B))+dim(ker(A)).
This important inequality is due to Sylvester.
Show that equality holds if and only if A(kerBA)=ker(B).
32.
Generalizetheresultofthepreviousproblemtoanyﬁniteproductoflinearmappings.
33.
If W ⊆V for W,V two subspaces of Fn and if dim(W)=dim(V), show W =V.
34.
LetV beasubspaceofFnandletV ,··· ,V besubspaces,eachcontainedinV.
Then 1 m V =V ⊕···⊕V (2.35) 1 m if every v ∈V can be written in a unique way in the form v =v +···+v 1 m where each v ∈V .
This is called a direct sum.
If this uniqueness condition does not i i hold, then one writes V =V +···+V 1 m and this symbol means all vectors of the form v +···+v , v ∈V for each j.
1 m j j Show (2.35) is equivalent to saying that if 0=v +···+v , v ∈V for each j, 1 m j j { } then each v = 0.
Next show that in the situation of (2.35), if β = ui,··· ,ui is a basis for Vj , then {β ,··· ,β } is a basis for V. i 1 mi i 1 m Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation2.7.
EXERCISES 75 35.
↑Suppose you have ﬁnitely many linear mappings L ,L ,··· ,L which map V to V 1 2 m where V is a subspace of Fn and suppose they commute.
That is, L L =L L for all i j j i i,j.
Also suppose L is one to one on ker(L ) whenever j ̸=k.
Letting P denote the k j product of these linear transformations, P =L L ···L , ﬁrst show 1 2 m ker(L )+···+ker(L )⊆ker(P) 1 m Next show L :ker(L )→ker(L ).
Then show j i i ker(L )+···+ker(L )=ker(L )⊕···⊕ker(L ).
1 m 1 m Using Sylvester’s theorem, and the result of Problem 33, show ker(P)=ker(L )⊕···⊕ker(L ) 1 m Hint: By Sylvester’s theorem and the above problem, ∑ dim(ker(P)) ≤ dim(ker(L )) i i = dim(ker(L )⊕···⊕ker(L ))≤dim(ker(P)) 1 m Now consider Problem 33.
36.
LetM(Fn,Fn)denotethesetofalln×nmatriceshavingentriesinF.
Withtheusual operations of matrix addition and scalar multiplications, explain why M(Fn,Fn) can be considered as Fn2.
Give a basis for M(Fn,Fn).
If A ∈ M(Fn,Fn), explain why there exists a monic (leading coeﬃcient equals 1) polynomial of the form λk+ak−1λk−1+···+a1λ+a0 such that Ak+ak−1Ak−1+···+a1A+a0I =0 The minimal polynomial of A is the polynomial like the above, for which p(A) = 0 whichhassmallestdegree.
Iwilldiscusstheuniquenessofthispolynomiallater.
Hint: ConsiderthematricesI,A,A2,··· ,An2.Therearen2+1ofthesematrices.
Canthey belinearlyindependent?
Nowconsiderallpolynomialsandpickoneofsmallestdegree and then divide by the leading coeﬃcient.
37.
↑Suppose the ﬁeld of scalars is C and A is an n×n matrix.
From the preceding problem, and the fundamental theorem of algebra, this minimal polynomial factors (λ−λ )r1(λ−λ )r2···(λ−λ )rk 1 2 k where r is the algebraic multiplicity of λ , and the λ are distinct.
Thus j j j (A−λ I)r1(A−λ I)r2···(A−λ I)rk =0 1 2 k and so, letting P = (A−λ I)r1(A−λ I)r2···(A−λ I)rk and L = (A−λ I)rj 1 2 k j j apply the result of Problem 35 to verify that Cn =ker(L )⊕···⊕ker(L ) 1 k and that A : ker(L ) → ker(L ).
In this context, ker(L ) is called the generalized j j j eigenspaceforλ .
Youneedtoverifytheconditionsoftheresultofthisproblemhold.
j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation76 MATRICES AND LINEAR TRANSFORMATIONS 38.
In the context of Problem 37, show there exists a nonzero vector x such that (A−λ I)x=0.
j Thisiscalledaneigenvectorandtheλ iscalledaneigenvalue.
Hint:Theremustexist j a vector y such that (A−λ I)r1(A−λ I)r2···(A−λ I)rj−1···(A−λ I)rky=z̸=0 1 2 j k Why?
Now what happens if you do (A−λ I) to z?
j 39.
Suppose Q(t) is an orthogonal matrix.
This means Q(t) is a real n×n matrix which satisﬁes Q(t)Q(t)T =I ( ) Suppose also the entries of Q(t) are diﬀerentiable.
Show QT ′ =−QTQ′QT.
40.
Remember the Coriolis force was 2Ω×v where Ω was a particular vector which B came from the matrix Q(t) as described above.
Show that   i(t)·i(t ) j(t)·i(t ) k(t)·i(t ) 0 0 0 Q(t)= i(t)·j(t ) j(t)·j(t ) k(t)·j(t ) .
0 0 0 i(t)·k(t ) j(t)·k(t ) k(t)·k(t ) 0 0 0 There will be no Coriolis force exactly when Ω=0 which corresponds to Q′(t) = 0.
When will Q′(t)=0?
41.
An illustration used in many beginning physics books is that of ﬁring a riﬂe hori- zontally and dropping an identical bullet from the same height above the perfectly ﬂat ground followed by an assertion that the two bullets will hit the ground at ex- actly the same time.
Is this true on the rotating earth assuming the experiment takes place over a large perfectly ﬂat ﬁeld so the curvature of the earth is not an issue?
Explain.
What other irregularities will occur?
Recall the Coriolis acceleration is 2ω[(−y′cosϕ)i+(x′cosϕ+z′sinϕ)j−(y′sinϕ)k] where k points away from the center of the earth, j points East, and i points South.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationDeterminants 3.1 Basic Techniques And Properties Let A be an n×n matrix.
The determinant of A, denoted as det(A) is a number.
If the matrix is a 2×2 matrix, this number is very easy to ﬁnd.
( ) a b Deﬁnition 3.1.1 Let A= .
Then c d det(A)≡ad−cb.
The determinant is also often denoted by enclosing the matrix with two vertical lines.
Thus ( ) (cid:12) (cid:12) (cid:12) (cid:12) a b (cid:12) a b (cid:12) det =(cid:12) (cid:12).
c d c d ( ) 2 4 Example 3.1.2 Find det .
−1 6 From the deﬁnition this is just (2)(6)−(−1)(4)=16.
Assuming the determinant has been deﬁned for k×k matrices for k ≤ n−1, it is now time to deﬁne it for n×n matrices.
Deﬁnition 3.1.3 Let A=(a ) be an n×n matrix.
Then a new matrix called the cofactor ij matrix, cof(A) is deﬁned by cof(A) = (c ) where to obtain c delete the ith row and the ij ij jth column of A, take the determinant of the (n−1)×(n−1) matrix which results, (This is called the ijth minor of A. )
and then multiply this number by (−1)i+j.
To make the formulas easier to remember, cof(A) will denote the ijth entry of the cofactor matrix.
ij Now here is the deﬁnition of the determinant given recursively.
Theorem 3.1.4 Let A be an n×n matrix where n≥2.
Then ∑n ∑n det(A)= a cof(A) = a cof(A) .
(3.1) ij ij ij ij j=1 i=1 The ﬁrst formula consists of expanding the determinant along the ith row and the second expands the determinant along the jth column.
77 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation78 DETERMINANTS Note that for a n×n matrix, you will need n!
terms to evaluate the determinant in this way.
If n=10, this is 10!=3,628,800 terms.
This is a lot of terms.
In addition to the diﬃculties just discussed, why is the determinant well deﬁned?
Why should you get the same thing when you expand along any row or column?
I think you shouldregardthisclaimthatyoualwaysgetthesameanswerbypickinganyroworcolumn with considerable skepticism.
It is incredible and not at all obvious.
However, it requires a little eﬀort to establish it.
This is done in the section on the theory of the determinant which follows.
Notwithstanding the diﬃculties involved in using the method of Laplace expansion, certain types of matrices are very easy to deal with.
Deﬁnition 3.1.5 A matrix M, is upper triangular if M = 0 whenever i > j.
Thus such ij a matrix equals zero below the main diagonal, the entries of the form M , as shown.
ii   ∗ ∗ ··· ∗  0 ∗ ... ...   ... ... ... ∗  0 ··· 0 ∗ A lower triangular matrix is deﬁned similarly as a matrix for which all entries above the main diagonal are equal to zero.
You should verify the following using the above theorem on Laplace expansion.
Corollary 3.1.6 Let M be an upper (lower) triangular matrix.
Then det(M) is obtained by taking the product of the entries on the main diagonal.
Proof: The corollary is true if the matrix is one to one.
Suppose it is n×n.
Then the matrix is of the form ( ) m a 11 0 M 1 whereM1 is(n−1)×(n−1).Thenexpandingalongtheﬁrstr∏ow,yougetm11det(M1)+0.
Then use the induction hypothesis to obtain that det(M )= n m .
(cid:4) 1 i=2 ii Example 3.1.7 Let   1 2 3 77    0 2 6 7  A=  0 0 3 33.7 0 0 0 −1 Find det(A).
From the above corollary, this is −6.
There are many properties satisﬁed by determinants.
Some of the most important are listed in the following theorem.
Theorem 3.1.8 If two rows or two columns in an n×n matrix A are switched, the deter- minant of the resulting matrix equals (−1) times the determinant of the original matrix.
If A is an n×n matrix in which two rows are equal or two columns are equal then det(A)=0.
Suppose the ith row of A equals (xa +yb ,··· ,xa +yb ).
Then 1 1 n n det(A)=xdet(A )+ydet(A ) 1 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.1.
BASIC TECHNIQUES AND PROPERTIES 79 where the ith row of A is (a ,··· ,a ) and the ith row of A is (b ,··· ,b ), all other rows 1 1 n 2 1 n of A and A coinciding with those of A.
In other words, det is a linear function of each 1 2 row A.
The same is true with the word “row” replaced with the word “column”.
In addition to this, if A and B are n×n matrices, then det(AB)=det(A)det(B), and if A is an n×n matrix, then ( ) det(A)=det AT .
This theorem implies the following corollary which gives a way to ﬁnd determinants.
As I pointed out above, the method of Laplace expansion will not be practical for any matrix of large size.
Corollary 3.1.9 LetAbeann×nmatrixandletB bethematrixobtainedbyreplacingthe ith row (column) of A with the sum of the ith row (column) added to a multiple of another row (column).
Then det(A)=det(B).
If B is the matrix obtained from A be replacing the ith row (column) of A by a times the ith row (column) then adet(A)=det(B).
Here is an example which shows how to use this corollary to ﬁnd a determinant.
Example 3.1.10 Find the determinant of the matrix   1 2 3 4    5 1 2 3  A=  4 5 4 3 2 2 −4 5 Replace the second row by (−5) times the ﬁrst row added to it.
Then replace the third row by (−4) times the ﬁrst row added to it.
Finally, replace the fourth row by (−2) times the ﬁrst row added to it.
This yields the matrix   1 2 3 4  0 −9 −13 −17  B = 0 −3 −8 −13  0 −2 −10 −3 and from the above co(roll)ary, it has the same determinant as A.
Now using the corollary some more, det(B)= −1 det(C) where 3   1 2 3 4    0 0 11 22  C = 0 −3 −8 −13 .
0 6 30 9 Thesecondrowwasreplacedby(−3)timesthethirdrowaddedtothesecondrowandthen the last row was multiplied by (−3).
Now replace the last row with 2 times the third added to it and then switch the third and second rows.
Then det(C)=−det(D) where   1 2 3 4  0 −3 −8 −13  D =  0 0 11 22 0 0 14 −17 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation80 DETERMINANTS Youcoulddomorerowoperationsoryoucouldnotethatthiscanbeeasilyexpandedalong theﬁrstcolumnfollowedbyexpandingthe3×3matrixwhichresultsalongitsﬁrstcolumn.
Thus (cid:12) (cid:12) (cid:12) (cid:12) det(D)=1(−3)(cid:12)(cid:12) 1114 −2127 (cid:12)(cid:12)=1485 ( ) and so det(C)=−1485 and det(A)=det(B)= −1 (−1485)=495.
3 The theorem about expanding a matrix along any row or column also provides a way to give a formula for the inverse of a matrix.
Recall the deﬁnition of the inverse of a matrix in Deﬁnition 2.1.22 on Page 47.
The following theorem gives a formula for the inverse of a matrix.
It is proved in the next section.
( ) Theorem 3.1.11 A−1 exists if and only if det(A) ̸= 0.
If det(A) ̸= 0, then A−1 = a−1 ij where a−1 =det(A)−1cof(A) ij ji for cof(A) the ijth cofactor of A. ij Theorem 3.1.11 says that to ﬁnd the inverse, take the transpose of the cofactor matrix and divide by the determinant.
The transpose of the cofactor matrix is called the adjugate orsometimestheclassicaladjointofthematrixA.
Itisanabominationtocallittheadjoint althoughyoudosometimesseeitreferredtointhisway.
Inwords, A−1 isequaltooneover the determinant of A times the adjugate matrix of A.
Example 3.1.12 Find the inverse of the matrix   1 2 3   A= 3 0 1 1 2 1 First ﬁnd the determinant of this matrix.
This is seen to be 12.
The cofactor matrix of A is   −2 −2 6  4 −2 0 .
2 8 −6 EachentryofAwasreplacedbyitscofactor.
Therefore,fromtheabovetheorem,theinverse of A should equal     −2 −2 6 T −1 1 1 1  4 −2 0  = −61 −31 62 .
12 6 6 3 2 8 −6 1 0 −1 2 2 This wayof ﬁnding inversesis especially useful in the case where it is desired to ﬁnd the inverse of a matrix whose entries are functions.
Example 3.1.13 Suppose   et 0 0   A(t)= 0 cost sint 0 −sint cost Find A(t)−1.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.2.
EXERCISES 81 First note det(A(t)) = et.
A routine computation using the above theorem shows that this inverse is     1 0 0 T e−t 0 0 1  0 etcost etsint  = 0 cost −sint .
et 0 −etsint etcost 0 sint cost This formula for the inverse also implies a famous procedure known as Cramer’s rule.
Cramer’s rule gives a formula for the solutions, x, to a system of equations, Ax=y.
Incaseyouaresolvingasystemofequations,Ax=y forx,itfollowsthatifA−1 exists, ( ) x= A−1A x=A−1(Ax)=A−1y thus solving the system.
Now in the case that A−1 exists, there is a formula for A−1 given above.
Using this formula, ∑n ∑n 1 x = a−1y = cof(A) y .
i ij j det(A) ji j j=1 j=1 By the formula for the expansion of a determinant along a column,   ∗ ··· y ··· ∗ 1 1  .
.
.
 x = det .
.
.
, i det(A) .
.
.
∗ ··· y ··· ∗ n where here the ith column of A is replaced with the column vector, (y ····,y )T, and the 1 n determinantofthismodiﬁedmatrixistakenanddividedbydet(A).
Thisformulaisknown as Cramer’s rule.
Procedure 3.1.14 Suppose A is an n × n matrix and it is desired to solve the system Ax=y,y=(y ,··· ,y )T for x=(x ,··· ,x )T .
Then Cramer’s rule says 1 n 1 n detA x = i i detA whereA isobtainedfromAbyreplacingtheith columnofAwiththecolumn(y ,··· ,y )T .
i 1 n Thefollowingtheoremisoffundamentalimportanceandtiestogethermanyoftheideas presented above.
It is proved in the next section.
Theorem 3.1.15 Let A be an n×n matrix.
Then the following are equivalent.
1.
A is one to one.
2.
A is onto.
3. det(A)̸=0.
3.2 Exercises 1.
Find the determinants of the following matrices.
  1 2 3   (a) 3 2 2 (The answer is 31.)
0 9 8 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation82 DETERMINANTS   4 3 2   (b) 1 7 8 (The answer is 375.)
3 −9 3   1 2 3 2   (c)  1 3 2 3 , (The answer is −2.)
4 1 5 0 1 2 1 2 ( ) 2.
If A−1 exist, what is the relationship between det(A) and det A−1 .
Explain your answer.
3.
Let A be an n×n matrix where n is odd.
Suppose also that A is skew symmetric.
This means AT =−A.
Show that det(A)=0.
4.
Is it true that det(A+B) = det(A)+det(B)?
If this is so, explain why it is so and if it is not so, give a counter example.
5.
LetAbeanr×r matrixandsupposetherearer−1rows(columns)suchthatallrows (columns) are linear combinations of these r−1 rows (columns).
Show det(A)=0.
6.
Show det(aA)=andet(A) where here A is an n×n matrix and a is a scalar.
7.
Suppose A is an upper triangular matrix.
Show that A−1 exists if and only if all elements of the main diagonal are non zero.
Is it true that A−1 will also be upper triangular?
Explain.
Is everything the same for lower triangular matrices?
8.
Let A and B be two n×n matrices.
A∼B (A is similar to B) means there exists an invertible matrix S such that A = S−1BS.
Show that if A ∼ B, then B ∼ A.
Show also that A∼A and that if A∼B and B ∼C, then A∼C.
9.
In the context of Problem 8 show that if A∼B, then det(A)=det(B).
10.
Let A be an n×n matrix and let x be a nonzero vector such that Ax=λx for some scalar, λ.
When this occurs, the vector, x is called an eigenvector and the scalar, λ is called an eigenvalue.
It turns out that not every number is an eigenvalue.
Only certain ones are.
Why?
Hint: Show that if Ax = λx, then (λI −A)x=0.
Explain whythisshowsthat(λI−A)isnotonetooneandnotonto.
NowuseTheorem3.1.15 to argue det(λI −A)=0.
What sort of equation is this?
How many solutions does it have?
11.
Suppose det(λI−A) = 0.
Show using Theorem 3.1.15 there exists x̸=0 such that (λI−A)x=0.
( ) a(t) b(t) 12.
Let F (t)=det .
Verify c(t) d(t) ( ) ( ) a′(t) b′(t) a(t) b(t) ′ F (t)=det +det .
c(t) d(t) c′(t) d′(t) Now suppose   a(t) b(t) c(t)   F (t)=det d(t) e(t) f(t) .
g(t) h(t) i(t) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 83 Use Laplace expansion and the ﬁrst part to verify F′(t)=     a′(t) b′(t) c′(t) a(t) b(t) c(t) det d(t) e(t) f(t) +det d′(t) e′(t) f′(t)  g(t) h(t) i(t) g(t) h(t) i(t)   a(t) b(t) c(t)   +det d(t) e(t) f(t) .
g′(t) h′(t) i′(t) Conjecture a general result valid for n×n matrices and explain why it will be true.
Can a similar thing be done with the columns?
13.
Use the formula for the inverse in terms of the cofactor matrix to ﬁnd the inverse of the matrix   et 0 0 A= 0 etcost etsint .
0 etcost−etsint etcost+etsint 14.
LetAbeanr×r matrixandletB beanm×mmatrixsuchthatr+m=n.Consider the following n×n block matrix ( ) A 0 C = .
D B where the D is an m×r matrix, and the 0 is a r×m matrix.
Letting I denote the k k×k identity matrix, tell why ( )( ) A 0 I 0 C = r .
D I 0 B m Now explain why det(C)=det(A)det(B).
Hint: Part of this will require an expla- nation of why ( ) A 0 det =det(A).
D I m See Corollary 3.1.9.
15.
SupposeQisanorthogonalmatrix.
ThismeansQisarealn×nmatrixwhichsatisﬁes QQT =I Find the possible values for det(Q).
16.
Suppose Q(t) is an orthogonal matrix.
This means Q(t) is a real n×n matrix which satisﬁes Q(t)Q(t)T =I Suppose Q(t) is continuous for t∈[a,b], some interval.
Also suppose det(Q(t))=1.
Show that it follows det(Q(t))=1 for all t∈[a,b].
3.3 The Mathematical Theory Of Determinants It is easiest to give a diﬀerent deﬁnition of the determinant which is clearly well deﬁned andthenprovetheearlieroneintermsofLaplaceexpansion.
Let(i ,··· ,i )beanordered 1 n list of numbers from {1,··· ,n}.
This means the order is important so (1,2,3) and (2,1,3) are diﬀerent.
There will be some repetition between this section and the earlier section on determinants.
The main purpose is to give all the missing proofs.
Two books which give a good introduction to determinants are Apostol [1] and Rudin [22].
A recent book which also has a good introduction is Baker [3] Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation84 DETERMINANTS 3.3.1 The Function sgn The following Lemma will be essential in the deﬁnition of the determinant.
Lemma 3.3.1 There exists a unique function, sgn which maps each ordered list of num- n bers from {1,··· ,n} to one of the three numbers, 0,1, or −1 which also has the following properties.
sgn (1,··· ,n)=1 (3.2) n sgn (i ,··· ,p,··· ,q,··· ,i )=−sgn (i ,··· ,q,··· ,p,··· ,i ) (3.3) n 1 n n 1 n In words, the second property states that if two of the numbers are switched, the value of the function is multiplied by −1.
Also, in the case where n>1 and {i ,··· ,i }={1,··· ,n} so 1 n that every number from {1,··· ,n} appears in the ordered list, (i ,··· ,i ), 1 n sgnn(i1,··· ,iθ−1,n,iθ+1,··· ,in)≡ (−1)n−θsgnn−1(i1,··· ,iθ−1,iθ+1,··· ,in) (3.4) where n=i in the ordered list, (i ,··· ,i ).
θ 1 n Proof: To begin with, it is necessary to show the existence of such a function.
This is clearlytrueifn=1.Deﬁnesgn (1)≡1andobservethatitworks.
Noswitchingispossible.
1 In the case where n = 2, it is also clearly true.
Let sgn (1,2) = 1 and sgn (2,1) = −1 2 2 whilesgn (2,2)=sgn (1,1)=0andverifyitworks.
Assumingsuchafunctionexistsforn, 2 2 sgn willbedeﬁnedintermsofsgn .Ifthereareanyrepeatednumbersin(i ,··· ,i ), n+1 n 1 n+1 sgn (i ,··· ,i ) ≡ 0.
If there are no repeats, then n+1 appears somewhere in the n+1 1 n+1 ordered list.
Let θ be the position of the number n+1 in the list.
Thus, the list is of the form (i1,··· ,iθ−1,n+1,iθ+1,··· ,in+1).
From (3.4) it must be that sgnn+1(i1,··· ,iθ−1,n+1,iθ+1,··· ,in+1)≡ (−1)n+1−θsgnn(i1,··· ,iθ−1,iθ+1,··· ,in+1).
It is necessary to verify this satisﬁes (3.2) and (3.3) with n replaced with n+1.
The ﬁrst of these is obviously true because sgn (1,··· ,n,n+1)≡(−1)n+1−(n+1)sgn (1,··· ,n)=1.
n+1 n If there are repeated numbers in (i ,··· ,i ), then it is obvious (3.3) holds because both 1 n+1 sideswouldequalzerofromtheabovedeﬁnition.
Itremainstoverify(3.3)inthecasewhere there are no numbers repeated in (i ,··· ,i ).
Consider 1 n+1 ( ) sgn i ,··· ,pr,··· ,qs,··· ,i , n+1 1 n+1 where the r above the p indicates the number p is in the rth position and the s above the q indicates that the number, q is in the sth position.
Suppose ﬁrst that r <θ <s.
Then ( ) sgn i ,··· ,pr,··· ,n+θ 1,··· ,qs,··· ,i ≡ n+1 1 n+1 ( ) (−1)n+1−θsgn i ,··· ,pr,··· ,s−q1,··· ,i n 1 n+1 while ( ) sgn i ,··· ,qr,··· ,n+θ 1,··· ,ps,··· ,i ≡ n+1 1 n+1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 85 ( ) (−1)n+1−θsgn i ,··· ,qr,··· ,s−p1,··· ,i n 1 n+1 and so, by induction, a switch of p and q introduces a minus sign in the result.
Similarly, if θ > s or if θ < r it also follows that (3.3) holds.
The interesting case is when θ = r or θ =s.
Consider the case where θ =r and note the other case is entirely similar.
( ) sgn i ,··· ,n+r 1,··· ,qs,··· ,i ≡ n+1 1 n+1 ( ) (−1)n+1−rsgn i ,··· ,s−q1,··· ,i (3.5) n 1 n+1 while ( ) sgn i ,··· ,qr,··· ,n+s 1,··· ,i = n+1 1 n+1 ( ) (−1)n+1−ssgn i ,··· ,qr,··· ,i .
(3.6) n 1 n+1 By making s−1−r switches, move the q which is in the s−1th position in (3.5) to the rth position in (3.6).
By induction, each of these switches introduces a factor of −1 and so ( ) ( ) sgn i ,··· ,s−q1,··· ,i =(−1)s−1−rsgn i ,··· ,qr,··· ,i .
n 1 n+1 n 1 n+1 Therefore, ( ) ( ) sgn i ,··· ,n+r 1,··· ,qs,··· ,i =(−1)n+1−rsgn i ,··· ,s−q1,··· ,i n+1 1 n+1 n 1 n+1 ( ) =(−1)n+1−r(−1)s−1−rsgn i ,··· ,qr,··· ,i n 1 n+1 ( ) ( ) =(−1)n+ssgn i ,··· ,qr,··· ,i =(−1)2s−1(−1)n+1−ssgn i ,··· ,qr,··· ,i n 1 n+1 n 1 n+1 ( ) =−sgn i ,··· ,qr,··· ,n+s 1,··· ,i .
n+1 1 n+1 Thisprovestheexistenceofthedesiredfunction.
Uniquenessfollowseasilyfromthefollow- ing lemma.
Lemma 3.3.2 Every ordered list of {1,2,··· ,n} can be obtained from every other ordered list by a ﬁnite number of switches.
Also, sgn is unique.
Proof: This is obvious if n = 1 or 2.
Suppose then that it is true for sets of n−1 elements.
Take two ordered lists of numbers, P ,P .
To get from P to P using switches, 1 2 1 2 ﬁrst make a switch to obtain the last element in the list coinciding with the last element of P .
By induction, there are switches which will arrange the ﬁrst n−1 to the right order.
2 To see sgn is unique, if there exist two functions, f and g both satisfying (3.2) and n (3.3), you could start with f(1,··· ,n) = g(1,··· ,n) and applying the same sequence of switches, eventually arrive at f(i ,··· ,i ) = g(i ,··· ,i ).
If any numbers are repeated, 1 n 1 n then (3.3) gives both functions are equal to zero for that ordered list.
(cid:4) Deﬁnition 3.3.3 Whenyouhaveanorderedlistofdistinctnumbersfrom{1,2,··· ,n},say (i ,··· ,i ), this ordered list is called a permutation.
The symbol for all such permutations 1 n is S .
The number sgn (i ,··· ,i ) is called the sign of the permutation.
n n 1 n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation86 DETERMINANTS A permutation can also be considered as a function from the set {1,2,··· ,n} to {1,2,··· ,n} as follows.
Let f(k) = i .
Permutations are of fundamental importance in certain areas k of math.
For example, it was by considering permutations that Galois was able to give a criterion for solution of polynomial equations by radicals, but this is a diﬀerent direction than what is being attempted here.
In what follows sgn will often be used rather than sgn because the context supplies the n appropriate n. 3.3.2 The Deﬁnition Of The Determinant Deﬁnition 3.3.4 Letf bearealvaluedfunctionwhichhasthesetoforderedlistsofnumbers from {1,··· ,n} as its domain.
Deﬁne ∑ f(k ···k ) 1 n (k1,···,kn) to be the sum of all the f(k ···k ) for all possible choices of ordered lists (k ,··· ,k ) of 1 n 1 n numbers of {1,··· ,n}.
For example, ∑ f(k ,k )=f(1,2)+f(2,1)+f(1,1)+f(2,2).
1 2 (k1,k2) Deﬁnition 3.3.5 Let (a ) = A denote an n×n matrix.
The determinant of A, denoted ij by det(A) is deﬁned by ∑ det(A)≡ sgn(k ,··· ,k )a ···a 1 n 1k1 nkn (k1,···,kn) where the sum is taken over all ordered lists of numbers from {1,··· ,n}.
Note it suﬃces to take the sum over only those ordered lists in which there are no repeats because if there are, sgn(k ,··· ,k )=0 and so that term contributes 0 to the sum.
1 n Let A be an n×n matrix A = (a ) and let (r ,··· ,r ) denote an ordered list of n ij 1 n numbers from {1,··· ,n}.
Let A(r ,··· ,r ) denote the matrix whose kth row is the r row 1 n k of the matrix A.
Thus ∑ det(A(r ,··· ,r ))= sgn(k ,··· ,k )a ···a (3.7) 1 n 1 n r1k1 rnkn (k1,···,kn) and A(1,··· ,n)=A.
Proposition 3.3.6 Let (r ,··· ,r ) be an ordered list of numbers from {1,··· ,n}.
Then 1 n ∑ sgn(r ,··· ,r )det(A) = sgn(k ,··· ,k )a ···a (3.8) 1 n 1 n r1k1 rnkn (k1,···,kn) = det(A(r ,··· ,r )).
(3.9) 1 n Proof: Let (1,··· ,n)=(1,··· ,r,···s,··· ,n) so r <s.
det(A(1,··· ,r,··· ,s,··· ,n))= (3.10) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 87 ∑ sgn(k ,··· ,k ,··· ,k ,··· ,k )a ···a ···a ···a , 1 r s n 1k1 rkr sks nkn (k1,···,kn) and renaming the variables, calling k ,k and k , k , this equals s r r s ∑ = sgn(k ,··· ,k ,··· ,k ,··· ,k )a ···a ···a ···a 1 s r n 1k1 rks skr nkn (k1,···,kn)   ∑ Thezsego}t|swit{ched = −sgnk ,··· , k ,··· ,k ,··· ,k a ···a ···a ···a 1 r s n 1k1 skr rks nkn (k1,···,kn) =−det(A(1,··· ,s,··· ,r,··· ,n)).
(3.11) Consequently, det(A(1,··· ,s,··· ,r,··· ,n))=−det(A(1,··· ,r,··· ,s,··· ,n))=−det(A) NowlettingA(1,··· ,s,··· ,r,··· ,n)playtheroleofA, andcontinuinginthisway, switch- ing pairs of numbers, det(A(r ,··· ,r ))=(−1)pdet(A) 1 n whereittookpswitchestoobtain(r ,··· ,r )from(1,··· ,n).
ByLemma3.3.1,thisimplies 1 n det(A(r ,··· ,r ))=(−1)pdet(A)=sgn(r ,··· ,r )det(A) 1 n 1 n and proves the proposition in the case when there are no repeated numbers in the ordered list, (r ,··· ,r ).
However, if there is a repeat, say the rth row equals the sth row, then the 1 n reasoning of (3.10) -(3.11) shows that det(A(r ,··· ,r )) = 0 and also sgn(r ,··· ,r ) = 0 1 n 1 n so the formula holds in this case also.
(cid:4) Observation 3.3.7 There are n!
ordered lists of distinct numbers from {1,··· ,n}.
To see this, consider n slots placed in order.
There are n choices for the ﬁrst slot.
For each of these choices, there are n−1 choices for the second.
Thus there are n(n−1) ways toﬁlltheﬁrsttwoslots.
Thenforeachofthesewaystherearen−2choicesleftforthethird slot.
Continuing this way, there are n!
ordered lists of distinct numbers from {1,··· ,n} as stated in the observation.
3.3.3 A Symmetric Deﬁnition Withtheabove, itispossibletogivea(mor)esymmetricdescriptionofthedeterminantfrom which it will follow that det(A)=det AT .
Corollary 3.3.8 The following formula for det(A) is valid.
∑ ∑ 1 det(A)= · sgn(r ,··· ,r )sgn(k ,··· ,k )a ···a .
(3.12) n!
1 n 1 n r1k1 rnkn (r1,···,rn)(k1,···,kn) ( ) ( ) And also det AT = det(A) where AT is the transpose of A.
(Recall that for AT = aT , ij aT =a .)
ij ji Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation88 DETERMINANTS Proof: From Proposition 3.3.6, if the r are distinct, i ∑ det(A)= sgn(r ,··· ,r )sgn(k ,··· ,k )a ···a .
1 n 1 n r1k1 rnkn (k1,···,kn) Summing over all ordered lists, (r ,··· ,r ) where the r are distinct, (If the r are not 1 n i i distinct, sgn(r ,··· ,r )=0 and so there is no contribution to the sum.)
1 n ∑ ∑ n!det(A)= sgn(r ,··· ,r )sgn(k ,··· ,k )a ···a .
1 n 1 n r1k1 rnkn (r1,···,rn)(k1,···,kn) This proves the corollary since the formula gives the same number for A as it does for AT.
(cid:4) Corollary 3.3.9 If two rows or two columns in an n × n matrix A, are switched, the determinantoftheresultingmatrixequals(−1)timesthedeterminantoftheoriginalmatrix.
IfAisann×nmatrixinwhichtworowsareequalortwocolumnsareequalthendet(A)=0.
Suppose the ith row of A equals (xa +yb ,··· ,xa +yb ).
Then 1 1 n n det(A)=xdet(A )+ydet(A ) 1 2 where the ith row of A is (a ,··· ,a ) and the ith row of A is (b ,··· ,b ), all other rows 1 1 n 2 1 n of A and A coinciding with those of A.
In other words, det is a linear function of each 1 2 row A.
The same is true with the word “row” replaced with the word “column”.
Proof: By Proposition 3.3.6 when two rows are switched, the determinant of the re- sulting matrix is (−1) times the determinant of the original matrix.
By Corollary 3.3.8 the sameholds for columnsbecause the columns ofthe matrix equalthe rowsof the transposed matrix.
Thus if A is the matrix obtained from A by switching two columns, 1 ( ) ( ) det(A)=det AT =−det AT =−det(A ).
1 1 If A has two equal columns or two equal rows, then switching them results in the same matrix.
Therefore, det(A)=−det(A) and so det(A)=0.
It remains to verify the last assertion.
∑ det(A)≡ sgn(k ,··· ,k )a ···(xa +yb )···a 1 n 1k1 rki rki nkn (k1,···,kn) ∑ =x sgn(k ,··· ,k )a ···a ···a 1 n 1k1 rki nkn ∑ (k1,···,kn) +y sgn(k ,··· ,k )a ···b ···a ≡xdet(A )+ydet(A ).
1 n 1k1 rki nkn 1 2 (k1,···,kn) ( ) Thesameistrueofcolumnsbecausedet AT =det(A)andtherowsofAT arethecolumns of A.
(cid:4) 3.3.4 Basic Properties Of The Determinant Deﬁnition 3.3.10 A vector, w, is a∑linear combination of the vectors {v1,··· ,vr} if there exist scalars c ,···c such that w= r c v .
This is the same as saying 1 r k=1 k k w∈span(v ,··· ,v ).
1 r Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 89 The following corollary is also of great use.
Corollary 3.3.11 Suppose A is an n×n matrix and some column (row) is a linear com- bination of r other columns (rows).
Then det(A)=0.
( ) Proof: Let A = a ··· a be the columns of A and suppose the condition that 1 n one column is a linear combination of r of the others is satisﬁed.
Then by using Corollary 3.3.9 you may rearrange the∑columns to have the nth column a linear combination of the ﬁrst r columns.
Thus a = r c a and so n k=1 k k ( ∑ ) det(A)=det a1 ··· ar ··· an−1 rk=1ckak .
By Corollary 3.3.9 ∑r ( ) det(A)= ckdet a1 ··· ar ··· an−1 ak =0.
k=1 ( ) The case for rows follows from the fact that det(A)=det AT .
(cid:4) Recall the following deﬁnition of matrix multiplication.
Deﬁnition ∑3.3.12 If A and B are n×n matrices, A = (aij) and B = (bij), AB = (cij) where c ≡ n a b .
ij k=1 ik kj Oneofthemostimportantrulesaboutdeterminantsisthatthedeterminantofaproduct equals the product of the determinants.
Theorem 3.3.13 Let A and B be n×n matrices.
Then det(AB)=det(A)det(B).
Proof: Let c be the ijth entry of AB.
Then by Proposition 3.3.6, ij ∑ det(AB)= sgn(k ,··· ,k )c ···c 1 n 1k1 nkn (k1,···,kn) ( ) ( ) ∑ ∑ ∑ = sgn(k ,··· ,k ) a b ··· a b 1 n 1r1 r1k1 nrn rnkn (k1∑,···,kn) ∑ r1 rn = sgn(k ,··· ,k )b ···b (a ···a ) 1 n r1k1 rnkn 1r1 nrn (r1∑···,rn)(k1,···,kn) = sgn(r ···r )a ···a det(B)=det(A)det(B).
(cid:4) 1 n 1r1 nrn (r1···,rn) TheBinetCauchyformulaisageneralizationofthetheoremwhichsaysthedeterminant ofaproductistheproductofthedeterminants.
Thesituationisillustratedinthefollowing picture where A,B are matrices.
B A Theorem 3.3.14 Let A be an n×m matrix with n≥m and let B be a m×n matrix.
Also let A i i=1,··· ,C(n,m) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation90 DETERMINANTS be the m×m submatrices of A which are obtained by deleting n−m rows and let B be the i m×m submatrices of B which are obtained by deleting corresponding n−m columns.
Then C(∑n,m) det(BA)= det(B )det(A ) k k k=1 Proof: This follows from a computation.
By Corollary 3.3.8 on Page 87, det(BA)= ∑ ∑ 1 sgn(i ···i )sgn(j ···j )(BA) (BA) ···(BA) m!
1 m 1 m i1j1 i2j2 imjm (i1···im)(j1···jm) ∑ ∑ 1 sgn(i ···i )sgn(j ···j )· m!
1 m 1 m (i1···im)(j1···jm) ∑n ∑n ∑n B A B A ··· B A i1r1 r1j1 i2r2 r2j2 imrm rmjm r1=1 r2=1 rm=1 Now denote by I one of the r subsets of {1,··· ,n}.
Thus there are C(n,m) of these.
k C(∑n,m) ∑ ∑ ∑ 1 = sgn(i ···i )sgn(j ···j )· m!
1 m 1 m k=1 {r1,···,rm}=Ik (i1···im)(j1···jm) B A B A ···B A i1r1 r1j1 i2r2 r2j2 imrm rmjm C∑(n,m) ∑ ∑ 1 = sgn(i ···i )B B ···B · m!
1 m i1r1 i2r2 imrm k∑=1 {r1,···,rm}=Ik (i1···im) sgn(j ···j )A A ···A 1 m r1j1 r2j2 rmjm (j1···jm) C(∑n,m) ∑ 1 = sgn(r ···r )2det(B )det(A )B m!
1 m k k k=1 {r1,···,rm}=Ik C(∑n,m) = det(B )det(A ) k k k=1 since there are m!
ways of arranging the indices {r ,··· ,r }.
(cid:4) 1 m 3.3.5 Expansion Using Cofactors Lemma 3.3.15 Suppose a matrix is of the form ( ) A ∗ M = (3.13) 0 a or ( ) A 0 M = (3.14) ∗ a where a is a number and A is an (n−1)×(n−1) matrix and ∗ denotes either a column or a row having length n−1 and the 0 denotes either a column or a row of length n−1 consisting entirely of zeros.
Then det(M)=adet(A).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 91 Proof: Denote M by (m ).
Thus in the ﬁrst case, m =a and m =0 if i̸=n while ij nn ni in the second case, m =a and m =0 if i̸=n.
From the deﬁnition of the determinant, nn in ∑ det(M)≡ sgn (k ,··· ,k )m ···m n 1 n 1k1 nkn (k1,···,kn) Letting θ denote the position of n in the ordered list, (k ,··· ,k ) then using the earlier 1 n conventions used to prove Lemma 3.3.1, det(M) equals ( ) ∑ (−1)n−θsgnn−1 k1,··· ,kθ−1,kθθ+1,··· ,nk−n1 m1k1···mnkn (k1,···,kn) Now suppose (3.14).
Then if k ̸= n, the term involving m in the above expression n nkn equals zero.
Therefore, the only terms which survive are those for which θ = n or in other words, those for which k =n.
Therefore, the above expression reduces to n ∑ a sgnn−1(k1,···kn−1)m1k1···m(n−1)kn−1 =adet(A).
(k1,···,kn−1) To get the assertion in the situation of (3.13) use Corollary 3.3.8 and (3.14) to write (( )) ( ) AT 0 ( ) det(M)=det MT =det =adet AT =adet(A).
(cid:4) ∗ a In terms of the theory of determinants, arguably the most important idea is that of Laplace expansion along a row or a column.
This will follow from the above deﬁnition of a determinant.
Deﬁnition 3.3.16 LetA=(a )beann×nmatrix.
Thenanewmatrixcalledthecofactor ij matrix cof(A) is deﬁned by cof(A) = (c ) where to obtain c delete the ith row and the ij ij jth column of A, take the determinant of the (n−1)×(n−1) matrix which results, (This is called the ijth minor of A. )
and then multiply this number by (−1)i+j.
To make the formulas easier to remember, cof(A) will denote the ijth entry of the cofactor matrix.
ij Thefollowingisthemainresult.
Earlierthiswasgivenasadeﬁnitionandtheoutrageous totallyunjustiﬁedassertionwasmadethatthesamenumberwouldbeobtainedbyexpanding the determinant along any row or column.
The following theorem proves this assertion.
Theorem 3.3.17 Let A be an n×n matrix where n≥2.
Then ∑n ∑n det(A)= a cof(A) = a cof(A) .
(3.15) ij ij ij ij j=1 i=1 The ﬁrst formula consists of expanding the determinant along the ith row and the second expands the determinant along the jth column.
Proof: Let(a ,··· ,a )be the ith rowof A.
Let B be the matrixobtained from Aby i1 in j leaving every row the same except the ith row which in B equals (0,··· ,0,a ,0,··· ,0).
j ij Then by Corollary 3.3.9, ∑n det(A)= det(B ) j j=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation92 DETERMINANTS For example if   a b c   A= d e f h i j and i=2, then       a b c a b c a b c       B = d 0 0 ,B = 0 e 0 ,B = 0 0 f 1 2 3 h i j h i j h i j Denote by Aij the (n−1)×(n−1) matrix obtained by deleting the ith row and the jth ( ) columnofA.Thuscof(A) ≡(−1)i+jdet Aij .Atthispoint,recallthatfromProposition ij 3.3.6,whentworowsortwocolumnsinamatrixM,areswitched,thisresultsinmultiplying thedeterminantoftheoldmatrixby−1togetthedeterminantofthenewmatrix.
Therefore, by Lemma 3.3.15, (( )) det(B ) = (−1)n−j(−1)n−idet Aij ∗ j 0 a (( )) ij Aij ∗ = (−1)i+jdet =a cof(A) .
0 a ij ij ij Therefore, ∑n det(A)= a cof(A) ij ij j=1 which is the formula for expanding det(A) along the ith row.
Also, ( ) ∑n ( ) ∑n det(A)=det AT = aT cof AT = a cof(A) ij ij ji ji j=1 j=1 which is the formula for expanding det(A) along the ith column.
(cid:4) 3.3.6 A Formula For The Inverse Notethatthisgivesaneasywaytowriteaformulafortheinverseofann×nmatrix.
Recall the deﬁnition of the inverse of a matrix in Deﬁnition 2.1.22 on Page 47.
( ) Theorem 3.3.18 A−1 exists if and only if det(A) ̸= 0.
If det(A) ̸= 0, then A−1 = a−1 ij where a−1 =det(A)−1cof(A) ij ji for cof(A) the ijth cofactor of A. ij Proof: By Theorem 3.3.17 and letting (a )=A, if det(A)̸=0, ir ∑n a cof(A) det(A)−1 =det(A)det(A)−1 =1.
ir ir i=1 Now in the matrix A, replace the kth column with the rth column and then expand along the kth column.
This yields for k ̸=r, ∑n a cof(A) det(A)−1 =0 ir ik i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 93 because there are two equal columns by Corollary 3.3.9.
Summarizing, ∑n a cof(A) det(A)−1 =δ .
ir ik rk i=1 Using the other formula in Theorem 3.3.17, and similar reasoning, ∑n a cof(A) det(A)−1 =δ rj kj rk j=1 ( ) This proves that if det(A)̸=0, then A−1 exists with A−1 = a−1 , where ij a−1 =cof(A) det(A)−1.
ij ji Now suppose A−1 exists.
Then by Theorem 3.3.13, ( ) ( ) 1=det(I)=det AA−1 =det(A)det A−1 so det(A)̸=0.
(cid:4) Thenextcorollarypointsoutthatifann×nmatrixAhasarightoraleftinverse,then it has an inverse.
Corollary 3.3.19 Let A be an n×n matrix and suppose there exists an n×n matrix B such that BA = I.
Then A−1 exists and A−1 = B.
Also, if there exists C an n×n matrix such that AC =I, then A−1 exists and A−1 =C.
Proof: Since BA=I, Theorem 3.3.13 implies detBdetA=1 and so detA̸=0.
Therefore from Theorem 3.3.18, A−1 exists.
Therefore, ( ) A−1 =(BA)A−1 =B AA−1 =BI =B.
The case where CA=I is handled similarly.
(cid:4) The conclusion of this corollary is that left inverses, right inverses and inverses are all the same in the context of n×n matrices.
Theorem 3.3.18 says that to ﬁnd the inverse, take the transpose of the cofactor matrix and divide by the determinant.
The transpose of the cofactor matrix is called the adjugate orsometimestheclassicaladjointofthematrixA.
Itisanabominationtocallittheadjoint althoughyoudosometimesseeitreferredtointhisway.
Inwords, A−1 isequaltooneover the determinant of A times the adjugate matrix of A. Incaseyouaresolvingasystemofequations,Ax=y forx,itfollowsthatifA−1 exists, ( ) x= A−1A x=A−1(Ax)=A−1y thus solving the system.
Now in the case that A−1 exists, there is a formula for A−1 given above.
Using this formula, ∑n ∑n 1 x = a−1y = cof(A) y .
i ij j det(A) ji j j=1 j=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation94 DETERMINANTS By the formula for the expansion of a determinant along a column,   ∗ ··· y ··· ∗ 1 1  .
.
.
 x = det .
.
.
, i det(A) .
.
.
∗ ··· y ··· ∗ n where here the ith column of A is replaced with the column vector, (y ····,y )T, and the 1 n determinantofthismodiﬁedmatrixistakenanddividedbydet(A).
Thisformulaisknown as Cramer’s rule.
Deﬁnition 3.3.20 A matrix M, is upper triangular if M =0 whenever i>j.
Thus such ij a matrix equals zero below the main diagonal, the entries of the form M as shown.
ii   ∗ ∗ ··· ∗  0 ∗ ... ...   ... ... ... ∗  0 ··· 0 ∗ A lower triangular matrix is deﬁned similarly as a matrix for which all entries above the main diagonal are equal to zero.
With this deﬁnition, here is a simple corollary of Theorem 3.3.17.
Corollary 3.3.21 Let M be an upper (lower) triangular matrix.
Then det(M) is obtained by taking the product of the entries on the main diagonal.
3.3.7 Rank Of A Matrix Deﬁnition 3.3.22 A submatrix of a matrix A is the rectangular array of numbers obtained by deleting some rows and columns of A.
Let A be an m×n matrix.
The determinant rank of the matrix equals r where r is the largest number such that some r×r submatrix of A has a non zero determinant.
The row rank is deﬁned to be the dimension of the span of the rows.
The column rank is deﬁned to be the dimension of the span of the columns.
Theorem 3.3.23 If A, an m×n matrix has determinant rank r, then there exist r rows of the matrix such that every other row is a linear combination of these r rows.
Proof: SupposethedeterminantrankofA=(a )equalsr.
Thussomer×r submatrix ij has non zero determinant and there is no larger square submatrix which has non zero determinant.
Suppose such a submatrix is determined by the r columns whose indices are j <···<j 1 r and the r rows whose indices are i <···<i 1 r I want to show that every row is a linear combination of these rows.
Consider the lth row and let p be an index between 1 and n. Form the following (r+1)×(r+1) matrix   a ··· a a  .i1j1 .i1jr .i1p   .
.
.
  .
.
.
  a ··· a a  irj1 irjr irp a ··· a a lj1 ljr lp Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.3.
THE MATHEMATICAL THEORY OF DETERMINANTS 95 Of course you can assume l ∈/ {i ,··· ,i } because there is nothing to prove if the lth 1 r row is one of the chosen ones.
The above matrix has determinant 0.
This is because if p ∈/ {j ,··· ,j } then the above would be a submatrix of A which is too large to have non 1 r zero determinant.
On the other hand, if p ∈ {j ,··· ,j } then the above matrix has two 1 r columns which are equal so its determinant is still 0.
Expand the determinant of the above matrix along the last column.
Let C denote the k cofactorassociatedwiththeentrya .Thisisnotdependentonthechoiceofp.Remember, ikp you delete the column and the row the entry is in and take the determinant of what is left and multiply by −1 raised to an appropriate power.
Let C denote the cofactor associated with a .
This is given to be nonzero, it being the determinant of the matrix lp   a ··· a  .i1j1 .i1jr   .
.
 .
.
a ··· a irj1 irjr Thus ∑r 0=a C+ C a lp k ikp k=1 which implies ∑r −C ∑r a = ka ≡ m a lp C ikp k ikp k=1 k=1 Since this is true for every p and since m does not depend on p, this has shown the lth row k is a linear combination of the i ,i ,··· ,i rows.
(cid:4) 1 2 r Corollary 3.3.24 The determinant rank equals the row rank.
Proof: From Theorem 3.3.23, every row is in the span of r rows where r is the deter- minant rank.
Therefore, the row rank (dimension of the span of the rows) is no larger than the determinant rank.
Could the row rank be smaller than the determinant rank?
If so, it follows from Theorem 3.3.23 that there exist p rows for p < r ≡ determinant rank, such that the span of these p rows equals the row space.
But then you could consider the r×r sub matrix which determines the determinant rank and it would follow that each of these rows would be in the span of the restrictions of the p rows just mentioned.
By Theorem 2.4.4, the exchange theorem, the rows of this sub matrix would not be linearly independent and so some row is a linear combination of the others.
By Corollary 3.3.11 the determinant would be 0, a contradiction.
(cid:4) Corollary 3.3.25 If A has determinant rank r, then there exist r columns of the matrix such that every other column is a linear combination of these r columns.
Also the column rank equals the determinant rank.
Proof: ThisfollowsfromtheabovebyconsideringAT.TherowsofAT arethecolumns ofAandthedeterminantrankofAT andAarethesame.
Therefore,fromCorollary3.3.24, column rank of A= row rank of AT = determinant rank of AT = determinant rank of A. Thefollowingtheoremisoffundamentalimportanceandtiestogethermanyoftheideas presented above.
Theorem 3.3.26 Let A be an n×n matrix.
Then the following are equivalent.
1. det(A)=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation96 DETERMINANTS 2.
A,AT are not one to one.
3.
A is not onto.
Proof: Suppose det(A) = 0.
Then the determinant rank of A = r < n. Therefore, thereexistr columnssuchthateveryothercolumnisalinearcombinationofthesecolumns by Theorem 3.3.23.
In particular, it follows that fo(r some m, the mth column) is a linear combination of all the others.
Thus letting A = a ··· a ··· a where the 1 m n columns are denoted by a , there exists scalars α such that i i ∑ a = α a .
m k k k̸=m ( ) Now consider the column vector, x≡ α ··· −1 ··· α T. Then 1 n ∑ Ax=−a + α a =0.
m k k k̸=m Since also A0=0, it follows A is not one to one.
Similarly, AT is not one to one by the same argument applied to AT.
This veriﬁes that 1.)
implies 2.).
Nowsuppose2.
).ThensinceAT isnotonetoone,itfollowsthereexistsx̸=0suchthat ATx=0.
Taking the transpose of both sides yields xTA=0T where the 0T is a 1×n matrix or row vector.
Now if Ay=x, then ( ) |x|2=xT (Ay)= xTA y=0y=0 contrary to x̸=0.
Consequently there can be no y such that Ay=x and so A is not onto.
This shows that 2.)
implies 3.).
Finally,suppose3.).
If1.
)doesnothold,thendet(A)̸=0butthenfromTheorem3.3.18 A−1 exists and so for every y ∈Fn there exists a unique x∈Fn such that Ax=y.
In fact x=A−1y.
Thus A would be onto contrary to 3.).
This shows 3.)
implies 1.).
(cid:4) Corollary 3.3.27 Let A be an n×n matrix.
Then the following are equivalent.
1. det(A)̸=0.
2.
A and AT are one to one.
3.
A is onto.
Proof: This follows immediately from the above theorem.
3.3.8 Summary Of Determinants In all the following A,B are n×n matrices 1. det(A) is a number.
2. det(A) is linear in each row and in each column.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.4.
THE CAYLEY HAMILTON THEOREM 97 3.
If you switch two rows or two columns, the determinant of the resulting matrix is −1 times the determinant of the unswitched matrix.
(This and the previous one say (a ···a )→det(a ···a ) 1 n 1 n is an alternating multilinear function or alternating tensor.
4. det(e ,··· ,e )=1.
1 n 5. det(AB)=det(A)det(B) 6. det(A)canbeexpandedalonganyroworanycolumnandthesameresultisobtained.
( ) 7. det(A)=det AT 8.
A−1 exists if and only if det(A)̸=0 and in this case ( ) 1 A−1 = cof(A) (3.16) ij det(A) ji 9.
Determinant rank, row rank and column rank are all the same number for any m×n matrix.
3.4 The Cayley Hamilton Theorem Deﬁnition 3.4.1 Let A be an n×n matrix.
The characteristic polynomial is deﬁned as p (t)≡det(tI−A) A and the solutions to p (t) = 0 are called eigenvalues.
For A a matrix and p(t) = tn + A an−1tn−1+···+a1t+a0, denote by p(A) the matrix deﬁned by p(A)≡An+an−1An−1+···+a1A+a0I.
The explanation for the last term is that A0 is interpreted as I, the identity matrix.
The Cayley Hamilton theorem states that every matrix satisﬁes its characteristic equa- tion, thatequationdeﬁnedbyp (t)=0.
Itisoneofthemostimportanttheoremsinlinear A algebra1.
The following lemma will help with its proof.
Lemma 3.4.2 Suppose for all |λ| large enough, A +A λ+···+A λm =0, 0 1 m where the A are n×n matrices.
Then each A =0.
i i Proof: Multiply by λ−m to obtain A0λ−m+A1λ−m+1+···+Am−1λ−1+Am =0.
Now let |λ|→∞ to obtain A =0.
With this, multiply by λ to obtain m A0λ−m+1+A1λ−m+2+···+Am−1 =0.
Now let |λ| → ∞ to obtain Am−1 = 0.
Continue multiplying by λ and letting λ → ∞ to obtain that all the A =0.
(cid:4) i With the lemma, here is a simple corollary.
1AspecialcasewasﬁrstprovedbyHamiltonin1853.
ThegeneralcasewasannouncedbyCayleysome timelaterandaproofwasgivenbyFrobeniusin1878.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation98 DETERMINANTS Corollary 3.4.3 Let A and B be n×n matrices and suppose i i A +A λ+···+A λm =B +B λ+···+B λm 0 1 m 0 1 m for all |λ| large enough.
Then A =B for all i. Consequently if λ is replaced by any n×n i i matrix, the two sides will be equal.
That is, for C any n×n matrix, A +A C+···+A Cm =B +B C+···+B Cm.
0 1 m 0 1 m Proof: Subtract and use the result of the lemma.
(cid:4) With this preparation, here is a relatively easy proof of the Cayley Hamilton theorem.
Theorem 3.4.4 Let A be an n×n matrix and let p(λ)≡det(λI−A) be the characteristic polynomial.
Then p(A)=0.
Proof: Let C(λ) equal the transpose of the cofactor matrix of (λI−A) for |λ| large.
(If |λ| is large enough, then λ cannot be in the ﬁnite list of eigenvalues of A and so for such λ, (λI−A)−1 exists.)
Therefore, by Theorem 3.3.18 C(λ)=p(λ)(λI−A)−1.
Note that each entry in C(λ) is a polynomial in λ having degree no more than n − 1.
Therefore, collecting the terms, C(λ)=C0+C1λ+···+Cn−1λn−1 for C some n×n matrix.
It follows that for all |λ| large enough, j ( ) (λI−A) C0+C1λ+···+Cn−1λn−1 =p(λ)I andsoCorollary3.4.3maybeused.
Itfollowsthematrixcoeﬃcientscorrespondingtoequal powers of λ are equal on both sides of this equation.
Therefore, if λ is replaced with A, the two sides will be equal.
Thus ( ) 0=(A−A) C0+C1A+···+Cn−1An−1 =p(A)I =p(A).
(cid:4) 3.5 Block Multiplication Of Matrices Consider the following problem ( )( ) A B E F C D G H You know how to do this.
You get ( ) AE+BG AF +BH .
CE+DG CF +DH Now what if instead of numbers, the entries, A,B,C,D,E,F,G are matrices of a size such that the multiplications and additions needed in the above formula all make sense.
Would the formula be true in this case?
I will show below that this is true.
Suppose A is a matrix of the form   A ··· A 11 1m A= ... ... ...  (3.17) A ··· A r1 rm Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.5.
BLOCK MULTIPLICATION OF MATRICES 99 where A is a s ×p matrix where s is constant for j = 1,··· ,m for each i = 1,··· ,r. ij i j i Such a matrix is called a block matrix, also a partitioned matrix.
How do you get the block A ?
Here is how for A an m×n matrix: ij z n×}|pj {   z( si}×|m ){ 0   0 Isi×si 0 A Ipj×pj .
(3.18) 0 In the block column matrix on the right, you need to have c −1 rows of zeros above the j small p ×p identity matrix where the columns of A involved in A are c ,··· ,c +p −1 j j ij j j j and in the block row matrix on the left, you need to have r −1 columns of zeros to the left i of the s ×s identity matrix where the rows of A involved in A are r ,··· ,r +s .
An i i ij i i i important observation to make is that the matrix on the right speciﬁes columns to use in the block and the one on the left speciﬁes the rows used.
Thus the block A in this case ij is a matrix of size s ×p .
There is no overlap between the blocks of A.
Thus the identity i j n×n identity matrix corresponding to multiplication on the right of A is of the form    Ip1×p1 0   ...  0 Ipm×pm where these little identity matrices don’t overlap.
A similar conclusion follows from consid- erationofthematricesIsi×si.
Notethatin (3.18)thematrix ontherightisablockcolumn matrix for the above block diagonal matrix and the matrix on the left in (3.18) is a block row matrix taken from a similar block diagonal matrix consisting of the Isi×si.
Next consider the question of multiplication of two block matrices.
Let B be a block matrix of the form   B ··· B 11 1p  ... ... ...  (3.19) B ··· B r1 rp and A is a block matrix of the form   A ··· A 11 1m  ... ... ...  (3.20) A ··· A p1 pm and that for all i,j, it makes sense to multiply B A for all s ∈ {1,··· ,p}.
(That is the is sj two matrices, Bis and Asj are conformable.)
and th∑at for ﬁxed ij, it follows BisAsj is the same size for each s so that it makes sense to write B A .
s is sj The following theorem says essentially that when you take the product of two matrices, you can do it two ways.
One way is to simply multiply them forming BA.
The other way is to partition both matrices, formally multiply the blocks to get another block matrix and this one will be BA partitioned.
Before presenting this theorem, here is a simple lemma which is really a special case of the theorem.
Lemma 3.5.1 Consider the following product.
  0 ( )   I 0 I 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation100 DETERMINANTS where the ﬁrst is n×r and the second is r×n.
The small identity matrix I is an r×r matrix and there are l zero rows above I and l zero columns to the left of I in the right matrix.
Then the product of these matrices is a block matrix of the form   0 0 0   0 I 0 0 0 0 Proof: From the deﬁnition of the way you multiply matrices, the product is               0 0 0 0 0 0   I 0 ··· I 0  I e ···  I e  I 0 ···  I 0  1 r 0 0 0 0 0 0 which yields the claimed result.
In the formula e refers to the column vector of length r j which has a 1 in the jth position.
(cid:4) Theorem 3.5.2 Let B be a q×p block matrix as in (3.19) and let A be a p×n block matrix asin(3.20)suchthatB isconformablewithA andeachproduct, B A fors=1,··· ,p is sj is sj is of the same size so they can be added.
Then BA can be obtained as a block matrix such that the ijth block is of the form ∑ B A .
(3.21) is sj s Proof: From (3.18)     ( ) 0 ( ) 0     BisAsj = 0 Iri×ri 0 B Ips×ps 0 Ips×ps 0 A Iqj×qj 0 0 where here it is assumed B is r ×p and A is p ×q .
The product involves the sth is i s sj s j block in the ith row of blocks for B and the sth block in the jth column of A.
Thus there are the same number of rows above the Ips×ps as there are columns to the left of Ips×ps in those two inside matrices.
Then from Lemma 3.5.1     0 ( ) 0 0 0     Ips×ps 0 Ips×ps 0 = 0 Ips×ps 0 0 0 0 0 Since the blocks of small identity matrices do not overlap,     ∑ 0 0 0  Ip1×p1 0   0 Ips×ps 0 = ... =I s 0 0 0 0 Ipp×pp and so     ∑ ∑( ) 0 ( ) 0     BisAsj = 0 Iri×ri 0 B Ips×ps 0 Ips×ps 0 A Iqj×qj s s 0 0     ( ) ∑ 0 ( ) 0     = 0 Iri×ri 0 B Ips×ps 0 Ips×ps 0 A Iqj×qj s 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.5.
BLOCK MULTIPLICATION OF MATRICES 101     ( ) 0 ( ) 0     = 0 Iri×ri 0 BIA Iqj×qj = 0 Iri×ri 0 BA Iqj×qj 0 0 which equals the ijth block of BA.
Hence t∑he ijth block of BA equals the formal multipli- cation according to matrix multiplication, B A .
(cid:4) s is sj ( ) a b Example 3.5.3 Letann×nmatrixhavetheformA= whereP isn−1×n−1.
c P ( ) p q Multiply it by B = where B is also an n×n matrix and Q is n−1×n−1.
r Q You use block multiplication ( )( ) ( ) a b p q ap+br aq+bQ = c P r Q pc+Pr cq+PQ Note that this all makes sense.
For example, b = 1×n−1 and r = n−1×1 so br is a 1×1.
Similar considerations apply to the other blocks.
Hereisaninterestingandsigniﬁcantapplicationofblockmultiplication.
Inthistheorem, p (t)denotesthecharacteristicpolynomial,det(tI −M).Thezerosofthispolynomialwill M beshownlatertobeeigenvaluesofthematrixM.
Firstnotethatfromblockmultiplication, for the following block matrices consisting of square blocks of an appropriate size, ( ) ( )( ) A 0 A 0 I 0 = so B C B I 0 C ( ) ( ) ( ) A 0 A 0 I 0 det =det det =det(A)det(C) B C B I 0 C Theorem 3.5.4 Let A be an m×n matrix and let B be an n×m matrix for m≤n.
Then p (t)=tn−mp (t), BA AB so the eigenvalues of BA and AB are the same including multiplicities except that BA has n−mextrazeroeigenvalues.
Herep (t)denotesthecharacteristicpolynomialofthematrix A A.
Proof: Use block multiplication to write ( )( ) ( ) AB 0 I A AB ABA = B 0 0 I B BA ( )( ) ( ) I A 0 0 AB ABA = .
0 I B BA B BA Therefore, ( ) ( )( ) ( ) −1 I A AB 0 I A 0 0 = 0 I B 0 0 I B BA ( ) ( ) 0 0 AB 0 Sincethetwomatricesabovearesimilar,itfollowsthat and have B BA B 0 thesamecharacteristicpolynomials.
SeeProblem8onPage82.
Therefore, notingthatBA is an n×n matrix and AB is an m×m matrix, tmdet(tI −BA)=tndet(tI−AB) and so det(tI−BA)=p (t)=tn−mdet(tI−AB)=tn−mp (t).
(cid:4) BA AB Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation102 DETERMINANTS 3.6 Exercises 1.
Let m < n and let A be an m×n matrix.
Show that A is not one to one.
Hint: Consider the n×n matrix A which is of the form 1 ( ) A A ≡ 1 0 where the 0 denotes an (n−m)×n matrix of zeros.
Thus detA = 0 and so A is 1 1 not one to one.
Now observe that A x is the vector, 1 ( ) Ax A x= 1 0 which equals zero if and only if Ax=0.
2.
Let v ,··· ,v be vectors in Fn and let M(v ,··· ,v ) denote the matrix whose ith 1 n 1 n column equals v .
Deﬁne i d(v ,··· ,v )≡det(M(v ,··· ,v )).
1 n 1 n Prove that d is linear in each variable, (multilinear), that d(v ,··· ,v ,··· ,v ,··· ,v )=−d(v ,··· ,v ,··· ,v ,··· ,v ), (3.22) 1 i j n 1 j i n and d(e ,··· ,e )=1 (3.23) 1 n where here e is the vector in Fn which has a zero in every position except the jth j position in which it has a one.
3.
Supposef :Fn×···×Fn →Fsatisﬁes(3.22)and(3.23) andislinearineachvariable.
Show that f =d.
4.
Show that if you replace a row (column) of an n×n matrix A with itself added to somemultipleofanotherrow(column)thenthenewmatrixhasthesamedeterminant as the original one.
5.
Use the result of Problem 4 to evaluate by hand the determinant   1 2 3 2  −6 3 2 3  det .
5 2 2 3 3 4 6 4 6.
Find the inverse if it exists of the matrix   et cost sint  et −sint cost .
et −cost −sint 7.
Let Ly = y(n) + an−1(x)y(n−1) + ··· + a1(x)y′ + a0(x)y where the ai are given continuous functions deﬁned on an interval, (a,b) and y is some function which has n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation3.6.
EXERCISES 103 derivatives so it makes sense to write Ly.
Suppose Ly = 0 for k = 1,2,··· ,n. The k Wronskian of these functions, y is deﬁned as i   y (x) ··· y (x) 1 n  y1′ (x) ··· yn′ (x)  W (y ,··· ,y )(x)≡det .
.
 1 n  .
.
 .
.
y(n−1)(x) ··· y(n−1)(x) 1 n Show that for W (x)=W (y ,··· ,y )(x) to save space, 1 n   y (x) ··· y (x) 1 n  .
.
 W′(x)=det .. ··· .. .
 y(n−2)(x) y(n−2)(x)  1 n y(n)(x) ··· y(n)(x) 1 n Nowusethediﬀerentialequation,Ly =0whichissatisﬁedbyeachofthesefunctions, yi andpropertiesofdeterminantspresentedabovetoverifythatW′+an−1(x)W =0.
Give an explicit solution of this linear diﬀerential equation, Abel’s formula, and use your answer to verify that the Wronskian of these solutions to the equation, Ly = 0 either vanishes identically on (a,b) or never.
8.
Two n×n matrices, A and B, are similar if B = S−1AS for some invertible n×n matrix S. Show that if two matrices are similar, they have the same characteristic polynomials.
The characteristic polynomial of A is det(λI−A).
9.
Suppose the characteristic polynomial of an n×n matrix A is of the form tn+an−1tn−1+···+a1t+a0 and that a ̸= 0.
Find a formula A−1 in terms of powers of the matrix A.
Show that 0 A−1 exists if and only if a ̸=0.
In fact, show that a =(−1)ndet(A).
0 0 10.
↑Letting p(t) denote the characteristic polynomial of A, show that p (t) ≡ p(t−ε) ε is the characteristic polynomial of A+εI.
Then show that if det(A) = 0, it follows that det(A+εI)̸=0 whenever |ε| is suﬃciently small.
11.
Inconstitutiv∑emodelingofthestressandstraintensors,onesometimesconsiderssums of the form ∞ a Ak where A is a 3×3 matrix.
Show using the Cayley Hamilton k=0 k theoremthatifsuchathingmakesanysense, youcanalwaysobtainitasaﬁnitesum having no more than n terms.
12.
Recall you can ﬁnd the determinant from expanding along the jth column.
∑ det(A)= A (cof(A)) ij ij i Think of det(A) as a function of the entries, A .
Explain why the ijth cofactor is ij really just ∂det(A) .
∂A ij Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation104 DETERMINANTS 13.
Let U be an open set in Rn and let g:U → Rn be such that all the ﬁrst partial derivatives of all components of g exist and are continuous.
Under these conditions form the matrix Dg(x) given by ∂g (x) Dg(x) ≡ i ≡g (x) ij ∂x i,j j The best kept secret in calculus courses is that the linear transformation determined by this matrix Dg(x) is called the derivative of g and is the correct generalization of the concept of derivative of a function of one variable.
Suppose the second partial derivatives also exist and are continuous.
Then show that ∑ (cof(Dg)) =0.
ij,j j ∑ Hint: First explain why g cof(Dg) = δ det(Dg).
Next diﬀerentiate with i i,k ij jk respect to x and sum on j using the equality of mixed partial derivatives.
Assume j det(Dg)̸=0toprovetheidentityinthisspecialcase.
ThenexplainusingProblem10 whythereexistsasequenceε →0suchthatforg (x)≡g(x)+ε x,det(Dg )̸=0 k εk k εk andsotheidentityholdsforg .
Thentakealimittogetthedesiredresultingeneral.
εk This is an extremely important identity which has surprising implications.
One can build degree theory on it for example.
It also leads to simple proofs of the Brouwer ﬁxed point theorem from topology.
14.
A determinant of the form (cid:12) (cid:12) (cid:12)(cid:12) 1 1 ··· 1 (cid:12)(cid:12) (cid:12)(cid:12) a0 a1 ··· an (cid:12)(cid:12) (cid:12)(cid:12) a20 a21 ··· a2n (cid:12)(cid:12) (cid:12) .
.
.
(cid:12) (cid:12) .
.
.
(cid:12) (cid:12) .
.
.
(cid:12) (cid:12)(cid:12) an−1 an−1 ··· an−1 (cid:12)(cid:12) (cid:12) a0n a1n ··· ann (cid:12) 0 1 n is called a Vandermonde determinant.
Show this determinant equals ∏ (a −a ) j i 0≤i<j≤n Bythisismeanttotaketheproductofalltermsoftheform(a −a )suchthatj >i.
j i Hint: Show it works if n=1 so you are looking at (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 1 (cid:12) (cid:12) (cid:12) a a 0 1 Then suppose it holds for n−1 and consider the case n. Consider the polynomial in t,p(t)whichisobtainedfromtheabovebyreplacingthelastcolumnwiththecolumn ( ) 1 t ··· tn T .
Explain why p(a )=0 for i=0,··· ,n−1.
Explain why j n∏−1 p(t)=c (t−a ).
i i=0 Of course c is the coeﬃcient of tn.
Find this coeﬃcient from the above description of p(t) and the induction hypothesis.
Then plug in t = a and observe you have the n formula valid for n. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationRow Operations 4.1 Elementary Matrices The elementary matrices result from doing a row operation to the identity matrix.
Deﬁnition 4.1.1 The row operations consist of the following 1.
Switch two rows.
2.
Multiply a row by a nonzero number.
3.
Replace a row by a multiple of another row added to it.
The elementary matrices are given in the following deﬁnition.
Deﬁnition 4.1.2 The elementary matrices consist of those matrices which result by apply- ingarowoperationtoanidentitymatrix.
Thosewhichinvolveswitchingrowsoftheidentity are called permutation matrices.
More generally, if (i ,i ,··· ,i ) is a permutation, a ma- 1 2 n trix which has a 1 in the i position in row k and zero in every other position of that row is k called a permutation matrix.
Thus each permutation corresponds to a unique permutation matrix.
As an example of why these elementary matrices are interesting, consider the following.
     0 1 0 a b c d x y z w      1 0 0 x y z w = a b c d 0 0 1 f g h i f g h i A3×4matrixwasmultipliedontheleftbyanelementarymatrixwhichwasobtainedfrom row operation 1 applied to the identity matrix.
This resulted in applying the operation 1 to the given matrix.
This is what happens in general.
Now consider what these elementary matrices look like.
First consider the one which involves switching row i and row j where i<j.
This matrix is of the form   1 0    ...   0 ··· 1   .
.
  .
.
  .
.
  1 ··· 0     ...  0 1 105 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation106 ROW OPERATIONS The two exceptional rows are shown.
The ith row was the jth and the jth row was the ith in the identity matrix.
Now consider what this does to a column vector.
     1 0 v v 1 1  ...  ...   ...   0 ··· 1  vi   vj   .
.
 .
  .
  .
.
 .
= .
  .
.
 .
  .
  1 ··· 0  vj   vi        ...  ...   ...  0 1 v v n n Now denote by Pij the elementary matrix which comes from the identity from switching rows i and j.
From what was just explained consider multiplication on the left by this elementary matrix.
  a a ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
  ai1 ai2 ··· aip   .
.
.
 Pij .
.
.
  .
.
.
  aj1 aj2 ··· ajp    .
.
.
 .
.
.
 .
.
.
a a ··· a n1 n2 np From the way you multiply matrices this is a matrix which has the indicated columns.
       a a a 11 12 1p   .
  .
  .
   .
  .
  .
   .
  .
  .
          ai1   ai2   aip  Pij .. ,Pij .. ,··· ,Pij ..    .
  .
  .
          aj1   aj2   ajp         .
.
.
  .
  .
  .
 .
.
.
a a a n1 n2 np       a a a 11 12 1p  .
  .
  .
  .
  .
  .
  .
  .
  .
        aj1   aj2   ajp  = .. , .. ,··· , ..   .
  .
  .
        ai1   ai2   aip        .
.
.
 .
  .
  .
 .
.
.
a a a n1 n2 np Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.1.
ELEMENTARY MATRICES 107   a a ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
  aj1 aj2 ··· ajp   .
.
.
 = .
.
.
  .
.
.
  ai1 ai2 ··· aip    .
.
.
 .
.
.
 .
.
.
a a ··· a n1 n2 np This has established the following lemma.
Lemma 4.1.3 Let Pij denote the elementary matrix which involves switching the ith and the jth rows.
Then PijA=B where B is obtained from A by switching the ith and the jth rows.
As a consequence of the above lemma, if you have any permutation (i ,··· ,i ), it 1 n follows from Lemma 3.3.2 that the corresponding permutation matrix can be obtained by multiplying ﬁnitely many permutation matrices, each of which switch only two rows.
Now every such permutation matrix in which only two rows are switched has determinant −1.
Therefore, the determinant of the permutation matrix for (i ,··· ,i ) equals (−1)p where 1 n the given permutation can be obtained by making p switches.
Now p is not unique.
There aremanywaystomakeswitchesandendupwithagivenpermutation,butwhatthisshows is that the total number of switches is either always odd or always even.
That is, you could notobtainagivenpermutationbymaking2mswitchesand2k+1switches.
Apermutation is said to be even if p is even and odd if p is odd.
This is an interesting result in abstract algebra which is obtained very easily from a consideration of elementary matrices and of coursethetheoryofthedeterminant.
Also,thisshowsthatthecompositionofpermutations corresponds to the product of the corresponding permutation matrices.
Toseepermutationsconsideredmoredirectlyinthecontextofgrouptheory, youshould see a good abstract algebra book such as [17] or [13].
Next consider the row operation which involves multiplying the ith row by a nonzero constant, c. The elementary matrix which results from applying this operation to the ith row of the identity matrix is of the form   1 0    ...     c     ...  0 1 Now consider what this does to a column vector.
     1 0 v v 1 1  ...  ...   ...        c  vi = cvi   ...  ...   ...  0 1 v v n n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation108 ROW OPERATIONS DenotebyE(c,i)thiselementarymatrixwhichmultipliestheith rowoftheidentitybythe nonzeroconstant,c.Thenfromwhatwasjustdiscussedandthewaymatricesaremultiplied,   a a ··· ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
 E(c,i) ai1 ai2 ··· ··· aip   .
.
.
  .
.
.
 .
.
.
a a ··· ··· a n1 n2 np equals a matrix having the columns indicated below.
       a a a 11 12 1p   .
  .
  .
   .
  .
  .
   .
  .
  .
 = E(c,i) ai1 ,E(c,i) ai2 ,··· ,E(c,i) aip    .
  .
  .
   .
  .
  .
 .
.
.
a a a  n1 n2 np a a ··· ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
 =  cai1 cai2 ··· ··· caip   .
.
.
  .
.
.
 .
.
.
a a ··· ··· a n1 n2 np This proves the following lemma.
Lemma 4.1.4 Let E(c,i) denote the elementary matrix corresponding to the row opera- tion in which the ith row is multiplied by the nonzero constant, c. Thus E(c,i) involves multiplying the ith row of the identity matrix by c. Then E(c,i)A=B where B is obtained from A by multiplying the ith row of A by c. Finallyconsiderthethirdoftheserowoperations.
DenotebyE(c×i+j)theelementary matrix which replaces the jth row with itself added to c times the ith row added to it.
In case i<j this will be of the form   1 0    ...     1   ... ...   c ··· 1     ...  0 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.1.
ELEMENTARY MATRICES 109 Now consider what this does to a column vector.
     1 0 v v 1 1  ...  ...   ...        1  vi   vi   ... ...  ... = ...   c ··· 1  vj   cvi+vj        ...  ...   ...  0 1 v v n n Now from this and the way matrices are multiplied,   a a ··· ··· ··· ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
  ai1 ai2 ··· ··· ··· ··· aip  E(c×i+j) .. .. ..   .
.
.
  aj2 aj2 ··· ··· ··· ··· ajp    .
.
.
 .
.
.
 .
.
.
a a ··· ··· ··· ··· a n1 n2 np equals a matrix of the following form having the indicated columns.
       a a a 11 12 1p   .
  .
  .
   .
  .
  .
   .
  .
  .
          ai1   ai2   aip  E(c×i+j) .. ,E(c×i+j) .. ,···E(c×i+j) ..    .
  .
  .
          aj2   aj2   ajp         .
.
.
  .
  .
  .
 .
.
.
a a a n1 n2 np   a a ··· a 11 12 1p  .
.
.
  .
.
.
  .
.
.
  ai1 ai2 ··· aip   .
.
.
 = .
.
.
  .
.
.
  aj2+cai1 aj2+cai2 ··· ajp+caip    .
.
.
 .
.
.
 .
.
.
a a ··· a n1 n2 np The case where i>j is handled similarly.
This proves the following lemma.
Lemma 4.1.5 Let E(c×i+j) denote the elementary matrix obtained from I by replacing the jth row with c times the ith row added to it.
Then E(c×i+j)A=B where B is obtained from A by replacing the jth row of A with itself added to c times the ith row of A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation110 ROW OPERATIONS The next theorem is the main result.
Theorem 4.1.6 To perform any of the three row operations on a matrix A it suﬃces to do the row operation on the identity matrix obtaining an elementary matrix E and then take the product, EA.
Furthermore, each elementary matrix is invertible and its inverse is an elementary matrix.
Proof: The ﬁrst part of this theorem has been proved in Lemmas 4.1.3 - 4.1.5.
It only remains to verify the claim about the inverses.
Consider ﬁrst the elementary matrices corresponding to row operation of type three.
E(−c×i+j)E(c×i+j)=I Thisfollowsbecausetheﬁrstmatrixtakesctimesrowiintheidentityandaddsittorowj.
When multiplied on the left by E(−c×i+j) it follows from the ﬁrst part of this theorem that you take the ith row of E(c×i+j) which coincides with the ith row of I since that row was not changed, multiply it by −c and add to the jth row of E(c×i+j) which was the jth row of I added to c times the ith row of I.
Thus E(−c×i+j) multiplied on the left, undoes the row operation which resulted in E(c×i+j).
The same argument applied to the product E(c×i+j)E(−c×i+j) replacing c with −c in the argument yields that this product is also equal to I.
Therefore, E(c×i+j)−1 = E(−c×i+j).
Similar reasoning shows that for E(c,i) the elementary matrix which comes from mul- tiplying the ith row by the nonzero constant, c, ( ) E(c,i)−1 =E c−1,i .
Finally, consider Pij which involves switching the ith and the jth rows.
PijPij =I because by the ﬁrst part of this theorem, multiplying on the left by Pij switches the ith and jth rows of Pij which was obtained from switching the ith and jth rows of the identity.
First you switch them to get Pij and then you multiply on the left by Pij which switches ( ) these rows again and restores the identity matrix.
Thus Pij −1 =Pij.
(cid:4) 4.2 The Rank Of A Matrix Recall the following deﬁnition of rank of a matrix.
Deﬁnition 4.2.1 A submatrix of a matrix A is the rectangular array of numbers obtained by deleting some rows and columns of A.
Let A be an m×n matrix.
The determinant rank of the matrix equals r where r is the largest number such that some r×r submatrix of A has a non zero determinant.
The row rank is deﬁned to be the dimension of the span of the rows.
The column rank is deﬁned to be the dimension of the span of the columns.
The rank of A is denoted as rank(A).
The following theorem is proved in the section on the theory of the determinant and is restated here for convenience.
Theorem 4.2.2 Let A be an m×n matrix.
Then the row rank, column rank and determi- nant rank are all the same.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.2.
THE RANK OF A MATRIX 111 Sohowdoyouﬁndtherank?
Itturnsoutthatrowoperationsarethekeytothepractical computation of the rank of a matrix.
Inroughterms, thefollowinglemmastatesthatlinear relationshipsbetweencolumns in a matrix are preserved by row operations.
Lemma 4.2.3 Let B and A be two m × n matrices and suppose B results from a row operation applied to A.
Then the kth column of B is a linear combination of the i ,··· ,i 1 r columns of B if and only if the kth column of A is a linear combination of the i ,··· ,i 1 r columns of A.
Furthermore, the scalars in the linear combination are the same.
(The linear relationship between the kth column of A and the i ,··· ,i columns of A is the same as the 1 r linear relationship between the kth column of B and the i ,··· ,i columns of B.)
1 r Proof: Let A equal the following matrix in which the a are the columns k ( ) a a ··· a 1 2 n and let B equal the following matrix in which the columns are given by the b k ( ) b b ··· b 1 2 n Then by Theorem 4.1.6 on Page 110 b =Ea where E is an elementary matrix.
Suppose k k then that one of the columns of A is a linear combination of some other columns of A.
Say ∑ a = c a .
k r r r∈S Then multiplying by E, ∑ ∑ b =Ea = c Ea = c b .
(cid:4) k k r r r r r∈S r∈S Corollary 4.2.4 Let A and B be two m×n matrices such that B is obtained by applying a row operation to A.
Then the two matrices have the same rank.
Proof: Lemma 4.2.3 says the linear relationships are the same between the columns of A and those of B.
Therefore, the column rank of the two matrices is the same.
(cid:4) This suggests that to ﬁnd the rank of a matrix, one should do row operations until a matrix is obtained in which its rank is obvious.
Example 4.2.5 Find the rank of the following matrix and identify columns whose linear combinations yield all the other columns.
  1 2 1 3 2   1 3 6 0 2 (4.1) 3 7 8 6 6 Take (−1) times the ﬁrst row and add to the second and then take (−3) times the ﬁrst row and add to the third.
This yields   1 2 1 3 2  0 1 5 −3 0  0 1 5 −3 0 By the above corollary, this matrix has the same rank as the ﬁrst matrix.
Now take (−1) times the second row and add to the third row yielding   1 2 1 3 2  0 1 5 −3 0  0 0 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation112 ROW OPERATIONS At this point it is clear the rank is 2.
This is because every column is in the span of the ﬁrst two and these ﬁrst two columns are linearly independent.
Example 4.2.6 Find the rank of the following matrix and identify columns whose linear combinations yield all the other columns.
  1 2 1 3 2   1 2 6 0 2 (4.2) 3 6 8 6 6 Take (−1) times the ﬁrst row and add to the second and then take (−3) times the ﬁrst row and add to the last row.
This yields   1 2 1 3 2  0 0 5 −3 0  0 0 5 −3 0 Now multiply the second row by 1/5 and add 5 times it to the last row.
  1 2 1 3 2  0 0 1 −3/5 0  0 0 0 0 0 Add (−1) times the second row to the ﬁrst.
  1 2 0 18 2  0 0 1 −35/5 0  (4.3) 0 0 0 0 0 It is now clear the rank of this matrix is 2 because the ﬁrst and third columns form a basis for the column space.
The matrix (4.3) is the row reduced echelon form for the matrix (4.2).
4.3 The Row Reduced Echelon Form The following deﬁnition is for the row reduced echelon form of a matrix.
Deﬁnition 4.3.1 Let e denote the column vector which has all zero entries except for the i ith slotwhichisone.
Anm×nmatrixissaidtobeinrowreducedechelonformif,inviewing successive columns from left to right, the ﬁrst nonzero column encountered is e and if you 1 have encountered e ,e ,··· ,e , the next column is either e or is a linear combination 1 2 k k+1 of the vectors, e ,e ,··· ,e .
1 2 k For example, here are some matrices which are in row reduced echelon form.
    0 1 3 0 3 1 0 3 −11 0     0 0 0 1 5 , 0 1 4 4 0 .
0 0 0 0 0 0 0 0 0 1 Theorem 4.3.2 Let A be an m × n matrix.
Then A has a row reduced echelon form determined by a simple process.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.3.
THE ROW REDUCED ECHELON FORM 113 Proof: Viewing the columns of A from left to right take the ﬁrst nonzero column.
Pick anonzeroentryinthiscolumnandswitchtherowcontainingthisentrywiththetoprowof A.Nowdividethisnewtoprowbythevalueofthisnonzeroentrytogeta1inthisposition and then use row operations to make all entries below this entry equal to zero.
Thus the ﬁrstnonzerocolumnisnowe .
DenotetheresultingmatrixbyA .
Considerthesubmatrix 1 1 of A to the right of this column and below the ﬁrst row.
Do exactly the same thing for it 1 that was done for A.
This time the e will refer to Fm−1.
Use this 1 and row operations 1 to zero out every entry above it in the rows of A .
Call the resulting matrix A .
Thus A 1 2 2 satisﬁes the conditions of the above deﬁnition up to the column just encountered.
Continue thiswaytilleverycolumnhasbeendealtwithandtheresultmustbeinrowreducedechelon form.
(cid:4) Thefollowingdiagramillustratestheaboveprocedure.
Saythematrixlookedsomething like the following.
  0 ∗ ∗ ∗ ∗ ∗ ∗  0 ∗ ∗ ∗ ∗ ∗ ∗   .
.
.
.
.
.
.
  .
.
.
.
.
.
.
 .
.
.
.
.
.
.
0 ∗ ∗ ∗ ∗ ∗ ∗ First step would yield something like   0 1 ∗ ∗ ∗ ∗ ∗  0 0 ∗ ∗ ∗ ∗ ∗   .
.
.
.
.
.
.
  .
.
.
.
.
.
.
 .
.
.
.
.
.
.
0 0 ∗ ∗ ∗ ∗ ∗ For the second step you look at the lower right corner as described,   ∗ ∗ ∗ ∗ ∗  .
.
.
.
.
  .
.
.
.
.
 .
.
.
.
.
∗ ∗ ∗ ∗ ∗ and if the ﬁrst column consists of all zeros but the next one is not all zeros, you would get something like this.
  0 1 ∗ ∗ ∗  .
.
.
.
.
  .
.
.
.
.
 .
.
.
.
.
0 0 ∗ ∗ ∗ Thus, after zeroing out the term in the top row above the 1, you get the following for the next step in the computation of the row reduced echelon form for the original matrix.
  0 1 ∗ 0 ∗ ∗ ∗  0 0 0 1 ∗ ∗ ∗   .
.
.
.
.
.
.
.
 .
.
.
.
.
.
.
 .
.
.
.
.
.
.
0 0 0 0 ∗ ∗ ∗ Next you look at the lower right matrix below the top two rows and to the right of the ﬁrst four columns and repeat the process.
Deﬁnition 4.3.3 The ﬁrst pivot column of A is the ﬁrst nonzero column of A.
The next pivotcolumnistheﬁrstcolumnafterthiswhichisnotalinearcombinationofthecolumnsto itsleft.
Thethirdpivotcolumnisthenextcolumnafterthiswhichisnotalinearcombination Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation114 ROW OPERATIONS of those columns to its left, and so forth.
Thus by Lemma 4.2.3 if a pivot column occurs as the jth column from the left, it follows that in the row reduced echelon form there will be one of the e as the jth column.
k There are three choices for row operations at each step in the above theorem.
A natural question is whether the same row reduced echelon matrix always results in the end from following the above algorithm applied in any way.
The next corollary says this is the case.
Deﬁnition 4.3.4 Two matrices are said to be row equivalent if one can be obtained from the other by a sequence of row operations.
Sinceeveryrowoperationcanbeobtainedbymultiplicationontheleftbyanelementary matrixandsinceeachoftheseelementarymatriceshasaninversewhichisalsoanelementary matrix,itfollowsthatrowequivalenceisasimilarityrelation.
Thusonecanclassifymatrices according to which similarity class they are in.
Later in the book, another more profound way of classifying matrices will be presented.
It has been shown above that every matrix is row equivalent to one which is in row reduced echelon form.
Note   x 1  .. =x e +···+x e .
1 1 n n x n sotosaytwocolumnvectorsareequalistosaytheyarethesamelinearcombinationofthe special vectors e .
j Corollary 4.3.5 The row reduced echelon form is unique.
That is if B,C are two matrices in row reduced echelon form and both are row equivalent to A, then B =C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix A.
Then theyclearlyhavethesamezerocolumnssincerowoperationsleavezerocolumnsunchanged.
IfB hasthesequencee ,e ,··· ,e occurringfortheﬁrsttimeinthepositions,i ,i ,··· ,i , 1 2 r 1 2 r the description of the row reduced echelon form means that each of these columns is not a linearcombinationoftheprecedingcolumns.
Therefore,byLemma4.2.3,thesameistrueof thecolumnsinpositionsi ,i ,··· ,i forC.
Itfollowsfromthedescriptionoftherowreduced 1 2 r echelon form, that e ,··· ,e occur respectively for the ﬁrst time in columns i ,i ,··· ,i 1 r 1 2 r for C. Thus B,C have the same columns in these positions.
By Lemma 4.2.3, the other columns in the two matrices are linear combinations, involving the same scalars, of the columns in the i ,··· ,i position.
Thus each column of B is identical to the corresponding 1 k column in C. (cid:4) The above corollary shows that you can determine whether two matrices are row equiv- alent by simply checking their row reduced echelon forms.
The matrices are row equivalent if and only if they have the same row reduced echelon form.
The following corollary follows.
Corollary 4.3.6 Let A be an m×n matrix and let R denote the row reduced echelon form obtained from A by row operations.
Then there exists a sequence of elementary matrices, E ,··· ,E such that 1 p (EpEp−1···E1)A=R.
Proof: This follows from the fact that row operations are equivalent to multiplication on the left by an elementary matrix.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.3.
THE ROW REDUCED ECHELON FORM 115 Corollary 4.3.7 Let A be an invertible n×n matrix.
Then A equals a ﬁnite product of elementary matrices.
Proof: Since A−1 is given to exist, it follows A must have rank n because by Theorem 3.3.18 det(A) ̸= 0 which says the determinant rank and hence the column rank of A is n and so the row reduced echelon form of A is I because the columns of A form a linearly independent set.
Therefore, by Corollary 4.3.6 there is a sequence of elementary matrices, E ,··· ,E such that 1 p (EpEp−1···E1)A=I.
But now multiply on the left on both sides by E−1 then by E−1 and then by E−1 etc.
p p−1 p−2 until you get A=E−1E−1···E−1 E−1 1 2 p−1 p and by Theorem 4.1.6 each of these in this product is an elementary matrix.
Corollary 4.3.8 The rank of a matrix equals the number of nonzero pivot columns.
Fur- thermore, every column is contained in the span of the pivot columns.
Proof: Write the row reduced echelon form for the matrix.
From Corollary 4.2.4 this row reduced matrix has the same rank as the original matrix.
Deleting all the zero rows and all the columns in the row reduced echelon form which do not correspond to a pivot column, yieldsan r×r identitysubmatrix inwhichr isthenumber ofpivotcolumns.
Thus the rank is at least r. From Lemma 4.2.3 every column of A is a linear combination of the pivot columns since this is true by deﬁnition for the row reduced echelon form.
Therefore, the rank is no more than r. (cid:4) Here is a fundamental observation related to the above.
Corollary 4.3.9 SupposeAisanm×nmatrixandthatm<n.Thatis,thenumberofrows is less than the number of columns.
Then one of the columns of A is a linear combination of the preceding columns of A.
Proof: Since m < n, not all the columns of A can be pivot columns.
That is, in the row reduced echelon form say e occurs for the ﬁrst time at r where r < r < ··· < r i i 1 2 p where p≤m.
It follows since m<n, there exists some column in the row reduced echelon form which is a linear combination of the preceding columns.
By Lemma 4.2.3 the same is true of the columns of A.
(cid:4) Deﬁnition 4.3.10 LetAbeanm×nmatrixhavingrank,r.ThenthenullityofAisdeﬁned to be n−r.
Also deﬁne ker(A)≡{x∈Fn :Ax=0}.
This is also denoted as N(A).
Observation 4.3.11 Note that ker(A) is a subspace because if a,b are scalars and x,y are vectors in ker(A), then A(ax+by)=aAx+bAy=0+0=0 Recall that the dimension of the column space of a matrix equals its rank and since the column space is just A(Fn), the rank is just the dimension of A(Fn).
The next theorem shows that the nullity equals the dimension of ker(A).
Theorem 4.3.12 Let A be an m×n matrix.
Then rank(A)+dim(ker(A))=n.. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation116 ROW OPERATIONS Proof: Since ker(A) is a subspace, there exists a basis for ker(A),{x ,··· ,x }.
Also 1 k let {Ay ,··· ,Ay } be a basis for A(Fn).
Let u ∈ Fn.
Then there exist unique scalars c 1 l i such that ∑l Au= c Ay i i i=1 It follows that ( ) ∑l A u− c y =0 i i i=1 and so the vector in parenthesis is in ker(A).
Thus there exist unique b such that j ∑l ∑k u= c y + b x i i j j i=1 j=1 Since u was arbitrary, this shows {x ,··· ,x ,y ,··· ,y } spans Fn.
If these vectors are 1 k 1 l independent,thentheywillformabasisandtheclaimedequationwillbeobtained.
Suppose then that ∑l ∑k c y + b x =0 i i j j i=1 j=1 Apply A to both sides.
This yields ∑l c Ay =0 i i i=1 and so each c =0.
Then the independence of the x imply each b =0.
(cid:4) i j j 4.4 Rank And Existence Of Solutions To Linear Sys- tems Consider the linear system of equations, Ax=b (4.4) where A is an m×n matrix, x is a n×1 column vector, and b is an m×1 column vector.
Suppose ( ) A= a ··· a 1 n where the a denote the columns of A.
Then x=(x ,··· ,x )T is a solution of the system k 1 n (4.4), if and only if x a +···+x a =b 1 1 n n which says that b is a vector in span(a ,··· ,a ).
This shows that there exists a solution 1 n to the system, (4.4) if and only if b is contained in span(a ,··· ,a ).
In words, there is a 1 n solution to (4.4) if and only if b is in the column space of A.
In terms of rank, the following proposition describes the situation.
Proposition 4.4.1 Let A be an m×n matrix and let b be an m×1 column vector.
Then there exists a solution to (4.4) if and only if ( ) rank A | b =rank(A).
(4.5) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.5.
FREDHOLM ALTERNATIVE 117 ( ) Proof: Place A | b and A in row reduced echelon form, respectively B and C. If the above condition on rank is true, then both B and C have the same number of nonzero rows.
In particular, you cannot have a row of the form ( ) 0 ··· 0 ⋆ where ⋆̸=0 in B.
Therefore, there will exist a solution to the system (4.4).
Conversely,supposethereexistsasolution.
ThismeanstherecannotbesucharowinB described above.
Therefore, B and C must have the same number of zero rows and so they have the same number of nonzero rows.
Therefore, the rank of the two matrices in (4.5) is the same.
(cid:4) 4.5 Fredholm Alternative There is a very useful version of Proposition 4.4.1 known as the Fredholm alternative.
I will only present this for the case of real matrices here.
Later a much more elegant and general approach is presented which allows for the general case of complex matrices.
The following deﬁnition is used to state the Fredholm alternative.
Deﬁnition 4.5.1 LetS ⊆Rm.ThenS⊥ ≡{z∈Rm :z·s=0 for every s∈S}.Thefunny exponent, ⊥ is called “perp”.
Now note { } ( ) { } ∑m ker AT ≡ z:ATz=0 = z: z a =0 k k k=1 Lemma 4.5.2 Let A be a real m×n matrix, let x∈Rn and y∈Rm.
Then ( ) (Ax·y)= x·ATy Proof: This follows right away from the deﬁnition of the inner product and matrix multiplication.
∑ ∑( ) ( ) (Ax·y)= A x y = AT x y = x·ATy .
(cid:4) kl l k lk l k k,l k,l NowitistimetostatetheFredholmalternative.
Theﬁrstversionofthisisthefollowing theorem.
Theorem 4.5.3 Let A be a real m×n matrix and let b∈Rm.
There exists a solution, x ( ) ⊥ to the equation Ax=b if and only if b∈ker AT .
( ) ⊥ Proof: First suppose b ∈ ker AT .
Then this says that if ATx=0, it follows that b·x=0.
In other words, taking the transpose, if xTA=0,then xTb=0.
Thus,ifP isaproductofelementarymatricessuchthatPAisinrowreducedechelonform, thenifPAhasaro(wofzeros,i)nthekth position,thenthereisalsoazerointhekth position of Pb.
Thus rank A | b = rank(A).By Proposition 4.4.1, there exists a solution, x to the system A(x=)b.
It remains to go the other direction.
Let z∈ ker AT and suppose Ax=b.
I need to verify b·z=0.
By Lemma 4.5.2, b·z=Ax·z=x·ATz=x·0=0 (cid:4) This implies the following corollary which is also called the Fredholm alternative.
The “alternative” becomes more clear in this corollary.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation118 ROW OPERATIONS Corollary 4.5.4 Let A be an m×n matrix.
Then A maps Rn onto Rm if and only if the only solution to ATx=0 is x=0.
( ) ( ) ⊥ Proof: IftheonlysolutiontoATx=0isx=0,thenker AT ={0}andsoker AT = Rm because every b ∈ Rm has the property that b·0 = 0.
Therefore, Ax=b has a solu- ( ) ⊥ tion for any b ∈ Rm because the b for which there is a solution are those in ker AT by Theorem 4.5.3.
In other words, A maps Rn onto Rm.
( ) ⊥ Conversely if A is onto, then by Theorem 4.5.3 every b ∈ Rm is in ker AT and so if ATx=0, then b·x=0 for every b.
In particular, this holds for b=x.
Hence if ATx=0, then x=0.
(cid:4) Here is an amusing example.
Example 4.5.5 Let A be an m×n matrix in which m>n.
Then A cannot map onto Rm.
ThereasonforthisisthatAT isann×mwherem>nandsointheaugmentedmatrix ( ) AT|0 theremustbesomefreevariables.
ThusthereexistsanonzerovectorxsuchthatATx=0.
4.6 Exercises 1.
Let {u ,··· ,u } be vectors in Rn.
The parallelepiped determined by these vectors 1 n P (u ,··· ,u ) is deﬁned as 1 n { } ∑n P (u ,··· ,u )≡ t u :t ∈[0,1] for all k .
1 n k k k k=1 Now let A be an n×n matrix.
Show that {Ax:x∈P (u ,··· ,u )} 1 n is also a parallelepiped.
2.
InthecontextofProblem1,drawP (e ,e )wheree ,e arethestandardbasisvectors 1 2 1 2 for R2.
Thus e =(1,0),e =(0,1).
Now suppose 1 2 ( ) 1 1 E = 0 1 where E is the elementary matrix which takes the third row and adds to the ﬁrst.
Draw {Ex:x∈P (e ,e )}.
1 2 In other words, draw the result of doing E to the vectors in P (e ,e ).
Next draw the 1 2 results of doing the other elementary matrices to P (e ,e ).
1 2 3.
In the context of Problem 1, either draw or describe the result of doing elementary matrices to P (e ,e ,e ).
Describe geometrically the conclusion of Corollary 4.3.7.
1 2 3 4.
Considerapermutationof{1,2,··· ,n}.
Thisisanorderedlistofnumberstakenfrom thislistwithnorepeats,{i ,i ,··· ,i }.
DeﬁnethepermutationmatrixP (i ,i ,··· ,i ) 1 2 n 1 2 n as the matrix which is obtained from the identity matrix by placing the jth column of I as the ith column of P (i ,i ,··· ,i ).
Thus the 1 in the ith column of this per- j 1 2 n j mutation matrix occurs in the jth slot.
What does this permutation matrix do to the column vector (1,2,··· ,n)T?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.6.
EXERCISES 119 5.
↑Consider the 3×3 permutation matrices.
List all of them and then determine the dimension of their span.
Recall that you can consider an m×n matrix as something in Fnm.
6.
Determine which matrices are in row reduced echelon form.
( ) 1 2 0 (a) 0 1 7   1 0 0 0   (b) 0 0 1 2 0 0 0 0   1 1 0 0 0 5   (c) 0 0 1 2 0 4 0 0 0 0 1 3 7.
Row reduce the following matrices to obtain the row reduced echelon form.
List the pivot columns in the original matrix.
  1 2 0 3   (a) 2 1 2 2 1 1 0 3   1 2 3  2 1 −2  (b)   3 0 0 3 2 1   1 2 1 3 (c)  −3 2 1 0  3 2 1 1 8.
Findtherankandnullityofthefollowingmatrices.
Iftherankisr,identifyr columns in the original matrix which have the property that every other column may be written as a linear combination of these.
  0 1 0 2 1 2 2    0 3 2 12 1 6 8  (a)   0 1 1 5 0 2 3 0 2 1 7 0 3 4   0 1 0 2 0 1 0    0 3 2 6 0 5 4  (b)   0 1 1 2 0 2 2 0 2 1 4 0 3 2   0 1 0 2 1 1 2    0 3 2 6 1 5 1  (c)   0 1 1 2 0 2 1 0 2 1 4 0 3 1 9.
Find the rank of the following matrices.
If the rank is r, identify r columns in the original matrix which have the property that every other column may be written as a linear combination of these.
Also ﬁnd a basis for the row and column spaces of the matrices.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation120 ROW OPERATIONS   1 2 0    3 2 1  (a)   2 1 0 0 2 1   1 0 0    4 1 1  (b)   2 1 0 0 2 0   0 1 0 2 1 2 2    0 3 2 12 1 6 8  (c)   0 1 1 5 0 2 3 0 2 1 7 0 3 4   0 1 0 2 0 1 0    0 3 2 6 0 5 4  (d)   0 1 1 2 0 2 2 0 2 1 4 0 3 2   0 1 0 2 1 1 2    0 3 2 6 1 5 1  (e)   0 1 1 2 0 2 1 0 2 1 4 0 3 1 10.
Suppose A is an m×n matrix.
Explain why the rank of A is always no larger than min(m,n).
11.
SupposeAisanm×nmatrixinwhichm≤n.SupposealsothattherankofAequals m. Show that A maps Fn onto Fm.
Hint: The vectors e ,··· ,e occur as columns 1 m in the row reduced echelon form for A.
12.
Suppose A is an m×n matrix and that m > n. Show there exists b ∈ Fm such that there is no solution to the equation Ax=b.
13.
Suppose A is an m×n matrix in which m ≥ n. Suppose also that the rank of A equals n. Show that A is one to one.
Hint: If not, there exists a vector, x̸=0 such that Ax=0, and this implies at least one column of A is a linear combination of the others.
Show this would require the column rank to be less than n. 14.
Explain why an n×n matrix A is both one to one and onto if and only if its rank is n. 15.
Suppose A is an m × n matrix and {w ,··· ,w } is a linearly independent set of 1 k vectors in A(Fn) ⊆ Fm.
Suppose also that Az = w .
Show that {z ,··· ,z } is also i i 1 k linearly independent.
16.
Show rank(A+B)≤rank(A)+rank(B).
17.
Suppose A is an m×n matrix, m ≥ n and the columns of A are independent.
Sup- pose also that {z ,··· ,z } is a linearly independent set of vectors in Fn.
Show that 1 k {Az ,··· ,Az } is linearly independent.
1 k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation4.6.
EXERCISES 121 18.
Suppose A is an m×n matrix and B is an n×p matrix.
Show that dim(ker(AB))≤dim(ker(A))+dim(ker(B)).
Hint: Consider the subspace, B(Fp)∩ker(A) and suppose a basis for this subspace is {w ,··· ,w }.
Now suppose {u ,··· ,u } is a basis for ker(B).
Let {z ,··· ,z } 1 k 1 r 1 k be such that Bz =w and argue that i i ker(AB)⊆span(u ,··· ,u ,z ,··· ,z ).
1 r 1 k 19.
Let m<n and let A be an m×n matrix.
Show that A is not one to one.
20.
Let A be an m×n real matrix and let b∈Rm.
Show there exists a solution, x to the system ATAx=ATb Next show that if x,x are two solutions, then Ax = Ax .
Hint: First show that ( ) 1 ( ) 1 ATA T =ATA.
Next show if x∈ker ATA , then Ax=0.
Finally apply the Fred- holm alternative.
Show ATb∈ker(ATA)⊥.
This will give existence of a solution.
21.
ShowthatinthecontextofProblem20thatifxisthesolutionthere,then|b−Ax|≤ |b−Ay| for every y.
Thus Ax is the point of A(Rn) which is closest to b of every point in A(Rn).
This is a solution to the least squares problem.
    1 0     22.
↑HereisapointinR4 :(1,2,3,4)T .Findthepointinspan 0 , 1 which 2 3 3 2 is closest to the given point.
23.
↑HereisapointinR4 :(1,2,3,4)T .Findthepointontheplanedescribedbyx+2y− 4z+4w =0 which is closest to the given point.
24.
Suppose A,B are two invertible n×n matrices.
Show there exists a sequence of row operations which when done to A yield B.
Hint: Recall that every invertible matrix is a product of elementary matrices.
25.
If A is invertible and n×n and B is n×p, show that AB has the same null space as B and also the same rank as B.
26.
Here are two matrices in row reduced echelon form     1 0 1 1 0 0     A= 0 1 1 , B = 0 1 1 0 0 0 0 0 0 Does there exist a sequence of row operations which when done to A will yield B?
Explain.
27.
Is it true that an upper triagular matrix has rank equal to the number of nonzero entries down the main diagonal?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation122 ROW OPERATIONS 28.
Let{v1,··· ,vn−1}bevectorsinFn.
Describeasystematicwaytoobtainavectorvn which is perpendicular to each of these vectors.
Hint: You might consider something like this   e e ··· e 1 2 n  v11 v12 ··· v1n  det .
.
.
  .
.
.
 .
.
.
v(n−1)1 v(n−1)2 ··· v(n−1)n where v is the jth entry of the vector v .
This is a lot like the cross product.
ij i 29.
Let A be an m×n matrix.
Then ker(A) is a subspace of Fn.
Is it true that every subspace of Fn is the kernel or null space of some matrix?
Prove or disprove.
30.
LetAbeann×nmatrixandletPij bethepermutationmatrixwhichswitchestheith and jth rows of the identity.
Show that PijAPij produces a matrix which is similar to A which switches the ith and jth entries on the main diagonal.
31.
RecalltheprocedureforﬁndingtheinverseofamatrixonPage48.
Itwasshownthat the procedure, when it works, ﬁnds the inverse of the matrix.
Show that whenever the matrix has an inverse, the procedure works.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationSome Factorizations 5.1 LU Factorization An LU factorization of a matrix involves writing the given matrix as the product of a lower triangular matrix which has the main diagonal consisting entirely of ones, L, and an upper triangular matrix U in the indicated order.
The L goes with “lower” and the U with “upper”.
It turns out many matrices can be written in this way and when this is possible, peoplegetexcitedaboutslickwaysofsolvingthesystemofequations,Ax=y.Themethod lacks generality but is of interest just the same.
( ) 0 1 Example 5.1.1 Can you write in the form LU as just described?
1 0 To do so you would need ( )( ) ( ) ( ) 1 0 a b a b 0 1 = = .
x 1 0 c xa xb+c 1 0 Therefore, b = 1 and a = 0.
Also, from the bottom rows, xa = 1 which can’t happen and havea=0.Therefore,youcan’twritethismatrixintheformLU.IthasnoLU factorization.
This is what I mean above by saying the method lacks generality.
Which matrices have an LU factorization?
It turns out it is those whose row reduced echelonformcanbeachievedwithoutswitchingrowsandwhichonlyinvolverowoperations of type 3 in which row j is replaced with a multiple of row i added to row j for i<j.
5.2 Finding An LU Factorization There is a convenient procedure for ﬁnding an LU factorization.
It turns out that it is only necessary to keep track of the multipliers which are used to row reduce to upper triangular form.
This procedure is described in the following examples and is called the multiplier method.
It is due to Dolittle.
  1 2 3 Example 5.2.1 Find an LU factorization for A= 2 1 −4  1 5 2 Write the matrix next to the identity matrix as shown.
   1 0 0 1 2 3  0 1 0  2 1 −4 .
0 0 1 1 5 2 123 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation124 SOME FACTORIZATIONS The process involves doing row operations to the matrix on the right while simultaneously updatingsuccessivecolumnsofthematrixontheleft.
Firsttake−2timestheﬁrstrowand add to the second in the matrix on the right.
   1 0 0 1 2 3  2 1 0  0 −3 −10  0 0 1 1 5 2 Note the method for updating the matrix on the left.
The 2 in the second entry of the ﬁrst column is there because −2 times the ﬁrst row of A added to the second row of A produced a 0.
Now replace the third row in the matrix on the right by −1 times the ﬁrst row added to the third.
Thus the next step is    1 0 0 1 2 3  2 1 0  0 −3 −10  1 0 1 0 3 −1 Finally, add the second row to the bottom row and make the following changes    1 0 0 1 2 3  2 1 0  0 −3 −10 .
1 −1 1 0 0 −11 Atthispoint,stopbecausethematrixontherightisuppertriangular.
AnLU factorization is the above.
The justiﬁcation for this gimmick will be given later.
  1 2 1 2 1    2 0 2 1 1  Example 5.2.2 Find an LU factorization for A= .
2 3 1 3 2 1 0 1 1 2 This time everything is done at once for a whole column.
This saves trouble.
First multiply the ﬁrst row by (−1) and then add to the last row.
Next take (−2) times the ﬁrst and add to the second and then (−2) times the ﬁrst and add to the third.
   1 0 0 0 1 2 1 2 1  2 1 0 0  0 −4 0 −3 −1   2 0 1 0  0 −1 −1 −1 0 .
1 0 0 1 0 −2 0 −1 1 This ﬁnishes the ﬁrst column of L and the ﬁrst column of U.
Now take −(1/4) times the second row in the matrix on the right and add to the third followed by −(1/2) times the second added to the last.
   1 0 0 0 1 2 1 2 1  2 1 0 0  0 −4 0 −3 −1   2 1/4 1 0  0 0 −1 −1/4 1/4  1 1/2 0 1 0 0 0 1/2 3/2 This ﬁnishes the second column of L as well as the second column of U.
Since the matrix on the right is upper triangular, stop.
The LU factorization has now been obtained.
This technique is called Dolittle’s method.
II This process is entirely typical of the general case.
The matrix U is just the ﬁrst upper triangular matrix you come to in your quest for the row reduced echelon form using only Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation5.3.
SOLVING LINEAR SYSTEMS USING AN LU FACTORIZATION 125 the row operation which involves replacing a row by itself added to a multiple of another row.
The matrix L is what you get by updating the identity matrix as illustrated above.
You should note that for a square matrix, the number of row operations necessary to reduce to LU form is about half the number needed to place the matrix in row reduced echelonform.
ThisiswhyanLU factorizationisofinterestinsolvingsystemsofequations.
5.3 Solving Linear Systems Using An LU Factorization ThereasonpeoplecareabouttheLU factorizationisitallowsthequicksolutionofsystems of equations.
Here is an example.
    x 1 2 3 2     y  Example 5.3.1 Suppose you want to ﬁnd the solutions to 4 3 1 1   = z 1 2 3 0 w   1   2 .
3 Of course one way is to write the augmented matrix and grind away.
However, this involves more row operations than the computation of an LU factorization and it turns out thatanLU factorizationcangivethesolutionquickly.
Hereishow.
ThefollowingisanLU factorization for the matrix.
     1 2 3 2 1 0 0 1 2 3 2  4 3 1 1 = 4 1 0  0 −5 −11 −7 .
1 2 3 0 1 0 1 0 0 0 −2 Let Ux=y and consider Ly=b where in this case, b=(1,2,3)T. Thus      1 0 0 y 1 1      4 1 0 y = 2 2 1 0 1 y 3 3   1 which yields very quickly that y=  −2 .
Now you can ﬁnd x by solving Ux=y.
Thus 2 in this case,       x 1 2 3 2   1  0 −5 −11 −7  y = −2  z 0 0 0 −2 2 w which yields   −3 + 7t x= 955− 1515t ,t∈R.
t −1 Work this out by hand and you will see the advantage of working only with triangular matrices.
It may seem like a trivial thing but it is used because it cuts down on the number of operations involved in ﬁnding a solution to a system of equations enough that it makes a diﬀerence for large systems.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation126 SOME FACTORIZATIONS 5.4 The PLU Factorization As indicated above, some matrices don’t have an LU factorization.
Here is an example.
  1 2 3 2   M = 1 2 3 0 (5.1) 4 3 1 1 In this case, there is another factorization which is useful called a PLU factorization.
Here P is a permutation matrix.
Example 5.4.1 Find a PLU factorization for the above matrix in (5.1).
Proceed as before trying to ﬁnd the row echelon form of the matrix.
First add −1 times the ﬁrst row to the second row and then add −4 times the ﬁrst to the third.
This yields    1 0 0 1 2 3 2  1 1 0  0 0 0 −2  4 0 1 0 −5 −11 −7 There is no way to do only row operations involving replacing a row with itself added to a multipleofanotherrowtothesecondmatrixinsuchawayastoobtainanuppertriangular matrix.
Therefore, consider M with the bottom two rows switched.
  1 2 3 2 M′ = 4 3 1 1 .
1 2 3 0 Now try again with this matrix.
First take −1 times the ﬁrst row and add to the bottom row and then take −4 times the ﬁrst row and add to the second row.
This yields    1 0 0 1 2 3 2  4 1 0  0 −5 −11 −7  1 0 1 0 0 0 −2 The second matrix is upper triangular and so the LU factorization of the matrix M′ is    1 0 0 1 2 3 2  4 1 0  0 −5 −11 −7 .
1 0 1 0 0 0 −2 Thus M′ =PM =LU where L and U are given above.
Therefore, M =P2M =PLU and so       1 2 3 2 1 0 0 1 0 0 1 2 3 2  1 2 3 0 = 0 0 1  4 1 0  0 −5 −11 −7  4 3 1 1 0 1 0 1 0 1 0 0 0 −2 This process can always be followed and so there always exists a PLU factorization of a given matrix even though there isn’t always an LU factorization.
  1 2 3 2 Example 5.4.2 Use a PLU factorization of M ≡  1 2 3 0  to solve the system 4 3 1 1 Mx=b where b=(1,2,3)T .
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation5.5.
JUSTIFICATION FOR THE MULTIPLIER METHOD 127 Let Ux=y and consider PLy=b.
In other words, solve,       1 0 0 1 0 0 y 1 1       0 0 1 4 1 0 y = 2 .
2 0 1 0 1 0 1 y 3 3 Then multiplying both sides by P gives      1 0 0 y 1 1      4 1 0 y = 3 2 1 0 1 y 2 3 and so     y 1 1 y= y = −1 .
2 y 1 3 Now Ux=y and so it only remains to solve       x 1 2 3 2  1  1  0 −5 −11 −7  x2 = −1  x 0 0 0 −2 3 1 x 4 which yields     x 1 + 7t  x12 = 1950 − 5151t :t∈R.
x t 3 x −1 4 2 5.5 Justiﬁcation For The Multiplier Method Why does the multiplier method work for ﬁnding an LU factorization?
Suppose A is a matrix which has the property that the row reduced echelon form for A may be achieved using only the row operations which involve replacing a row with itself added to a multiple of another row.
It is not ever necessary to switch rows.
Thus every row which is replaced using this row operation in obtaining the echelon form may be modiﬁed by using a row which is above it.
Furthermore, in the multiplier method for ﬁnding the LU factorization, we zero out the elements below the pivot entry in ﬁrst column and then the next and so on whenscanningfromtheleft.
Intermsofelementarymatrices,thismeanstherowoperations used to reduce A to upper triangular form correspond to multiplication on the left by lower triangular matrices having all ones down the main diagonal and the sequence of elementary matrices which row reduces A has the property that in scanning the list of elementary matrices from the right to the left, this list consists of several matrices which involve only changes from the identity in the ﬁrst column, then several which involve only changes from the identity in the second column and so forth.
More precisely, E ···E A = U where U p 1 is upper triangular, each E is a lower triangular elementary matrix having all ones down i the main diagonal, for some r , each of E ···E diﬀers from the identity only in the ﬁrst i r1 1 column, each of E ···E diﬀers from the identity only in the second column and so r2 r1+1 z Wil}l|beL { forth.
Therefore, A = E−1···E−1 E−1U.
You multiply the inverses in the reverse order.
1 p−1 p Now each of the E−1 is also lower triangular with 1 down the main diagonal.
Therefore i their product has this property.
Recall also that if E equals the identity matrix except i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation128 SOME FACTORIZATIONS for having an a in the jth column somewhere below the main diagonal, E−1 is obtained by i replacing the a in E with −a, thus explaining why we replace with −1 times the multiplier i in computing L. In the case where A is a 3×m matrix, E−1···E−1 E−1 is of the form 1 p−1 p       1 0 0 1 0 0 1 0 0 1 0 0       a 1 0 0 1 0 0 1 0 = a 1 0 .
0 0 1 b 0 1 0 c 1 b c 1 Note that scanning from left to right, the ﬁrst two in the product involve changes in the identity only in the ﬁrst column while in the third matrix, the change is only in the second.
If the entries in the ﬁrst column had been zeroed out in a diﬀerent order, the following would have resulted.
      1 0 0 1 0 0 1 0 0 1 0 0       0 1 0 a 1 0 0 1 0 = a 1 0 b 0 1 0 0 1 0 c 1 b c 1 However, it is important to be working from the left to the right, one column at a time.
Asimilarobservationholdsinanydimension.
Multiplyingtheelementarymatriceswhich involve a change only in the jth column you obtain A equal to an upper triangular, n×m matrix U which is multiplied by a sequence of lower triangular matrices on its left which is of the following form, in which the a are negatives of multipliers used in row reducing to ij an upper triangular matrix.
     1 0 ··· 0 1 0 ··· 0 1 0 ··· 0  .
 .
  .
  .
 .
  .
  a11 1 .
 0 1 .
··· 0 1 .
  ... ... 0  ... ... ... 0   ... ... 0  a1,n−1 0 ··· 1 0 a2,n−2 ··· 1 0 ··· an,n−1 1 From the matrix multiplication, this product equals   1    a11 1   ... ...  a1,n−1 ··· an,n−1 1 Notice how the end result of the matrix multiplication made no change in the a .
It just ij ﬁlledintheemptyspaceswiththea whichoccurredinoneofthematricesintheproduct.
ij This is why, in computing L, it is suﬃcient to begin with the left column and work column bycolumntowardtheright,replacingentrieswiththenegativeofthemultiplierusedinthe row operation which produces a zero in that entry.
5.6 Existence For The PLU Factorization Here I will consider an invertible n×n matrix and show that such a matrix always has a PLU factorization.
More general matrices could also be considered but this is all I will present.
Let A be such an invertible matrix and consider the ﬁrst column of A.
If A ̸= 0, use 11 this to zero out everything below it.
The entry A is called the pivot.
Thus in this case 11 there is a lower triangular matrix L which has all ones on the diagonal such that 1 ( ) ∗ ∗ L P A= (5.2) 1 1 0 A 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation5.6.
EXISTENCE FOR THE PLU FACTORIZATION 129 Here P = I.
In case A = 0, let r be such that A ̸=0 and r is the ﬁrst entry for which 1 11 r1 this happens.
In this case, let P be the permutation matrix which switches the ﬁrst row 1 and the rth row.
Then as before, there exists a lower triangular matrix L which has all 1 ones on the diagonal such that (5.2) holds in this case also.
In the ﬁrst column, this L has 1 zeros between the ﬁrst row and the rth row.
GotoA .Followingthesameprocedureasabove, thereexistsalowertriangularmatrix 1 and permutation matrix L′,P′ such that 2 2 ( ) ∗ ∗ ′ ′ L P A = 2 2 1 0 A 2 Let ( ) ( ) 1 0 1 0 L = , P = 2 0 L′ 2 0 P′ 2 2 Then using block multiplication, Theorem 3.5.2, ( )( )( ) 1 0 1 0 ∗ ∗ = 0 L′ 0 P′ 0 A 2 2 1 ( )( ) ( ) 1 0 ∗ ∗ ∗ ∗ = = 0 L′ 0 P′A 0 L′P′A 2 2 1 2 2 1   ∗ ··· ∗  0 ∗ ∗ =L P L P A 2 2 1 1 0 0 A 2 and L has all the subdiagonal entries equal to 0 except possibly some nonzero entries in 2 the second column starting with position r where P switches rows r and 2.
Continuing 2 2 2 thisway, itfollowstherearelowertriangularmatricesL havingallonesdownthediagonal j and permutation matrices P which switch only two rows such that i Ln−1Pn−1Ln−2Pn−2Ln−3···L2P2L1P1A=U (5.3) where U is upper triangular.
The matrix L has all zeros below the main diagonal except j for the jth column and even in this column it has zeros between position j and r where P j j switchesrowsj andr .
Ofcourseinthecasewherenoswitchingisnecessary, youcouldget j all nonzero entries below the main diagonal in the jth column for L .
j The fact that L is the identity except for the jth column means that each P for k >j j k almost commutes with L .
Say P switches the kth and the qth rows for q ≥ k > j.
When j k you place P on the right of L it just switches the kth and the qth columns and leaves the k j jth column unchanged.
Therefore, the same result as placing P on the left of L can be k j obtainedbyplacingP ontherightofL andmodifyingL byswitchingthekth andtheqth k j j entriesinthejth column.
(Notethiscouldpossiblyinterchangea0forsomethingnonzero.)
It follows from (5.3) there exists P, the product of permutation matrices, P = Pn−1···P1 each of which switches two rows, and L a lower triangular matrix having all ones on the maindiagonal, L=L′ ···L′L′,wheretheL′ areobtainedasjustdescribedbymovinga n−1 2 1 j succession of P from the left to the right of L and modifying the jth column as indicated, k j such that LPA=U.
Then A=PTL−1U Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation130 SOME FACTORIZATIONS It is customary to write this more simply as A=PLU whereLisanuppertriangularmatrixhavingallonesonthediagonalandP isapermutation matrix consisting of P1···Pn−1 as described above.
This proves the following theorem.
Theorem 5.6.1 Let A be any invertible n×n matrix.
Then there exists a permutation matrix P and a lower triangular matrix L having all ones on the main diagonal and an upper triangular matrix U such that A=PLU 5.7 The QR Factorization As pointed out above, the LU factorization is not a mathematically respectable thing be- cause it does not always exist.
There is another factorization which does always exist.
Much more can be said about it than I will sayhere.
At this time, I will only deal with real matrices and so the inner product will be the usual real dot product.
Deﬁnition 5.7.1 An n×n real matrix Q is called an orthogonal matrix if QQT =QTQ=I.
Thus an orthogonal matrix is one whose inverse is equal to its transpose.
First note that if a matrix is orthogonal this says ∑ ∑ QTQ = Q Q =δ ij jk ji jk ik j j Thus   2 ∑ ∑ ∑∑∑ |Qx|2 =  Q x  = Q x Q x ij j is s ir r i j i r s ∑∑∑ ∑∑∑ = Q Q x x = Q Q x x is ir s r is ir s r i r s r s i ∑∑ ∑ = δ x x = x2 =|x|2 sr s r r r s r This shows that orthogonal transformations preserve distances.
You can show that if you have a matrix which does preserve distances, then it must be orthogonal also.
Example 5.7.2 One of the most important examples of an orthogonal matrix is the so called Householder matrix.
You have v a unit vector and you form the matrix I−2vvT This is an orthogonal matrix which is also symmetric.
To see this, you use the rules of matrix operations.
( ) ( ) I−2vvT T = IT − 2vvT T = I−2vvT Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation5.7.
THE QR FACTORIZATION 131 so it is symmetric.
Now to show it is orthogonal, ( )( ) I−2vvT I−2vvT = I−2vvT −2vvT +4vvTvvT = I−4vvT +4vvT =I because vTv=v·v=|v|2 =1.
Therefore, this is an example of an orthogonal matrix.
Consider the following problem.
Problem 5.7.3 Given two vectors x,y such that |x|=|y|̸=0 but x̸=y and you want an orthogonal matrix Q such that Qx=y and Qy=x.
The thing which works is the House- holder matrix x−y Q≡I−2 (x−y)T |x−y|2 Here is why this works.
x−y Q(x−y) = (x−y)−2 (x−y)T (x−y) |x−y|2 x−y = (x−y)−2 |x−y|2 =y−x |x−y|2 x−y Q(x+y) = (x+y)−2 (x−y)T (x+y) |x−y|2 x−y = (x+y)−2 ((x−y)·(x+y)) |x−y|2 ( ) x−y = (x+y)−2 |x|2−|y|2 =x+y |x−y|2 Hence Qx+Qy = x+y Qx−Qy = y−x Adding these equations, 2Qx=2y and subtracting them yields 2Qy=2x.
A picture of the geometric signiﬁcance follows.
x y The orthogonal matrix Q reﬂects across the dotted line taking x to y and y to x. Deﬁnition 5.7.4 Let A be an m×n matrix.
Then a QR factorization of A consists of two matrices, Q orthogonal and R upper triangular (right triangular) having all the entries on the main diagonal nonnegative such that A=QR.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation132 SOME FACTORIZATIONS With the solution to this simple problem, here is how to obtain a QR factorization for any matrix A.
Let A=(a ,a ,··· ,a ) 1 2 n where the a are the columns.
If a =0, let Q =I.
If a ̸=0, let i 1 1 1   |a | 1    0  b≡ .
  .
 .
0 and form the Householder matrix (a −b) Q ≡I−2 1 (a −b)T 1 |a −b|2 1 1 As in the above problem Q a =b and so 1 1 ( ) |a | ∗ Q A= 1 1 0 A 2 whereA isam−1×n−1matrix.
Nowﬁndinthesamewayaswasjustdoneam−1×m−1 2 b matrix Q2 such that ( ) ∗ ∗ b Q A = 2 2 0 A 3 Let ( ) 1 0 Q2 ≡ 0 Qb .
2 Then ( )( ) 1 0 |a | ∗ Q2Q1A= 0 Qb 01 A 2 2   |a | ∗ ∗ 1  .
 = .. ∗ ∗  0 0 A 3 Continuing this way until the result is upper triangular, you get a sequence of orthogonal matrices QpQp−1···Q1 such that QpQp−1···Q1A=R (5.4) where R is upper triangular.
Now if Q and Q are orthogonal, then from properties of matrix multiplication, 1 2 Q Q (Q Q )T =Q Q QTQT =Q IQT =I 1 2 1 2 1 2 2 1 1 1 and similarly (Q Q )T Q Q =I.
1 2 1 2 Thustheproductoforthogonalmatricesisorthogonal.
Alsothetransposeofanorthogonal matrix is orthogonal directly from the deﬁnition.
Therefore, from (5.4) A=(QpQp−1···Q1)T R≡QR.
This proves the following theorem.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation5.8.
EXERCISES 133 Theorem 5.7.5 Let A be any real m×n matrix.
Then there exists an orthogonal matrix Q and an upper triangular matrix R having nonnegative entries on the main diagonal such that A=QR and this factorization can be accomplished in a systematic manner.
I I 5.8 Exercises   1 2 0   1.
Find a LU factorization of 2 1 3 .
1 2 3   1 2 3 2   2.
Find a LU factorization of 1 3 2 1 .
5 0 1 3   1 2 1   3.
Find a PLU factorization of 1 2 2 .
2 1 1   1 2 1 2 1   4.
Find a PLU factorization of 2 4 2 4 1 .
1 2 1 3 2   1 2 1    1 2 2  5.
Find a PLU factorization of  .
2 4 1 3 2 1 6.
Is there only one LU factorization for a given matrix?
Hint: Consider the equation ( ) ( )( ) 0 1 1 0 0 1 = .
0 1 1 1 0 0 7.
Here is a matrix and an LU factorization of it.
     1 2 5 0 1 0 0 1 2 5 0 A= 1 1 4 9 = 1 1 0  0 −1 −1 9  0 1 2 5 0 −1 1 0 0 1 14 Use this factorization to solve the system of equations   1   Ax= 2 3 8.
Find a QR factorization for the matrix   1 2 1  3 −2 1  1 0 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation134 SOME FACTORIZATIONS 9.
Find a QR factorization for the matrix   1 2 1 0   3 0 1 1 1 0 2 1 10.
If you had a QR factorization, A = QR, describe how you could use it to solve the equation Ax=b.
11.
If Q is an orthogonal matrix, show the columns are an orthonormal set.
That is show that for ( ) Q= q ··· q 1 n it follows that q ·q =δ .
Also show that any orthonormal set of vectors is linearly i j ij independent.
12.
Show you can’t expect uniqueness for QR factorizations.
Consider   0 0 0   0 0 1 0 0 1 and verify this equals   √  √0 1 √0 0 0 2  1√2 0 1 √2  0 0 0  2 2 1 2 0 −1 2 0 0 0 2 2 and also    1 0 0 0 0 0    0 1 0 0 0 1 .
0 0 1 0 0 1 Using Deﬁnition 5.7.4, can it be concluded that if A is an invertible matrix it will follow there is only one QR factorization?
13.
Suppose {a ,··· ,a } are linearly independent vectors in Rn and let 1 n ( ) A= a ··· a 1 n Form a QR factorization for A.
  r r ··· r 11 12 1n ( ) ( ) 0 r22 ··· r2n  a1 ··· an = q1 ··· qn  ... ...  0 0 ··· r nn Show that for each k ≤n, span(a ,··· ,a )=span(q ,··· ,q ) 1 k 1 k Prove that every subspace of Rn has an orthonormal basis.
The procedure just de- scribed is similar to the Gram Schmidt procedure which will be presented later.
14.
Suppose Q R converges to an orthogonal matrix Q where Q is orthogonal and R n n n n is upper triangular having all positive entries on the diagonal.
Show that then Q n converges to Q and R converges to the identity.
n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationLinear Programming 6.1 Simple Geometric Considerations One of the most important uses of row operations is in solving linear program problems which involve maximizing a linear function subject to inequality constraints determined from linear equations.
Here is an example.
A certain hamburger store has 9000 hamburger patties to use in one week and a limitless supply of special sauce, lettuce, tomatoes, onions, andbuns.
Theyselltwotypesofhamburgers,thebigstackandthebasicburger.
Ithasalso been determined that the employees cannot prepare more than 9000 of either type in one week.
The big stack, popular with the teenagers from the local high school, involves two patties, lots of delicious sauce, condiments galore, and a divider between the two patties.
The basic burger, very popular with children, involves only one patty and some pickles and ketchup.
Demand for the basic burger is twice what it is for the big stack.
What is the maximum number of hamburgers which could be sold in one week given the above limitations?
Letxbethenumberofbasicburgersandy thenumberofbigstackswhichcouldbesold in a week.
Thus it is desired to maximize z = x+y subject to the above constraints.
The totalnumberofpattiesis9000andsothenumberofpattyusedisx+2y.
Thisnumbermust satisfy x+2y ≤9000 because there are only 9000 patty available.
Because of the limitation on the number the employees can prepare and the demand, it follows 2x + y ≤ 9000.
You never sell a negative number of hamburgers and so x,y ≥ 0.
In simpler terms the problem reduces to maximizing z =x+y subject to the two constraints, x+2y ≤9000 and 2x+y ≤ 9000.
This problem is pretty easy to solve geometrically.
Consider the following pictureinwhichRlabelstheregiondescribedbytheaboveinequalitiesandthelinez =x+y is shown for a particular value of z. x+y =z 2x+y =4 R x+2y =4 Asyoumakez largerthislinemovesawayfromtheorigin,alwayshavingthesameslope 135 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation136 LINEAR PROGRAMMING and the desired solution would consist of a point in the region, R which makes z as large as possible or equivalently one for which the line is as far as possible from the origin.
Clearly this point is the point of intersection of the two lines, (3000,3000) and so the maximum valueofthegivenfunctionis6000.
Ofcoursethistypeofprocedureisﬁneforasituationin whichthereareonlytwovariablesbutwhataboutasimilarprobleminwhichtherearevery many variables.
In reality, this hamburger store makes many more types of burgers than those two and there are many considerations other than demand and available patty.
Each will likely give you a constraint which must be considered in order to solve a more realistic problem and the end result will likely be a problem in many dimensions, probably many more than three so your ability to draw a picture will get you nowhere for such a problem.
Another method is needed.
This method is the topic of this section.
I will illustrate with thisparticularproblem.
Letx =xandy =x .Alsoletx andx benonnegativevariables 1 2 3 4 such that x +2x +x =9000, 2x +x +x =9000.
1 2 3 1 2 4 Tosaythatx andx arenonnegativeisthesameassayingx +2x ≤9000and2x +x ≤ 3 4 1 2 1 2 9000andthesevariablesarecalledslackvariablesatthispoint.
Theyarecalledthisbecause they “take up the slack”.
I will discuss these more later.
First a general situation is considered.
6.2 The Simplex Tableau Here is some notation.
Deﬁnition 6.2.1 Let x,y be vectors in Rq.
Then x≤y means for each i,x ≤y .
i i The problem is as follows: Let A be an m×(m+n) real matrix of rank m. It is desired to ﬁnd x ∈ Rn+m such that x satisﬁes the constraints, x≥0,Ax=b (6.1) and out of all such x, m∑+n z ≡ c x i i i=1 is as large (or small) as possible.
This is usually referred to as maximizing or minimizing z subject to the(above constraints).
First I will consider the constraints.
Let A = a ··· a .
First you ﬁnd a vector, x0≥0, Ax0=b such that n of 1 n+m the components of this vector equal 0.
Letting i ,··· ,i be the positions of x0 for which 1 n x0 = 0, suppose also that {a ,··· ,a } is linearly independent for j the other positions i j1 jm i of x0.
Geometrically, this means that x0 is a corner of the feasible region, those x which satisfy the constraints.
This is called a basic feasible solution.
Also deﬁne c ≡ (c .··· ,c ), c ≡(c ,··· ,c ) B j1 jm F i1 in x ≡ (x ,··· ,x ), x ≡(x ,··· ,x ).
B j1 jm F i1 in and ( ) ( ) ( ) x0 z0 ≡z x0 = c c B =c x0 B F x0 B B F sincex0 =0.
Thevariableswhicharethecomponentsofthevectorx arecalledthebasic F B variables andthevariableswhicharetheentriesofx arecalledthefree variables.
You F Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.2.
THE SIMPLEX TABLEAU 137 ( ) set x =0.
Now x0,z0 T is a solution to F ( )( ) ( ) A 0 x b = −c 1 z 0 along with the constraints x≥0.
Writing the above in augmented matrix form yields ( ) A 0 b (6.2) −c 1 0 Permute the columns and variables on the left if necessary to write the above in the form   ( ) ( ) x B F 0  B  b x = (6.3) −c −c 1 F 0 B F z or equivalently in the augmented matrix form keeping track of the variables on the bottom as   B F 0 b  −c −c 1 0 .
(6.4) B F x x 0 0 B F Here B pertains to the variables x ,··· ,x and is an m×m matrix with linearly inde- i1 jm pendent columns, {a ,··· ,a }, and F is an m×n matrix.
Now it is assumed that j1 jm ( ) ( ) ( ) x0 ( ) x0 B F B = B F B =Bx0 =b x0 0 B F and since B is assumed to have rank m, it follows x0 =B−1b≥0.
(6.5) B This is very important to observe.
B−1b≥0!
This is by the assumption that x0 ≥0.
Do row operations on the top part of the matrix ( ) B F 0 b (6.6) −c −c 1 0 B F andobtainitsrowreducedechelonform.
Thenaftertheserowoperationstheabovebecomes ( ) I B−1F 0 B−1b .
(6.7) −c −c 1 0 B F where B−1b≥0.
Next do another row operation in order to get a 0 where you see a −c .
B Thus ( ) I B−1F 0 B−1b (6.8) 0 c B−1F′−c 1 c B−1b ( B F B ) I B−1F 0 B−1b = 0 c B−1F′−c 1 c x0 ( B F B B ) I B−1F 0 B−1b = (6.9) 0 c B−1F −c 1 z0 B F ( ) The reason there is a z0 on the bottom right corner is that x = 0 and x0,x0,z0 T is a F B F solutionofthesystemofequationsrepresentedbytheaboveaugmentedmatrixbecauseitis Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation138 LINEAR PROGRAMMING a solution to the system of equations corresponding to the system of equations represented by(6.6)androwoperationsleavesolutionsetsunchanged.
Notehowattractivethisis.
The z is the value of z at the point x0.
The augmented matrix of (6.9) is called the simplex 0 tableau and it is the beginning point for the simplex algorithm to be described a little later.
It is very convenient to express the simplex(table)au in the above form in which the I variables are possibly permuted in order to have on the left side.
However, as far 0 as the simplex algorithm is concerned it is not necessary to be permuting the variables in this manner.
Starting with (6.9) you could permute the variables and columns to obtain an augmentedmatrixinwhichthevariablesareintheiroriginalorder.
Whatisreallyrequired for the simplex tableau?
It is an augmented m+1×m+n+2 matrix which represents a system of equations which has the same set of solutions, (x,z)T as the system whose augmented matrix is ( ) A 0 b −c 1 0 (Possibly the variables for x are taken in another order.)
There are m linearly independent columns in the ﬁrst m+n columns for which there is only one nonzero entry, a 1 in one of the ﬁrst m rows, the “simple columns”, the other ﬁrst m+n columns being the “nonsimple columns”.
As in the above, the variables corresponding to the simple columns are x , B the basic variables and those corresponding to the nonsimple columns are x , the free F variables.
Also, the top m entries of the last column on the right are nonnegative.
This is the description of a simplex tableau.
In a simplex tableau it is easy to spot a basic feasible solution.
You can see one quickly by setting the variables, x corresponding to the nonsimple columns equal to zero.
Then F the other variables, corresponding to the simple columns are each equal to a nonnegative entry in the far right column.
Lets call this an “obvious basic feasible solution”.
If a solution is obtained by setting the variables corresponding to the nonsimple columns equal to zero and the variables corresponding to the simple columns equal to zero this will be referred to as an “obvious” solution.
Lets also call the ﬁrst m+n entries in the bottom row the “bottom left row”.
In a simplex tableau, the entry in the bottom right corner gives the value of the variable being maximized or minimized when the obvious basic feasible solution is chosen.
Thefollowingisaspecialcaseofthegeneraltheorypresentedaboveandshowshowsuch a special case can be ﬁt into the above framework.
The following example is rather typical of the sorts of problems considered.
It involves inequality constraints instead of Ax=b.
This is handled by adding in “slack variables” as explained below.
Theideaistoobtainanaugmentedmatrixfortheconstraintssuchthatobvioussolutions are also feasible.
Then there is an algorithm, to be presented later, which takes you from one obvious feasible solution to another until you obtain the maximum.
Example 6.2.2 Considerz =x −x subjecttotheconstraints,x +2x ≤10,x +2x ≥2, 1 2 1 2 1 2 and 2x +x ≤ 6,x ≥ 0.
Find a simplex tableau for a problem of the form x≥0,Ax=b 1 2 i which is equivalent to the above problem.
Youaddinslackvariables.
Thesearepositivevariables,oneforeachoftheﬁrstthreecon- straints,whichchangetheﬁrstthreeinequalitiesintoequations.
Thustheﬁrstthreeinequal- itiesbecomex +2x +x =10,x +2x −x =2,and2x +x +x =6,x ,x ,x ,x ,x ≥0.
1 2 3 1 2 4 1 2 5 1 2 3 4 5 Now it is necessary to ﬁnd a basic feasible solution.
You mainly need to ﬁnd a positive so- Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.2.
THE SIMPLEX TABLEAU 139 lution to the equations, x +2x +x =10 1 2 3 x +2x −x =2 1 2 4 2x +x +x =6 1 2 5 the solution set for the above system is given by 2 2 1 1 10 2 x = x − + x ,x =− x + − x ,x =−x +8.
2 3 4 3 3 5 1 3 4 3 3 5 3 4 An easy way to get a basic feasible solution is to let x = 8 and x = 1.
Then a feasible 4 5 solution is (x ,x ,x ,x ,x )=(0,5,0,8,1).
1 2 3 4 5 ( ) A 0 b It follows z0 = −5 and the matrix (6.2), with the variables kept track of −c 1 0 on the bottom is   1 2 1 0 0 0 10  1 2 0 −1 0 0 2     2 1 0 0 1 0 6   −1 1 0 0 0 1 0  x x x x x 0 0 1 2 3 4 5 andtheﬁrstthingtodoistopermutethecolumnssothatthelistofvariablesonthebottom will have x and x at the end.
1 3   2 0 0 1 1 0 10  2 −1 0 1 0 0 2     1 0 1 2 0 0 6   1 0 0 −1 0 1 0  x x x x x 0 0 2 4 5 1 3 Next, as described above, take the row reduced echelon form of the top three lines of the above matrix.
This yields   1 0 0 1 1 0 5  2 2  0 1 0 0 1 0 8 .
0 0 1 3 −1 0 1 2 2 Now do row operations to   1 0 0 1 1 0 5  2 2   0 1 0 0 1 0 8   0 0 1 3 −1 0 1  2 2 1 0 0 −1 0 1 0 to ﬁnally obtain   1 0 0 1 1 0 5  2 2   0 1 0 0 1 0 8   0 0 1 3 −1 0 1  2 2 0 0 0 −3 −1 1 −5 2 2 and this is a simplex tableau.
The variables are x ,x ,x ,x ,x ,z.
2 4 5 1 3 It isn’t as hard as it may appear from the above.
Lets not permute the variables and simply ﬁnd an acceptable simplex tableau as described above.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation140 LINEAR PROGRAMMING Example 6.2.3 Considerz =x −x subjecttotheconstraints,x +2x ≤10,x +2x ≥2, 1 2 1 2 1 2 and 2x +x ≤6,x ≥0.
Find a simplex tableau.
1 2 i Adding in slack variables, an augmented matrix which is descriptive of the constraints is   1 2 1 0 0 10  1 2 0 −1 0 6  2 1 0 0 1 6 The obvious solution is not feasible because of that -1 in the fourth column.
When you let x ,x =0, you end up having x =−6 which is negative.
Consider the second column and 1 2 4 select the 2 as a pivot to zero out that which is above and below the 2.
  0 0 1 1 0 4  1 1 0 −1 0 3  2 2 3 0 0 1 1 3 2 2 This one is good.
When you let x = x = 0, you ﬁnd that x = 3,x = 4,x = 3.
The 1 4 2 3 5 obvious solution is now feasible.
You can now assemble the simplex tableau.
The ﬁrst step is to include a column and row for z.
This yields   0 0 1 1 0 0 4  1 1 0 −1 0 0 3   32 0 0 12 1 0 3  2 2 −1 0 1 0 0 1 0 Now you need to get zeros in the right places so the simple columns will be preserved as simple columns in this larger matrix.
This means you need to zero out the 1 in the third column on the bottom.
A simplex tableau is now   0 0 1 1 0 0 4  1 1 0 −1 0 0 3   32 0 0 12 1 0 3 .
2 2 −1 0 0 −1 0 1 −4 Note it is not the same one obtained earlier.
There is no reason a simplex tableau should be unique.
In fact, it follows from the above general description that you have one for each basic feasible point of the region determined by the constraints.
6.3 The Simplex Algorithm 6.3.1 Maximums The simplex algorithm takes you from one basic feasible solution to another while maxi- mizing or minimizing the function you are trying to maximize or minimize.
Algebraically, it takes you from one simplex tableau to another in which the lower right corner either increases in the case of maximization or decreases in the case of minimization.
Iwillcontinuewritingthesimplextableauinsuchawaythatthesimplecolumnshaving only one entry nonzero are on the left.
As explained above, this amounts to permuting the variables.
I will do this because it is possible to describe what is going on without onerous notation.
However, in the examples, I won’t worry so much about it.
Thus, from a basic feasible solution, a simplex tableau of the following form has been obtained in which the columns for the basic variables, x are listed ﬁrst and b≥0.
B ( ) I F 0 b (6.10) 0 c 1 z0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.3.
THE SIMPLEX ALGORITHM 141 ( ) Let x0i =bi for i=1,··· ,m and x0i(=0 fo)r i>m.
Then x0,z0 is a solution to the above system and since b≥0, it follows x0,z0 is a basic feasible solutio(n. ) F If c < 0 for some i, and if F ≤ 0 so that a whole column of is ≤ 0 with the i ji c bottom entry <0, then letting x be the variable corresponding to that column, you could i leave all the other entries of x equal to zero but change x to be positive.
Let the new F i vector be denoted by x′ and letting x′ =b−Fx′ it follows F B F ∑ (x′ ) = b − F (x ) B k k kj F j j = b −F x ≥0 k ki i Now this shows (x′ ,x′ ) is feasible whenever x > 0 and so you could let x become B F i i arbitrarily large and positive and conclude there is no maximum for z because z =(−c )x +z0 (6.11) i i If this happens in a simplex tableau, you can say there is no maximum and stop.
What if c≥0?
Then z = z0 −cx and to satisfy the constraints, you need x ≥ 0.
F F Therefore, in this case, z0 is the largest possible value of z and so the maximum has been found.
Youstopwhenthisoccurs.
NextIexplainwhattodoifneitheroftheabovestopping conditions hold.
(The o)nly case which remains is that some ci <0 and some Fji >0.
You pick a column F in in which c < 0, usually the one for which c is the largest in absolute value.
c i i You pick F > 0 as a pivot element, divide the jth row by F and then use to obtain ji ji zeros above F and below F , thus obtaining a new simple column.
This row operation ji ji also makes exactly one of the other simple columns into a nonsimple column.
(In terms of variables,itissaidthatafreevariablebecomesabasicvariableandabasicvariablebecomes a free variable.)
Now permuting the columns and variables, yields ( ) I F′ 0 b′ 0 c′ 1 z0′ ( ) where z0′ ≥ z0 because z0′ = z0 −c bj and c < 0.
If b′ ≥ 0, you are in the same i Fji i position you were at the beginning but now z0 is larger.
Now here is the important thing.
Youdon’tpickjustanyF whenyoudotheserowoperations.
Youpick the positive one ji for which the row operation results in b′ ≥ 0.
Otherwise the obvious basic feasible solution obtained by letting x′ =0 will fail to satisfy the constraint that x≥0.
F How is this done?
You need F b b′ ≡b − ki j ≥0 (6.12) k k F ji for each k =1,··· ,m or equivalently, F b b ≥ ki j.
(6.13) k F ji Now if F ≤ 0 the above holds.
Therefore, you only need to check F for F > 0.
The ki pi pi pivot, F is the one which makes the quotients of the form ji b p F pi Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation142 LINEAR PROGRAMMING for all positive F the smallest.
This will work because for F >0, pi ki b b F b p ≤ k ⇒b ≥ ki p F F k F pi ki pi Having gotten a new simplex tableau, you do the same thing to it which was just done and continue.
As long as b>0, so you don’t encounter the degenerate case, the values for z associated with setting x = 0 keep getting strictly larger every time the process is F repeated.
You keep going until you ﬁnd c ≥ 0.
Then you stop.
You are at a maximum.
Problems can occur in the process in the so called degenerate case when at some stage of the process some b = 0.
In this case you can cycle through diﬀerent values for x with no j improvement in z.
This case will not be discussed here.
Example 6.3.1 Maximize 2x +3x subject to the constraints x +x ≥ 1,2x +x ≤ 1 2 1 2 1 2 6,x +2x ≤6, x ,x ≥0.
1 2 1 2 The constraints are of the form x +x −x = 1 1 2 3 2x +x +x = 6 1 2 4 x +2x +x = 6 1 2 5 where the x ,x ,x are the slack variables.
An augmented matrix for these equations is of 3 4 5 the form   1 1 −1 0 0 1   2 1 0 1 0 6 1 2 0 0 1 6 Obviously the obvious solution is not feasible.
It results in x < 0.
We need to exchange 3 basic variables.
Lets just try something.
  1 1 −1 0 0 1  0 −1 2 1 0 4  0 1 1 0 1 5 Now this one is all right because the obvious solution is feasible.
Letting x = x = 0, 2 3 it follows that the obvious solution is feasible.
Now we add in the objective function as described above.
  1 1 −1 0 0 0 1  0 −1 2 1 0 0 4    0 1 1 0 1 0 5 −2 −3 0 0 0 1 0 Then do row operations to leave the simple columns the same.
Then   1 1 −1 0 0 0 1  0 −1 2 1 0 0 4    0 1 1 0 1 0 5 0 −1 −2 0 0 1 2 Now there are negative numbers on the bottom row to the left of the 1.
Lets pick the ﬁrst.
(Itwouldbemoresensibletopickthesecond.)
Theratiostolookatare5/1,1/1sopickfor Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.3.
THE SIMPLEX ALGORITHM 143 the pivot the 1 in the second column and ﬁrst row.
This will leave the right column above the lower right corner nonnegative.
Thus the next tableau is   1 1 −1 0 0 0 1    1 0 1 1 0 0 5   −1 0 2 0 1 0 4  1 0 −3 0 0 1 3 Thereisstillanegativenumbertheretotheleftofthe1inthebottomrow.
Thenewratios are 4/2,5/1 so the new pivot is the 2 in the third column.
Thus the next tableau is   1 1 0 0 1 0 3  23 0 0 1 −21 0 3   −21 0 2 0 12 0 4  −1 0 0 0 3 1 9 2 2 Still, there is a negative number in the bottom row to the left of the 1 so the process does not stop yet.
The ratios are 3/(3/2) and 3/(1/2) and so the new pivot is that 3/2 in the ﬁrst column.
Thus the new tableau is   0 1 0 −1 2 0 2  3 0 0 13 −31 0 3   02 0 2 2 22 0 6  3 3 0 0 0 1 4 1 10 3 3 Now stop.
The maximum value is 10.
This is an easy enough problem to do geometrically and so you can easily verify that this is the right answer.
It occurs when x =x =0,x = 4 5 1 2,x =2,x =3.
2 3 6.3.2 Minimums How does it diﬀer if you are ﬁnding a minimum?
From a basic feasible solution, a simplex tableau of the following form has been obtained in which the simple columns for the basic variables, x are listed ﬁrst and b≥0.
B ( ) I F 0 b (6.14) 0 c 1 z0 ( ) Let x0i =bi for i=1,··· ,m and x0i(=0 fo)r i>m.
Then x0,z0 is a solution to the above system and since b≥0, it follows x0,z0 is a basic feasible solution.
So far, there is no change.
Supposeﬁrstthatsomec >0andF ≤0foreachj.Thenletx′ consistofchangingx i ji F i by making it positive but leaving the other entries of x equal to 0.
Then from the bottom F row, z =−c x +z0 i i and you let x′ = b−Fx′ ≥ 0.
Thus the constraints continue to hold when x is made B F i increasingly positive and it follows from the above equation that there is no minimum for z.
You stop when this happens.
Next suppose c≤0.
Then in this case, z =z0−cx and from the constraints, x ≥0 F F and so −cx ≥ 0 and so z0 is the minimum value and you stop since this is what you are F looking for.
What do you do in the case where some c >0 and some F >0?
In this case, you use i ji thesimplexalgorithmasinthecaseofmaximumstoobtainanewsimplextableauinwhich Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation144 LINEAR PROGRAMMING z0′ is smaller.
You choose F the same way to be the positive entry of the ith column such ji that b /F ≥b /F for all positive entries, F and do the same row operations.
Now this p pi j ji pi time, ( ) b z0′ =z0−c j <z0 i F ji As in the case of maximums no problem can occur and the process will converge unless you have the degenerate case in which some b = 0.
As in the earlier case, this is most j unfortunate when it occurs.
You see what happens of course.
z0 does not change and the algorithm just delivers diﬀerent values of the variables forever with no improvement.
Tosummarizethegeometricalsigniﬁcanceofthesimplexalgorithm,ittakesyoufromone corner of the feasible region to another.
You go in one direction to ﬁnd the maximum and inanothertoﬁndtheminimum.
Forthemaximumyoutrytogetridofnegativeentriesofc andforminimumsyoutrytoeliminatepositiveentriesofc,wherethemethodofelimination involves the auspicious use of an appropriate pivot element and row operations.
Now return to Example 6.2.2.
It will be modiﬁed to be a maximization problem.
Example 6.3.2 Maximize z =x −x subject to the constraints, 1 2 x +2x ≤10,x +2x ≥2, 1 2 1 2 and 2x +x ≤6,x ≥0.
1 2 i Recall this is the same as maximizing z =x −x subject to 1 2     x1     1 2 1 0 0  x2  10  1 2 0 −1 0  x3 = 2 ,x≥0,   2 1 0 0 1 x 6 4 x 5 the variables, x ,x ,x being slack variables.
Recall the simplex tableau was 3 4 5   1 0 0 1 1 0 5  2 2   0 1 0 0 1 0 8   0 0 1 3 −1 0 1  2 2 0 0 0 −3 −1 1 −5 2 2 with the variables ordered as x ,x ,x ,x ,x and so x = (x ,x ,x ) and 2 4 5 1 3 B 2 4 5 x =(x ,x ).
F 1 3 Apply the simplex algorithm to the fourth column because −3 <0 and this is the most 2 negative entry in the bottom row.
The pivot is 3/2 because 1/(3/2) = 2/3 < 5/(1/2).
Dividing this row by 3/2 and then using this to zero out the other elements in that column, the new simplex tableau is   1 0 −1 0 2 0 14  3 3 3   0 1 0 0 1 0 8   0 0 2 1 −1 0 2 .
3 3 3 0 0 1 0 −1 1 −4 Now there is still a negative number in the bottom left row.
Therefore, the process should be continued.
This timethe pivotisthe 2/3inthe top ofthe column.
Dividingthe toprow Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.3.
THE SIMPLEX ALGORITHM 145 by 2/3 and then using this to zero out the entries below it,   3 0 −1 0 1 0 7  −23 1 12 0 0 0 1   12 0 21 1 0 0 3 .
2 2 3 0 1 0 0 1 3 2 2 Now all the numbers on the bottom left row are nonnegative so the process stops.
Now recall the variables and columns were ordered as x ,x ,x ,x ,x .
The solution in terms of 2 4 5 1 3 x and x is x = 0 and x = 3 and z = 3.
Note that in the above, I did not worry about 1 2 2 1 permuting the columns to keep those which go with the basic variables on the left.
Here is a bucolic example.
Example 6.3.3 Consider the following table.
F F F F 1 2 3 4 iron 1 2 1 3 protein 5 3 2 1 folic acid 1 2 2 1 copper 2 1 1 1 calcium 1 1 1 1 This information is available to a pig farmer and F denotes a particular feed.
The numbers i in the table contain the number of units of a particular nutrient contained in one pound of the given feed.
Thus F has 2 units of iron in one pound.
Now suppose the cost of each feed 2 in cents per pound is given in the following table.
F F F F 1 2 3 4 2 3 2 3 A typical pig needs 5 units of iron, 8 of protein, 6 of folic acid, 7 of copper and 4 of calcium.
(The units may change from nutrient to nutrient.)
How many pounds of each feed per pig should the pig farmer use in order to minimize his cost?
His problem is to minimize C ≡2x +3x +2x +3x subject to the constraints 1 2 3 4 x +2x +x +3x ≥ 5, 1 2 3 4 5x +3x +2x +x ≥ 8, 1 2 3 4 x +2x +2x +x ≥ 6, 1 2 3 4 2x +x +x +x ≥ 7, 1 2 3 4 x +x +x +x ≥ 4.
1 2 3 4 where each x ≥0.
Add in the slack variables, i x +2x +x +3x −x = 5 1 2 3 4 5 5x +3x +2x +x −x = 8 1 2 3 4 6 x +2x +2x +x −x = 6 1 2 3 4 7 2x +x +x +x −x = 7 1 2 3 4 8 x +x +x +x −x = 4 1 2 3 4 9 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation146 LINEAR PROGRAMMING The augmented matrix for this system is   1 2 1 3 −1 0 0 0 0 5  5 3 2 1 0 −1 0 0 0 8   1 2 2 1 0 0 −1 0 0 6   2 1 1 1 0 0 0 −1 0 7  1 1 1 1 0 0 0 0 −1 4 How in the world can you ﬁnd a basic feasible solution?
Remember the simplex algorithm is designed to keep the entries in the right column nonnegative so you use this algorithm a few times till the obvious solution is a basic feasible solution.
Considertheﬁrstcolumn.
Thepivotisthe5.
Usingtherowoperationsdescribedinthe algorithm, you get   0 7 3 14 −1 1 0 0 0 17  1 53 52 51 0 −51 0 0 0 58   0 57 58 54 0 15 −1 0 0 252   0 −51 51 53 0 52 0 −1 0 159  5 5 5 5 5 0 2 3 4 0 1 0 0 −1 12 5 5 5 5 5 Now go to the second column.
The pivot in this column is the 7/5.
This is in a diﬀerent row than the pivot in the ﬁrst column so I will use it to zero out everything below it.
This will get rid of the zeros in the ﬁfth column and introduce zeros in the second.
This yields   0 1 3 2 −5 1 0 0 0 17  1 0 71 −1 37 −72 0 0 0 71   0 0 17 −2 17 07 −1 0 0 17   0 0 2 1 −1 3 0 −1 0 30  7 7 7 7 0 0 3 0 2 1 0 0 −1 10 7 7 7 7 Now consider another column, this time the fourth.
I will pick this one because it has some negative numbers in it so there are fewer entries to check in looking for a pivot.
Unfortunately, the pivot is the top 2 and I don’t want to pivot on this because it would destroy the zeros in the second column.
Consider the ﬁfth column.
It is also not a good choicebecausethepivotisthesecondelementfromthetopandthiswoulddestroythezeros in the ﬁrst column.
Consider the sixth column.
I can use either of the two bottom entries as the pivot.
The matrix is   0 1 0 2 −1 0 0 0 1 1  1 0 1 −1 1 0 0 0 −2 3   0 0 1 −2 1 0 −1 0 0 1   0 0 −1 1 −1 0 0 −1 3 0  0 0 3 0 2 1 0 0 −7 10 Next consider the third column.
The pivot is the 1 in the third row.
This yields   0 1 0 2 −1 0 0 0 1 1  1 0 0 1 0 0 1 0 −2 2   0 0 1 −2 1 0 −1 0 0 1 .
 0 0 0 −1 0 0 −1 −1 3 1  0 0 0 6 −1 1 3 0 −7 7 There are still 5 columns which consist entirely of zeros except for one entry.
Four of them have that entry equal to 1 but one still has a -1 in it, the -1 being in the fourth column.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.3.
THE SIMPLEX ALGORITHM 147 I need to do the row operations on a nonsimple column which has the pivot in the fourth row.
Such a column is the second to the last.
The pivot is the 3.
The new matrix is   0 1 0 7 −1 0 1 1 0 2  1 0 0 31 0 0 31 −32 0 38   0 0 1 −32 1 0 −31 03 0 13 .
(6.15)  0 0 0 −1 0 0 −1 −1 1 1  3 3 3 3 0 0 0 11 −1 1 2 −7 0 28 3 3 3 3 Now the obvious basic solution is feasible.
You let x = 0 = x = x = x and x = 4 5 7 8 1 8/3,x = 2/3,x = 1, and x = 28/3.
You don’t need to worry too much about this.
It is 2 3 6 the above matrix which is desired.
Now you can assemble the simplex tableau and begin the algorithm.
Remember C ≡2x +3x +2x +3x .
First add the row and column which 1 2 3 4 deal with C. This yields   0 1 0 7 −1 0 1 1 0 0 2  1 0 0 31 0 0 31 −32 0 0 38   0 0 1 −32 1 0 −31 03 0 0 13   0 0 0 −1 0 0 −1 −1 1 0 1  (6.16)  0 0 0 113 −1 1 23 −37 0 0 238  3 3 3 3 −2 −3 −2 −3 0 0 0 0 0 1 0 Nowyoudorowoperationstokeepthesimplecolumnsof(6.15)simplein(6.16).
Ofcourse you could permute the columns if you wanted but this is not necessary.
This yields the following for a simplex tableau.
Now it is a matter of getting rid of the positive entries in the bottom row because you are trying to minimize.
  0 1 0 7 −1 0 1 1 0 0 2  1 0 0 13 0 0 31 −32 0 0 83   0 0 1 −32 1 0 −31 03 0 0 13   0 0 0 −1 0 0 −1 −1 1 0 1   0 0 0 113 −1 1 23 −37 0 0 238  3 3 3 3 0 0 0 2 −1 0 −1 −1 0 1 28 3 3 3 3 Themostpositiveofthemisthe2/3andsoIwillapplythealgorithmtothisoneﬁrst.
The pivot is the 7/3.
After doing the row operation the next tableau is   0 3 0 1 −3 0 1 1 0 0 2  1 −71 0 0 17 0 72 −75 0 0 178   0 67 1 0 17 0 −75 27 0 0 171   0 71 0 0 −71 0 −72 −72 1 0 73   0 −711 0 0 47 1 17 −270 0 0 578  7 7 7 7 7 0 −2 0 0 −5 0 −3 −3 0 1 64 7 7 7 7 7 andyouseethatalltheentriesarenegativeandsotheminimumis64/7anditoccurswhen x =18/7,x =0,x =11/7,x =2/7.
1 2 3 4 There is no maximum for the above problem.
However, I will pretend I don’t know this and attempt to use the simplex algorithm.
You set up the simiplex tableau the same way.
Recall it is   0 1 0 7 −1 0 1 1 0 0 2  1 0 0 13 0 0 31 −32 0 0 83   0 0 1 −32 1 0 −31 03 0 0 13   0 0 0 −1 0 0 −1 −1 1 0 1   0 0 0 113 −1 1 23 −37 0 0 238  3 3 3 3 0 0 0 2 −1 0 −1 −1 0 1 28 3 3 3 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation148 LINEAR PROGRAMMING Now to maximize, you try to get rid of the negative entries in the bottom left row.
The most negative entry is the -1 in the ﬁfth column.
The pivot is the 1 in the third row of this column.
The new tableau is   0 1 1 1 0 0 −2 1 0 0 5  1 0 0 13 0 0 13 −32 0 0 38   0 0 1 −32 1 0 −31 03 0 0 13   0 0 0 −1 0 0 −1 −1 1 0 1 .
 0 0 1 53 0 1 −13 −37 0 0 331  3 3 3 3 0 0 1 −4 0 0 −4 −1 0 1 31 3 3 3 3 Consider the fourth column.
The pivot is the top 1/3.
The new tableau is   0 3 3 1 0 0 −2 1 0 0 5  1 −1 −1 0 0 0 1 −1 0 0 1   0 6 7 0 1 0 −5 2 0 0 11   0 1 1 0 0 0 −1 0 1 0 2   0 −5 −4 0 0 1 3 −4 0 0 2  0 4 5 0 0 0 −4 1 0 1 17 There is still a negative in the bottom, the -4.
The pivot in that column is the 3.
The algorithm yields   0 −1 1 1 0 2 0 −5 0 0 19  1 23 31 0 0 −31 0 13 0 0 13   0 −37 31 0 1 53 0 −314 0 0 433   0 −32 −31 0 0 31 0 −34 1 0 83   0 −35 −34 0 0 31 1 −34 0 0 23  3 3 3 3 3 0 −8 −1 0 0 4 0 −13 0 1 59 3 3 3 3 3 Note how z keeps getting larger.
Consider the column having the −13/3 in it.
The pivot is the single positive entry, 1/3.
The next tableau is   5 3 2 1 0 −1 0 0 0 0 8  3 2 1 0 0 −1 0 1 0 0 1   14 7 5 0 1 −3 0 0 0 0 19   4 2 1 0 0 −1 0 0 1 0 4 .
 4 1 0 0 0 −1 1 0 0 0 2  13 6 4 0 0 −3 0 0 0 1 24 There is a column consisting of all negative entries.
There is therefore, no maximum.
Note also how there is no way to pick the pivot in that column.
Example 6.3.4 Minimize z = x −3x +x subject to the constraints x +x +x ≤ 1 2 3 1 2 3 10,x +x +x ≥2, x +x +3x ≤8 and x +2x +x ≤7 with all variables nonnegative.
1 2 3 1 2 3 1 2 3 There exists an answer because the region deﬁned by the constraints is closed and bounded.
Adding in slack variables you get the following augmented matrix corresponding to the constraints.
  1 1 1 1 0 0 0 10  1 1 1 0 −1 0 0 2    1 1 3 0 0 1 0 8 1 2 1 0 0 0 1 7 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.3.
THE SIMPLEX ALGORITHM 149 Of course there is a problem with the obvious solution obtained by setting to zero all variables corresponding to a nonsimple column because of the simple column which has the −1 in it.
Therefore, I will use the simplex algorithm to make this column non simple.
The third column has the 1 in the second row as the pivot so I will use this column.
This yields   0 0 0 1 1 0 0 8  1 1 1 0 −1 0 0 2   −2 −2 0 0 3 1 0 2  (6.17) 0 1 0 0 1 0 1 5 and the obvious solution is feasible.
Now it is time to assemble the simplex tableau.
First addinthe bottomrowandsecond tolast columncorresponding tothe equationforz.
This yields   0 0 0 1 1 0 0 0 8  1 1 1 0 −1 0 0 0 2   −2 −2 0 0 3 1 0 0 2    0 1 0 0 1 0 1 0 5 −1 3 −1 0 0 0 0 1 0 Next you need to zero out the entries in the bottom row which are below one of the simple columns in (6.17).
This yields the simplex tableau   0 0 0 1 1 0 0 0 8  1 1 1 0 −1 0 0 0 2   −2 −2 0 0 3 1 0 0 2 .
  0 1 0 0 1 0 1 0 5 0 4 0 0 −1 0 0 1 2 Thedesireistominimizethissoyouneedtogetridofthepositiveentriesintheleftbottom row.
There is only one such entry, the 4.
In that column the pivot is the 1 in the second row of this column.
Thus the next tableau is   0 0 0 1 1 0 0 0 8  1 1 1 0 −1 0 0 0 2     0 0 2 0 1 1 0 0 6   −1 0 −1 0 2 0 1 0 3  −4 0 −4 0 3 0 0 1 −6 There is still a positive number there, the 3.
The pivot in this column is the 2.
Apply the algorithm again.
This yields   1 0 1 1 0 0 −1 0 13  21 1 21 0 0 0 12 0 27   21 0 25 0 0 1 −21 0 29 .
 −21 0 −21 0 1 0 12 0 23  2 2 2 2 −5 0 −5 0 0 0 −3 1 −21 2 2 2 2 Now all the entries in the left bottom row are nonpositive so the process has stopped.
The minimum is −21/2.
It occurs when x =0, x =7/2,x =0.
1 2 3 Now consider the same problem but change the word, minimize to the word, maximize.
Example 6.3.5 Maximize z = x −3x +x subject to the constraints x +x +x ≤ 1 2 3 1 2 3 10,x +x +x ≥2, x +x +3x ≤8 and x +2x +x ≤7 with all variables nonnegative.
1 2 3 1 2 3 1 2 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation150 LINEAR PROGRAMMING The ﬁrst part of it is the same.
You wind up with the same simplex tableau,   0 0 0 1 1 0 0 0 8  1 1 1 0 −1 0 0 0 2   −2 −2 0 0 3 1 0 0 2    0 1 0 0 1 0 1 0 5 0 4 0 0 −1 0 0 1 2 but this time, you apply the algorithm to get rid of the negative entries in the left bottom row.
There is a −1.
Use this column.
The pivot is the 3.
The next tableau is   2 2 0 1 0 −1 0 0 22  31 31 1 0 0 13 0 0 38   −32 −32 0 0 1 31 0 0 32   23 53 0 0 0 −31 1 0 133  3 3 3 3 −2 10 0 0 0 1 0 1 8 3 3 3 3 There is still a negative entry, the −2/3.
This will be the new pivot column.
The pivot is the 2/3 on the fourth row.
This yields   0 −1 0 1 0 0 −1 0 3  0 −1 1 0 0 1 −1 0 1   2 2 2 2   0 1 0 0 1 0 1 0 5   1 5 0 0 0 −1 3 0 13  2 2 2 2 0 5 0 0 0 0 1 1 7 and the process stops.
The maximum for z is 7 and it occurs when x =13/2,x =0,x = 1 2 3 1/2.
6.4 Finding A Basic Feasible Solution Bynowitshouldbefairlyclearthatﬁndingabasicfeasiblesolutioncancreateconsiderable diﬃculty.
Indeed,givenasystemoflinearinequalitiesalongwiththerequirementthateach variable be nonnegative, do there even exist points satisfying all these inequalities?
If you have many variables, you can’t answer this by drawing a picture.
Is there some other way to do this which is more systematic than what was presented above?
The answer is yes.
It is called the method of artiﬁcial variables.
I will illustrate this method with an example.
Example 6.4.1 Findabasicfeasiblesolutiontothesystem2x +x −x ≥3,x +x +x ≥ 1 2 3 1 2 3 2,x +x +x ≤7 and x≥0.
1 2 3 If you write the appropriate augmented matrix with the slack variables,   2 1 −1 −1 0 0 3  1 1 1 0 −1 0 2  (6.18) 1 1 1 0 0 1 7 The obvious solution is not feasible.
This is why it would be hard to get started with the simplex method.
What is the problem?
It is those −1 entries in the fourth and ﬁfth columns.
To get around this, you add in artiﬁcial variables to get an augmented matrix of the form   2 1 −1 −1 0 0 1 0 3  1 1 1 0 −1 0 0 1 2  (6.19) 1 1 1 0 0 1 0 0 7 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.4.
FINDING A BASIC FEASIBLE SOLUTION 151 Thus the variables are x ,x ,x ,x ,x ,x ,x ,x .
Suppose you can ﬁnd a feasible solution 1 2 3 4 5 6 7 8 to the system of equations represented by the above augmented matrix.
Thus all variables are nonnegative.
Suppose also that it can be done in such a way that x and x happen 8 7 to be 0.
Then it will follow that x ,··· ,x is a feasible solution for (6.18).
Conversely, if 1 6 you can ﬁnd a feasible solution for (6.18), then letting x and x both equal zero, you have 7 8 obtained a feasible solution to (6.19).
Since all variables are nonnegative, x and x both 7 8 equallingzeroisequivalenttosayingtheminimumofz =x +x subjecttotheconstraints 7 8 represented by the above augmented matrix equals zero.
This has proved the following simple observation.
Observation 6.4.2 There exists a feasible solution to the constraints represented by the augmented matrix of (6.18) and x≥0 if and only if the minimum of x +x subject to the 7 8 constraints of (6.19) and x≥0 exists and equals 0.
Of course a similar observation would hold in other similar situations.
Now the point of all this is that it is trivial to see a feasible solution to (6.19), namely x =7,x =3,x =2 6 7 8 and all the other variables may be set to equal zero.
Therefore, it is easy to ﬁnd an initial simplextableaufortheminimizationproblemjustdescribed.
Firstaddthecolumnandrow for z   2 1 −1 −1 0 0 1 0 0 3  1 1 1 0 −1 0 0 1 0 2    1 1 1 0 0 1 0 0 0 7 0 0 0 0 0 0 −1 −1 1 0 Nextitisnecessarytomakethelasttwocolumnsonthebottomleftrowintosimplecolumns.
Performing the row operation, this yields an initial simplex tableau,   2 1 −1 −1 0 0 1 0 0 3  1 1 1 0 −1 0 0 1 0 2    1 1 1 0 0 1 0 0 0 7 3 2 0 −1 −1 0 0 0 1 5 Nowthealgorithminvolvesgetting ridofthepositiveentriesonthe leftbottomrow.
Begin with the ﬁrst column.
The pivot is the 2.
An application of the simplex algorithm yields the new tableau   1 1 −1 −1 0 0 1 0 0 3  0 21 32 12 −1 0 −21 1 0 12   0 21 23 21 0 1 −21 0 0 121  2 2 2 2 2 0 1 3 1 −1 0 −3 0 1 1 2 2 2 2 2 Now go to the third column.
The pivot is the 3/2 in the second row.
An application of the simplex algorithm yields   1 2 0 −1 −1 0 1 1 0 5  0 31 1 13 −32 0 −31 32 0 31   0 03 0 03 13 1 03 −31 0 53  (6.20) 0 0 0 0 0 0 −1 −1 1 0 and you see there are only nonpositive numbers on the bottom left column so the process stopsandyields0fortheminimumofz =x +x .Asfortheothervariables,x =5/3,x = 7 8 1 2 0,x = 1/3,x = 0,x = 0,x = 5.
Now as explained in the above observation, this is a 3 4 5 6 basic feasible solution for the original system (6.18).
Now consider a maximization problem associated with the above constraints.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation152 LINEAR PROGRAMMING Example 6.4.3 Maximize x −x +2x subject to the constraints, 2x +x −x ≥3,x + 1 2 3 1 2 3 1 x +x ≥2,x +x +x ≤7 and x≥0.
2 3 1 2 3 From (6.20) you can immediately assemble an initial simplex tableau.
You begin with the ﬁrst 6 columns and top 3 rows in (6.20).
Then add in the column and row for z.
This yields   1 2 0 −1 −1 0 0 5  0 31 1 13 −32 0 0 13   3 3 3 3  0 0 0 0 1 1 0 5 −1 1 −2 0 0 0 1 0 and you ﬁrst do row operations to make the ﬁrst and third columns simple columns.
Thus the next simplex tableau is   1 2 0 −1 −1 0 0 5  0 31 1 13 −32 0 0 13   3 3 3 3  0 0 0 0 1 1 0 5 0 7 0 1 −5 0 1 7 3 3 3 3 You are trying to get rid of negative entries in the bottom left row.
There is only one, the −5/3.
The pivot is the 1.
The next simplex tableau is then   1 2 0 −1 0 1 0 10  0 31 1 13 0 32 0 131   3 3 3 3  0 0 0 0 1 1 0 5 0 7 0 1 0 5 1 32 3 3 3 3 andsothemaximumvalueofz is32/3anditoccurswhenx =10/3,x =0andx =11/3.
1 2 3 6.5 Duality You can solve minimization problems by solving maximization problems.
You can also go theotherdirectionandsolvemaximizationproblemsbyminimizationproblems.
Sometimes this makes things much easier.
To be more speciﬁc, the two problems to be considered are A.)
Minimize z = cx subject to x≥0 and Ax≥b and B.)
Maximize w = yb such that y≥0 and yA≤c, ( ) equivalently ATyT ≥cT and w =bTyT .
In these problems it is assumed A is an m×p matrix.
I will show how a solution of the ﬁrst yields a solution of the second and then show how a solution of the second yields a solution of the ﬁrst.
The problems, A.)
and B.)
are called dual problems.
Lemma 6.5.1 Let x be a solution of the inequalities of A.)
and let y be a solution of the inequalities of B.).
Then cx≥yb.
and if equality holds in the above, then x is the solution to A.)
and y is a solution to B.).
Proof: This follows immediately.
Since c≥yA,cx≥yAx≥yb.
It follows from this lemma that if y satisﬁes the inequalities of B.)
and x satisﬁes the inequalities of A.)
then if equality holds in the above lemma, it must be that x is a solution of A.)
and y is a solution of B.).
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.5.
DUALITY 153 Now recall that to solve either of these problems using the simplex method, you ﬁrst add in slack variables.
Denote by x′ and y′ the enlarged list of variables.
Thus x′ has at least m entries and so does y′ and the inequalities involving A were replaced by equalities whose augmented matrices were of the form ( ) ( ) A −I b , and AT I cT Then you included the row and column for z and w to obtain ( ) ( ) A −I 0 b AT I 0 cT and .
(6.21) −c 0 1 0 −bT 0 1 0 Then the problems have basic feasible solutions if it is possible to permute the ﬁrst p+m columns in the above two matrices and obtain matrices of the form ( ) ( ) B F 0 b B F 0 cT and 1 1 (6.22) −c −c 1 0 −bT −bT 1 0 B F B1 F1 where B,B are invertible m×m and p×p matrices and denoting the variables associated 1 with these columns by x ,y and those variables associated with F or F by x and y , B B 1 F F it fol(lows that )letting BxB =b and xF =0, the resulting vector, x′ is a solution to x′ ≥0 and A −I x′ =b with similar constraints holding for y′.
In other words, it is possible to obtain simplex tableaus, ( ) ( ) I B−1F 0 B−1b I B−1F 0 B−1cT 0 c B−1F −c 1 c B−1b , 0 bT B−11F −1 bT 1 bT 1B−1cT (6.23) B F B B1 1 F1 B1 1 Similarconsiderationsapplytothesecondproblem.
Thusasjustdescribed, abasicfeasible solutionisonewhichdeterminesasimplextableauliketheaboveinwhichyougetafeasible solution by setting all but the ﬁrst m variables equal to zero.
The simplex algorithm takes you from one basic feasible solution to another till eventually, if there is no degeneracy, you obtain a basic feasible solution which yields the solution of the problem of interest.
Theorem 6.5.2 Suppose there exists a solution x to A.)
where x is a basic feasible solution of the inequalities of A.).
Then there exists a solution y to B.)
and cx=by.
It is also possible to ﬁnd y from x using a simple formula.
Proof: Since the solution to A.)
is basic and feasible, there exists a simplex tableau like (6.23) such that x′ can be split into x and x such that x = 0 and x = B−1b.
B F F B Now since it is a minimizer, it follows c B−1F −c ≤0 and the minimum value for cx is B F c B−1b.
Stating this again, cx=c B−1b.
Is it possible you can take y=c B−1?
From B B B Lemma 6.5.1 this will be so if cBB−1 solves the constraints of problem B.).
I(s cBB−1 ≥)0?
I(s cBB−)1A ≤ c?
These two conditions are satisﬁed if and only if cBB−1 A −I ≤ c 0 .
Referring to the process of permuting the columns of the ﬁrst augmen(ted matrix) of (6(.21) to )get (6.22) and doing the same permutations on th(e column)s of( A −I ) and c 0 , the desired ineq(uality holds if an)d on(ly if cBB−1) B F ≤ cB cF which is equivalent to saying c c B−1F ≤ c c and this is true because B B B F c B−1F −c ≤0 due to the assumption that x is a minimizer.
The simple formula is just B F y=c B−1.
(cid:4) B The proof of the following corollary is similar.
Corollary 6.5.3 Supposethereexistsasolution,ytoB.
)whereyisabasicfeasiblesolution of the inequalities of B.).
Then there exists a solution, x to A.)
and cx=by.
It is also possible to ﬁnd x from y using a simple formula.
In this case, and referring to (6.23), the simple formula is x=B−Tb .
1 B1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation154 LINEAR PROGRAMMING As an example, consider the pig farmers problem.
The main diﬃculty in this problem was ﬁnding an initial simplex tableau.
Now consider the following example and marvel at how all the diﬃculties disappear.
Example 6.5.4 minimize C ≡2x +3x +2x +3x subject to the constraints 1 2 3 4 x +2x +x +3x ≥ 5, 1 2 3 4 5x +3x +2x +x ≥ 8, 1 2 3 4 x +2x +2x +x ≥ 6, 1 2 3 4 2x +x +x +x ≥ 7, 1 2 3 4 x +x +x +x ≥ 4.
1 2 3 4 where each x ≥0.
i Here the dual problem is to maximize w = 5y +8y +6y +7y +4y subject to the 1 2 3 4 5 constraints       y 1 5 1 2 1  1  2  21 32 22 11 11  yy23 ≤ 32 .
y 3 1 1 1 1 4 3 y 5 Addinginslackvariables,theseinequalitiesareequivalenttothesystemofequationswhose augmented matrix is   1 5 1 2 1 1 0 0 0 2    2 3 2 1 1 0 1 0 0 3    1 2 2 1 1 0 0 1 0 2 3 1 1 1 1 0 0 0 1 3 Now the obvious solution is feasible so there is no hunting for an initial obvious feasible solution required.
Now add in the row and column for w. This yields   1 5 1 2 1 1 0 0 0 0 2    2 3 2 1 1 0 1 0 0 0 3     1 2 2 1 1 0 0 1 0 0 2 .
  3 1 1 1 1 0 0 0 1 0 3 −5 −8 −6 −7 −4 0 0 0 0 1 0 Itisamaximizationproblemsoyouwanttoeliminatethenegativesinthebottomleftrow.
Pick the column having the one which is most negative, the −8.
The pivot is the top 5.
Then apply the simplex algorithm to obtain   1 1 1 2 1 1 0 0 0 0 2  57 0 57 −51 52 −53 1 0 0 0 59   53 0 58 15 53 −52 0 1 0 0 56 .
 154 0 54 53 54 −51 0 0 1 0 153  5 5 5 5 5 5 −17 0 −22 −19 −12 8 0 0 0 1 16 5 5 5 5 5 5 There are still negative entries in the bottom left row.
Do the simplex algorithm to the column which has the −22.
The pivot is the 8.
This yields 5 5   1 1 0 3 1 1 0 −1 0 0 1  87 0 0 −83 −81 −41 1 −87 0 0 34   83 0 1 18 38 −41 0 58 0 0 34   85 0 0 81 81 04 0 −81 1 0 24  2 2 2 2 −7 0 0 −13 −3 1 0 11 0 1 13 4 4 4 2 4 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation6.5.
DUALITY 155 and there are still negative numbers.
Pick the column which has the −13/4.
The pivot is the 3/8 in the top.
This yields   1 8 0 1 1 2 0 −1 0 0 2  13 13 0 0 03 03 1 −13 0 0 13   1 −1 1 0 1 −1 0 2 0 0 2   37 −34 0 0 13 −31 0 −31 1 0 53  3 3 3 3 3 3 −2 26 0 0 1 8 0 5 0 1 26 3 3 3 3 3 3 which has only one negative entry on the bottom left.
The pivot for this ﬁrst column is the 7.
The next tableau is 3   0 20 0 1 2 5 0 −2 −1 0 3  0 171 0 0 −71 71 1 −76 −73 0 72   0 −71 1 0 27 −72 0 57 −71 0 73   1 −74 0 0 71 −71 0 −71 37 0 75  7 7 7 7 7 7 0 58 0 0 3 18 0 11 2 1 64 7 7 7 7 7 7 and all the entries in the left bottom row are nonnegative so the answer is 64/7.
This is the same as obtained before.
So what values for x are needed?
Here the basic variables are y ,y ,y ,y .
Consider the original augmented matrix, one step before the simplex tableau.
1 3 4 7   1 5 1 2 1 1 0 0 0 0 2    2 3 2 1 1 0 1 0 0 0 3     1 2 2 1 1 0 0 1 0 0 2 .
  3 1 1 1 1 0 0 0 1 0 3 −5 −8 −6 −7 −4 0 0 0 0 1 0 Permute the columns to put the columns associated with these basic variables ﬁrst.
Thus   1 1 2 0 5 1 1 0 0 0 2    2 2 1 1 3 1 0 0 0 0 3     1 2 1 0 2 1 0 1 0 0 2    3 1 1 0 1 1 0 0 1 0 3 −5 −6 −7 0 −8 −4 0 0 0 1 0 The matrix B is   1 1 2 0    2 2 1 1    1 2 1 0 3 1 1 0 and so B−T equals   −1 −2 5 1  7 7 7 7   0 0 0 1   −1 5 −2 −6  7 7 7 7 3 −1 −1 −3 7 7 7 7 ( ) Also bT = 5 6 7 0 and so from Corollary 6.5.3, B      −1 −2 5 1 5 18  7 7 7 7    7   0 0 0 1  6   0  x= −1 5 −2 −6  7 = 11  7 7 7 7 7 3 −1 −1 −3 0 2 7 7 7 7 7 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation156 LINEAR PROGRAMMING which agrees with the original way of doing the problem.
Two good books which give more discussion of linear programming are Strang [25] and Nobel and Daniels [20].
Also listed in these books are other references which may prove useful if you are interested in seeing more on these topics.
There is a great deal more which can be said about linear programming.
6.6 Exercises 1.
Maximize and minimize z =x −2x +x subject to the constraints x +x +x ≤ 1 2 3 1 2 3 10, x +x +x ≥2, and x +2x +x ≤7 if possible.
All variables are nonnegative.
1 2 3 1 2 3 2.
Maximize and minimize the following if possible.
All variables are nonnegative.
(a) z =x −2x subject to the constraints x +x +x ≤10, x +x +x ≥1, and 1 2 1 2 3 1 2 3 x +2x +x ≤7 1 2 3 (b) z =x −2x −3x subjecttotheconstraintsx +x +x ≤8, x +x +3x ≥1, 1 2 3 1 2 3 1 2 3 and x +x +x ≤7 1 2 3 (c) z =2x +x subject to the constraints x −x +x ≤10, x +x +x ≥1, and 1 2 1 2 3 1 2 3 x +2x +x ≤7 1 2 3 (d) z =x +2x subject to the constraints x −x +x ≤10, x +x +x ≥1, and 1 2 1 2 3 1 2 3 x +2x +x ≤7 1 2 3 3.
Consider contradictory constraints, x +x ≥ 12 and x +2x ≤ 5,x ≥ 0,x ≥ 0.
1 2 1 2 1 2 Youknowthesetwocontradictbutshowtheycontradictusingthesimplexalgorithm.
4.
Find a solution to the following inequalities for x,y ≥0 if it is possible to do so.
If it is not possible, prove it is not possible.
6x+3y ≥4 (a) 8x+4y ≤5 6x +4x ≤11 1 3 (b) 5x +4x +4x ≥8 1 2 3 6x +6x +5x ≤11 1 2 3 6x +4x ≤11 1 3 (c) 5x +4x +4x ≥9 1 2 3 6x +6x +5x ≤9 1 2 3 x −x +x ≤2 1 2 3 (d) x +2x ≥4 1 2 3x +2x ≤7 1 3 5x −2x +4x ≤1 1 2 3 (e) 6x −3x +5x ≥2 1 2 3 5x −2x +4x ≤5 1 2 3 5.
Minimize z = x +x subject to x +x ≥ 2, x +3x ≤ 20, x +x ≤ 18.
Change 1 2 1 2 1 2 1 2 to a maximization problem and solve as follows: Let y =M−x .
Formulate in terms i i of y ,y .
1 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationSpectral Theory Spectral Theory refers to the study of eigenvalues and eigenvectors of a matrix.
It is of fundamentalimportanceinmanyareas.
Rowoperationswillnolongerbesuchausefultool in this subject.
7.1 Eigenvalues And Eigenvectors Of A Matrix The ﬁeld of scalars in spectral theory is best taken to equal C although I will sometimes refer to it as F when it could be either C or R. Deﬁnition 7.1.1 Let M be an n×n matrix and let x∈Cn be a nonzero vector for which Mx=λx (7.1) for some scalar, λ.
Then x is called an eigenvector and λ is called an eigenvalue (charac- teristic value) of the matrix M. Eigenvectors are never equal to zero!
The set of all eigenvalues of an n×n matrix M, is denoted by σ(M) and is referred to as the spectrum of M. Eigenvectors are vectors which are shrunk, stretched or reﬂected upon multiplication by a matrix.
How can they be identiﬁed?
Suppose x satisﬁes (7.1).
Then (λI−M)x=0 for some x̸=0.
Therefore, the matrix M −λI cannot have an inverse and so by Theorem 3.3.18 det(λI −M)=0.
(7.2) Inotherwords,λmustbeazeroofthecharacteristicpolynomial.
SinceM isann×nmatrix, it follows from the theorem on expanding a matrix by its cofactor that this is a polynomial equation of degree n. As such, it has a solution, λ ∈ C. Is it actually an eigenvalue?
The answer is yes and this follows from Theorem 3.3.26 on Page 95.
Since det(λI −M) = 0 the matrix λI −M cannot be one to one and so there exists a nonzero vector, x such that (λI−M)x=0.
This proves the following corollary.
Corollary 7.1.2 LetM beann×nmatrixanddet(M −λI)=0.
Thenthereexistsx∈Cn such that (M −λI)x=0.
157 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation158 SPECTRAL THEORY As an example, consider the following.
Example 7.1.3 Find the eigenvalues and eigenvectors for the matrix   5 −10 −5   A= 2 14 2 .
−4 −8 6 Youﬁrstneedtoidentifytheeigenvalues.
Recallthisrequiresthesolutionoftheequation      1 0 0 5 −10 −5 detλ 0 1 0 − 2 14 2 =0 0 0 1 −4 −8 6 When you expand this determinant, you ﬁnd the equation is ( ) (λ−5) λ2−20λ+100 =0 and so the eigenvalues are 5,10,10.
I have listed 10 twice because it is a zero of multiplicity two due to λ2−20λ+100=(λ−10)2.
Having found the eigenvalues, it only remains to ﬁnd the eigenvectors.
First ﬁnd the eigenvectors for λ=5.
As explained above, this requires you to solve the equation,         1 0 0 5 −10 −5 x 0 5 0 1 0 − 2 14 2  y = 0 .
0 0 1 −4 −8 6 z 0 That is you need to ﬁnd the solution to      0 10 5 x 0  −2 −9 −2  y = 0  4 8 −1 z 0 Bynowthisisanoldproblem.
Yousetuptheaugmentedmatrixandrowreducetogetthe solution.
Thus the matrix you must row reduce is   0 10 5 0  −2 −9 −2 0 .
(7.3) 4 8 −1 0 The reduced row echelon form is   1 0 −5 0  0 1 14 0  2 0 0 0 0 and so the solution is any vector of the form     5z 5  −41z =z −41  2 2 z 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.1.
EIGENVALUES AND EIGENVECTORS OF A MATRIX 159 where z ∈ F. You would obtain the same collection of vectors if you replaced z with 4z.
Thus a simpler description for the solutions to this system of equations whose augmented matrix is in (7.3) is   5 z −2  (7.4) 4 where z ∈ F. Now you need to remember that you can’t take z = 0 because this would result in the zero vector and Eigenvectors are never equal to zero!
Other than this value, every other choice of z in (7.4) results in an eigenvector.
It is a good ideatocheckyourwork!
Todoso,Iwilltaketheoriginalmatrixandmultiplybythisvector and see if I get 5 times this vector.
       5 −10 −5 5 25 5  2 14 2  −2 = −10 =5 −2  −4 −8 6 4 20 4 so it appears this is correct.
Always check your work on these problems if you care about getting the answer right.
The variable, z is called a free variable or sometimes a parameter.
The set of vectors in (7.4)iscalledtheeigenspaceanditequalsker(λI−A).Youshouldobservethatinthiscase the eigenspace has dimension 1 because there is one vector which spans the eigenspace.
In general, you obtain the solution from the row echelon form and the number of diﬀerent free variables gives you the dimension of the eigenspace.
Just remember that not every vector in the eigenspace is an eigenvector.
The vector, 0 is not an eigenvector although it is in the eigenspace because Eigenvectors are never equal to zero!
Next consider the eigenvectors for λ=10.
These vectors are solutions to the equation,         1 0 0 5 −10 −5 x 0 10 0 1 0 − 2 14 2  y = 0  0 0 1 −4 −8 6 z 0 That is you must ﬁnd the solutions to      5 10 5 x 0  −2 −4 −2  y = 0  4 8 4 z 0 which reduces to consideration of the augmented matrix   5 10 5 0  −2 −4 −2 0  4 8 4 0 The row reduced echelon form for this matrix is   1 2 1 0   0 0 0 0 0 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation160 SPECTRAL THEORY and so the eigenvectors are of the form       −2y−z −2 −1       y =y 1 +z 0 .
z 0 1 You can’t pick z and y both equal to zero because this would result in the zero vector and Eigenvectors are never equal to zero!
However, every other choice of z and y does result in an eigenvector for the eigenvalue λ = 10.
As in the case for λ = 5 you should check your work if you care about getting it right.
       5 −10 −5 −1 −10 −1        2 14 2 0 = 0 =10 0 −4 −8 6 1 10 1 so it worked.
The other vector will also work.
Check it.
The above example shows how to ﬁnd eigenvectors and eigenvalues algebraically.
You may have noticed it is a bit long.
Sometimes students try to ﬁrst row reduce the matrix before looking for eigenvalues.
This is a terrible idea because row operations destroy the value of the eigenvalues.
The eigenvalue problem is really not about row operations.
A general rule to remember about the eigenvalue problem is this.
If it is not long and hard it is usually wrong!
The eigenvalue problem is the hardest problem in algebra and people still do research on ways to ﬁnd eigenvalues.
Now if you are so fortunate as to ﬁnd the eigenvalues as in the above example, then ﬁnding the eigenvectors does reduce to row operations and this part of the problem is easy.
However, ﬁnding the eigenvalues is anything but easy because for an n×n matrix, it involves solving a polynomial equation of degree n and none of us are very good at doing this.
If you only ﬁnd a good approximation to the eigenvalue, it won’t work.
It either is or is not an eigenvalue and if it is not, the only solution to the equation, (λI−M)x=0 will be the zero solution as explained above and Eigenvectors are never equal to zero!
Here is another example.
Example 7.1.4 Let   2 2 −2 A= 1 3 −1  −1 1 1 First ﬁnd the eigenvalues.
     1 0 0 2 2 −2 detλ 0 1 0 − 1 3 −1 =0 0 0 1 −1 1 1 This is λ3−6λ2+8λ=0 and the solutions are 0, 2, and 4.
0 Can be an Eigenvalue!
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.1.
EIGENVALUES AND EIGENVECTORS OF A MATRIX 161 Now ﬁnd the eigenvectors.
For λ=0 the augmented matrix for ﬁnding the solutions is   2 2 −2 0  1 3 −1 0  −1 1 1 0 and the row reduced echelon form is   1 0 −1 0   0 1 0 0 0 0 0 0 Therefore, the eigenvectors are of the form   1   z 0 1 where z ̸=0.
Next ﬁnd the eigenvectors for λ=2.
The augmented matrix for the system of equations needed to ﬁnd these eigenvectors is   0 −2 2 0  −1 −1 1 0  1 −1 1 0 and the row reduced echelon form is   1 0 0 0  0 1 −1 0  0 0 0 0 and so the eigenvectors are of the form   0   z 1 1 where z ̸=0.
Finallyﬁndtheeigenvectorsforλ=4.Theaugmentedmatrixforthesystemofequations needed to ﬁnd these eigenvectors is   2 −2 2 0  −1 1 1 0  1 −1 3 0 and the row reduced echelon form is   1 −1 0 0   0 0 1 0 .
0 0 0 0 Therefore, the eigenvectors are of the form   1   y 1 0 where y ̸=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation162 SPECTRAL THEORY Example 7.1.5 Let   2 −2 −1 A= −2 −1 −2 .
14 25 14 Find the eigenvectors and eigenvalues.
In this case the eigenvalues are 3,6,6 where I have listed 6 twice because it is a zero of algebraic multiplicity two, the characteristic equation being (λ−3)(λ−6)2 =0.
It remains to ﬁnd the eigenvectors for these eigenvalues.
First consider the eigenvectors for λ=3.
You must solve         1 0 0 2 −2 −1 x 0 3 0 1 0 − −2 −1 −2  y = 0 .
0 0 1 14 25 14 z 0 Using routine row operations, the eigenvectors are nonzero vectors of the form     z 1  −z =z −1  z 1 Next consider the eigenvectors for λ=6.
This requires you to solve         1 0 0 2 −2 −1 x 0 6 0 1 0 − −2 −1 −2  y = 0  0 0 1 14 25 14 z 0 and using the usual procedures yields the eigenvectors for λ=6 are of the form   −1 z −81  4 1 or written more simply,   −1 z −2  8 where z ∈F.
Note that in this example the eigenspace for the eigenvalue λ = 6 is of dimension 1 because there is only one parameter which can be chosen.
However, this eigenvalue is of multiplicity two as a root to the characteristic equation.
Deﬁnition 7.1.6 If A is an n×n matrix with the property that some eigenvalue has alge- braicmultiplicity as a rootof thecharacteristicequationwhich is greaterthan the dimension of the eigenspace associated with this eigenvalue, then the matrix is called defective.
There may be repeated roots to the characteristic equation, (7.2) and it is not known whetherthedimensionoftheeigenspaceequalsthemultiplicityoftheeigenvalue.
However, the following theorem is available.
Theorem 7.1.7 Suppose Mv =λ v ,i=1,··· ,r , v ̸=0, and that if i̸=j, then λ ̸=λ .
i i i i i j Then the set of eigenvectors, {v ,··· ,v } is linearly independent.
1 r Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.1.
EIGENVALUES AND EIGENVECTORS OF A MATRIX 163 Proof.
Suppose the claim of the lemma is not true.
Then there exists a subset of this set of vectors {w ,··· ,w }⊆{v ,··· ,v } 1 r 1 k such that ∑r c w =0 (7.5) j j j=1 where each c ̸=0.
Say Mw =µ w where j j j j {µ ,··· ,µ }⊆{λ ,··· ,λ }, 1 r 1 k the µ being distinct eigenvalues of M. Out of all such subsets, let this one be such that r j is as small as possible.
Then necessarily, r > 1 because otherwise, c w = 0 which would 1 1 imply w =0, which is not allowed for eigenvectors.
1 Now apply M to both sides of (7.5).
∑r c µ w =0.
(7.6) j j j j=1 Next pick µ ̸= 0 and multiply both sides of (7.5) by µ .
Such a µ exists because r > 1. k k k Thus ∑r c µ w =0 (7.7) j k j j=1 Subtract the sum in (7.7) from the sum in (7.6) to obtain ∑r ( ) c µ −µ w =0 j k j j j=1 ( ) Now one of the constants c µ −µ equals 0, when j =k.
Therefore, r was not as small j k j as possible after all.
(cid:4) In words, this theorem says that eigenvectors associated with distinct eigenvalues are linearly independent.
Sometimes you have to consider eigenvalues which are complex numbers.
This occurs in diﬀerential equations for example.
You do these problems exactly the same way as you do the ones in which the eigenvalues are real.
Here is an example.
Example 7.1.8 Find the eigenvalues and eigenvectors of the matrix   1 0 0 A= 0 2 −1 .
0 1 2 You need to ﬁnd the eigenvalues.
Solve      1 0 0 1 0 0 detλ 0 1 0 − 0 2 −1 =0.
0 0 1 0 1 2 ( ) This reduces to (λ−1) λ2−4λ+5 =0.
The solutions are λ=1,λ=2+i,λ=2−i.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation164 SPECTRAL THEORY Thereisnothingnewaboutﬁndingtheeigenvectorsforλ=1soconsidertheeigenvalue λ=2+i.
You need to solve         1 0 0 1 0 0 x 0 (2+i) 0 1 0 − 0 2 −1  y = 0  0 0 1 0 1 2 z 0 In other words, you must consider the augmented matrix   1+i 0 0 0   0 i 1 0 0 −1 i 0 for the solution.
Divide the top row by (1+i) and then take −i times the second row and add to the bottom.
This yields   1 0 0 0   0 i 1 0 0 0 0 0 Now multiply the second row by −i to obtain   1 0 0 0  0 1 −i 0  0 0 0 0 Therefore, the eigenvectors are of the form   0   z i .
1 You should ﬁnd the eigenvectors for λ=2−i.
These are   0 z −i .
1 As usual, if you want to get it right you had better check it.
       1 0 0 0 0 0  0 2 −1  −i = −1−2i =(2−i) −i  0 1 2 1 2−i 1 so it worked.
7.2 Some Applications Of Eigenvalues And Eigenvec- tors Recall that n×n matrices can be considered as linear transformations.
If F is a 3×3 real matrix having positive determinant, it can be shown that F = RU where R is a rotation matrix and U is a symmetric real matrix having positive eigenvalues.
An application of this wonderful result, known to mathematicians as the right polar decomposition, is to continuum mechanics where a chunk of material is identiﬁed with a set of points in three dimensional space.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.2.
SOME APPLICATIONS OF EIGENVALUES AND EIGENVECTORS 165 The linear transformation, F in this context is called the deformation gradient and it describes the local deformation of the material.
Thus it is possible to consider this deformation in terms of two processes, one which distorts the material and the other which just rotates it.
It is the matrix U which is responsible for stretching and compressing.
This is why in continuum mechanics, the stress is often taken to depend on U which is known in this context as the right Cauchy Green strain tensor.
This process of writing a matrix as a product of two such matrices, one of which preserves distance and the other which distorts is also important in applications to geometric measure theory an interesting ﬁeld of study in mathematics and to the study of quadratic forms which occur in many applications such as statistics.
Here I am emphasizing the application to mechanics in which the eigenvectors of U determine the principle directions, those directions in which the material is stretched or compressed to the maximum extent.
Example 7.2.1 Find the principle directions determined by the matrix   29 6 6  161 1411 1119  11 44 44 6 19 41 11 44 44 The eigenvalues are 3,1, and 1.
2 It is nice to be given the eigenvalues.
The largest eigenvalue is 3 which means that in the direction determined by the eigenvector associated with 3 the stretch is three times as large.
The smallest eigenvalue is 1/2 and so in the direction determined by the eigenvector for 1/2 the material is compressed, becoming locally half as long.
It remains to ﬁnd these directions.
First consider the eigenvector for 3.
It is necessary to solve         1 0 0 29 6 6 x 0 3 0 1 0 − 161 1411 1119  y = 0  11 44 44 0 0 1 6 19 41 z 0 11 44 44 Thus the augmented matrix for this system of equations is   4 − 6 − 6 0  −116 9111 −1119 0  11 44 44 − 6 −19 91 0 11 44 44 The row reduced echelon form is   1 0 −3 0  0 1 −1 0  0 0 0 0 and so the principle direction for the eigenvalue 3 in which the material is stretched to the maximum extent is   3   1 .
1 A direction vector in this direction is  √  3/√11   1/√11 .
1/ 11 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation166 SPECTRAL THEORY You should show that the direction in which the material is compressed the most is in the direction   0√  −1/√ 2  1/ 2 Note this is meaningful information which you would have a hard time ﬁnding without the theory of eigenvectors and eigenvalues.
Another application is to the problem of ﬁnding solutions to systems of diﬀerential equations.
It turns out that vibrating systems involving masses and springs can be studied in the form ′′ x =Ax (7.8) where A is a real symmetric n × n matrix which has nonpositive eigenvalues.
This is analogous to the case of the scalar equation for undamped oscillation, x′′+ω2x = 0.
The main diﬀerence is that here the scalar ω2 is replaced with the matrix −A.
Consider the problem of ﬁnding solutions to (7.8).
You look for a solution which is in the form x(t)=veλt (7.9) and substitute this into (7.8).
Thus x′′ =vλ2eλt =eλtAv and so λ2v=Av.
Therefore, λ2 needs to be an eigenvalue of A and v needs to be an eigenvector.
Since A has nonpositive eigenvalues, λ2 = −a2 and so λ = ±ia where −a2 is an eigenvalue of A.
Corresponding to this you obtain solutions of the form x(t)=vcos(at),vsin(at).
Note these solutions oscillate because of the cos(at) and sin(at) in the solutions.
Here is an example.
Example 7.2.2 Find oscillatory solutions to the system of diﬀerential equations, x′′ =Ax where   −5 −1 −1 A= −31 −133 53 .
3 6 6 −1 5 −13 3 6 6 The eigenvalues are −1,−2, and −3.
Accordingtotheabove, youcanﬁndsolutionsbylookingfortheeigenvectors.
Consider the eigenvectors for −3.
The augmented matrix for ﬁnding the eigenvectors is   −4 1 1 0  13 −35 −35 0  3 6 6 1 −5 −5 0 3 6 6 and its row echelon form is   1 0 0 0   0 1 1 0 .
0 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.3.
EXERCISES 167 Therefore, the eigenvectors are of the form   0 v=z −1 .
1 It follows     0 (√ ) 0 (√ )  −1 cos 3t ,  −1 sin 3t 1 1 are both solutions to the system of diﬀerential equations.
You can ﬁnd other oscillatory solutions in the same way by considering the other eigenvalues.
You might try checking these answers to verify they work.
This is just a special case of a procedure used in diﬀerential equations to obtain closed form solutions to systems of diﬀerential equations using linear algebra.
The overall philos- ophy is to take one of the easiest problems in analysis and change it into the eigenvalue problem which is the most diﬃcult problem in algebra.
However, when it works, it gives precise solutions in terms of known functions.
7.3 Exercises 1.
If A is the matrix of a linear transformation which rotates all vectors in R2 through 30◦, explain why A cannot have any real eigenvalues.
2.
If A is an n×n matrix and c is a nonzero constant, compare the eigenvalues of A and cA.
3.
If A is an invertible n × n matrix, compare the eigenvalues of A and A−1.
More generally, for m an arbitrary integer, compare the eigenvalues of A and Am.
4.
Let A,B be invertible n×n matrices which commute.
That is, AB = BA.
Suppose x is an eigenvector of B.
Show that then Ax must also be an eigenvector for B.
5.
Suppose A is an n×n matrix and it satisﬁes Am = A for some m a positive integer larger than 1.
Show that if λ is an eigenvalue of A then |λ| equals either 0 or 1.
6.
Show that if Ax=λx and Ay=λy, then whenever a,b are scalars, A(ax+by)=λ(ax+by).
Does this imply that ax+by is an eigenvector?
Explain.
  −1 −1 7 7.
Find the eigenvalues and eigenvectors of the matrix  −1 0 4 .
Determine −1 −1 5 whether the matrix is defective.
  −3 −7 19 8.
Find the eigenvalues and eigenvectors of the matrix  −2 −1 8 .Determine −2 −3 10 whether the matrix is defective.
  −7 −12 30 9.
Find the eigenvalues and eigenvectors of the matrix  −3 −7 15 .
−3 −6 14 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation168 SPECTRAL THEORY   7 −2 0 10.
Find the eigenvalues and eigenvectors of the matrix  8 −1 0 .
Determine −2 4 6 whether the matrix is defective.
  3 −2 −1   11.
Find the eigenvalues and eigenvectors of the matrix 0 5 1 .
0 2 4   6 8 −23 12.
Find the eigenvalues and eigenvectors of the matrix  4 5 −16 .
Determine 3 4 −12 whether the matrix is defective.
  5 2 −5 13.
Find the eigenvalues and eigenvectors of the matrix  12 3 −10 .
Determine 12 4 −11 whether the matrix is defective.
  20 9 −18 14.
Find the eigenvalues and eigenvectors of the matrix  6 5 −6 .
Determine 30 14 −27 whether the matrix is defective.
  1 26 −17 15.
Findtheeigenvaluesandeigenvectorsofthematrix 4 −4 4 .Determine −9 −18 9 whether the matrix is defective.
  3 −1 −2 16.
Find the eigenvalues and eigenvectors of the matrix  11 3 −9 .
Determine 8 0 −6 whether the matrix is defective.
  −2 1 2 17.
Find the eigenvalues and eigenvectors of the matrix  −11 −2 9 .
Determine −8 0 7 whether the matrix is defective.
  2 1 −1 18.
Findtheeigenvaluesandeigenvectorsofthematrix 2 3 −2 .Determinewhether 2 2 −1 the matrix is defective.
  4 −2 −2 19.
Find the complex eigenvalues and eigenvectors of the matrix  0 2 −2 .
2 0 2   9 6 −3   20.
Find the eigenvalues and eigenvectors of the matrix 0 6 0 .
Determine −3 −6 9 whether the matrix is defective.
  4 −2 −2 21.
Find the complex eigenvalues and eigenvectors of the matrix  0 2 −2 .
De- 2 0 2 termine whether the matrix is defective.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.3.
EXERCISES 169   −4 2 0 22.
Find the complex eigenvalues and eigenvectors of the matrix  2 −4 0 .
−2 2 −2 Determine whether the matrix is defective.
  1 1 −6 23.
Find the complex eigenvalues and eigenvectors of the matrix  7 −5 −6 .
−1 7 2 Determine whether the matrix is defective.
  4 2 0 24.
Find the complex eigenvalues and eigenvectors of the matrix  −2 4 0 .
Deter- −2 2 6 mine whether the matrix is defective.
25.
Here is a matrix.
  1 a 0 0    0 1 b 0    0 0 2 c 0 0 0 2 Find values of a,b,c for which the matrix is defective and values of a,b,c for which it is nondefective.
26.
Here is a matrix.
  a 1 0   0 b 1 0 0 c where a,b,c are numbers.
Show this is sometimes defective depending on the choice of a,b,c.
What is an easy case which will ensure it is not defective?
27.
SupposeAisann×nmatrixconsistingentirelyofrealentriesbuta+ibisacomplex eigenvalue having the eigenvector, x+iy.
Here x and y are real vectors.
Show that then a−ib is also an eigenvalue with the eigenvector, x−iy.
Hint: You should remember that the conjugate of a product of complex numbers equals the product of the conjugates.
Here a+ib is a complex number whose conjugate equals a−ib.
28.
Recall an n×n matrix is said to be symmetric if it has all real entries and if A=AT.
Show the eigenvalues of a real symmetric matrix are real and for each eigenvalue, it has a real eigenvector.
29.
Recall an n×n matrix is said to be skew symmetric if it has all real entries and if A=−AT.
Show that any nonzero eigenvalues must be of the form ib where i2 =−1.
In words, the eigenvalues are either 0 or pure imaginary.
30.
Is it possible for a nonzero matrix to have only 0 as an eigenvalue?
31.
Show that the eigenvalues and eigenvectors of a real matrix occur in conjugate pairs.
32.
Suppose A is an n×n matrix having all real eigenvalues which are distinct.
Show there exists S such that S−1AS =D, a diagonal matrix.
If   λ 0 1   D = ...  0 λ n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation170 SPECTRAL THEORY deﬁne eD by   eλ1 0   eD ≡ ...  0 eλn and deﬁne eA ≡SeDS−1.
Next show that if A is as just described, so is tA where t is a real number and the eigenvalues of At are tλ .
If you diﬀerentiate a matrix of functions entry by entry so k that for the ijth entry of A′(t) you get a′ (t) where a (t) is the ijth entry of A(t), ij ij show ( ) d eAt =AeAt dt ( ) Next show det eAt ̸= 0.
This is called the matrix exponential.
Note I have only deﬁneditforthecasewheretheeigenvaluesofAarereal,butthesameprocedurewill work even for complex eigenvalues.
All you have to do is to deﬁne what is meant by ea+ib.
  7 −1 1 33.
Find the principle directions determined by the matrix  −121 74 −61 .
The 4 12 6 1 −1 2 6 6 3 eigenvalues are 1,1, and 1 listed according to multiplicity.
3 2 34.
Find the principle directions determined by the matrix   5 −1 −1  −31 73 13  The eigenvalues are 1,2, and 1.
What is the physical interpreta- 3 6 6 −1 1 7 3 6 6 tion of the repeated eigenvalue?
35.
Find oscillatory solutions to the system of diﬀerential equations, x′′ =Ax where A=   −3 −1 −1  −1 −2 0  The eigenvalues are −1,−4, and −2.
−1 0 −2 36.
Let A and B be n×n matrices and let the columns of B be b ,··· ,b 1 n and the rows of A are aT,··· ,aT.
1 n Show the columns of AB are Ab ···Ab 1 n and the rows of AB are aTB···aTB.
1 n 37.
Let M be an n×n matrix.
Then deﬁne the adjoint of M, denoted by M∗ to be the transpose of the conjugate of M. For example, ( ) ( ) 2 i ∗ 2 1−i = .
1+i 3 −i 3 A matrix M, is self adjoint if M∗ =M.
Show the eigenvalues of a self adjoint matrix are all real.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.3.
EXERCISES 171 38.
Let M be an n×n matrix and suppose x ,··· ,x are n eigenvectors which form a 1 n linearly independent set.
Form the matrix S by making the columns these vectors.
Show that S−1 exists and that S−1MS is a diagonal matrix (one having zeros every- whereexceptonthemaindiagonal)havingtheeigenvaluesofM onthemaindiagonal.
When this can be done the matrix is said to be diagonalizable.
39.
Showthatan×nmatrixM isdiagonalizableifandonlyifFn hasabasisofeigenvec- tors.
Hint: The ﬁrst part is done in Problem 38.
It only remains to show that if the matrix can be diagonalized by some matrix S giving D = S−1MS for D a diagonal matrix, then it has a basis of eigenvectors.
Try using the columns of the matrix S. 40.
Let   1 2 2   A= 3 4 0  0 1 3 and let   0 1   B = 1 1  2 1 ( ) 1 2 MultiplyABverifyingtheblockmultiplicationformula.
HereA = ,A = 11 3 4 12 ( ) ( ) 2 ,A = 0 1 and A =(3).
0 21 22 41.
SupposeA,B aren×nmatricesandλisanonzeroeigenvalueofAB.Showthatthen it is also an eigenvalue of BA.
Hint: Use the deﬁnition of what it means for λ to be an eigenvalue.
That is, ABx=λx where x̸=0.
Maybe you should multiply both sides by B.
42.
Using the above problem show that if A,B are n×n matrices, it is not possible that AB − BA = aI for any a ̸= 0.
Hint: First show that if A is a matrix, then the eigenvalues of A−aI are λ−a where λ is an eigenvalue of A.
43.
Consider the following matrix.
  0 ··· 0 −a 0  1 0 −a1  C = ... ... ...  0 1 −an−1 Showdet(λI−C)=a0+λa1+···an−1λn−1+λn.
Thismatrixiscalledacompanion matrix for the given polynomial.
44.
A discreet dynamical system is of the form x(k+1)=Ax(k), x(0)=x 0 where A is an n×n matrix and x(k) is a vector in Rn.
Show ﬁrst that x(k)=Akx 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation172 SPECTRAL THEORY for all k ≥ 1.
If A is nondefective so that it has a basis of eigenvectors, {v ,··· ,v } 1 n where Av =λ v j j j youcanwritetheinitialconditionx inauniquewayasalinearcombinationofthese 0 eigenvectors.
Thus ∑n x = a v 0 j j j=1 Now explain why ∑n ∑n x(k)= a Akv = a λkv j j j j j j=1 j=1 which gives a formula for x(k), the solution of the dynamical system.
45.
Suppose A is an n×n matrix and let v be an eigenvector such that Av = λv.
Also suppose the characteristic polynomial of A is det(λI−A)=λn+an−1λn−1+···+a1λ+a0 Explain why ( ) An+an−1An−1+···+a1A+a0I v=0 If A is nondefective, give a very easy proof of the Cayley Hamilton theorem based on this.
Recall this theorem says A satisﬁes its characteristic equation, An+an−1An−1+···+a1A+a0I =0.
46.
Suppose an n×n nondefective matrix A has only 1 and −1 as eigenvalues.
Find A12.
47.
Supposethecharacteristicpolynomialofann×nmatrixAis1−λn.
FindAmn where m is an integer.
Hint: Note ﬁrst that A is nondefective.
Why?
48.
Sometimes sequences come in terms of a recursion formula.
An example is the Fi- bonacci sequence.
x0 =1=x1, xn+1 =xn+xn−1 Show this can be considered as a discreet dynamical system as follows.
( ) ( )( ) ( ) ( ) x 1 1 x x 1 n+1 = n , 1 = xn 1 0 xn−1 x0 1 Now use the technique of Problem 44 to ﬁnd a formula for x .
n 49.
Let A be an n×n matrix having characteristic polynomial det(λI−A)=λn+an−1λn−1+···+a1λ+a0 Show that a =(−1)ndet(A).
0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.4.
SCHUR’S THEOREM 173 7.4 Schur’s Theorem Everymatrixisrelatedtoanuppertriangularmatrixinaparticularlysigniﬁcantway.
This is Schur’s theorem and it is the most important theorem in the spectral theory of matrices.
Lemma 7.4.1 Let {x ,··· ,x } be a basis for Fn.
Then there exists an orthonormal ba- 1 n sis for Fn, {u ,··· ,u } which has the property that for each k ≤ n, span(x ,··· ,x ) = 1 n 1 k span(u ,··· ,u ).
1 k Proof: Let {x ,··· ,x } be a basis for Fn.
Let u ≡ x /|x |.
Thus for k = 1, 1 n 1 1 1 span(u )=span(x )and{u }isanorthonormalset.
Nowsupposeforsomek <n,u ,···, 1 1 1 1 u have been chosen such that (u ·u ) = δ and span(x ,··· ,x ) = span(u ,··· ,u ).
k j l jl 1 k 1 k Then deﬁne ∑ x − k (x ·u )u uk+1 ≡ (cid:12)(cid:12) k+1 ∑j=1 k+1 j j(cid:12)(cid:12), (7.10) (cid:12)x − k (x ·u )u (cid:12) k+1 j=1 k+1 j j where the denominator is not equal to zero because the x form a basis and so j x ∈/ span(x ,··· ,x )=span(u ,··· ,u ) k+1 1 k 1 k Thus by induction, u ∈span(u ,··· ,u ,x )=span(x ,··· ,x ,x ).
k+1 1 k k+1 1 k k+1 Also, x ∈ span(u ,··· ,u ,u ) which is seen easily by solving (7.10) for x and it k+1 1 k k+1 k+1 follows span(x ,··· ,x ,x )=span(u ,··· ,u ,u ).
1 k k+1 1 k k+1 If l≤k,   ∑k (u ·u )=C(x ·u )− (x ·u )(u ·u )= k+1 l k+1 l k+1 j j l j=1   ∑k C(x ·u )− (x ·u )δ =C((x ·u )−(x ·u ))=0.
k+1 l k+1 j lj k+1 l k+1 l j=1 The vectors, {u }n , generated in this way are therefore an orthonormal basis because j j=1 each vector has unit length.
(cid:4) The process by which these vectors were generated is called the Gram Schmidt process.
Here is a fundamental deﬁnition.
Deﬁnition 7.4.2 An n×n matrix U, is unitary if UU∗ =I =U∗U where U∗ is deﬁned to be the transpose of the conjugate of U.
Proposition 7.4.3 Ann×nmatrixisunitaryifandonlyifthecolumnsareanorthonormal set.
Proof: This follows right away from the way we multiply matrices.
If U is an n×n complex matrix, then ∗ ∗ (U U) =u u =(u ,u ) ij i j i j and the matrix is unitary if and only if this equals δ if and only if the columns are ij orthonormal.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation174 SPECTRAL THEORY Theorem 7.4.4 Let A be an n×n matrix.
Then there exists a unitary matrix U such that ∗ U AU =T, (7.11) where T is an upper triangular matrix having the eigenvalues of A on the main diagonal listed according to multiplicity as roots of the characteristic equation.
Proof: The theorem is clearly true if A is a 1×1 matrix.
Just let U = 1 the 1×1 matrix which has 1 down the main diagonal and zeros elsewhere.
Suppose it is true for (n−1)×(n−1) matrices and let A be an n×n matrix.
Then let v be a unit eigenvector 1 for A .
Then there exists λ such that 1 Av =λ v , |v |=1.
1 1 1 1 Extend {v } to a basis and then use Lemma 7.4.1 to obtain {v ,··· ,v }, an orthonormal 1 1 n basis in Fn.
Let U be a matrix whose ith column is v .
Then from the above, it follows U 0 i 0 is unitary.
Then U∗AU is of the form 0 0   λ ∗ ··· ∗ 1    0   .
  .
 .
A 1 0 where A is an n−1×n−1 matrix.
Now by induction there exists an (n−1)×(n−1) 1 e unitary matrix U such that 1 e∗ e U1A1U1 =Tn−1, an upper triangular matrix.
Consider ( ) 1 0 U1 ≡ 0 Ue 1 This is a unitary matrix and ( )( )( ) ( ) 1 0 λ ∗ 1 0 λ ∗ U1∗U0∗AU0U1 = 0 Ue1∗ 01 A1 0 Ue1 = 01 Tn−1 ≡T where T is upper triangular.
Then let U = U U .
Since (U U )∗ = U∗U∗, it follows A 0 1 0 1 1 0 is similar to T and that U U is unitary.
Hence A and T have the same characteristic 0 1 polynomials and since the eigenvalues of T are the diagonal entries listed according to algebraic multiplicity, (cid:4) As a simple consequence of the above theorem, here is an interesting lemma.
Lemma 7.4.5 Let A be of the form   P ··· ∗ 1 A= ... ... ...  0 ··· P s where P is an m ×m matrix.
Then k k k ∏ det(A)= det(P ).
k k Also, the eigenvalues of A consist of the union of the eigenvalues of the P .
j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.4.
SCHUR’S THEOREM 175 Proof: Let U be an m ×m unitary matrix such that k k k ∗ U P U =T k k k k where T is upper triangular.
Then it follows that for k     U ··· 0 U∗ ··· 0 1 1 U ≡ ... ... ... , U∗ = ... ... ...  0 ··· U 0 ··· U∗ s s and also       U∗ ··· 0 P ··· ∗ U ··· 0 T ··· ∗ 1 1 1 1  ... ... ...  ... ... ...  ... ... ... = ... ... ... .
0 ··· U∗ 0 ··· P 0 ··· U 0 ··· T s s s s Therefore,sincethedeterminantofanuppertriangularmatrixistheproductofthediagonal entries, ∏ ∏ det(A)= det(T )= det(P ).
k k k k From the above formula, the eigenvalues of A consist of the eigenvalues of the upper trian- gular matrices T , and each T has the same eigenvalues as P .
(cid:4) k k k What if A is a real matrix and you only want to consider real unitary matrices?
Theorem 7.4.6 Let A be a real n×n matrix.
Then there exists a real unitary matrix Q and a matrix T of the form   P ··· ∗ 1 T = ... ...  (7.12) 0 P r where P equals either a real 1×1 matrix or P equals a real 2×2 matrix having as its i i eigenvalues a conjugate pair of eigenvalues of A such that QTAQ = T. The matrix T is called the real Schur form of the matrix A.
Recall that a real unitary matrix is also called an orthogonal matrix.
Proof: Suppose Av =λ v , |v |=1 1 1 1 1 where λ is real.
Then let {v ,··· ,v } be an orthonormal basis of vectors in Rn.
Let Q 1 1 n 0 be a matrix whose ith column is v .
Then Q∗AQ is of the form i 0 0   λ ∗ ··· ∗ 1    0   .
  .
 .
A 1 0 where A is a real n−1×n−1 matrix.
This is just like the proof of Theorem 7.4.4 up to 1 this point.
Now consider the case where λ = α+iβ where β ̸= 0.
It follows since A is real that 1 v = z +iw and that v = z −iw is an eigenvector for the eigenvalue α−iβ.
Here 1 1 1 1 1 1 z and w are real vectors.
Since v and v are eigenvectors corresponding to distinct 1 1 1 1 eigenvalues, they form a linearly independent set.
From this it follows that {z ,w } is an 1 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation176 SPECTRAL THEORY independent set of vectors in Cn, hence in Rn.
Indeed,{v ,v } is an independent set and 1 1 alsospan(v ,v )=span(z ,w ).NowusingtheGramSchmidttheoreminRn,thereexists 1 1 1 1 {u ,u }, an orthonormal set of real vectors such that span(u ,u ) = span(v ,v ).
For 1 2 1 2 1 1 example, |z |2w −(w ·z )z u1 =z1/|z1|, u2 = (cid:12)(cid:12) 1 1 1 1 1(cid:12)(cid:12) (cid:12)|z |2w −(w ·z )z (cid:12) 1 1 1 1 1 Let {u ,u ,··· ,u } be an orthonormal basis in Rn and let Q be a unitary matrix whose 1 2 n 0 ith column is u so Q is a real orthogonal matrix.
Then Au are both in span(u ,u ) for i 0 j 1 2 j =1,2 and so uTAu =0 whenever k ≥3.
It follows that Q∗AQ is of the form k j 0 0   ∗ ∗ ··· ∗  ∗ ∗  ( ) Q∗AQ = 0 = P1 ∗ 0 0  ..  0 A1 .
A 1 0 where A is now an n−2×n−2 matrix and P is a 2×2 matrix.
Now this is similar to A 1 1 and so two of its eigenvalues are α+iβ and α−iβ.
Now ﬁnd Qe an n−2×n−2 matrix to put A in an appropriate form as above and 1 1 come up with A either an n−4×n−4 matrix or an n−3×n−3 matrix.
Then the only 2 other diﬀerence is to let   1 0 0 ··· 0  0 1 0 ··· 0    Q = 0 0  1  .
.
  .
.
e  .
.
Q 1 0 0 thusputtinga2×2identitymatrixintheupperleftcornerratherthanaone.
Repeatingthis process with the above modiﬁcation for the case of a complex eigenvalue leads eventually to (7.12) where Q is the product of real unitary matrices Q above.
When the block P is i i 2×2, its eigenvalues are a conjugate pair of eigenvalues of A and if it is 1×1 it is a real eigenvalue of A.
Here is why this last claim is true   λI −P ··· ∗ 1 1 λI−T = ... ...  0 λI −P r r where I is the 2×2 identity matrix in the case that P is 2×2 and is the number 1 in the k k case where P is a 1×1 matrix.
Now by Lemma 7.4.5, k ∏r det(λI −T)= det(λI −P ).
k k k=1 Therefore, λ is an eigenvalue of T if and only if it is an eigenvalue of some P .
This proves k the theorem since the eigenvalues of T are the same as those of A including multiplicity because they have the same characteristic polynomial due to the similarity of A and T. (cid:4) Corollary 7.4.7 Let A be a real n×n matrix having only real eigenvalues.
Then there exists a real orthogonal matrix Q and an upper triangular matrix T such that QTAQ=T Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.4.
SCHUR’S THEOREM 177 and furthermore, if the eigenvalues of A are listed in decreasing order, λ ≥λ ≥···≥λ 1 2 n Q can be chosen such that T is of the form   λ ∗ ··· ∗ 1  0 λ2 ... ...   ... ... ... ∗  0 ··· 0 λ n Proof: Most of this follows right away from Theorem 7.4.6.
It remains to verify the claim that the diagonal entries can be arranged in the desired order.
However, this follows from a simple modiﬁcation of the above argument.
When you ﬁnd v the eigenvalue of λ , 1 1 just be sure λ is chosen to be the largest eigenvalue.
Then observe that from Lemma 7.4.5 1 applied to the characteristic equation, the eigenvalues of the (n−1)×(n−1) matrix A 1 are {λ ,··· ,λ }.
Then pick λ to continue the process of construction with A .
(cid:4) 1 n 2 1 Of course there is a similar conclusion which can be proved exactly the same way in the case where A has complex eigenvalues.
Corollary 7.4.8 Let A be a real n×n matrix.
Then there exists a real orthogonal matrix Q and an upper triangular matrix T such that   P ··· ∗ 1 QTAQ=T = ... ...  0 P r where P equals either a real 1×1 matrix or P equals a real 2×2 matrix having as its i i eigenvalues a conjugate pair of eigenvalues of A.
If P corresponds to the two eigenvalues k α ±iβ ≡σ(P ), Q can be chosen such that k k k |σ(P )|≥|σ(P )|≥··· 1 2 where √ |σ(P )|≡ α2 +β2 k k k The blocks, P can be arranged in any other order also.
k Deﬁnition 7.4.9 When a linear transformation, A, mapping a linear space, V to V has a basis of eigenvectors, the linear transformation is called non defective.
Otherwise it is called defective.
An n×n matrix A, is called normal if AA∗ =A∗A.
An important class of normal matrices is that of the Hermitian or self adjoint matrices.
An n×n matrix A is self adjoint or Hermitian if A=A∗.
Thenextlemmaisthebasisforconcludingthateverynormalmatrixisunitarilysimilar to a diagonal matrix.
Lemma 7.4.10 If T is upper triangular and normal, then T is a diagonal matrix.
Proof:This is obviously true if T is 1×1.
In fact, it can’t help being diagonal in this case.
Suppose then that the lemma is true for (n−1)×(n−1) matrices and let T be an upper triangular normal n×n matrix.
Thus T is of the form ( ) ( ) t a∗ t 0T T = 11 , T∗ = 11 0 T a T∗ 1 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation178 SPECTRAL THEORY Then ( )( ) ( ) TT∗ = t11 a∗ t11 0T = |t11|2+a∗a a∗T1∗ 0 T a T∗ T a T T∗ ( 1 )( 1 ) ( 1 1 1 ) T∗T = t11 0T t11 a∗ = |t11|2 t11a∗ a T∗ 0 T at aa∗+T∗T 1 1 11 1 1 Since these two matrices are equal, it follows a=0.
But now it follows that T∗T = T T∗ 1 1 1 1 and so by induction T is a diagonal matrix D .
Therefore, 1 1 ( ) t 0T T = 11 0 D 1 a diagonal matrix.
Now here is a proof which doesn’t involve block multiplication.
Since T is normal, T∗T = TT∗.
Writing this in terms of components and using the description of the adjoint as the transpose of the conjugate, yields the following for the ikth entry of T∗T =TT∗.
z T}T|∗ { z T}∗|T { ∑ ∑ ∑ ∑ ∗ ∗ t t = t t = t t = t t .
ij jk ij kj ij jk ji jk j j j j Now use the fact that T is upper triangular and let i=k =1 to obtain the following from the above.
∑ ∑ |t |2 = |t |2 =|t |2 1j j1 11 j j You see, t =0 unless j =1 due to the assumption that T is upper triangular.
This shows j1 T is of the form   ∗ 0 ··· 0  0 ∗ ··· ∗   ... ... ... ... .
0 ··· 0 ∗ Now do the same thing only this time take i = k = 2 and use the result just established.
Thus, from the above, ∑ ∑ |t |2 = |t |2 =|t |2, 2j j2 22 j j showing that t =0 if j >2 which means T has the form 2j   ∗ 0 0 ··· 0  0 ∗ 0 ··· 0   0 0 ∗ ··· ∗ .
 ... ... ... ... ...  0 0 0 0 ∗ Next let i = k = 3 and obtain that T looks like a diagonal matrix in so far as the ﬁrst 3 rows and columns are concerned.
Continuing in this way it follows T is a diagonal matrix.
(cid:4) Theorem 7.4.11 Let A be a normal matrix.
Then there exists a unitary matrix U such that U∗AU is a diagonal matrix.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.4.
SCHUR’S THEOREM 179 Proof: From Theorem 7.4.4 there exists a unitary matrix U such that U∗AU equals an upper triangular matrix.
The theorem is now proved if it is shown that the property of being normal is preserved under unitary similarity transformations.
That is, verify that if A is normal and if B =U∗AU, then B is also normal.
But this is easy.
∗ ∗ ∗ ∗ ∗ ∗ B B = U A UU AU =U A AU ∗ ∗ ∗ ∗ ∗ ∗ = U AA U =U AUU A U =BB .
Therefore, U∗AU is a normal and upper triangular matrix and by Lemma 7.4.10 it mustbe a diagonal matrix.
(cid:4) Corollary 7.4.12 If A is Hermitian, then all the eigenvalues of A are real and there exists an orthonormal basis of eigenvectors.
Proof: Since A is normal, there exists unitary, U such that U∗AU = D, a diagonal matrixwhosediagonalentriesaretheeigenvaluesofA.Therefore,D∗ =U∗A∗U =U∗AU = D showing D is real.
Finally, let ( ) U = u u ··· u 1 2 n where the u denote the columns of U and i   λ 0 1   D = ...  0 λ n The equation, U∗AU =D implies ( ) AU = Au Au ··· Au 1( 2 n ) = UD = λ u λ u ··· λ u 1 1 2 2 n n where the entries denote the columns of AU and UD respectively.
Therefore, Au = λ u i i i and since the matrix is unitary, the ijth entry of U∗U equals δ and so ij δ =u∗u ≡u ·u .
ij i j j i This proves the corollary because it shows the vectors {u } are orthonormal.
Therefore, i they form a basis because every orthonormal set of vectors is linearly independent.
(cid:4) Corollary 7.4.13 If A is a real symmetric matrix, then A is Hermitian and there exists a real unitary matrix U such that UTAU = D where D is a diagonal matrix whose diagonal entries are the eigenvalues of A.
By arranging the columns of U the diagonal entries of D can be made to appear in any order.
Proof: This follows from Theorem 7.4.6 and Corollary 7.4.12.
Let ( ) U = u ··· u 1 n Then AU =UD so ( ) ( ) ( ) AU = Au ··· Au = u ··· u D = λ u ··· λ u 1 n 1 n 1 1 n n HenceeachcolumnofU isaneigenvectorofA.
Itfollowsthatbyrearrangingthesecolumns, the entries of D on the main diagonal can be made to appear in any order.
To see this, consider such a rearrangement resulting in an orthogonal matrix U′ given by ( ) U′ = u ··· u i1 in Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation180 SPECTRAL THEORY Then ( ) U′TAU′ =U′T Au ··· Au   i1  in  uT λ 0 = ...i1 ( λi1ui1 ··· λinuin )= i1 ...  (cid:4) uT 0 λ in in 7.5 Trace And Determinant The determinant has already been discussed.
It is also clear that if A = S−1BS so that A,B are similar, then ( ) ( ) det(A) = det S−1 det(S)det(B)=det S−1S det(B) = det(I)det(B)=det(B) The trace is deﬁned in the following deﬁnition.
Deﬁnition 7.5.1 Let A be an n×n matrix whose ijth entry is denoted as a .
Then ij ∑ trace(A)≡ a ii i In other words it is the sum of the entries down the main diagonal.
With this deﬁnition, it is easy to see that if A=S−1BS, then trace(A)=trace(B).
Here is why.
∑ ∑( ) ∑ ∑ ( ) trace(A) ≡ A = S−1 B S = B S S−1 ii ij jk ki jk ki ij ∑i i,j,k∑ j,k i = B δ = B =trace(B).
jk kj kk j,k k Alternatively, ∑ trace(AB)≡ A B =trace(BA).
ij ji ij Therefore, ( ) ( ) trace S−1AS =trace ASS−1 =trace(A).
Theorem 7.5.2 LetAbeann×nmatrix.
Thentrace(A)equalsthesumoftheeigenvalues of A and det(A) equals the product of the eigenvalues of A.
This is proved using Schur’s theorem and is in Problem 17 below.
Another important property of the trace is in the following theorem.
Theorem 7.5.3 Let A be an m×n matrix and let B be an n×m matrix.
Then trace(AB)=trace(BA).
Proof: ( ) ∑ ∑ ∑∑ trace(AB)≡ A B = B A =trace(BA) (cid:4) ik ki ki ik i k k i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.6.
QUADRATIC FORMS 181 7.6 Quadratic Forms Deﬁnition 7.6.1 A quadratic form in three dimensions is an expression of the form   ( ) x   x y z A y (7.13) z where A is a 3×3 symmetric matrix.
In higher dimensions the idea is the same except you use a larger symmetric matrix in place of A.
In two dimensions A is a 2×2 matrix.
For example, consider    ( ) 3 −4 1 x x y z  −4 0 −4  y  (7.14) 1 −4 3 z whichequals3x2−8xy+2xz−8yz+3z2.Thisisveryawkwardbecauseofthemixedterms such as −8xy.
The idea is to pick diﬀerent axes such that if x,y,z are taken with respect to these axes, the quadratic form is much simpler.
In other words, look for new variables, x′,y′, and z′ and a unitary matrix U such that     x′ x U y′ = y  (7.15) z′ z andifyouwritethequadraticformintermsoftheprimedvariables, therewillbenomixed terms.
Any symmetric real matrix is Hermitian and is therefore normal.
From Corollary 7.4.13, it follows there exists a real unitary matrix U, (an orthogonal matrix) such that UTAU =D a diagonal matrix.
Thus in the quadratic form, (7.13)     ( ) x ( ) x′ x y z A y  = x′ y′ z′ UTAU y′  z z′   ( ) x′ = x′ y′ z′ D y′  z′ and in terms of these new variables, the quadratic form becomes ′ 2 ′ 2 ′ 2 λ (x) +λ (y ) +λ (z ) 1 2 3 where D = diag(λ ,λ ,λ ).
Similar considerations apply equally well in any other dimen- 1 2 3 sion.
For the given example,  √ √   −1√ 2 √0 1√2 3 −4 1  12√6 1 √6 21√6  −4 0 −4 · 6 3 6 1 3 −1 3 1 3 1 −4 3 3 3 3     −√1 √1 √1 2 0 0  2 6 3   0 √2 −√1 = 0 −4 0  6 3 √1 √1 √1 0 0 8 2 6 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation182 SPECTRAL THEORY and so if the new variables are given by      −√1 √1 √1 x′ x  2 6 3   0 √2 −√1  y′ = y , 6 3 √1 √1 √1 z′ z 2 6 3 it follows that in terms of the new variables the quadratic form is 2(x′)2−4(y′)2+8(z′)2.
You can work other examples the same way.
7.7 Second Derivative Test Undercertainconditionsthemixed partial derivativeswillalwaysbeequal.
Thisaston- ishing fact was ﬁrst observed by Euler around 1734.
It is also called Clairaut’s theorem.
Theorem 7.7.1 Suppose f :U ⊆F2 →R where U is an open set on which f ,f , f and x y xy f exist.
Then if f and f are continuous at the point (x,y)∈U, it follows yx xy yx f (x,y)=f (x,y).
xy yx Proof: Since U is open, there exists r >0 such that B((x,y),r)⊆U.
Now let |t|,|s|< r/2,t,s real numbers and consider z h}(|t) { z h}(|0) { 1 ∆(s,t)≡ {f(x+t,y+s)−f(x+t,y)−(f(x,y+s)−f(x,y))}.
(7.16) st Note that (x+t,y+s)∈U because ( ) |(x+t,y+s)−(x,y)| = |(t,s)|= t2+s2 1/2 ( ) r2 r2 1/2 r ≤ + = √ <r.
4 4 2 Asimpliedabove,h(t)≡f(x+t,y+s)−f(x+t,y).
Therefore,bythemeanvaluetheorem from calculus and the (one variable) chain rule, 1 1 ∆(s,t) = (h(t)−h(0))= h′(αt)t st st 1 = (f (x+αt,y+s)−f (x+αt,y)) s x x for some α∈(0,1).
Applying the mean value theorem again, ∆(s,t)=f (x+αt,y+βs) xy where α,β ∈(0,1).
If the terms f(x+t,y) and f(x,y+s) are interchanged in (7.16), ∆(s,t) is unchanged and the above argument shows there exist γ,δ ∈(0,1) such that ∆(s,t)=f (x+γt,y+δs).
yx Letting (s,t)→(0,0) and using the continuity of f and f at (x,y), xy yx lim ∆(s,t)=f (x,y)=f (x,y).
(cid:4) xy yx (s,t)→(0,0) Thefollowingisobtainedfromtheabovebysimplyﬁxingallthevariablesexceptforthe two of interest.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.7.
SECOND DERIVATIVE TEST 183 Corollary 7.7.2 Suppose U is an open subset of Fn and f : U → R has the property that for two indices, k,l, f , f ,f , and f exist on U and f and f are both xk xl xlxk xkxl xkxl xlxk continuous at x∈U.
Then f (x)=f (x).
xkxl xlxk Thus the theorem asserts that the mixed partial derivatives are equal at x if they are deﬁned near x and continuous at x.
Now recall the Taylor formula with the Lagrange form of the remainder.
What follows is a proof of this important result based on the mean value theorem or Rolle’s theorem.
Theorem 7.7.3 Suppose f has n+1 derivatives on an interval, (a,b) and let c ∈ (a,b).
Then if x∈(a,b), there exists ξ between c and x such that ∑n f(k)(c) f(n+1)(ξ) f(x)=f(c)+ (x−c)k+ (x−c)n+1.
k!
(n+1)!
k=1 ∑ (In this formula, the symbol 0 a will denote the number 0.)
k=1 k Proof: If n = 0 then the theorem is true because it is just the mean value theorem.
Supposethetheoremistrueforn−1,n≥1.Itcanbeassumedx̸=cbecauseifx=cthere is nothing to show.
Then there exists K such that ( ) ∑n f(k)(c) f(x)− f(c)+ (x−c)k+K(x−c)n+1 =0 (7.17) k!
k=1 In fact, ( ) ∑ −f(x)+ f(c)+ n f(k)(c)(x−c)k k=1 k!
K = .
(x−c)n+1 Now deﬁne F (t) for t in the closed interval determined by x and c by ( ) ∑n f(k)(c) F (t)≡f(x)− f(t)+ (x−t)k+K(x−t)n+1 .
k!
k=1 The c in (7.17) got replaced by t. Therefore, F (c) = 0 by the way K was chosen and also F (x) = 0.
By the mean value theoremorRolle’stheorem,thereexistst betweenxandcsuchthatF′(t )=0.Therefore, 1 1 0 = f′(t )−∑n f(k)(c)k(x−t )k−1−K(n+1)(x−t )n 1 k!
1 1 k(=1 ) n∑−1f(k+1)(c) = f′(t )− f′(c)+ (x−t )k −K(n+1)(x−t )n 1 k!
1 1 ( k=1 ) n∑−1f′(k)(c) = f′(t )− f′(c)+ (x−t )k −K(n+1)(x−t )n 1 k!
1 1 k=1 By induction applied to f′, there exists ξ between x and t such that the above simpliﬁes 1 to f′(n)(ξ)(x−t )n 0 = 1 −K(n+1)(x−t )n n!
1 f(n+1)(ξ)(x−t )n = 1 −K(n+1)(x−t )n n!
1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation184 SPECTRAL THEORY therefore, f(n+1)(ξ) f(n+1)(ξ) K = = (n+1)n!
(n+1)!
and the formula is true for n. (cid:4) The following is a special case and is what will be used.
Theorem 7.7.4 Leth:(−δ,1+δ)→R havem+1derivatives.
Thenthereexistst∈[0,1] such that ∑m h(k)(0) h(m+1)(t) h(1)=h(0)+ + .
k!
(m+1)!
k=1 Now let f :U →R where U ⊆Rn and suppose f ∈Cm(U).
Let x∈U and let r >0 be such that B(x,r)⊆U.
Then for ||v||<r, consider f(x+tv)−f(x)≡h(t) for t∈[0,1].
Then by the chain rule, ∑n ∂f ∑n ∑n ∂2f h′(t)= (x+tv)v , h′′(t)= (x+tv)v v (cid:4) ∂x k ∂x ∂x k j k j k k=1 k=1j=1 Then from the Taylor formula stopping at the second derivative, the following theorem can be obtained.
Theorem 7.7.5 Let f :U →R and let f ∈C2(U).
Then if B(x,r)⊆U, and ||v||<r, there exists t∈(0,1) such that.
∑n ∂f 1∑n ∑n ∂2f f(x+v)=f(x)+ (x)v + (x+tv)v v (7.18) ∂x k 2 ∂x ∂x k j k j k k=1 k=1j=1 Deﬁnition 7.7.6 Deﬁne the following matrix.
∂2f(x+tv) H (x+tv)≡ .
ij ∂x ∂x j i It is called the Hessian matrix.
From Corollary 7.7.2, this is a symmetric matrix.
Then in terms of this matrix, (7.18) can be written as ∑n ∂f 1 f(x+v)=f(x)+ (x)v + vTH(x+tv)v ∂x k 2 j j=1 Then this implies f(x+v)= ∑n ( ) ∂f 1 1 f(x)+ (x)v + vTH(x)v+ vT (H(x+tv)−H(x))v .
(7.19) ∂x k 2 2 j j=1 Using the above formula, here is the second derivative test.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.7.
SECOND DERIVATIVE TEST 185 Theorem 7.7.7 In the above situation, suppose f (x)=0 for each x .
Then if H(x) has xj j all positive eigenvalues, x is a local minimum for f. If H(x) has all negative eigenvalues, then x is a local maximum.
If H(x) has a positive eigenvalue, then there exists a direction in which f has a local minimum at x, while if H(x) has a negative eigenvalue, there exists a direction in which H(x) has a local maximum at x.
Proof: Since f (x)=0 for each x , formula (7.19) implies xj j ( ) 1 1 f(x+v)=f(x)+ vTH(x)v+ vT (H(x+tv)−H(x))v 2 2 whereH(x)isasymmetricmatrix.
Thus,byCorollary7.4.12H(x)hasallrealeigenvalues.
Suppose ﬁrst that H(x) has all positive eigenvalues and that all are larger than δ2 > 0.
Then∑H(x)hasanorthonormalbasisofeigenvectors,{vi}ni=1 andifuisanarbitraryvector, u= n u v where u =u·v .
Thus j=1 j j j j ( )   ∑n ∑n ∑n ∑n uTH(x)u= u vT H(x) u v = u2λ ≥δ2 u2 =δ2|u|2.
k k j j j j j k=1 j=1 j=1 j=1 From (7.19) and the continuity of H, if v is small enough, 1 1 δ2 f(x+v)≥f(x)+ δ2|v|2− δ2|v|2 =f(x)+ |v|2.
2 4 4 This shows the ﬁrst claim of the theorem.
The second claim follows from similar reasoning.
SupposeH(x)hasapositiveeigenvalueλ2.Thenletvbeaneigenvectorforthiseigenvalue.
From (7.19), ( ) 1 1 f(x+tv)=f(x)+ t2vTH(x)v+ t2 vT (H(x+tv)−H(x))v 2 2 which implies ( ) 1 1 f(x+tv) = f(x)+ t2λ2|v|2+ t2 vT (H(x+tv)−H(x))v 2 2 1 ≥ f(x)+ t2λ2|v|2 4 whenever t is small enough.
Thus in the direction v the function has a local minimum at x.
The assertion about the local maximum in some direction follows similarly.
(cid:4) This theorem is an analogue of the second derivative test for higher dimensions.
As in one dimension, when there is a zero eigenvalue, it may be impossible to determine from the Hessianmatrixwhatthelocalqualitativebehaviorofthefunctionis.
Forexample,consider f (x,y)=x4+y2, f (x,y)=−x4+y2.
1 2 Then Df (0,0)=0 and for both functions, the Hessian matrix evaluated at (0,0)equals i ( ) 0 0 0 2 but the behavior of the two functions is very diﬀerent near the origin.
The second has a saddle point while the ﬁrst has a minimum there.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation186 SPECTRAL THEORY 7.8 The Estimation Of Eigenvalues There are ways to estimate the eigenvalues for matrices.
The most famous is known as Gerschgorin’stheorem.
Thistheoremgivesaroughideawheretheeigenvaluesarejustfrom looking at the matrix.
Theorem 7.8.1 Let A be an n×n matrix.
Consider the n Gerschgorin discs deﬁned as    ∑  D ≡ λ∈C:|λ−a |≤ |a | .
i  ii ij  j̸=i Then every eigenvalue is contained in some Gerschgorin disc.
This theorem says to add up the absolute values of the entries of the ith row which are oﬀ the main diagonal and form the disc centered at a having this radius.
The union of ii these discs contains σ(A).
Proof: Suppose Ax=λx where x̸=0.
Then for A=(a ) ij ∑ a x =(λ−a )x .
ij j ii i j̸=i Therefore, picking k such that |x | ≥ |x | for all x , it follows that |x | ̸= 0 since |x| ̸= 0 k j j k and ∑ ∑ |x | |a |≥ |a ||x |≥|λ−a ||x |.
k ij ij j ii k j̸=i j̸=i Now dividing by |x |, it follows λ is contained in the kth Gerschgorin disc.
(cid:4) k Example 7.8.2 Here is a matrix.
Estimate its eigenvalues.
  2 1 1   3 5 0 0 1 9 According to Gerschgorin’s theorem the eigenvalues are contained in the disks D ={λ∈C:|λ−2|≤2},D ={λ∈C:|λ−5|≤3}, 1 2 D ={λ∈C:|λ−9|≤1} 3 It is important to observe that these disks are in the complex plane.
In general this is the case.
If you want to ﬁnd eigenvalues they will be complex numbers.
iy x 2 5 9 So what are the values of the eigenvalues?
In this case they are real.
You can compute them by graphing the characteristic polynomial, λ3 − 16λ2 + 70λ − 66 and then zoom- ing in on the zeros.
If you do this you ﬁnd the solution is {λ=1.2953},{λ=5.5905}, {λ=9.1142}.
Of course these are only approximations and so this information is useless Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.9.
ADVANCED THEOREMS 187 for ﬁnding eigenvectors.
However, in many applications, it is the size of the eigenvalues which is important and so these numerical values would be helpful for such applications.
In this case, you might think there is no real reason for Gerschgorin’s theorem.
Why not just compute the characteristic equation and graph and zoom?
This is ﬁne up to a point, but what if the matrix was huge?
Then it might be hard to ﬁnd the characteristic polynomial.
Remember the diﬃculties in expanding a big matrix along a row or column.
Also, what if the eigenvalues were complex?
You don’t see these by following this procedure.
However, Gerschgorin’s theorem will at least estimate them.
7.9 Advanced Theorems More can be said but this requires some theory from complex variables1.
The following is a fundamental theorem about counting zeros.
Theorem 7.9.1 Let U be a region and let γ : [a,b] → U be closed, continuous, bounded variation, and the winding number, n(γ,z) = 0 for all z ∈/ U.
Suppose also that f is analytic on U having zeros a ,··· ,a where the zeros are repeated according to multiplicity, 1 m and suppose that none of these zeros are on γ([a,b]).
Then ∫ 1 f′(z) ∑m dz = n(γ,a ).
2πi f(z) k γ k=1 ∏ Proof: It is given that f(z) = m (z−a )g(z) where g(z) ̸= 0 on U.
Hence using j=1 j the product rule, f′(z) ∑m 1 g′(z) = + f(z) z−a g(z) j j=1 where g′(z) is analytic on U and so g(z) ∫ ∫ 1 f′(z) ∑m 1 g′(z) ∑m dz = n(γ,a )+ dz = n(γ,a ).
(cid:4) 2πi f(z) j 2πi g(z) j γ j=1 γ j=1 Now let A be an n×n matrix.
Recall that the eigenvalues of A are given by the zeros of the polynomial, p (z) = det(zI −A) where I is the n × n identity.
You can argue A that small changes in A will produce small changes in p (z) and p′ (z).
Let γ denote a A A k very small closed circle which winds around z , one of the eigenvalues of A, in the counter k clockwise direction so that n(γ ,z )=1.
This circle is to enclose only z and is to have no k k k other eigenvalue on it.
Then apply Theorem 7.9.1.
According to this theorem ∫ 1 p′ (z) A dz 2πi p (z) γ A is always an integer equal to the multiplicity of z as a root of p (t).
Therefore, small k A changesinAresultinnochangetotheabovecontourintegralbecauseitmustbeaninteger andsmallchangesinAresultinsmallchangesintheintegral.
ThereforewheneverB isclose enough to A, the two matrices have the same number of zeros inside γ , the zeros being k countedaccordingtomultiplicity.
Bymakingtheradiusofthesmallcircleequaltoεwhere ε is less than the minimum distance between any two distinct eigenvalues of A, this shows that if B is close enough to A, every eigenvalue of B is closer than ε to some eigenvalue of A.
(cid:4) 1Ifyouhaven’tstudiedthetheoryofacomplexvariable,youshouldskipthissectionbecauseyouwon’t understandanyofit.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation188 SPECTRAL THEORY Theorem 7.9.2 If λ is an eigenvalue of A, then if all the entries of B are close enough to the corresponding entries of A, some eigenvalue of B will be within ε of λ.
Consider the situation that A(t) is an n×n matrix and that t→A(t) is continuous for t∈[0,1].
Lemma 7.9.3 Let λ(t)∈σ(A(t)) for t<1 and let Σt =∪s≥tσ(A(s)).
Also let Kt be the connected component of λ(t) in Σ .
Then there exists η >0 such that K ∩σ(A(s))̸=∅ for t t all s∈[t,t+η].
Proof: Denote by D(λ(t),δ) the disc centered at λ(t) having radius δ >0, with other occurrences of this notation being deﬁned similarly.
Thus D(λ(t),δ)≡{z ∈C:|λ(t)−z|≤δ}.
Suppose δ > 0 is small enough that λ(t) is the only element of σ(A(t)) contained in D(λ(t),δ)andthatp hasnozeroesontheboundaryofthisdisc.
Thenbycontinuity,and A(t) the above discussion and theorem, there exists η >0,t+η <1, such that for s∈[t,t+η], p also has no zeroes on the boundary of this disc and A(s) has the same number A(s) of eigenvalues, counted according to multiplicity, in the disc as A(t).
Thus σ(A(s)) ∩ D(λ(t),δ)̸=∅ for all s∈[t,t+η].
Now let ∪ H = σ(A(s))∩D(λ(t),δ).
s∈[t,t+η] It will be shown that H is connected.
Suppose not.
Then H = P ∪Q where P,Q are separatedandλ(t)∈P.Lets ≡inf{s:λ(s)∈Q for some λ(s)∈σ(A(s))}.Thereexists 0 λ(s ) ∈ σ(A(s ))∩D(λ(t),δ).
If λ(s ) ∈/ Q, then from the above discussion there are 0 0 0 λ(s)∈σ(A(s))∩Qfors>s arbitrarilyclosetoλ(s ).Therefore,λ(s )∈Qwhichshows 0 0 0 that s > t because λ(t) is the only element of σ(A(t)) in D(λ(t),δ) and λ(t) ∈ P. Now 0 lets ↑s .Thenλ(s )∈P foranyλ(s )∈σ(A(s ))∩D(λ(t),δ)andalsoitfollowsfrom n 0 n n n the above discussion that for some choice of s → s , λ(s ) → λ(s ) which contradicts P n 0 n 0 and Q separated and nonempty.
Since P is nonempty, this shows Q = ∅.
Therefore, H is connected as claimed.
But K ⊇H and so K ∩σ(A(s))̸=∅ for all s∈[t,t+η].
(cid:4) t t Theorem 7.9.4 Suppose A(t) is an n×n matrix and that t → A(t) is continuous for t ∈ [0,1].
Let λ(0) ∈σ(A(0)) and deﬁne Σ ≡∪t∈[0,1]σ(A(t)).
Let Kλ(0) = K0 denote the connected component of λ(0) in Σ.
Then K ∩σ(A(t))̸=∅ for all t∈[0,1].
0 Proof: Let S ≡ {t∈[0,1]:K ∩σ(A(s))̸=∅ for all s∈[0,t]}.
Then 0 ∈ S. Let t = 0 0 sup(S).
Say σ(A(t ))=λ (t ),··· ,λ (t ).
0 1 0 r 0 Claim: At least one of these is a limit point of K and consequently must be in K 0 0 which shows that S has a last point.
Why is this claim true?
Let s ↑ t so s ∈ S. n 0 n Now let the discs, D(λ (t ),δ),i=1,··· ,r be disjoint with p having no zeroes on γ i 0 A(t0) i the boundary of D(λ (t ),δ).
Then for n large enough it follows from Theorem 7.9.1 and i 0 the discussion following it that σ(A(s )) is contained in ∪r D(λ (t ),δ).
It follows that n i=1 i 0 K ∩(σ(A(t ))+D(0,δ)) ̸= ∅ for all δ small enough.
This requires at least one of the 0 0 λ (t ) to be in K .
Therefore, t ∈S and S has a last point.
i 0 0 0 Now by Lemma 7.9.3, if t < 1, then K ∪K would be a strictly larger connected set 0 0 t containing λ(0).
(The reason this would be strictly larger is that K ∩σ(A(s)) = ∅ for 0 some s∈(t,t+η) while K ∩σ(A(s))̸=∅ for all s∈[t,t+η].)
Therefore, t =1.
(cid:4) t 0 Corollary 7.9.5 Suppose one of the Gerschgorin discs, D is disjoint from the union of i the others.
Then D contains an eigenvalue of A.
Also, if there are n disjoint Gerschgorin i discs, then each one contains an eigenvalue of A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.9.
ADVANCED THEOREMS 189 ( ) Proof: DenotebyA(t)thematrix at whereifi̸=j,at =ta andat =a .Thusto ij ij ij ii ii getA(t)multiplyallnondiagonaltermsbyt.Lett∈[0,1].ThenA(0)=diag(a ,··· ,a ) 11 nn and A(1) = A.
Furthermore, the map, t → A(t) is continuous.
Denote by Dt the Ger- j schgorin disc obtained from the jth row for the matrix A(t).
Then it is clear that Dt ⊆D j j the jth Gerschgorin disc for A.
It follows a is the eigenvalue for A(0) which is contained ii in the disc, consisting of the single point a which is contained in D .
Letting K be the ii i connected component in Σ for Σ deﬁned in Theorem 7.9.4 which is determined by a , Ger- ii schgorin’s theorem implies that K ∩σ(A(t)) ⊆ ∪nj=1Djt ⊆ ∪nj=1Dj = Di ∪(∪j̸=iDj) and also,sinceK isconnected,therearenotpointsofK inbothDi and(∪j̸=iDj).Sinceatleast one point of K is in D ,(a ), it follows all of K must be contained in D .
Now by Theorem i ii i 7.9.4 this shows there are points of K∩σ(A) in D .
The last assertion follows immediately.
i (cid:4) This can be improved even more.
This involves the following lemma.
Lemma 7.9.6 In the situation of Theorem 7.9.4 suppose λ(0) = K ∩σ(A(0)) and that 0 λ(0) is a simple root of the characteristic equation of A(0).
Then for all t∈[0,1], σ(A(t))∩K =λ(t) 0 where λ(t) is a simple root of the characteristic equation of A(t).
Proof:LetS ≡{t∈[0,1]:K ∩σ(A(s))=λ(s), a simple eigenvalue for all s∈[0,t]}.
0 Then 0 ∈ S so it is nonempty.
Let t = sup(S) and suppose λ ̸= λ are two elements of 0 1 2 σ(A(t ))∩K .Thenchoosingη >0smallenough,andlettingD bedisjointdiscscontaining 0 0 i λ respectively, similar arguments to those of Lemma 7.9.3 can be used to conclude i Hi ≡∪s∈[t0−η,t0]σ(A(s))∩Di is a connected and nonempty set for i = 1,2 which would require that H ⊆ K .
But i 0 then there would be two diﬀerent eigenvalues of A(s) contained in K , contrary to the 0 deﬁnition of t .
Therefore, there is at most one eigenvalue λ(t ) ∈ K ∩σ(A(t )).
Could 0 0 0 0 it be a repeated root of the characteristic equation?
Suppose λ(t ) is a repeated root of 0 the characteristic equation.
As before, choose a small disc, D centered at λ(t ) and η small 0 enough that H ≡∪s∈[t0−η,t0]σ(A(s))∩D is a nonempty connected set containing either multiple eigenvalues of A(s) or else a single repeatedroottothecharacteristicequationofA(s).ButsinceH isconnectedandcontains λ(t ) it must be contained in K which contradicts the condition for s ∈ S for all these 0 0 s ∈ [t −η,t ].
Therefore, t ∈ S as hoped.
If t < 1, there exists a small disc centered 0 0 0 0 at λ(t ) and η > 0 such that for all s ∈ [t ,t +η], A(s) has only simple eigenvalues in 0 0 0 D and the only eigenvalues of A(s) which could be in K are in D. (This last assertion 0 follows from noting that λ(t ) is the only eigenvalue of A(t ) in K and so the others are 0 0 0 at a positive distance from K .
For s close enough to t , the eigenvalues of A(s) are either 0 0 close to these eigenvalues of A(t ) at a positive distance from K or they are close to the 0 0 eigenvalue λ(t ) in which case it can be assumed they are in D.) But this shows that t is 0 0 not really an upper bound to S. Therefore, t =1 and the lemma is proved.
(cid:4) 0 With this lemma, the conclusion of the above corollary can be sharpened.
Corollary 7.9.7 Suppose one of the Gerschgorin discs, D is disjoint from the union of i the others.
Then D contains exactly one eigenvalue of A and this eigenvalue is a simple i root to the characteristic polynomial of A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation190 SPECTRAL THEORY Proof: In the proof of Corollary 7.9.5, note that a is a simple root of A(0) since ii otherwise the ith Gerschgorin disc would not be disjoint from the others.
Also, K, the connected component determined by a must be contained in D because it is connected ii i and by Gerschgorin’s theorem above, K ∩σ(A(t)) must be contained in the union of the Gerschgorindiscs.
SincealltheothereigenvaluesofA(0),thea ,areoutsideD ,itfollows jj i that K∩σ(A(0))=a .
Therefore, by Lemma 7.9.6, K∩σ(A(1))=K∩σ(A) consists of ii a single simple eigenvalue.
(cid:4) Example 7.9.8 Consider the matrix   5 1 0   1 1 1 0 1 0 The Gerschgorin discs are D(5,1),D(1,2), and D(0,1).
Observe D(5,1) is disjoint from the other discs.
Therefore, there should be an eigenvalue in D(5,1).
The actual eigenvaluesarenoteasytoﬁnd.
Theyaretherootsofthecharacteristicequation,t3−6t2+ 3t+5=0.
The numerical values of these are −.66966,1.4231, and 5.24655, verifying the predictions of Gerschgorin’s theorem.
7.10 Exercises 1.
Explain why it is typically impossible to compute the upper triangular matrix whose existence is guaranteed by Schur’s theorem.
2.
Now recall the QR factorization of Theorem 5.7.5 on Page 133.
The QR algorithm is a technique which does compute the upper triangular matrix in Schur’s theorem.
There is much more to the QR algorithm than will be presented here.
In fact, what I am about to show you is not the way it is done in practice.
One ﬁrst obtains what is called a Hessenburg matrix for which the algorithm will work better.
However, the idea is as follows.
Start with A an n×n matrix having real eigenvalues.
Form A = QR where Q is orthogonal and R is upper triangular.
(Right triangular.)
This can be done using the technique of Theorem 5.7.5 using Householder matrices.
Next take A ≡RQ.
Show that A=QA QT.
In other words these two matrices, A,A are 1 1 1 similar.
Explainwhytheyhavethesameeigenvalues.
ContinuebylettingA playthe 1 role of A.
Thus the algorithm is of the form A =QR and A =R Q.
Explain n n n+1 n+1 why A = Q A QT for some Q orthogonal.
Thus A is a sequence of matrices each n n n n n similar to A.
The remarkable thing is that often these matrices converge to an upper triangular matrix T and A=QTQT for some orthogonal matrix, the limit of the Q n where the limit means the entries converge.
Then the process computes the upper triangular Schur form of the matrix A.
Thus the eigenvalues of A appear on the diagonal of T. You will see approximately what these are as the process continues.
3.
Try the QR algorithm on ( ) −1 −2 6 6 whichhaseigenvalues3and 2.
Isuggestyouusea computeralgebrasystem todothe computations.
4.
Now try the QR algorithm on ( ) 0 −1 2 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.10.
EXERCISES 191 Showthatthealgorithmcannotconvergeforthisexample.
Hint: Tryafewiterations of the algorithm.
( ) ( ) 0 −1 0 −2 5.
Show the two matrices A ≡ and B ≡ are similar; that is 4 0 2 0 there exists a matrix S such that A = S−1BS but there is no orthogonal matrix Q such that QTBQ = A.
Show the QR algorithm does converge for the matrix B although it fails to do so for A.
6.
Let F be an m×n matrix.
Show that F∗F has all real eigenvalues and furthermore, they are all nonnegative.
7.
IfAisarealn×nmatrixandλisacomplexeigenvalueλ=a+ib,b̸=0,ofAhaving eigenvector z+iw, show that w̸=0.
8.
Suppose A = QTDQ where Q is an orthogonal matrix and all the matrices are real.
Also D is a diagonal matrix.
Show that A must be symmetric.
9.
Suppose A is an n×n matrix and there exists a unitary matrix U such that ∗ A=U DU where D is a diagonal matrix.
Explain why A must be normal.
10.
If A is Hermitian, show that det(A) must be real.
11.
Show that every unitary matrix preserves distance.
That is, if U is unitary, |Ux|=|x|.
12.
Show that if a matrix does preserve distances, then it must be unitary.
13.
↑Show that a complex normal matrix A is unitary if and only if its eigenvalues have magnitude equal to 1.
14.
Suppose A is an n×n matrix which is diagonally dominant.
Recall this means ∑ |a |<|a | ij ii j̸=i show A−1 must exist.
15.
Give some disks in the complex plane whose union contains all the eigenvalues of the matrix   1+2i 4 2   0 i 3 5 6 7 16.
Show a square matrix is invertible if and only if it has no zero eigenvalues.
17.
Using Schur’s theorem, show the trace of an n × n matrix equals the sum of the eigenvaluesandthedeterminantofann×nmatrixistheproductoftheeigenvalues.
18.
UsingSchur’stheorem,showthatifAisa∑nycomplexn∑×nmatrixhavingeigenvalues {λ }listedaccordingtomultiplicity,then |A |2 ≥ n |λ |2.
Showthatequality i i,j ij i=1 i holds if and only if A is normal.
This inequality is called Schur’s inequality.
[19] Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation192 SPECTRAL THEORY 19.
Here is a matrix.
  1234 6 5 3  0 −654 9 123    98 123 10,000 11 56 78 98 400 I know this matrix has an inverse before doing any computations.
How do I know?
20.
Show the critical points of the following function are ( ) 1 (0,−3,0),(2,−3,0),and 1,−3,− 3 and classify them as local minima, local maxima or saddle points.
f(x,y,z)=−3x4+6x3−6x2+zx2−2zx−2y2−12y−18− 3z2.
2 2 21.
Here is a function of three variables.
f(x,y,z)=13x2+2xy+8xz+13y2+8yz+10z2 change the variables so that in the new variables there are no mixed terms, terms involving xy,yz etc.
Two eigenvalues are 12 and 18.
22.
Here is a function of three variables.
f(x,y,z)=2x2−4x+2+9yx−9y−3zx+3z+5y2−9zy−7z2 change the variables so that in the new variables there are no mixed terms, terms involving xy,yz etc.
The eigenvalues of the matrix which you will work with are −17,19,−1.
2 2 23.
Here is a function of three variables.
f(x,y,z)=−x2+2xy+2xz−y2+2yz−z2+x change the variables so that in the new variables there are no mixed terms, terms involving xy,yz etc.
24.
Show the critical points of the function, f(x,y,z)=−2yx2−6yx−4zx2−12zx+y2+2yz.
are points of the form, ( ) (x,y,z)= t,2t2+6t,−t2−3t for t∈R and classify them as local minima, local maxima or saddle points.
25.
Show the critical points of the function 1 1 f(x,y,z)= x4−4x3+8x2−3zx2+12zx+2y2+4y+2+ z2.
2 2 are (0,−1,0),(4,−1,0), and (2,−1,−12) and classify them as local minima, local maxima or saddle points.
26.
Let f(x,y) = 3x4−24x2+48−yx2+4y.
Find and classify the critical points using the second derivative test.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.10.
EXERCISES 193 27.
Letf(x,y)=3x4−5x2+2−y2x2+y2.Findandclassifythecriticalpointsusingthe second derivative test.
28.
Let f(x,y)=5x4−7x2−2−3y2x2+11y2−4y4.
Find and classify the critical points using the second derivative test.
29.
Let f(x,y,z)=−2x4−3yx2+3x2+5x2z+3y2−6y+3−3zy+3z+z2.
Find and classify the critical points using the second derivative test.
30.
Let f(x,y,z) = 3yx2−3x2−x2z−y2+2y−1+3zy−3z−3z2.
Find and classify the critical points using the second derivative test.
31.
Let Q be orthogonal.
Find the possible values of det(Q).
32.
Let U be unitary.
Find the possible values of det(U).
33.
If a matrix is nonzero can it have only zero for eigenvalues?
34.
A matrix A is called nilpotent if Ak = 0 for some positive integer k. Suppose A is a nilpotent matrix.
Show it has only 0 for an eigenvalue.
35.
If A is a nonzero nilpotent matrix, show it must be defective.
36.
Suppose A is a nondefective n×n matrix and its eigenvalues are all either 0 or 1.
Show A2 = A.
Could you say anything interesting if the eigenvalues were all either 0,1,or −1?
By DeMoivre’s theorem, an nth root of unity is of the form ( ( ) ( )) 2kπ 2kπ cos +isin n n Could you generalize the sort of thing just described to get An = A?
Hint: Since A is nondefective, there exists S such that S−1AS =D where D is a diagonal matrix.
37.
This and the following problems will present most of a diﬀerential equations course.
Most of the explanations are given.
You ﬁll in any details needed.
To begin with, consider the scalar initial value problem ′ y =ay, y(t )=y 0 0 When a is real, show the unique solution to this problem is y = y0ea(t−t0).
Next suppose ′ y =(a+ib)y, y(t )=y (7.20) 0 0 where y(t) = u(t)+iv(t).
Show there exists a unique solution and it is given by y(t)= y ea(t−t0)(cosb(t−t )+isinb(t−t ))≡e(a+ib)(t−t0)y .
(7.21) 0 0 0 0 Nextshowthatforarealorcomplexthereexistsauniquesolutiontotheinitialvalue problem ′ y =ay+f, y(t )=y 0 0 and it is given by ∫ t y(t)=ea(t−t0)y +eat e−asf(s)ds.
0 t0 Hint: For the ﬁrst part write as y′−ay = 0 and multiply both sides by e−at.
Then explain why you get ( ) d e−aty(t) =0, y(t )=0.
dt 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation194 SPECTRAL THEORY Now you ﬁnish the argument.
To show uniqueness in the second part, suppose ′ y =(a+ib)y, y(t )=0 0 and verify this requires y(t)=0.
To do this, note y′ =(a−ib)y, y(t )=0 0 and that |y|2(t )=0 and 0 d |y(t)|2 =y′(t)y(t)+y′(t)y(t) dt =(a+ib)y(t)y(t)+(a−ib)y(t)y(t)=2a|y(t)|2.
Thusfromtheﬁrstpart|y(t)|2 =0e−2at =0.Finallyobservebyasimplecomputation that (7.20) is solved by (7.21).
For the last part, write the equation as y′−ay =f and multiply both sides by e−at and then integrate from t to t using the initial 0 condition.
38.
NowconsiderAann×nmatrix.
BySchur’stheoremthereexistsunitaryQsuchthat Q−1AQ=T where T is upper triangular.
Now consider the ﬁrst order initial value problem ′ x =Ax, x(t )=x .
0 0 Show there exists a unique solution to this ﬁrst order system.
Hint: Let y = Q−1x and so the system becomes y′ =Ty, y(t )=Q−1x (7.22) 0 0 Now letting y=(y ,··· ,y )T , the bottom equation becomes 1 n ( ) y′ =t y , y (t )= Q−1x .
n nn n n 0 0 n Then use the solution you get in this to get the solution to the initial value problem which occurs one level up, namely ( ) yn′−1 =t(n−1)(n−1)yn−1+t(n−1)nyn, yn−1(t0)= Q−1x0 n−1 Continue doing this to obtain a unique solution to (7.22).
39.
Now suppose Φ(t) is an n×n matrix of the form ( ) Φ(t)= x (t) ··· x (t) (7.23) 1 n where ′ x (t)=Ax (t).
k k Explain why ′ Φ (t)=AΦ(t) if and only if Φ(t) is given in the form of (7.23).
Also explain why if c ∈ Fn,y(t) ≡ Φ(t)c solves the equation y′(t)=Ay(t).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.10.
EXERCISES 195 40.
In the above problem, consider the question whether all solutions to ′ x =Ax (7.24) are obtained in the form Φ(t)c for some choice of c ∈ Fn.
In other words, is the generalsolutiontothisequationΦ(t)cforc∈Fn?
Provethefollowingtheoremusing linear algebra.
Theorem 7.10.1 Suppose Φ(t) is an n×n matrix which satisﬁes Φ′(t) = AΦ(t).
Then the general solution to (7.24) is Φ(t)c if and only if Φ(t)−1 exists for some t. Furthermore, if Φ′(t) = AΦ(t), then either Φ(t)−1 exists for all t or Φ(t)−1 never exists for any t. (det(Φ(t))iscalledtheWronskianandthistheoremissometimescalledtheWronskian alternative.)
Hint: Suppose ﬁrst the general solution is of the form Φ(t)c where c is an arbitrary constant vector in Fn.
You need to verify Φ(t)−1 exists for some t. In fact, show Φ(t)−1 exists for every t. Suppose then that Φ(t )−1 does not exist.
Explain why 0 there exists c ∈ Fn such that there is no solution x to the equation c = Φ(t )x.
By 0 the existence part of Problem 38 there exists a solution to ′ x =Ax, x(t )=c 0 but this cannot be in the form Φ(t)c. Thus for every t, Φ(t)−1 exists.
Next suppose for some t ,Φ(t )−1 exists.
Let z′ =Az and choose c such that 0 0 z(t )=Φ(t )c 0 0 Then both z(t),Φ(t)c solve ′ x =Ax, x(t )=z(t ) 0 0 Apply uniqueness to conclude z = Φ(t)c. Finally, consider that Φ(t)c for c ∈ Fn either is the general solution or it is not the general solution.
If it is, then Φ(t)−1 exists for all t. If it is not, then Φ(t)−1 cannot exist for any t from what was just shown.
41.
Let Φ′(t)=AΦ(t).
Then Φ(t) is called a fundamental matrix if Φ(t)−1 exists for all t. Show there exists a unique solution to the equation ′ x =Ax+f, x(t )=x (7.25) 0 0 and it is given by the formula ∫ t x(t)=Φ(t)Φ(t )−1x +Φ(t) Φ(s)−1f(s)ds 0 0 t0 Nowthesefewproblemshavedonevirtuallyeverythingofsigniﬁcanceinanentireun- dergraduatediﬀerentialequationscourse,illustratingthesuperiorityoflinearalgebra.
The above formula is called the variation of constants formula.
Hint: Uniquenssiseasy.
Ifx ,x aretwosolutionsthenletu(t)=x (t)−x (t)and 1 2 1 2 argueu′ =Au, u(t )=0.ThenuseProblem38.
Toverifythereexistsasolution,you 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation196 SPECTRAL THEORY could just diﬀerentiate the above formula using the fundamental theorem of calculus and verify it works.
Another way is to assume the solution in the form x(t)=Φ(t)c(t) and ﬁnd c(t) to make it all work out.
This is called the method of variation of parameters.
42.
Show there exists a special Φ such that Φ′(t) = AΦ(t), Φ(0) = I, and suppose Φ(t)−1 exists for all t. Show using uniqueness that Φ(−t)=Φ(t)−1 and that for all t,s∈R Φ(t+s)=Φ(t)Φ(s) Explain why with this special Φ, the solution to (7.25) can be written as ∫ t x(t)=Φ(t−t )x + Φ(t−s)f(s)ds.
0 0 t0 Hint: Let Φ(t) be such that the jth column is x (t) where j ′ x =Ax , x (0)=e .
j j j j Use uniqueness as required.
43.
You can see more on this problem and the next one in the latest version of Horn and Johnson, [16].
Two n×n matrices A,B are said to be congruent if there is an invertible P such that ∗ B =PAP Let A be a Hermitian matrix.
Thus it has all real eigenvalues.
Let n be the number + of positive eigenvalues, n−, the number of negative eigenvalues and n0 the number of zero eigenvalues.
For k a positive integer, let I denote the k×k identity matrix and k O thek×k zeromatrix.
ThentheinertiamatrixofAisthefollowingblockdiagonal k n×n matrix.
  I  n+  I n− O n0 Show that A is congruent to its inertia matrix.
Next show that congruence is an equivalence relation.
Finally, show that if two Hermitian matrices have the same inertia matrix, then they must be congruent.
Hint: First recall that there is a unitary matrix, U such that   D U∗AU = n+ D  n− O n0 where the Dn+ is a diagon(cid:12)(cid:12)al m(cid:12)(cid:12)atrix having the positive eigenvalues of A, Dn− being deﬁnedsimilarly.
Nowlet D denotethediagonalmatrixwhichreplaceseachentry n− of D with its absolute value.
Consider the two diagonal matrices n−   D−1/2  n+ (cid:12) (cid:12)  D =D∗ = (cid:12)D (cid:12)−1/2  n− I n0 Now consider D∗U∗AUD.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation7.10.
EXERCISES 197 44.
Show that if A,B are two congruent Hermitian matrices, then they have the same inertia matrix.
Hint: Let A=SBS∗ where S is invertible.
Show that A,B have the same rank and this implies that they are each unitarily similar to a diagonal matrix which has the same number of zero entries on the main diagonal.
Therefore, letting V be the span of the eigenvectors associated with positive eigenvalues of A and V A B being deﬁned similarly, it suﬃces to show that these have the same dimensions.
Show that (Ax,x)>0 for all x∈V .
Next consider S∗V .
For x∈V , explain why A A A ( ) (BS∗x,S∗x) = S−1A(S∗)−1S∗x,S∗x ( ) ( ( ) ) = S−1Ax,S∗x = Ax, S−1 ∗S∗x =(Ax,x)>0 NextexplainwhythisshowsthatS∗V isasubspaceofV andsothedimensionofV A B B is at least as large as the dimension of V .
Hence there are at least as many positive A eigenvalues for B as there are for A.
Switching A,B you can turn the inequality around.
Thus the two have the same inertia matrix.
45.
Let A be an m×n matrix.
Then if you unraveled it, you could consider it as a vector inCnm.
TheFrobeniusinnerproductonthevectorspaceofm×nmatricesisdeﬁned as (A,B)≡trace(AB∗) Show that this really does satisfy the axioms of an inner product space and that it also amounts to nothing more than considering m×n matrices as vectors in Cnm.
46.
↑Consider the n×n unitary matrices.
Show that whenever U is such a matrix, it follows that √ |U| = n Cnn Next explain why if {U } is any sequence of unitary matrices, there exists a subse- k quence {Ukm}∞m=1 such that limm→∞Ukm = U where U is unitary.
Here the limit takes place in the sense that the entries of U converge to the corresponding entries km of U.
47.
↑Let A,B be two n×n matrices.
Denote by σ(A) the set of eigenvalues of A. Deﬁne dist(σ(A),σ(B))= max min{|λ−µ|:µ∈σ(B)} λ∈σ(A) Explain why dist(σ(A),σ(B)) is small if and only if every eigenvalue of A is close to some eigenvalue of B.
Now prove the following theorem using the above problem and Schur’s theorem.
This theorem says roughly that if A is close to B then the eigenvaluesofAareclosetothoseofB inthesensethateveryeigenvalueofAisclose to an eigenvalue of B. Theorem 7.10.2 Suppose limk→∞Ak =A.
Then lim dist(σ(A ),σ(A))=0 k k→∞ ( ) a b 48.
Let A = be a 2×2 matrix which is not a multiple of the identity.
Show c d that A is similar to a 2×2 matrix which has at least one diagonal entry equal to 0.
Hint: FirstnotethatthereexistsavectorasuchthatAaisnotamultipleofa.Then consider ( ) ( ) −1 B = a Aa A a Aa Show B has a zero on the main diagonal.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation198 SPECTRAL THEORY 49.
↑ Let A be a complex n×n matrix whichhas trace equal to 0.
Showthat A is similar to a matrix which has all zeros on the main diagonal.
Hint: Use Problem 30 on Page 122 to argue that you can say that a given matrix is similar to one which has the diagonal entries permuted in any order desired.
Then use the above problem and block multiplication to show that if the A has k nonzero entries, then it is similar to a matrix which has k−1 nonzero entries.
Finally, when A is similar to one which has at most one nonzero entry, this one must also be zero because of the condition on the trace.
50.
↑An n×n matrix X is a comutator if there are n×n matrices A,B such that X = AB−BA.
Show that the trace of any comutator is 0.
Next show that if a complex matrix X has trace equal to 0, then it is in fact a comutator.
Hint: Use the above problem to show that it suﬃces to consider X having all zero entries on the main diagonal.
Then deﬁne   1 0 {    2  Xij if i̸=j A= ... , Bij = i−0jif i=j 0 n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationVector Spaces And Fields 8.1 Vector Space Axioms It is time to consider the idea of a Vector space.
Deﬁnition 8.1.1 A vector space is an Abelian group of “vectors” satisfying the axioms of an Abelian group, v+w=w+v, the commutative law of addition, (v+w)+z=v+(w+z), the associative law for addition, v+0=v, the existence of an additive identity, v+(−v)=0, the existence of an additive inverse, along with a ﬁeld of “scalars”, F which are allowed to multiply the vectors according to the following rules.
(The Greek letters denote scalars.)
α(v+w)=αv+αw, (8.1) (α+β)v=αv+βv, (8.2) α(βv)=αβ(v), (8.3) 1v=v.
(8.4) The ﬁeld of scalars is usually R or C and the vector space will be called real or complex depending on whether the ﬁeld is R or C. However, other ﬁelds are also possible.
For example, one could use the ﬁeld of rational numbers or even the ﬁeld of the integers mod p for p a prime.
A vector space is also called a linear space.
For example, Rn with the usual conventions is an example of a real vector space and Cn is an example of a complex vector space.
Up to now, the discussion has been for Rn or Cn and all that is taking place is an increase in generality and abstraction.
There are many examples of vector spaces.
199 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation200 VECTOR SPACES AND FIELDS Example 8.1.2 Let Ω be a nonempty set and let V consist of all functions deﬁned on Ω which have values in some ﬁeld F. The vector operations are deﬁned as follows.
(f +g)(x) = f(x)+g(x) (αf)(x) = αf(x) Then it is routine to verify that V with these operations is a vector space.
NotethatFnactuallyﬁtsintothisframework.
YouconsiderthesetΩtobe{1,2,··· ,n} and then the mappings from Ω to F give the elements of Fn.
Thus a typical vector can be considered as a function.
Example 8.1.3 Generalize the above example by letting V denote all functions deﬁned on Ω which have values in a vector space W which has ﬁeld of scalars F. The deﬁnitions of scalar multiplication and vector addition are identical to those of the above example.
8.2 Subspaces And Bases 8.2.1 Basic Deﬁnitions Deﬁnition 8.2.1 If {v ,··· ,v }⊆V, a vector space, then 1 n { } ∑n span(v ,··· ,v )≡ α v :α ∈F .
1 n i i i i=1 A subset, W ⊆ V is said to be a subspace if it is also a vector space with the same ﬁeld of scalars.
Thus W ⊆ V is a subspace if ax+by ∈ W whenever a,b ∈ F and x,y ∈ W. The span of a set of vectors as just described is an example of a subspace.
Example 8.2.2 Consider the real valued functions deﬁned on an interval [a,b].
A subspace is the set of continuous real valued functions deﬁned on the interval.
Another subspace is the set of polynomials of degree no more than 4.
Deﬁnition 8.2.3 If {v ,··· ,v }⊆V, the set of vectors is linearly independent if 1 n ∑n α v =0 i i i=1 implies α =···=α =0 1 n and {v ,··· ,v } is called a basis for V if 1 n span(v ,··· ,v )=V 1 n and {v ,··· ,v } is linearly independent.
The set of vectors is linearly dependent if it is not 1 n linearly independent.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.2.
SUBSPACES AND BASES 201 8.2.2 A Fundamental Theorem Thenexttheoremiscalledtheexchangetheorem.
Itisveryimportantthatyouunderstand this theorem.
It is so important that I have given several proofs of it.
Some amount to the same thing, just worded diﬀerently.
Theorem 8.2.4 Let {x ,··· ,x } be a linearly independent set of vectors such that each x 1 r i is in the span{y ,··· ,y }.
Then r ≤s.
1 s Proof 1: Deﬁne span{y ,··· ,y } ≡ V, it follows there exist scalars c ,··· ,c such 1 s 1 s that ∑s x = c y .
(8.5) 1 i i i=1 Not all of these scalars can equal zero because if this were the case, it would follow that x∑1 = 0 and so {x1,··· ,xr} would not be linearly independent.
Indeed, if x1 = 0, 1x1 + r 0x = x = 0 and so there would exist a nontrivial linear combination of the vectors i=2 i 1 {x ,··· ,x } which equals zero.
1 r Say c ̸=0.
Then solve (8.5) for y and obtain k k   z s-1vec}to|rshere { yk ∈spanx1,y1,··· ,yk−1,yk+1,··· ,ys.
Deﬁne {z1,··· ,zs−1} by {z1,··· ,zs−1}≡{y1,··· ,yk−1,yk+1,··· ,ys} Therefore, span{x1,z1,··· ,zs−1} = V because if v ∈ V, there exist constants c1,··· ,cs such that ∑s−1 v= c z +c y .
i i s k i=1 Nowreplacetheyk intheabovewithalinearcombinationofthevectors,{x1,z1,··· ,zs−1} toobtainv∈span{x1,z1,··· ,zs−1}.Thevectoryk,inthelist{y1,··· ,ys},hasnowbeen replaced with the vector x and the resulting modiﬁed list of vectors has the same span as 1 the original list of vectors, {y ,··· ,y }.
1 s Now suppose that r > s and that span{x ,··· ,x ,z ,··· ,z } = V where the vectors, 1 l 1 p z ,··· ,z are each taken from the set, {y ,··· ,y } and l+p=s.
This has now been done 1 p 1 s for l=1 above.
Then since r >s, it follows that l≤s<r and so l+1≤r.
Therefore, x l+1 is a vector not in the list, {x ,··· ,x } and since span{x ,··· ,x ,z ,··· ,z } = V there 1 l 1 l 1 p exist scalars c and d such that i j ∑l ∑p x = c x + d z .
(8.6) l+1 i i j j i=1 j=1 Now not all the d can equal zero because if this were so, it would follow that {x ,··· ,x } j 1 r wouldbealinearlydependentsetbecauseoneofthevectorswouldequalalinearcombination oftheothers.
Therefore, ((8.6))canbesolvedforoneofthez ,sayz ,intermsofx and i k l+1 the other z and just as in the above argument, replace that z with x to obtain i i l+1   z p-1vec}t|orshere { spanx1,···xl,xl+1,z1,···zk−1,zk+1,··· ,zp=V.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation202 VECTOR SPACES AND FIELDS Continue this way, eventually obtaining span(x ,··· ,x )=V.
1 s But then x ∈ span{x ,··· ,x } contrary to the assumption that {x ,··· ,x } is linearly r 1 s 1 r independent.
Therefore, r ≤s as claimed.
Proof 2: Let ∑s x = a y k jk j j=1 If r > s, then the matrix A = (a ) has more columns than rows.
By Corollary 4.3.9 jk one of these columns is a linear combination of the others.
This implies there exist scalars c ,··· ,c , not all zero such that 1 r ∑r a c =0, j =1,··· ,r jk k k=1 Then ( ) ∑r ∑r ∑s ∑s ∑r c x = c a y = c a y =0 k k k jk j k jk j k=1 k=1 j=1 j=1 k=1 which contradicts the assumption that {x ,··· ,x } is linearly independent.
Hence r ≤s.
1 r Proof 3: Suppose r >s.
Let z denote a vector of {y ,··· ,y }.
Thus there exists j as k 1 s small as possible such that span(y ,··· ,y )=span(x ,··· ,x ,z ,··· ,z ) 1 s 1 m 1 j where m+j = s. It is given that m = 0, corresponding to no vectors of {x ,··· ,x } and 1 m j =s,correspondingtoallthey resultsintheaboveequationholding.
Ifj >0thenm<s k and so ∑m ∑j x = a x + b z m+1 k k i i k=1 i=1 Notalltheb canequal0andsoyoucansolveforoneofthemintermsofx ,x ,··· ,x , i m+1 m 1 and the other z .
Therefore, there exists k {z1,··· ,zj−1}⊆{y1,··· ,ys} such that span(y1,··· ,ys)=span(x1,··· ,xm+1,z1,··· ,zj−1) contradicting the choice of j.
Hence j =0 and span(y ,··· ,y )=span(x ,··· ,x ) 1 s 1 s It follows that x ∈span(x ,··· ,x ) s+1 1 s contrarytotheassumptionthex arelinearlyindependent.
Therefore, r ≤sasclaimed.
(cid:4) k Corollary 8.2.5 If {u ,··· ,u } and {v ,··· ,v } are two bases for V, then m=n.
1 m 1 n Proof: By Theorem 8.2.4, m≤n and n≤m.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.2.
SUBSPACES AND BASES 203 Deﬁnition 8.2.6 AvectorspaceV isofdimensionnifithasabasisconsistingofnvectors.
This is well deﬁned thanks to Corollary 8.2.5.
It is always assumed here that n<∞ and in this case, such a vector space is said to be ﬁnite dimensional.
Example 8.2.7 Consider the polynomials de{ﬁned on R}of degree no more than 3, denoted here as P .
Then show that a basis for P is 1,x,x2,x3 .
Here xk symbolizes the function 3 3 x7→xk.
It is obvious that the span of the given vectors yields P .
Why is this set of vectors 3 linearly independent?
Suppose c +c x+c x2+c x3 =0 0 1 2 3 where0isthezerofunctionwhichmapseverythingto0.
Thenyoucoulddiﬀerentiatethree times and obtain the following equations c +2c x+3c x2 = 0 1 2 3 2c +6c x = 0 2 3 6c = 0 3 Now this implies c = 0.
Then from the equations above the bottom one, you ﬁnd in 3 succession that c =0,c =0,c =0.
2 1 0 Thereisasomewhatinterestingtheoremaboutlinearindependenceofsmoothfunctions (those having plenty of derivatives) which I will show now.
It is often used in diﬀerential equations.
Deﬁnition 8.2.8 Let f ,··· ,f be smooth functions deﬁned on an interval [a,b].
The 1 n Wronskian of these functions is deﬁned as follows.
(cid:12) (cid:12) (cid:12)(cid:12) f1(x) f2(x) ··· fn(x) (cid:12)(cid:12) (cid:12)(cid:12) f1′(x) f2′(x) ··· fn′ (x) (cid:12)(cid:12) W (f ,··· ,f )(x)≡(cid:12) .
.
.
(cid:12) 1 n (cid:12) .
.
.
(cid:12) (cid:12) .
.
.
(cid:12) (cid:12) f(n−1)(x) f(n−1)(x) ··· f(n−1)(x) (cid:12) 1 2 n Note that to get from one row to the next, you just diﬀerentiate everything in that row.
The notation f(k)(x) denotes the kth derivative.
Withthisdeﬁnition,thefollowingisthetheorem.
Theinterestingtheoreminvolvingthe Wronskian has to do with the situation where the functions are solutions of a diﬀerential equation.
Then much more can be said and it is much more interesting than the following theorem.
Theorem 8.2.9 Let {f ,··· ,f } be smooth functions deﬁned on [a,b].
Then they are lin- 1 n early independent if there exists some point t∈[a,b] where W (f ,··· ,f )(t)̸=0.
1 n Proof: Form the linear combination of these vectors (functions) and suppose it equals 0.
Thus a f +a f +···+a f =0 1 1 2 2 n n The question you must answer is whether this requires each a to equal zero.
If they all j must equal 0, then this means these vectors (functions) are independent.
This is what it means to be linearly independent.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation204 VECTOR SPACES AND FIELDS Diﬀerentiate the above equation n−1 times yielding the equations   a f +a f +···+a f =0 1 1 2 2 n n  a f′ +a f′ +···+a f′ =0   1 1 2 2 n n   .
  .
 .
a f(n−1)+a f(n−1)+···+a f(n−1) =0 1 1 2 2 n n Now plug in t. Then the above yields      f1(t) f2(t) ··· fn(t) a1 0  f1′(t) f2′(t) ··· fn′ (t)  a2   0   ... ... ...  ... = ...  f1(n−1)(t) f2(n−1)(t) ··· fn(n−1)(t) an 0 Since the determinant of the matrix on the left is assumed to be nonzero, it follows this matrix has an inverse and so the only solution to the above system of equations is to have each a =0.
(cid:4) k Here is a useful lemma.
Lemma 8.2.10 Suppose v ∈/ span(u ,··· ,u ) and {u ,··· ,u } is linearly independent.
1 k 1 k Then {u ,··· ,u ,v} is also linearly independent.
1 k ∑ Proof: Suppose k c u +dv = 0.
It is required to verify that each c = 0 and that i=1 i i i d = 0.
But if d ̸= 0, then you can solve for v as a linear combination of the vectors, {u ,··· ,u }, 1 k ∑k ( ) c v=− i u d i i=1 ∑ contrary to assumption.
Therefore, d=0.
But then k c u =0 and the linear indepen- i=1 i i dence of {u ,··· ,u } implies each c =0 also.
(cid:4) 1 k i Givenaspanningset,youcandeletevectorstillyouendupwithabasis.
Givenalinearly independentset,youcanaddvectorstillyougetabasis.
Thisiswhatthefollowingtheorem is about, weeding and planting.
Theorem 8.2.11 If V =span(u ,··· ,u ) then some subset of {u ,··· ,u } is a basis for 1 n 1 n V. Also, if {u ,··· ,u } ⊆ V is linearly independent and the vector space is ﬁnite dimen- 1 k sional, then the set, {u ,··· ,u }, can be enlarged to obtain a basis of V. 1 k Proof: Let S ={E ⊆{u ,··· ,u } such that span(E)=V}.
1 n For E ∈S, let |E| denote the number of elements of E. Let m≡min{|E| such that E ∈S}.
Thus there exist vectors {v ,··· ,v }⊆{u ,··· ,u } 1 m 1 n such that span(v ,··· ,v )=V 1 m andmisassmallaspossibleforthistohappen.
Ifthissetislinearlyindependent,itfollows it is a basis for V and the theorem is proved.
On the other hand, if the set is not linearly independent, then there exist scalars c ,··· ,c 1 m Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 205 such that ∑m 0= c v i i i=1 and not all the c are equal to zero.
Suppose c ̸=0.
Then the vector, v may be solved for i k k in terms of the other vectors.
Consequently, V =span(v1,··· ,vk−1,vk+1,··· ,vm) contradicting the deﬁnition of m. This proves the ﬁrst part of the theorem.
To obtain the second part, begin with {u ,··· ,u } and suppose a basis for V is 1 k {v ,··· ,v }.
1 n If span(u ,··· ,u )=V, 1 k then k =n.
If not, there exists a vector, u ∈/ span(u ,··· ,u ).
k+1 1 k Then by Lemma 8.2.10, {u ,··· ,u ,u } is also linearly independent.
Continue adding 1 k k+1 vectors in this way until n linearly independent vectors have been obtained.
Then span(u ,··· ,u )=V 1 n because if it did not do so, there would exist u as just described and {u ,··· ,u } n+1 1 n+1 wouldbealinearlyindependentsetofvectorshavingn+1elementseventhough{v ,··· ,v } 1 n is a basis.
This would contradict Theorem 8.2.4.
Therefore, this list is a basis.
(cid:4) 8.2.3 The Basis Of A Subspace Every subspace of a ﬁnite dimensional vector space is a span of some vectors and in fact it has a basis.
This is the content of the next theorem.
Theorem 8.2.12 Let V be a nonzero subspace of a ﬁnite dimensional vector space, W of dimension, n. Then V has a basis with no more than n vectors.
Proof: Let v ∈ V where v ̸= 0.
If span{v } = V, stop.
{v } is a basis for V. 1 1 1 1 Otherwise, there exists v ∈ V which is not in span{v }.
By Lemma 8.2.10 {v ,v } is a 2 1 1 2 linearly independent set of vectors.
If span{v ,v } = V stop, {v ,v } is a basis for V. If 1 2 1 2 span{v ,v }̸= V, then there exists v ∈/ span{v ,v } and {v ,v ,v } is a larger linearly 1 2 3 1 2 1 2 3 independent set of vectors.
Continuing this way, the process must stop before n+1 steps because if not, it would be possible to obtain n+1 linearly independent vectors contrary to the exchange theorem, Theorem 8.2.4.
(cid:4) 8.3 Lots Of Fields 8.3.1 Irreducible Polynomials I mentioned earlier that most things hold for arbitrary ﬁelds.
However, I havenot bothered to give any examples of other ﬁelds.
This is the point of this section.
It also turns out that showing the algebraic numbers are a ﬁeld can be understood using vector space concepts Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation206 VECTOR SPACES AND FIELDS and it gives a very convincing application of the abstract theory presented earlier in this chapter.
Here I will give some basic algebra relating to polynomials.
This is interesting for its own sake but also provides the basis for constructing many diﬀerent kinds of ﬁelds.
The ﬁrst is the Euclidean algorithm for polynomials.
∑ Deﬁnition 8.3.1 A polynomial is an expression of the form p(λ) = n a λk where as k=0 k usual λ0 is deﬁned to equal 1.
Two polynomials are said to be equal if their corresponding coeﬃcients are the same.
Thus, in particular, p(λ) = 0 means each of the a = 0.
An k element of the ﬁeld λ is said to be a root of the polynomial if p(λ) = 0 in the sense that when you plug in λ into the formula and do the indicated operations, you get 0.
The degree of a nonzero polynomial is the highest exponent appearing on λ.
The degree of the zero polynomial p(λ)=0 is not deﬁned.
Example 8.3.2 Consider the polynomial p(λ)=λ2+λ where the coeﬃcients are in Z .
Is 2 this polynomial equal to 0?
Not according to the above deﬁnition, because its coeﬃcients are not all equal to 0.
However, p(1)=p(0)=0 so it sends every element of Z to 0.
Note the 2 distinction between saying it sends everything in the ﬁeld to 0 with having the polynomial be the zero polynomial.
Lemma 8.3.3 Letf(λ)andg(λ)̸=0bepolynomials.
Thenthereexistsapolynomial,q(λ) such that f(λ)=q(λ)g(λ)+r(λ) where the degree of r(λ) is less than the degree of g(λ) or r(λ)=0.
Proof: Consider the polynomials of the form f(λ) − g(λ)l(λ) and out of all these polynomials, pick one which has the smallest degree.
This can be done because of the well ordering of the natural numbers.
Let this take place when l(λ)=q (λ) and let 1 r(λ)=f(λ)−g(λ)q (λ).
1 It is required to show degree of r(λ)< degree of g(λ) or else r(λ)=0.
Suppose f(λ)−g(λ)l(λ) is never equal to zero for any l(λ).
Then r(λ) ̸= 0.
It is required to show the degree of r(λ) is smaller than the degree of g(λ).
If this doesn’t happen, then the degree of r ≥ the degree of g. Let r(λ) = b λm+···+b λ+b m 1 0 g(λ) = a λn+···+a λ+a n 1 0 where m≥n and b and a are nonzero.
Then let r (λ) be given by m n 1 λm−nb r (λ)=r(λ)− mg(λ) 1 a n λm−nb =(b λm+···+b λ+b )− m (a λn+···+a λ+a ) m 1 0 a n 1 0 n which has smaller degree than m, the degree of r(λ).
But z r}(λ|) { λm−nb r (λ) = f(λ)−g(λ)q (λ)− mg(λ) 1 1 a ( n ) λm−nb = f(λ)−g(λ) q (λ)+ m , 1 a n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 207 and this is not zero by the assumption that f(λ)−g(λ)l(λ) is never equal to zero for any l(λ) yet has smaller degree than r(λ) which is a contradiction to the choice of r(λ).
(cid:4) Now with this lemma, here is another one which is very fundamental.
First here is a deﬁnition.
A polynomial is monic means it is of the form λn+cn−1λn−1+···+c1λ+c0.
That is, the leading coeﬃcient is 1.
In what follows, the coeﬃcients of polynomials are in F, a ﬁeld of scalars which is completely arbitrary.
Think R if you need an example.
Deﬁnition 8.3.4 A polynomial f is said to divide a polynomial g if g(λ)=f(λ)r(λ) for some polynomial r(λ).
Let {ϕ (λ)} be a ﬁnite set of polynomials.
The greatest common i divisorwillbethemonicpolynomialq suchthatq(λ)divideseachϕ (λ)andifp(λ)divides i eachϕ (λ),thenp(λ)dividesq(λ).Theﬁnitesetofpolynomials{ϕ }issaidtoberelatively i i prime if their greatest common divisor is 1.
A polynomial f(λ) is irreducible if there is no polynomial with coeﬃcients in F which divides it except nonzero scalar multiples of f(λ) and constants.
Proposition 8.3.5 The greatest common divisor is unique.
Proof: Suppose both q(λ) and q′(λ) work.
Then q(λ) divides q′(λ) and the other way around and so ′ ′ ′ q (λ)=q(λ)l(λ), q(λ)=l (λ)q (λ) Therefore, the two must have the same degree.
Hence l′(λ),l(λ) are both constants.
How- ever, this constant must be 1 because both q(λ) and q′(λ) are monic.
(cid:4) Theorem 8.3.6 Let ψ(λ) be the greatest common divisor of {ϕ (λ)}, not all of which are i zero polynomials.
Then there exist polynomials r (λ) such that i ∑p ψ(λ)= r (λ)ϕ (λ).
i i i=1 Furthermore, ψ(λ) is the monic polynomial of smallest degree which can be written in the above form.
Proof: Let S denote the set of monic polynomials which are of the form ∑p r (λ)ϕ (λ) i i i=1 whereri(λ)isapolynomial.
ThenS ̸=∑∅becausesomeϕi(λ)̸=0.
Thenlettheri bechosen suchthatthedegreeoftheexpression p r (λ)ϕ (λ)isassmallaspossible.
Lettingψ(λ) i=1 i i equal this sum, it remains to verify it is the greatest common divisor.
First, does it divide each ϕ (λ)?
Suppose it fails to divide ϕ (λ).
Then by Lemma 8.3.3 i 1 ϕ (λ)=ψ(λ)l(λ)+r(λ) 1 wheredegreeofr(λ)islessthanthatofψ(λ).
Thendividingr(λ)bytheleadingcoeﬃcient if necessary and denoting the result by ψ (λ), it follows the degree of ψ (λ) is less than 1 1 the degree of ψ(λ) and ψ (λ) equals 1 ψ (λ)=(ϕ (λ)−ψ(λ)l(λ))a 1 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation208 VECTOR SPACES AND FIELDS ( ) ∑p = ϕ (λ)− r (λ)ϕ (λ)l(λ) a 1 i i ( i=1 ) ∑p = (1−r (λ))ϕ (λ)+ (−r (λ)l(λ))ϕ (λ) a 1 1 i i i=2 for a suitable a ∈ F. This is one of the polynomials in S. Therefore, ψ(λ) does not have the smallest degree after all because the degree of ψ (λ) is smaller.
This is a contradiction.
1 Therefore, ψ(λ) divides ϕ (λ).
Similarly it divides all the other ϕ (λ).
1 i Ifp∑(λ)dividesalltheϕi(λ),thenitdividesψ(λ)becauseoftheformulaforψ(λ)which equals p r (λ)ϕ (λ).
(cid:4) i=1 i i Lemma 8.3.7 Suppose ϕ(λ) and ψ(λ) are monic polynomials which are irreducible and not equal.
Then they are relatively prime.
Proof: Suppose η(λ) is a nonconstant polynomial.
If η(λ) divides ϕ(λ), then since ϕ(λ) is irreducible, η(λ) equals aϕ(λ) for some a ∈ F. If η(λ) divides ψ(λ) then it must be of the form bψ(λ) for some b∈F and so it follows a ψ(λ)= ϕ(λ) b but both ψ(λ) and ϕ(λ) are monic polynomials which implies a = b and so ψ(λ) = ϕ(λ).
This is assumed not to happen.
It follows the only polynomials which divide both ψ(λ) andϕ(λ)areconstantsandsothetwopolynomialsarerelativelyprime.
Thusapolynomial which divides them both must be a constant, and if it is monic, then it must be 1.
Thus 1 is the greatest common divisor.
(cid:4) Lemma 8.3.8 Let ψ(λ) be an irreducible monic polynomial not equal to 1 which divides ∏p ϕ (λ)ki, k a positive integer, i i i=1 where each ϕ (λ) is an irreducible monic polynomial.
Then ψ(λ) equals some ϕ (λ).
i i Proof : Suppose ψ(λ)̸=ϕ (λ) for all i.
Then by Lemma 8.3.7, there exist polynomials i m (λ),n (λ) such that i i 1=ψ(λ)m (λ)+ϕ (λ)n (λ).
i i i Hence (ϕ (λ)n (λ))ki =(1−ψ(λ)m (λ))ki ∏ i i i Then, letting ge(λ) = p n (λ)ki, and applying the binomial theorem, there exists a i=1 i polynomial h(λ) such that ∏p ∏p ∏p ge(λ) ϕ (λ)ki ≡ n (λ)ki ϕ (λ)ki i i i i=1 i=1 i=1 ∏p = (1−ψ(λ)m (λ))ki =1+ψ(λ)h(λ) i i=1 ∏ Thus, using the fact that ψ(λ) divides p ϕ (λ)ki, for a suitable polynomial g(λ), i=1 i g(λ)ψ(λ)=1+ψ(λ)h(λ) 1=ψ(λ)(h(λ)−g(λ)) which is impossible if ψ(λ) is non constant, as assumed.
(cid:4) Now here is a simple lemma about canceling monic polynomials.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 209 Lemma 8.3.9 Suppose p(λ) is a monic polynomial and q(λ) is a polynomial such that p(λ)q(λ)=0.
Then q(λ)=0.
Also if p(λ)q (λ)=p(λ)q (λ) 1 2 then q (λ)=q (λ).
1 2 Proof: Let ∑k ∑n p(λ)= p λj, q(λ)= q λi, p =1.
j i k j=1 i=1 Then the product equals ∑k ∑n p q λi+j.
j i j=1i=1 Then look at those terms involving λk+n.
This is p q λk+n and is given to be 0.
Since k n p =1, it follows q =0.
Thus k n ∑k n∑−1 p q λi+j =0.
j i j=1 i=1 Thenconsidertheterminvolvingλn−1+k andconcludethatsincepk =1,itfollowsqn−1 =0.
Continuing this way, each q =0.
This proves the ﬁrst part.
The second follows from i p(λ)(q (λ)−q (λ))=0.
(cid:4) 1 2 The following is the analog of the fundamental theorem of arithmetic for polynomials.
Theorem 8.3.10 Let f(λ) be a non∏constant polynomial with coeﬃcients in F. Then there is some a ∈ F such that f(λ) = a n ϕ (λ) where ϕ (λ) is an irreducible nonconstant i=1 i i monic polynomial and repeats are allowed.
Furthermore, this factorization is unique in the sense that any two of these factorizations have the same nonconstant factors in the product, possibly in diﬀerent order and the same constant a.
Proof: That such a factorization exists is obvious.
If f(λ) is irreducible, you are done.
Factor out the leading coeﬃcient.
If not, then f(λ)=aϕ (λ)ϕ (λ) where these are monic 1 2 polynomials.
Continue doing this with the ϕ and eventually arrive at a factorization of the i desired form.
It remains to argue the factorization is unique except for order of the factors.
Suppose ∏n ∏m a ϕ (λ)=b ψ (λ) i i i=1 i=1 wheretheϕ (λ)andtheψ (λ)areallirreduciblemonicnonconstantpolynomialsanda,b∈ i i F. If n > m, then by Lemma 8.3.8, each ψ (λ) equals one of the ϕ (λ).
By the above i j cancellation lemma, Lemma 8.3.9, you can cancel all these ψ (λ) with appropriate ϕ (λ) i j and obtain a contradiction because the resulting polynomials on either side would have diﬀerent degrees.
Similarly, it cannot happen that n < m. It follows n = m and the two products consist of the same polynomials.
Then it follows a=b.
(cid:4) Thefollowingcorollarywillbewellused.
Thiscorollaryseemsratherbelievablebutdoes require a proof.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation210 VECTOR SPACES AND FIELDS ∏ Corollary 8.3.11 Letq(λ)= p ϕ (λ)ki wherethek arepositiveintegersandtheϕ (λ) i=1 i i i are irreducible monic polynomials.
Suppose also that p(λ) is a monic polynomial which divides q(λ).
Then ∏p p(λ)= ϕ (λ)ri i i=1 where r is a nonnegative integer no larger than k .
i i ∏ Proof: Using Theorem 8.3.10, let p(λ) = b s ψ (λ)ri where the ψ (λ) are each i=1 i i irreducibleandmonicandb∈F.
Sincep(λ)ismonic,b=1.Thenthereexistsapolynomial g(λ) such that ∏s ∏p p(λ)g(λ)=g(λ) ψ (λ)ri = ϕ (λ)ki i i i=1 i=1 Hence g(λ) must be monic.
Therefore, z p}(λ|) { ∏s ∏l ∏p p(λ)g(λ)= ψ (λ)ri η (λ)= ϕ (λ)ki i j i i=1 j=1 i=1 for η monic and irreducible.
By uniqueness, each ψ equals one of the ϕ (λ) and the same j i j holding true of the η (λ).
Therefore, p(λ) is of the desired form.
(cid:4) i 8.3.2 Polynomials And Fields When you have a polynomial like x2−3 which has no rational roots, it turns out you can enlarge the ﬁeld of rational numbers to obtain a larger ﬁeld such that this polynomial does have roots in this larger ﬁeld.
I am going to discuss a systematic way to do this.
It will turnoutthatforanypolynomialwithcoeﬃcientsinanyﬁeld,therealwaysexistsapossibly larger ﬁeld such that the polynomial has roots in this larger ﬁeld.
This book has mainly featured the ﬁeld of real or complex numbers but this procedure will show how to obtain many other ﬁelds which could be used in most of what was presented earlier in the book.
Here is an important idea concerning equivalence relations which I hope is familiar.
Deﬁnition 8.3.12 Let S be a set.
The symbol, ∼ is called an equivalence relation on S if it satisﬁes the following axioms.
1. x∼x for all x∈S.
(Reﬂexive) 2.
If x∼y then y ∼x.
(Symmetric) 3.
If x∼y and y ∼z, then x∼z.
(Transitive) Deﬁnition 8.3.13 [x] denotes the set of all elements of S which are equivalent to x and [x] is called the equivalence class determined by x or just the equivalence class of x.
Also recall the notion of equivalence classes.
Theorem 8.3.14 Let ∼ be an equivalence class deﬁned on a set, S and let H denote the set of equivalence classes.
Then if [x] and [y] are two of these equivalence classes, either x∼y and [x]=[y] or it is not true that x∼y and [x]∩[y]=∅.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 211 Deﬁnition 8.3.15 Let F be a ﬁeld, for example the rational numbers, and denote by F[x] the polynomials having coeﬃcients in F. Suppose p(x) is a polynomial.
Let a(x) ∼ b(x) (a(x) is similar to b(x)) when a(x)−b(x)=k(x)p(x) for some polynomial k(x).
Proposition 8.3.16 In the above deﬁnition, ∼ is an equivalence relation.
Proof: First of all, note that a(x) ∼ a(x) because their diﬀerence equals 0p(x).
If a(x) ∼ b(x), then a(x) − b(x) = k(x)p(x) for some k(x).
But then b(x) − a(x) = −k(x)p(x) and so b(x) ∼ a(x).
Next suppose a(x) ∼ b(x) and b(x) ∼ c(x).
Then a(x)−b(x) = k(x)p(x) for some polynomial k(x) and also b(x)−c(x) = l(x)p(x) for some polynomial l(x).
Then a(x)−c(x)=a(x)−b(x)+b(x)−c(x) =k(x)p(x)+l(x)p(x)=(l(x)+k(x))p(x) and so a(x)∼c(x) and this shows the transitive law.
(cid:4) Withthisproposition,hereisanotherdeﬁnitionwhichessentiallydescribestheelements ofthenewﬁeld.
Itwilleventuallybenecessarytoassumethepolynomialp(x)intheabove deﬁnition is irreducible so I will begin assuming this.
Deﬁnition 8.3.17 Let F be a ﬁeld and let p(x) ∈F[x] be a monic irreducible polynomial.
ThismeansthereisnopolynomialhavingcoeﬃcientsinFwhichdividesp(x)exceptforitself and constants.
For the similarity relation deﬁned in Deﬁnition 8.3.15, deﬁne the following operations on the equivalence classes.
[a(x)] is an equivalence class means that it is the set of all polynomials which are similar to a(x).
[a(x)]+[b(x)] ≡ [a(x)+b(x)] [a(x)][b(x)] ≡ [a(x)b(x)] This collection of equivalence classes is sometimes denoted by F[x]/(p(x)).
Proposition 8.3.18 In the situation of Deﬁnition 8.3.17, p(x) and q(x) are relatively prime for any q(x)∈F[x] which is not a multiple of p(x).
Also the deﬁnitions of addition and multiplication are well deﬁned.
In addition, if a,b∈F and [a]=[b], then a=b.
Proof: First consider the claim about p(x),q(x) being relatively prime.
If ψ(x) is the greatest common divisor, it follows ψ(x) is either equal to p(x) or 1.
If it is p(x), then q(x) is a multiple of p(x).
If it is 1, then by deﬁnition, the two polynomials are relatively prime.
To show the operations are well deﬁned, suppose ′ ′ [a(x)]=[a (x)],[b(x)]=[b (x)] It is necessary to show ′ ′ [a(x)+b(x)]=[a (x)+b (x)] ′ ′ [a(x)b(x)]=[a (x)b (x)] Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation212 VECTOR SPACES AND FIELDS Consider the second of the two.
a′(x)b′(x)−a(x)b(x) = a′(x)b′(x)−a(x)b′(x)+a(x)b′(x)−a(x)b(x) = b′(x)(a′(x)−a(x))+a(x)(b′(x)−b(x)) Now by assumption (a′(x)−a(x)) is a multiple of p(x) as is (b′(x)−b(x)), so the above is a multiple of p(x) and by deﬁnition this shows [a(x)b(x)] = [a′(x)b′(x)].
The case for addition is similar.
Now suppose [a]= [b].
This means a−b = k(x)p(x) for some polynomial k(x).
Then k(x) must equal 0 since otherwise the two polynomials a−b and k(x)p(x) could not be equal because they would have diﬀerent degree.
(cid:4) Note that from this proposition and math induction, if each a ∈F, i [ ] anxn+an−1xn−1+···+a1x+a0 =[an][x]n+[an−1][x]n−1+···[a1][x]+[a0] (8.7) With the above preparation, here is a deﬁnition of a ﬁeld in which the irreducible poly- nomial p(x) has a root.
Deﬁnition 8.3.19 Let p(x)∈F[x] be irreducible and let a(x)∼b(x) when a(x)−b(x) is a multiple of p(x).
Let G denote the set of equivalence classes as described above with the operations also described in Deﬁnition 8.3.17.
Also here is another useful deﬁnition and a simple proposition which comes from it.
Deﬁnition 8.3.20 Let F ⊆ K be two ﬁelds.
Then clearly K is also a vector space over F. Then also, K is called a ﬁnite ﬁeld extension of F if the dimension of this vector space, denoted by [K :F] is ﬁnite.
There are some easy things to observe about this.
Proposition 8.3.21 Let F ⊆K ⊆L be ﬁelds.
Then [L:F]=[L:K][K :F].
Proof: Let{l }n beabasisforLoverK andlet{k }m beabasisofK overF.
Then i i=1 j j=1 if l∈L, there exist unique scalars x in K such that i ∑n l= x l i i i=1 Now x ∈K so there exist f such that i ji ∑m x = f k i ji j j=1 Then it follows that ∑n ∑m l= f k l ji j i i=1j=1 It follows that {k l } is a spanning set.
If j i ∑n ∑m f k l =0 ji j i i=1j=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 213 Then, since the l are independent, it follows that i ∑m f k =0 ji j j=1 and since {k } is independent, each f = 0 for each j for a given arbitrary i.
Therefore, j ji {k l } is a basis.
(cid:4) j i Theorem 8.3.22 The set of all equivalence classes G ≡ F/(p(x)) described above with the multiplicative identity given by [1] and the additive identity given by [0] along with the operations of Deﬁnition 8.3.17, is a ﬁeld and p([x]) = [0].
(Thus p has a root in this new ﬁeld.)
In addition to this, [G:F]=n, the degree of p(x).
Proof: Everything is obvious except for the existence of the multiplicative inverse and theassertionthatp([x])=0.
Supposethenthat[a(x)]̸=[0].Thatis,a(x)isnotamultiple of p(x).
Why does [a(x)]−1 exist?
By Theorem 8.3.6, a(x),p(x) are relatively prime and so there exist polynomials ψ(x),ϕ(x) such that 1=ψ(x)p(x)+a(x)ϕ(x) and so 1−a(x)ϕ(x)=ψ(x)p(x) which, by deﬁnition implies [1−a(x)ϕ(x)]=[1]−[a(x)ϕ(x)]=[1]−[a(x)][ϕ(x)]=[0] and so [ϕ(x)]=[a(x)]−1.
This shows G is a ﬁeld.
Now if p(x)=anxn+an−1xn−1+···+a1x+a0, p([x])=0 by (8.7) and the deﬁnition which says [p(x)]=[0].
[ ] Consider the claim about the dim[en]sion.
It[was]just shown that [1],[x], x2 ,··· ,[xn] is linearly dependent.
Also [1],[x], x2 ,··· , xn−1 is independent because if not, there wouldexistapolynomialq(x)ofdegreen−1whichisamultipleofp(x)whichisimpossible.
Now for [q(x)]∈G, you can write q(x)=p(x)l(x)+r(x) wherethedegreeofr(x)islesstha[n n] orels[eiteq]uals0.
Eitherway, [q(x)]=[r(x)]which is a linear combination of [1],[x], x2 ,··· , xn−1 .
Thus [G:F]= n as claimed.
(cid:4) Note that if p(x) were not irreducible, then you could ﬁnd a ﬁeld extension G such that [G:F]≤n.
You could do this by working with an irreducible factor of p(x).
Usually, people simply write b rather than [b] if b∈F.
Then with this convention, [bϕ(x)]=[b][ϕ(x)]=b[ϕ(x)].
This shows how to enlarge a ﬁeld to get a new one in which the polynomial has a root.
By using a succession of such enlargements, called ﬁeld extensions, there will exist a ﬁeld in which the given polynomial can be factored into a product of polynomials having degree one.
The ﬁeld you obtain in this process of enlarging in which the given polynomial factors in terms of linear factors is called a splitting ﬁeld.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation214 VECTOR SPACES AND FIELDS Theorem 8.3.23 Letp(x)=xn+an−1xn−1+···+a1x+a0 beapolynomialwithcoeﬃcients in a ﬁeld of scalars F. There exists a larger ﬁeld G such that there exist {z ,··· ,z } listed 1 n according to multiplicity such that ∏n p(x)= (x−z ) i i=1 This larger ﬁeld is called a splitting ﬁeld.
Furthermore, [G:F]≤n!
Proof: FromTheorem8.3.22, thereexistsaﬁeldF suchthatp(x)hasaroot, z (=[x] 1 1 if p is irreducible.)
Then by the Euclidean algorithm p(x)=(x−z )q (x)+r 1 1 where r ∈ F .
Since p(z ) = 0, this requires r = 0.
Now do the same for q (x) that was 1 1 1 done for p(x), enlarging the ﬁeld to F if necessary, such that in this new ﬁeld 2 q (x)=(x−z )q (x).
1 2 2 and so p(x)=(x−z )(x−z )q (x) 1 2 2 After n such extensions, you will have obtained the necessary ﬁeld G. Finally consider the claim about dimension.
By Theorem 8.3.22, there is a larger ﬁeld G such that p(x) has a root a in G and [G:F]≤n.
Then 1 1 1 p(x)=(x−a )q(x) 1 Continue this way until the polynomial equals the product of linear factors.
Then by Proposition 8.3.21 applied multiple times, [G:F]≤n!.
(cid:4) Example 8.3.24 The polynomial x2 +1 is irreducible in R(x), polynomials having real coeﬃcients.
To see this is the case, suppose ψ(x) divides x2+1.
Then x2+1=ψ(x)q(x) If the degree of ψ(x) is less than 2, then it must be either a constant or of the form ax+b.
In the latter case, −b/a must be a zero of the right side, hence of the left but x2+1 has no real zeros.
Therefore, the degree of ψ(x) must be two and q(x) must be a constant.
Thus the only polynomial which divides x2+1 are consta[nts and mu]ltiples of x2+1.
Therefore, this shows x(2+1 is)irreducible.
Find the inverse of x2+x+1 in the space of equivalence classes, R/ x2+1 .
You can solve this with partial fractions.
1 x x+1 =− + (x2+1)(x2+x+1) x2+1 x2+x+1 and so ( ) ( ) 1=(−x) x2+x+1 +(x+1) x2+1 which implies ( ) 1∼(−x) x2+x+1 and so the inverse is [−x].
Thefollowingpropositionisinteresting.
Itwasessentiallyprovedabovebuttoemphasize it, here it is again.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 215 Proposition 8.3.25 Suppose p(x) ∈ F[x] is irreducible and has degree n. Then every element of G = F[x]/(p(x)) is of the form [0] or [r(x)] where the degree of r(x) is less than n. Proof: This follows right away from the Euclidean algorithm for polynomials.
If k(x) has degree larger than n−1, then k(x)=q(x)p(x)+r(x) where r(x) is either equal to 0 or has degree less than n. Hence [k(x)]=[r(x)].
(cid:4) Example 8.3.26 In the situation of the above example, ﬁnd [ax+b]−1 assuming a2+b2 ̸= 0.
Note this includes all cases of interest thanks to the above proposition.
You can do it with partial fractions as above.
1 b−ax a2 = + (x2+1)(ax+b) (a2+b2)(x2+1) (a2+b2)(ax+b) and so 1 a2 ( ) 1= (b−ax)(ax+b)+ x2+1 a2+b2 (a2+b2) Thus 1 (b−ax)(ax+b)∼1 a2+b2 and so [ax+b]−1 = [(b−ax)] = b−a[x] a2+b2 a2+b2 You might ﬁnd it interesting to recall that (ai+b)−1 = b−ai .
a2+b2 8.3.3 The Algebraic Numbers EachpolynomialhavingcoeﬃcientsinaﬁeldFhasasplittingﬁeld.
Considerthecaseofall polynomials p(x) having coeﬃcients in a ﬁeld F⊆G and consider all roots which are also in G. The theory of vector spaces is very useful in the study of these algebraic numbers.
Here is a deﬁnition.
Deﬁnition 8.3.27 The algebraic numbers A are those numbers which are in G and also roots of some polynomial p(x) having coeﬃcients in F. Theorem 8.3.28 Let a∈A.
Then there exists a unique monic irreducible polynomial p(x) having coeﬃcients in F such that p(a)=0.
This is called the minimal polynomial for a.
Proof: By deﬁnition, there exists a polynomial q(x) having coeﬃcients in F such that q(a)=0.Ifq(x)isirreducible,dividebytheleadingcoeﬃcientandthisprovestheexistence.
Ifq(x)isnotirreducible,thenthereexistnonconstantpolynomialsr(x)andk(x)suchthat q(x)=r(x)k(x).
Thenoneofr(a),k(a)equals0.
Picktheonewhichequalszeroandletit play the role of q(x).
Continuing this way, in ﬁnitely many steps one obtains an irreducible polynomial p(x) such that p(a)=0.
Now divide by the leading coeﬃcient and this proves existence.
Suppose p ,i = 1,2 both work and they are not equal.
Then by Lemma 8.3.7 i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation216 VECTOR SPACES AND FIELDS they must be relatively prime because they are both assumed to be irreducible and so there exist polynomials l(x),k(x) such that 1=l(x)p (x)+k(x)p (x) 1 2 But now when a is substituted for x, this yields 0 = 1, a contradiction.
The polynomials are equal after all.
(cid:4) Deﬁnition 8.3.29 For a an algebraic number, let deg(a) denote the degree of the minimal polynomial of a.
Also, here is another deﬁnition.
Deﬁnition 8.3.30 Let a ,··· ,a be in A.
A polynomial in {a ,··· ,a } will be an ex- 1 m 1 m pression of the form ∑ ak1···knak11···aknn k1···kn where the ak1···kn are in F, each kj is a nonnegative integer, and all but ﬁnitely many of the ak1···kn equal zero.
The collection of such polynomials will be denoted by F[a ,··· ,a ].
1 m Now notice that for a an algebraic number, F[a] is a vector space with ﬁeld of scalars F. Similarly, for {a ,··· ,a } algebraic numbers, F[a ,··· ,a ] is a vector space with ﬁeld of 1 m 1 m scalars F. The following fundamental proposition is important.
Proposition 8.3.31 Let {a ,··· ,a } be algebraic numbers.
Then 1 m ∏m dimF[a ,··· ,a ]≤ deg(a ) 1 m j j=1 and for an algebraic number a, dimF[a]=deg(a) Every element of F[a ,··· ,a ] is in A and F[a ,··· ,a ] is a ﬁeld.
1 m 1 m Proof: First consider the second assertion.
Let the minimal polynomial of a be p(x)=xn+an−1xn−1+···+a1x+a0.
{ } Since p(a) = 0, it follows 1,a,a2,··· ,an is linearly dependent.
However, if the degree of q(x) is less than the degree of p(x), then if q(x) is not a constant, the two must be relatively prime because p(x) is irreducible and so there exist polynomials k(x),l(x) such that 1=l(x)q(x)+k(x)p(x) and this is a contradiction if q(a)=0 because it would imply upon replacing x with a that 1=0.
Therefore, no polynomial having degree less than n can have a as a root.
It follows { } 1,a,a2,··· ,an−1 is linearly independent.
Thus dimF[a] = deg(a) = n. Here is why this is.
If q(a) is any element of F[a], q(x)=p(x)k(x)+r(x) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.3.
LOTS OF FIELDS 217 ( ) where degr(x)<degp(x) and so q(a)=r(a) and r(a)∈span 1,a,a2,··· ,an−1 .
Now consider{the ﬁrst claim.
}By deﬁnition, F[a1,··· ,am] is obtained from all linear combinations of ak11,ak22,··· ,aknn where the ki are nonnegative integers.
From the ﬁrst part, it suﬃces to consider only k ≤ deg(a ).
Therefore, there exists a spanning set for j j F[a ,··· ,a ] which has 1 m ∏m deg(a ) i i=1 entries.
By Theorem 8.2.4 this proves the ﬁrst claim.
Finally consider the last claim.
Let g(a ,··· ,a ) be a polynomial in {a ,··· ,a } in 1 m 1 m F[a ,··· ,a ].
Since 1 m ∏m dimF[a ,··· ,a ]≡p≤ deg(a )<∞, 1 m j j=1 it follows 1,g(a ,··· ,a ),g(a ,··· ,a )2,··· ,g(a ,··· ,a )p 1 m 1 m 1 m are dependent.
It follows g(a ,··· ,a ) is the root of some polynomial having coeﬃcients 1 m in F. Thus everything in F[a ,··· ,a ] is algebraic.
Why is F[a ,··· ,a ] a ﬁeld?
Let 1 m 1 m g(a ,··· ,a ) be as just mentioned.
Then it has a minimal polynomial, 1 m p(x)=xp+ap−1xp−1+···+a1x+a0 where the a ∈F.
Then a ̸=0 or else the polynomial would not be minimal.
Therefore, i 0 ( ) g(a1,··· ,am) g(a1,··· ,am)p−1+ap−1g(a1,··· ,am)p−2+···+a1 =−a0 and so the multiplicative inverse for g(a ,··· ,a ) is 1 m g(a1,··· ,am)p−1+ap−1g(a1,··· ,am)p−2+···+a1 ∈F[a ,··· ,a ].
−a 1 m 0 The other axioms of a ﬁeld are obvious.
(cid:4) Now from this proposition, it is easy to obtain the following interesting result about the algebraic numbers.
Theorem 8.3.32 The algebraic numbers A, those roots of polynomials in F[x] which are in G, are a ﬁeld.
Proof: Let a be an algebraic number and let p(x) be its minimal polynomial.
Then p(x) is of the form xn+an−1xn−1+···+a1x+a0 where a ̸=0.
Then plugging in a yields 0 ( ) an−1+an−1an−2+···+a1 (−1) a =1.
a 0 and so a−1 = (an−1+an−1an−2+···+a1)(−1) ∈F[a].
By the proposition, every element of F[a] is in A and this shows thaat0 for every element of A, its inverse is also in A.
What about products and sums of things in A?
Are they still in A?
Yes.
If a,b ∈ A, then both a+b and ab∈F[a,b] and from the proposition, each element of F[a,b] is in A.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation218 VECTOR SPACES AND FIELDS A typical example of what is of interest here is when the ﬁeld F of scalars is Q, the rational numbers and the ﬁeld G is R. However, you can certainly conceive of many other examples by considering the integers mod a prime, for example (See Problem 34 on Page 222 for example.)
or any of the ﬁelds which occur as ﬁeld extensions in the above.
There is a very interesting thing about F[a ···a ] in the case where F is inﬁnite which 1 n says that there exists a single algebraic γ such that F[a ···a ] = F[γ].
In other words, 1 n every ﬁeld extension of this sort is a simple ﬁeld extension.
I found this fact in an early version of [5].
Proposition 8.3.33 There exists γ such that F[a ···a ]=F[γ].
1 n Proof: To begin with, consider F[α,β].
Let γ =α+λβ.
Then by Proposition 8.3.31 γ is an algebraic number and it is also clear F[γ]⊆F[α,β] I need to show the other inclusion.
This will be done for a suitable choice of λ.
To do this, it suﬃces to verify that both α and β are in F[γ].
Let the minimal polynomials of α and β be f(x) and g(x) respectively.
Let the distinct roots of f(x) and g(x) be {α ,α ,··· ,α } and {β ,β ,··· ,β } respectively.
These roots 1 2 n 1 2 m are in a ﬁeld which contains splitting ﬁelds of both f(x) and g(x).
Let α=α and β =β .
1 1 Now deﬁne h(x)≡f(α+λβ−λx)≡f(γ−λx) so that h(β) = f(α) = 0.
It follows (x−β) divides both h((x) and)g(x).
If (x−η) is a diﬀerent linear factor of both g(x) and h(x) then it must be x−β for some β for some j j j >1 because these are the only factors of g(x).
Therefore, this would require ( ) ( ) 0=h β =f α +λβ −λβ j 1 1 j and so it would be the case that α +λβ −λβ =α for some k. Hence 1 1 j k α −α λ= k 1 β −β 1 j Now there are ﬁnitely many quotients of the above form and if λ is chosen to not be any of them, then the above cannot happen and so in this case, the only linear factor of both g(x) and h(x) will be (x−β).
Choose such a λ.
Let ϕ(x) be the minimal polynomial of β with respect to the ﬁeld F[γ].
Then this minimal polynomial must divide both h(x) and g(x) because h(β) = g(β) = 0.
However, the only factor these two have in common is x−β and so ϕ(x) = x−β which requires β ∈ F[γ].
Now also α = γ −λβ and so α ∈ F[γ] also.
Therefore, both α,β ∈ F[γ] which forcesF[α,β]⊆F[γ].Thisprovesthepropositioninthecasethatn=2.
Thegeneralresult follows right away by observing that F[a1···an]=F[a1···an−1][an] and using induction.
(cid:4) When you have a ﬁeld F, F(a) denotes the smallest ﬁeld which contains both F and a.
When a is algebraic over F, it follows that F(a)=F[a].
The latter is easier to think about because it just involves polynomials.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.4.
EXERCISES 219 8.3.4 The Lindemannn Weierstrass Theorem And Vector Spaces Asanotherapplicationoftheabstractconceptofvectorspaces,thereisanamazingtheorem due to Weierstrass and Lindemannn.
Theorem 8.3.34 Suppose a ,··· ,a are algebraic numbers and suppose α ,··· ,α are 1 n 1 n distinct algebraic numbers.
Then ∑n a eαi ̸=0 i i=1 In other words, the {eα1,··· ,eαn} are independent as vectors with ﬁeld of scalars equal to the algebraic numbers.
There is a proof of this in the appendix.
It is long and hard but only depends on elementary considerations other than some algebra involving symmetric polynomials.
See Theorem F.3.5.
A number is transcendental if it is not a root of a polynomial which has integer co- eﬃcients.
Most numbers are this way but it is hard to verify that speciﬁc numbers are transcendental.
That π is transcendental follows from e0+eiπ =0.
Bytheabovetheorem,thiscouldnothappenifπ werealgebraicbecausetheniπ wouldalso be algebraic.
Recall these algebraic numbers form a ﬁeld and i is clearly algebraic, being a root of x2+1.
This fact about π was ﬁrst proved by Lindemannn in 1882 and then the generaltheoremabovewasprovedbyWeierstrassin1885.
Thisfactthatπ istranscendental solved an old problem called squaring the circle which was to construct a square with the same area as a circle using a straight edge and compass.
It can be shown that the fact π is transcendental implies this problem is impossible.1 8.4 Exercises         1 1 1 0         1.
Let H denote span 2 , 4 , 3 , 1 .
Find the dimension of H 0 0 1 1 and determine a basis.
{ } 2.
Let M = u=(u ,u ,u ,u )∈R4 :u =u =0 .
Is M a subspace?
Explain.
1 2 3 4 3 1 { } 3.
Let M = u=(u ,u ,u ,u )∈R4 :u ≥u .
Is M a subspace?
Explain.
1 2 3 4 3 1 { } 4.
Let w ∈ R4 and let M = u=(u ,u ,u ,u )∈R4 :w·u=0 .
Is M a subspace?
1 2 3 4 Explain.
{ } 5.
Let M = u=(u ,u ,u ,u )∈R4 :u ≥0 for each i=1,2,3,4 .
Is M a subspace?
1 2 3 4 i Explain.
6.
Let w,w be given vectors in R4 and deﬁne 1 { } M = u=(u ,u ,u ,u )∈R4 :w·u=0 and w ·u=0 .
1 2 3 4 1 Is M a subspace?
Explain.
1Gilbert, the librettist of the Savoy operas, may have heard about this great achievement.
In Princess Ida which opened in 1884 he has the following lines.
“As for fashion they forswear it, so the say - so they say;andthecircle-theywillsquareitsomeﬁnedaysomeﬁneday.”Ofcourseithadbeenprovedimpossible todothisacoupleofyearsbefore.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation220 VECTOR SPACES AND FIELDS { } 7.
Let M = u=(u ,u ,u ,u )∈R4 :|u |≤4 .
Is M a subspace?
Explain.
1 2 3 4 1 { } 8.
Let M = u=(u ,u ,u ,u )∈R4 :sin(u )=1 .
Is M a subspace?
Explain.
1 2 3 4 1 9.
Suppose {x ,··· ,x } is aset of vectorsfrom Fn.
Showthat 0 is in span(x ,··· ,x ).
1 k 1 k 10.
Consider the vectors of the form     2t+3s   s−t :s,t∈R .
  t+s IsthissetofvectorsasubspaceofR3?Ifso,explainwhy,giveabasisforthesubspace and ﬁnd its dimension.
11.
Consider the vectors of the form     2t+st+3−sst+u :s,t,u∈R.
u IsthissetofvectorsasubspaceofR4?Ifso,explainwhy,giveabasisforthesubspace and ﬁnd its dimension.
12.
Consider the vectors of the form     2t+u+1    tt++s3+uv :s,t,u,v ∈R.
u IsthissetofvectorsasubspaceofR4?Ifso,explainwhy,giveabasisforthesubspace and ﬁnd its dimension.
13.
Let V denote the set of functions deﬁned on [0,1].
Vector addition is deﬁned as (f +g)(x)≡f(x)+g(x) and scalar multiplication is deﬁned as (αf)(x)≡α(f(x)).
Verify V is a vector space.
What is its dimension, ﬁnite or inﬁnite?
Justify your answer.
14.
Let V denote the set of polynomial functions deﬁned on [0,1].
Vector addition is deﬁnedas(f +g)(x)≡f(x)+g(x)andscalarmultiplicationisdeﬁnedas(αf)(x)≡ α(f(x)).
Verify V is a vector space.
What is its dimension, ﬁnite or inﬁnite?
Justify your answer.
15.
Let V be the set of polynomials deﬁned on R having degree no more than 4.
Give a basis for this vector space.
√ 16.
Let the vectors be of the form a+b 2 where a,b are rational numbers and let the ﬁeld of scalars be F = Q, the rational numbers.
Show directly this is a vector space.
What is its dimension?
What is a basis for this vector space?
17.
Let V be a vector space with ﬁeld of scalars F and suppose {v ,··· ,v } is a basis for 1 n V. Now let W also be a vector space with ﬁeld of scalars F. Let L : {v ,··· ,v } → 1 n W be a function such that Lv = w .
Explain how L can be extended to a linear j j transformation mapping V to W in a unique way.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.4.
EXERCISES 221 18.
If you have 5 vectors in F5 and the vectors are linearly independent, can it always be concluded they span F5?
Explain.
19.
If you have 6 vectors in F5, is it possible they are linearly independent?
Explain.
20.
Suppose V,W are subspaces of Fn.
Show V ∩W deﬁned to be all vectors which are in both V and W is a subspace also.
21.
SupposeV andW bothhavedimensionequalto7andtheyaresubspacesofavector space of dimension 10.
What are the possibilities for the dimension of V ∩W?
Hint: Remember that a linear independent set can be extended to form a basis.
22.
Suppose V has dimension p and W has dimension q and they are each contained in a subspace, U which has dimension equal to n where n > max(p,q).
What are the possibilities for the dimension of V ∩W?
Hint: Remember that a linear independent set can be extended to form a basis.
23.
If b̸=0, can the solution set of Ax=b be a plane through the origin?
Explain.
24.
Suppose a system of equations has fewerequations than variablesand youhavefound asolutiontothissystemofequations.
Isitpossiblethatyoursolutionistheonlyone?
Explain.
25.
Supposeasystemoflinearequationshasa2×4augmentedmatrixandthelastcolumn is a pivot column.
Could the system of linear equations be consistent?
Explain.
26.
Suppose the coeﬃcient matrix of a system of n equations with n variables has the property that every column is a pivot column.
Does it follow that the system of equations must have a solution?
If so, must the solution be unique?
Explain.
27.
Supposethereisauniquesolutiontoasystemoflinearequations.
Whatmustbetrue of the pivot columns in the augmented matrix.
28.
State whether each of the following sets of data are possible for the matrix equation Ax=b.
If possible, describe the solution set.
That is, tell whether there exists a unique solution no solution or inﬁnitely many solutions.
(a) A is a 5×6 matrix, rank(A) = 4 and rank(A|b) = 4.
Hint: This says b is in the span of four of the columns.
Thus the columns are not independent.
(b) A is a 3×4 matrix, rank(A)=3 and rank(A|b)=2.
(c) A is a 4×2 matrix, rank(A) = 4 and rank(A|b) = 4.
Hint: This says b is in the span of the columns and the columns must be independent.
(d) A is a 5×5 matrix, rank(A) = 4 and rank(A|b) = 5.
Hint: This says b is not in the span of the columns.
(e) A is a 4×2 matrix, rank(A)=2 and rank(A|b)=2.
29.
SupposeAisanm×nmatrixinwhichm≤n.SupposealsothattherankofAequals m. Show that A maps Fn onto Fm.
Hint: The vectors e ,··· ,e occur as columns 1 m in the row reduced echelon form for A.
30.
SupposeAisanm×nmatrixinwhichm≥n.SupposealsothattherankofAequals n.ShowthatAisonetoone.
Hint: Ifnot, thereexistsavector,xsuchthatAx=0, and this implies at least one column of A is a linear combination of the others.
Show this would require the column rank to be less than n. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation222 VECTOR SPACES AND FIELDS 31.
Explain why an n×n matrix A is both one to one and onto if and only if its rank is n. 32.
If you have not done this already, here it is again.
It is a very important result.
Suppose A is an m×n matrix and B is an n×p matrix.
Show that dim(ker(AB))≤dim(ker(A))+dim(ker(B)).
Hint: Consider the subspace, B(Fp)∩ker(A) and suppose a basis for this subspace is {w ,··· ,w }.
Now suppose {u ,··· ,u } is a basis for ker(B).
Let {z ,··· ,z } 1 k 1 r 1 k be such that Bz =w and argue that i i ker(AB)⊆span(u ,··· ,u ,z ,··· ,z ).
1 r 1 k Here is how you do this.
Suppose ABx=0.
Then Bx ∈ ker(A) ∩ B(Fp) and so ∑ Bx= k Bz showing that i=1 i ∑k x− z ∈ker(B).
i i=1 33.
Recallthateverypositiveintegercanbefactoredintoaproductofprimesinaunique way.
Show there must be inﬁnitely many primes.
Hint: Show that if you have any ﬁnite set of primes and you multiply them and then add 1, the result cannot be divisible by any of the primes in your ﬁnite set.
This idea in the hint is due to Euclid who lived about 300 B.C.
34.
Therearelotsofﬁelds.
Thiswillgiveanexampleofaﬁniteﬁeld.
LetZdenotetheset of integers.
Thus Z = {··· ,−3,−2,−1,0,1,2,3,···}.
Also let p be a prime number.
We will say that two integers, a,b are equivalent and write a ∼ b if a−b is divisible by p. Thus they are equivalent if a−b = px for some integer x.
First show that a ∼ a.
Next show that if a ∼ b then b ∼ a.
Finally show that if a ∼ b and b ∼ c then a ∼ c. For a an integer, denote by [a] the set of all integers which is equivalent to a, the equivalence class of a.
Show ﬁrst that is suﬃces to consider only [a] for a=0,1,2,··· ,p−1 and that for 0≤a<b≤p−1,[a]̸=[b].
That is, [a]=[r] where r ∈ {0,1,2,··· ,p−1}.
Thus there are exactly p of these equivalence classes.
Hint: Recall the Euclidean algorithm.
For a > 0, a = mp+r where r < p. Next deﬁne the following operations.
[a]+[b] ≡ [a+b] [a][b] ≡ [ab] Show these operations are well deﬁned.
That is, if [a] = [a′] and [b] = [b′], then [a]+[b] = [a′]+[b′] with a similar conclusion holding for multiplication.
Thus for additionyouneedtoverify[a+b]=[a′+b′]andformultiplicationyouneedtoverify [ab]=[a′b′].
Forexample,ifp=5youhave[3]=[8]and[2]=[7].Is[2×3]=[8×7]?
Is [2+3]=[8+7]?
Clearly so in this example because when you subtract, the result is divisible by 5.
So why is this so in general?
Now verify that {[0],[1],··· ,[p−1]} with these operations is a Field.
This is called the integers modulo a prime and is written Z .
Since there are inﬁnitely many primes p, it follows there are inﬁnitely p many of these ﬁnite ﬁelds.
Hint: Most of the axioms are easy once you have shown the operations are well deﬁned.
The only two which are tricky are the ones which give the existence of the additive inverse and the multiplicative inverse.
Of these, the Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation8.4.
EXERCISES 223 ﬁrst is not hard.
−[x] = [−x].
Since p is prime, there exist integers x,y such that 1=px+kyandso1−ky =pxwhichsays1∼kyandso[1]=[ky].Nowyouﬁnishthe argument.
Whatisthemultiplicativeidentityinthiscollectionofequivalenceclasses?
Of course you could now consider ﬁeld extensions based on these ﬁelds.
35.
Suppose the ﬁeld of scalars is Z described above.
Show that 2 ( )( ) ( )( ) ( ) 0 1 0 0 0 0 0 1 1 0 − = 0 0 1 0 1 0 0 0 0 1 Thus the identity is a comutator.
Compare this with Problem 50 on Page 198.
36.
Suppose V is a vector space with ﬁeld of scalars F. Let T ∈ L(V,W), the space of linear transformations mapping V onto W where W is another vector space.
Deﬁne an equivalence relation on V as follows.
v∼w means v−w ∈ ker(T).
Recall that ker(T) ≡ {v:Tv=0}.
Show this is an equivalence relation.
Now for [v] an equiv- alence class deﬁne T′[v] ≡ Tv.
Show this is well deﬁned.
Also show that with the operations [v]+[w] ≡ [v+w] α[v] ≡ [αv] thissetofequivalenceclasses,denotedbyV/ker(T)isavectorspace.
Shownextthat T′ :V/ker(T)→W isonetoone, linear, andonto.
Thisnewvectorspace, V/ker(T) is called a quotient space.
Show its dimension equals the diﬀerence between the dimension of V and the dimension of ker(T).
37.
Let V be an n dimensional vector space and let W be a subspace.
Generalize the above problem to deﬁne and give properties of V/W.
What is its dimension?
What is a basis?
38.
If F and G are two ﬁelds and F⊆G, can you consider G as a vector space with ﬁeld of scalars F?
Explain.
39.
Let A denote the algebraic numbers, those numbers which are roots of polynomials having rational coeﬃcients which are in R. Show A can be considered a vector space with ﬁeld of scalars Q.
What is the dimension of this vector space, ﬁnite or inﬁnite?
40.
As mentioned, for distinct algebraic numbers αi, the complex numbers {eαi}ni=1 are linearlyindependentovertheﬁeldofscalarsAwhereAdenotesthealgebraicnumbers, those which are roots of a polynomial having integer (rational) coeﬃcients.
What is the dimension of the vector space C with ﬁeld of scalars A, ﬁnite or inﬁnite?
If the ﬁeld of scalars were C instead of A, would this change?
What if the ﬁeld of scalars were R?
41.
Suppose F is a countable ﬁeld and let A be the algebraic numbers, those numbers which are roots of a polynomial having coeﬃcients in F which are in G, some other ﬁeld containing F. Show A is also countable.
42.
This problem is on partial fractions.
Suppose you have p(x) R(x)= , degree of p(x)< degree of denominator.
q (x)···q (x) 1 m Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation224 VECTOR SPACES AND FIELDS where the polynomials q (x) are relatively prime and all the polynomials p(x) and i q (x) have coeﬃcients in a ﬁeld of scalars F. Thus there exist polynomials a (x) i i having coeﬃcients in F such that ∑m 1= a (x)q (x) i i i=1 Explain why ∑ p(x) m a (x)q (x) ∑m a (x)p(x) R(x)= i=1 i i = ∏i q (x)···q (x) q (x) 1 m i=1 j̸=i j Now continue doing this on each term in the above sum till ﬁnally you obtain an expression of the form ∑m b (x) i q (x) i i=1 Using the Euclidean algorithm for polynomials, explain why the above is of the form ∑m r (x) M(x)+ i q (x) i i=1 where the degree of each r (x) is less than the degree of q (x) and M(x) is a poly- i i nomial.
Now argue that M(x)=0.
From this explain why the usual partial fractions expansionofcalculusmustbetrue.
Youcanusethefactthateverypolynomialhaving real coeﬃcients factors into a product of irreducible quadratic polynomials and linear polynomials having real coeﬃcients.
This follows from the fundamental theorem of algebra in the appendix.
43.
Suppose{f ,··· ,f }isanindependentsetofsmoothfunctionsdeﬁnedonsomeinter- 1 n val (a,b).
Now let A be an invertible n×n matrix.
Deﬁne new functions {g ,··· ,g } 1 n as follows.
    g f 1 1  .
  .
  .
=A .
 .
.
g f n n Is it the case that {g ,··· ,g } is also independent?
Explain why.
1 n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationLinear Transformations 9.1 Matrix Multiplication As A Linear Transformation Deﬁnition 9.1.1 Let V and W be two ﬁnite dimensional vector spaces.
A function, L which maps V to W is called a linear transformation and written L ∈ L(V,W) if for all scalars α and β, and vectors v,w, L(αv+βw)=αL(v)+βL(w).
An example of a linear transformation is familiar matrix multiplication.
Let A = (a ) ij be an m×n matrix.
Then an example of a linear transformation L:Fn →Fm is given by ∑n (Lv) ≡ a v .
i ij j j=1 Here   v 1 v≡ .. ∈Fn.
.
v n 9.2 L(V;W) As A Vector Space Deﬁnition 9.2.1 Given L,M ∈ L(V,W) deﬁne a new element of L(V,W), denoted by L+M according to the rule1 (L+M)v ≡Lv+Mv.
For α a scalar and L∈L(V,W), deﬁne αL∈L(V,W) by αL(v)≡α(Lv).
You should verify that all the axioms of a vector space hold for L(V,W) with the above deﬁnitions of vector addition and scalar multiplication.
What about the dimension of L(V,W)?
Before answering this question, here is a useful lemma.
It gives a way to deﬁne linear transformations and a way to tell when two of them are equal.
1Notethatthisisthestandardwayofdeﬁningthesumoftwofunctions.
225 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation226 LINEAR TRANSFORMATIONS Lemma 9.2.2 Let V and W be vector spaces and suppose {v ,··· ,v } is a basis for V. 1 n Then if L:V →W is given by Lv =w ∈W and k k ( ) ∑n ∑n ∑n L a v ≡ a Lv = a w k k k k k k k=1 k=1 k=1 then L is well deﬁned and is in L(V,W).
Also, if L,M are two linear transformations such that Lv =Mv for all k, then M =L.
k k Proof: LiswelldeﬁnedonV because, since{v ,··· ,v }isabasis, thereisexactlyone 1 n way to write a given vector of V as a linear combination.
Next, o∑bserve that L is obviously linear from the deﬁnition.
If L,M are equal on the basis, then if n a v is an arbitrary k=1 k k vector of V, ( ) ( ) ∑n ∑n ∑n ∑n L a v = a Lv = a Mv =M a v k k k k k k k k k=1 k=1 k=1 k=1 and so L=M because they give the same result for every vector in V. (cid:4) The message is that when you deﬁne a linear transformation, it suﬃces to tell what it does to a basis.
Theorem 9.2.3 Let V and W be ﬁnite dimensional linear spaces of dimension n and m respectively Then dim(L(V,W))=mn.
Proof: Let two sets of bases be {v ,··· ,v } and {w ,··· ,w } 1 n 1 m for V and W respectively.
Using Lemma 9.2.2, let w v ∈ L(V,W) be the linear transfor- i j mation deﬁned on the basis, {v ,··· ,v }, by 1 n w v (v )≡w δ i k j i jk where δ =1 if i=k and 0 if i̸=k.
I will show that L∈L(V,W) is a linear combination ik of these special linear transformations called dyadics.
Then let L ∈ L(V,W).
Since {w ,··· ,w } is a basis, there exist constants, d such 1 m jk that ∑m Lv = d w r jr j j=1 Now consider the following sum of dyadics.
∑m ∑n d w v ji j i j=1i=1 Apply this to v .
This yields r ∑m ∑n ∑m ∑n ∑m d w v (v )= d w δ = d w =Lv ji j i r ji j ir jr i r j=1i=1 j=1i=1 j=1 ∑ ∑ Therefore, L= m n d w v showing the span of the dyadics is all of L(V,W).
j=1 i=1 ji j i Now consider whether these dyadics form a linearly independent set.
Suppose ∑ d w v =0.
ik i k i,k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 227 Are all the scalars d equal to 0?
ik ∑ ∑m 0= d w v (v )= d w ik i k l il i i,k i=1 and so, since {w ,··· ,w } is a basis, d = 0 for each i = 1,··· ,m. Since l is arbitrary, 1 m il this shows d = 0 for all i and l. Thus these linear transformations form a basis and this il shows that the dimension of L(V,W) is mn as claimed because there are m choices for the w and n choices for the v .
(cid:4) i j 9.3 The Matrix Of A Linear Transformation Deﬁnition 9.3.1 In Theorem 9.2.3, the matrix of the linear transformation L ∈ L(V,W) with respect to the ordered bases β ≡ {v1,··· ,vn} for V and γ ≡ {w∑1,··· ,wm} for W is deﬁned to be [L] where [L] =d .
Thus this matrix is deﬁned by L= [L] w v .
When ij ij i,j ij i i it is desired to feature the bases β,γ, this matrix will be denoted as [L] .
When there is γβ only one basis β, this is denoted as [L] .
β If V is an n dimensional vector space and β ={v ,··· ,v } is a basis for V, there exists 1 n a linear map q :Fn →V β deﬁned as ∑n q (a)≡ a v β i i i=1 where   a  .1  ∑n a= .
= a e , .
i i a i=1 n ( ) for e the standard basis vectors for Fn consisting of 0 ··· 1 ··· 0 T. Thus the 1 i is in the ith position and the other entries are 0.
It is clear that q deﬁned in this way, is one to one, onto, and linear.
For v ∈ V, q−1(v) β is a vector in Fn called the component vector of v with respect to the basis {v ,··· ,v }.
1 n Proposition 9.3.2 The matrix of a linear transformation with respect to orderedbases β,γ as described above is characterized by the requirement that multiplication of the components of v by [L] gives the components of Lv.
γβ ∑ Proof: This happens because by deﬁnition, if v = x v , then i i i ∑ ∑∑ ∑∑ Lv = x Lv ≡ [L] x w = [L] x w i i ji i j ji i j i i j j i ∑ and so the jth component of Lv is [L] x , the jth component of the matrix times the i ji i componentvectorofv.
Couldtherebesomeothermatrixwhichwilldothis?No,becauseif such a matrix is M, then for any x , it follows from what was just shown that [L]x=Mx.
Hence [L]=M.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation228 LINEAR TRANSFORMATIONS The above proposition shows that the following diagram determines the matrix of a linear transformation.
Here q and q are the maps deﬁned above with reference to the β γ ordered bases, {v ,··· ,v } and {w ,··· ,w } respectively.
1 n 1 m L β ={v ,··· ,v } V → W {w ,··· ,w }=γ 1 n 1 m q ↑ ◦ ↑q (9.1) β γ Fn → Fm [L] γβ In terms of this diagram, the matrix [L] is the matrix chosen to make the diagram γβ “commute” It may help to write the description of [L] in the form γβ ( ) ( ) Lv ··· Lv = w ··· w [L] (9.2) 1 n 1 m γβ with the understanding that you do the multiplications in a formal manner just as you would if everything were numbers.
If this helps, use it.
If it does not help, ignore it.
Example 9.3.3 Let V ≡{ polynomials of degree 3 or less}, W ≡{ polynomials of degree 2 or less}, { } and L≡D where D is the diﬀerentiation operator.
A basis for V is β = 1,x,x2,x3 and a basis for W is γ ={1,x, x2}.
What is the matrix of this linear transformation with respect to this basis?
Using (9.2), ( ) ( ) 0 1 2x 3x2 = 1 x x2 [D] .
γβ It follows from this that the ﬁrst column of [D] is γβ   0   0 0 The next three columns of [D] are γβ       1 0 0       0 , 2 , 0 0 0 3 and so   0 1 0 0   [D] = 0 0 2 0 .
γβ 0 0 0 3 Now consider the important case where V = Fn, W = Fm, and the basis chosen is the standard basis of vectors e described above.
i β ={e ,··· ,e }, γ ={e ,··· ,e } 1 n 1 m LetLbealineartransformationfromFntoFmandletAbethematrixofthetransformation with respect to these bases.
In this case the coordinate maps q and q are simply the β γ identity maps on Fn and Fm respectively, and can be accomplished by simply multiplying Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 229 by the appropriate sized identity matrix.
The requirement that A is the matrix of the transformation amounts to Lb=Ab What about the situation where diﬀerent pairs of bases are chosen for V and W?
How are the two matrices with respect to these choices related?
Consider the following diagram which illustrates the situation.
Fn A Fm −→2 q ↓ ◦ q ↓ β γ 2 2 V L W −→ q ↑ ◦ q ↑ β γ 1Fn A F1m −→1 In this diagram q and q are coordinate maps as described above.
From the diagram, β γ i i q−1q A q−1q =A , γ1 γ2 2 β2 β1 1 whereq−1q and q−1q areonetoone,onto,andlinearmapswhichmaybeaccomplished bymultβip2licβa1tionbyγa1 sqγ2uarematrix.
ThusthereexistmatricesP,QsuchthatP :Fn →Fn and Q:Fm →Fm are invertible and PA Q=A .
2 1 Example 9.3.4 Let β ≡ {v ,··· ,v } and γ ≡ {w ,··· ,w } be two bases for V. Let L 1 n 1 n be the linear transformation which maps v to w .
Find [L] .
In case V = Fn and letting i i γβ δ ={e ,··· ,e }, the usual basis for Fn, ﬁnd [L] .
1 n δ ∑ Letting δij be the symbol which equals 1 if i = j and 0 if i ̸= j, it follows that L = δ w v and so [L] =I the identity matrix.
For the second part, you must have i,j ij i j γβ ( ) ( ) w ··· w = v ··· v [L] 1 n 1 n δ and so ( ) ( ) [L] = v ··· v −1 w ··· w δ 1 n 1 n ( ) where w ··· w is the n×n matrix having ith column equal to w .
1 n i Deﬁnition 9.3.5 In the special case where V =W and only one basis is used for V =W, this becomes q−1q A q−1q =A .
β1 β2 2 β2 β1 1 LettingS bethematrixofthelineartransformationq−1q withrespecttothestandardbasis vectors in Fn, β2 β1 S−1A S =A .
(9.3) 2 1 When this occurs, A is said to be similar to A and A → S−1AS is called a similarity 1 2 transformation.
Recall the following.
Deﬁnition 9.3.6 Let S be a set.
The symbol ∼ is called an equivalence relation on S if it satisﬁes the following axioms.
1. x∼x for all x∈S.
(Reﬂexive) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation230 LINEAR TRANSFORMATIONS 2.
If x∼y then y ∼x.
(Symmetric) 3.
If x∼y and y ∼z, then x∼z.
(Transitive) Deﬁnition 9.3.7 [x] denotes the set of all elements of S which are equivalent to x and [x] is called the equivalence class determined by x or just the equivalence class of x.
Also recall the notion of equivalence classes.
Theorem 9.3.8 Let ∼ be an equivalence class deﬁned on a set S and let H denote the set of equivalence classes.
Then if [x] and [y] are two of these equivalence classes, either x∼y and [x]=[y] or it is not true that x∼y and [x]∩[y]=∅.
Theorem 9.3.9 In the vector space of n×n matrices, deﬁne A∼B if there exists an invertible matrix S such that A=S−1BS.
Then ∼ is an equivalence relation and A∼B if and only if whenever V is an n dimensional vector space, there exists L ∈ L(V,V) and bases {v ,··· ,v } and {w ,··· ,w } such that 1 n 1 n A is the matrix of L with respect to {v ,··· ,v } and B is the matrix of L with respect to 1 n {w ,··· ,w }.
1 n Proof: A∼A because S =I works in the deﬁnition.
If A∼B , then B ∼A, because A=S−1BS implies B =SAS−1.
If A∼B and B ∼C, then A=S−1BS, B =T−1CT and so A=S−1T−1CTS =(TS)−1CTS which implies A∼C.
This veriﬁes the ﬁrst part of the conclusion.
Now let V be an n dimensional vector space, A∼B so A=S−1BS and pick a basis for V, β ≡{v ,··· ,v }.
1 n Deﬁne L∈L(V,V) by ∑ Lv ≡ a v i ji j j whereA=(a ).ThusAisthematrixofthelineartransformationL.
Considerthediagram ij Fn B Fn −→ q ↓ ◦ q ↓ γ γ V L V −→ q ↑ ◦ q ↑ β β Fn A Fn −→ where q is chosen to make the diagram commute.
Thus we need S =q−1q which requires γ γ β q =q S−1 γ β Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 231 Then it follows that B is the matrix of L with respect to the basis {q e ,··· ,q e }≡{w ,··· ,w }.
γ 1 γ n 1 n That is, A and B are matrices of the same linear transformation L. Conversely, if A ∼ B, let L be as just described.
Thus L = q Aq−1 = q SBS−1q−1.
Let q ≡ q S and it follows β β β β γ β that B is the matrix of L with respect to {q Se ,··· ,q Se }.
(cid:4) β 1 β n WhatifthelineartransformationconsistsofmultiplicationbyamatrixAandyouwant to ﬁnd the matrix of this linear transformation with respect to another basis?
Is there an easy way to do it?
The next proposition considers this.
Proposition 9.3.10 LetAbeanm×nmatrixandletLbethelineartransformationwhich is deﬁned by ( ) ∑n ∑n ∑m ∑n L x e ≡ (Ae )x ≡ A x e k k k k ik k i k=1 k=1 i=1k=1 In simple language, to ﬁnd Lx, you multiply on the left of x by A.
(A is the matrix of L with respect to the standard basis.)
Then the matrix M of this linear transformation with respect to the bases β ={u ,··· ,u } for Fn and γ ={w ,··· ,w } for Fm is given by 1 n 1 m ( ) ( ) M = w ··· w −1A u ··· u 1 m 1 n ( ) where w ··· w is the m×m matrix which has w as its jth column.
1 m j Proof: Consider the following diagram.
L {u ,··· ,u } Fn → Fm {w ,··· ,w } 1 n 1 m q ↑ ◦ ↑q β γ Fn → Fm M Here the coordinate maps are deﬁned in the usual way.
Thus ( ) ∑n q x ··· x T ≡ x u .
β 1 n i i i=1 Therefore, q(β can be consid)ered the same as multiplication of a vector in Fn on the left by the matrix u ··· u .
Similar considerations apply to q .
Thus it is desired to have 1 n γ the following for an arbitrary x∈Fn.
( ) ( ) A u ··· u x= w ··· w Mx 1 n 1 n Therefore, the conclusion of the proposition follows.
(cid:4) In the special case where m = n and F = C or R and {u ,··· ,u } is an orthonormal 1 n basisandyouwantM,thematrixofLwithrespecttothisneworthonormalbasis,itfollows from the above that ( ) ( ) M = u ··· u ∗A u ··· u =U∗AU 1 m 1 n where U is a unitary matrix.
Thus matrices with respect to two orthonormal bases are unitarily similar.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation232 LINEAR TRANSFORMATIONS Deﬁnition 9.3.11 An n×n matrix A, is diagonalizable if there exists an invertible n×n matrix S such that S−1AS = D, where D is a diagonal matrix.
Thus D has zero entries everywhere except on the main diagonal.
Write diag(λ ··· ,λ ) to denote the diagonal 1 n matrix having the λ down the main diagonal.
i The following theorem is of great signiﬁcance.
Theorem 9.3.12 Let A be an n×n matrix.
Then A is diagonalizable if and only if Fn has a basis of eigenvectors of A.
In this case, S of Deﬁnition 9.3.11 consists of the n×n matrix whose columns are the eigenvectors of A and D =diag(λ ,··· ,λ ).
1 n Proof: SupposeﬁrstthatFnhasabasisofeigenvectors,{v1,··· ,vn}whereAvi =λivi.
uT Then let S denote the matrix ( v ··· v ) and let S−1 ≡ ..1  where 1 n .
uT n { 1 if i=j uTv =δ ≡ .
i j ij 0 if i̸=j S−1 exists because S has rank n. Then from block multiplication,     uT uT 1 1 S−1AS = .. (Av ···Av )= .. (λ v ···λ v ) .
1 n .
1 1 n n uT uT n n   λ 0 ··· 0 1  0 λ2 0 ···  = ... ... ... ... =D.
0 ··· 0 λ n NextsupposeAisdiagonalizablesoS−1AS =D ≡diag(λ ,··· ,λ ).Thenthecolumns 1 n of S form a basis because S−1 is given to exist.
(It only remains) to verify that these c(olumns of S are e)igen(vectors.
But letting) S = v1 ··· vn , AS = SD and so Av ··· Av = λ v ··· λ v which shows that Av =λ v .
(cid:4) 1 n 1 1 n n i i i Itmakessensetospeakofthedeterminantofalineartransformationasdescribedinthe following corollary.
Corollary 9.3.13 Let L∈L(V,V) where V is an n dimensional vector space and let A be the matrix of this linear transformation with respect to a basis on V. Then it is possible to deﬁne det(L)≡det(A).
Proof: Each choice of basis for V determines a matrix for L with respect to the basis.
If A and B are two such matrices, it follows from Theorem 9.3.9 that A=S−1BS and so ( ) det(A)=det S−1 det(B)det(S).
But ( ) ( ) 1=det(I)=det S−1S =det(S)det S−1 and so det(A)=det(B) (cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 233 Deﬁnition 9.3.14 Let A ∈ L(X,Y) where X and Y are ﬁnite dimensional vector spaces.
Deﬁne rank(A) to equal the dimension of A(X).
The following theorem explains how the rank of A is related to the rank of the matrix of A. Theorem 9.3.15 Let A∈L(X,Y).
Then rank(A)=rank(M) where M is the matrix of A taken with respect to a pair of bases for the vector spaces X, and Y.
Proof: Recall the diagram which describes what is meant by the matrix of A.
Here the two bases are as indicated.
β ={v ,··· ,v } X A Y {w ,··· ,w }=γ 1 n −→ 1 m q ↑ ◦ ↑q β γ Fn M Fm −→ Let {Ax ,··· ,Ax } be a basis for AX.
Thus 1 r { } q Mq−1x ,··· ,q Mq−1x γ β 1 γ β r is a basis for AX.
It follows that { } Mq−1x ,··· ,Mq−1x X 1 X r is linearly independent and so rank(A) ≤ rank(M).
However, one could interchange the roles of M and A in the above argument and thereby turn the inequality around.
(cid:4) The following result is a summary of many concepts.
Theorem 9.3.16 Let L∈L(V,V) where V is a ﬁnite dimensional vector space.
Then the following are equivalent.
1.
L is one to one.
2.
L maps a basis to a basis.
3.
L is onto.
4. det(L)̸=0 5.
If Lv =0 then v =0.
∑ Proof:Sup∑poseﬁrstLisonetooneandletβ ={vi}ni=1beabasis.
Thenif ni=1ciLvi = 0itfollowsL( ni=∑1civi)=0whichmeansthatsinceL(0)=0,andLisonetoone,itmust be the case that n c v = 0.
Since {v } is a basis, each c = 0 which shows {Lv } is a i=1 i i i i i linearly independent set.
Since there are n of these, it must be that this is a basis.
Now suppose 2.).
Then letting {vi}∑be a basis, and y∑∈V, it follows from part 2.)
that there are constants, {c } such that y = n c Lv =L( n c v ).
Thus L is onto.
It has i i=1 i i i=1 i i been shown that 2.)
implies 3.).
Nowsuppose3.).
ThentheoperationconsistingofmultiplicationbythematrixofL,[L], must be onto.
However, the vectors in Fn so obtained, consist of linear combinations of the columns of [L].
Therefore, the column rank of [L] is n. By Theorem 3.3.23 this equals the determinant rank and so det([L])≡det(L)̸=0.
Now assume 4.)
If Lv = 0 for some v ̸= 0, it follows that [L]x = 0 for some x̸=0.
Therefore, the columns of [L] are linearly dependent and so by Theorem 3.3.23, det([L])= det(L)=0 contrary to 4.).
Therefore, 4.)
implies 5.).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation234 LINEAR TRANSFORMATIONS Now suppose 5.)
and suppose Lv = Lw.
Then L(v−w) = 0 and so by 5.
), v−w = 0 showing that L is one to one.
(cid:4) Also it is important to note that composition of linear transformations corresponds to multiplication of the matrices.
Consider the following diagram in which [A] denotes the γβ matrix of A relative to the bases γ on Y and β on X,[B] deﬁned similarly.
δγ X A Y B Z −→ −→ q ↑ ◦ ↑q ◦ ↑q β γ δ Fn [A] Fm [B] Fp −−−γ→β −−−→δγ where A and B are two linear transformations, A ∈ L(X,Y) and B ∈ L(Y,Z).
Then B ◦ A ∈ L(X,Z) and so it has a matrix with respect to bases given on X and Z, the coordinate maps for these bases being q and q respectively.
Then β δ B◦A=q [B] q q−1[A] q−1 =q [B] [A] q−1.
δ δγ γ γ γβ β δ δγ γβ β But this shows that [B] [A] plays the role of [B◦A] , the matrix of B◦A.
Hence the δγ γβ δβ matrix of B ◦A equals the product of the two matrices [A] and [B] .
Of course it is γβ δγ interesting to note that although [B◦A] must be unique, the matrices, [A] and [B] δβ γβ δγ are not unique because they depend on γ, the basis chosen for Y. Theorem 9.3.17 The matrix of the composition of linear transformations equals the prod- uct of the matrices of these linear transformations.
9.3.1 Some Geometrically Deﬁned Linear Transformations If T is any linear transformation which maps Fn to Fm, there is always an m×n matrix A≡[T] with the property that Ax=Tx (9.4) for all x∈Fn.
You simply take the matrix of the linear transformation with respect to the standard basis.
What is the form of A?
Suppose T : Fn → Fm is a linear transformation and you want to ﬁnd the matrix deﬁned by this linear transformation as described in (9.4).
Then if x∈Fn it follows ∑n x= x e i i i=1 where e is the vector which has zeros in every slot but the ith and a 1 in this slot.
Then i since T is linear, ∑n Tx= x T (e ) i i i=1      | | x1 x1 = T (e ) ··· T (e )  .. ≡A ..  1 n .
.
| | x x n n and so you see that the matrix desired is obtained from letting the ith column equal T (e ).
i This proves the following theorem.
Theorem 9.3.18 Let T be a linear transformation from Fn to Fm.
Then the matrix A satisfying (9.4) is given by   | |  T (e ) ··· T (e )  1 n | | Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 235 where Te is the ith column of A. i Example 9.3.19 Determine the matrix for the transformation mapping R2 to R2 which consists of rotating every vector counter clockwise through an angle of θ.
( ) ( ) 1 0 Let e ≡ and e ≡ .
These identify the geometric vectors which point 1 0 2 1 along the positive x axis and positive y axis as shown.
e 26 - e 1 FromTheorem9.3.18,youonlyneedtoﬁndTe andTe ,theﬁrstbeingtheﬁrstcolumn 1 2 of the desired matrix A and the second being the second column.
From drawing a picture and doing a little geometry, you see that ( ) ( ) cosθ −sinθ Te = ,Te = .
1 sinθ 2 cosθ Therefore, from Theorem 9.3.18, ( ) cosθ −sinθ A= sinθ cosθ Example 9.3.20 Find the matrix of the linear transformation which is obtained by ﬁrst rotating all vectors through an angle of ϕ and then through an angle θ.
Thus you want the linear transformation which rotates all angles through an angle of θ+ϕ.
Let T denote the linear transformation which rotates every vector through an angle θ+ϕ of θ+ϕ.
Then to get T , you could ﬁrst do T and then do T where T is the linear θ+ϕ ϕ θ ϕ transformation which rotates through an angle of ϕ and T is the linear transformation θ which rotates through an angle of θ. Denoting the corresponding matrices by A , A , θ+ϕ ϕ and A , you must have for every x θ A x=T x=T T x=A A x. θ+ϕ θ+ϕ θ ϕ θ ϕ Consequently, you must have ( ) cos(θ+ϕ) −sin(θ+ϕ) A = =A A θ+ϕ sin(θ+ϕ) cos(θ+ϕ) θ ϕ ( )( ) cosθ −sinθ cosϕ −sinϕ = .
sinθ cosθ sinϕ cosϕ Therefore, ( ) ( ) cos(θ+ϕ) −sin(θ+ϕ) cosθcosϕ−sinθsinϕ −cosθsinϕ−sinθcosϕ = .
sin(θ+ϕ) cos(θ+ϕ) sinθcosϕ+cosθsinϕ cosθcosϕ−sinθsinϕ Don’t these look familiar?
They are the usual trig.
identities for the sum of two angles derived here using linear algebra concepts.
Example 9.3.21 Find the matrix of the linear transformation which rotates vectors in R3counterclockwise about the positive z axis.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation236 LINEAR TRANSFORMATIONS Let T be the name of this linear transformation.
In this case, Te = e ,Te = 3 3 1 (cosθ,sinθ,0)T ,andTe =(−sinθ,cosθ,0)T .Therefore,thematrixofthistransformation 2 is just   cosθ −sinθ 0   sinθ cosθ 0 (9.5) 0 0 1 In Physics it is important to consider the work done by a force ﬁeld on an object.
This involves the concept of projection onto a vector.
Suppose you want to ﬁnd the projection of a vector, v onto the given vector, u, denoted by proj (v) This is done using the dot u product as follows.
( ) v·u proj (v)= u u u·u Because of properties of the dot product, the map v→proj (v) is linear, u ( ) ( ) ( ) αv+βw·u v·u w·u proj (αv+βw) = u=α u+β u u u·u u·u u·u = αproj (v)+βproj (w).
u u Example 9.3.22 Let the projection map be deﬁned above and let u = (1,2,3)T .
Find the matrix of this linear transformation with respect to the usual basis.
You can ﬁnd this matrix in the same way as in earlier examples.
proj (e ) gives the ith u i column of the desired matrix.
Therefore, it is only necessary to ﬁnd ( ) e ·u proj (e )≡ i u u i u·u For the given vector in the example, this implies the columns of the desired matrix are       1 1 1 1   2   3   2 , 2 , 2 .
14 14 14 3 3 3 Hence the matrix is   1 2 3 1   2 4 6 .
14 3 6 9 Example 9.3.23 Find the matrix of the linear transformation which reﬂects all vectors in R3 through the xz plane.
Asillustratedabove,youjustneedtoﬁndTe whereT isthenameofthetransformation.
i But Te =e ,Te =e , and Te =−e so the matrix is 1 1 3 3 2 2   1 0 0  0 −1 0 .
0 0 1 Example 9.3.24 Find the matrix of the linear transformation which ﬁrst rotates counter clockwise about the positive z axis and then reﬂects through the xz plane.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 237 This linear transformation is just the composition of two linear transformations having matrices     cosθ −sinθ 0 1 0 0  sinθ cosθ 0 , 0 −1 0  0 0 1 0 0 1 respectively.
Thus the matrix desired is      1 0 0 cosθ −sinθ 0 cosθ −sinθ 0  0 −1 0  sinθ cosθ 0 = −sinθ −cosθ 0 .
0 0 1 0 0 1 0 0 1 9.3.2 Rotations About A Given Vector As an application, I will consider the problem of rotating counter clockwise about a given unit vector which is possibly not one of the unit vectors in coordinate directions.
First consider a pair of perpendicular unit vectors, u and u and the problem of rotating in the 1 2 counterclockwise direction about u where u = u ×u so that u ,u ,u forms a right 3 3 1 2 1 2 3 handed orthogonal coordinate system.
Thus the vector u is coming out of the page.
3 - θ θ u 1 (cid:9) u2R ?
Let T denote the desired rotation.
Then T (au +bu +cu )=aTu +bTu +cTu 1 2 3 1 2 3 =(acosθ−bsinθ)u +(asinθ+bcosθ)u +cu .
1 2 3 Thus in terms of the basis γ ≡{u ,u ,u }, the matrix of this transformation is 1 2 3   cosθ −sinθ 0 [T] ≡ sinθ cosθ 0 .
γ 0 0 1 Iwanttoobtainthematrixofthetransformationintermsoftheusualbasisβ ≡{e ,e ,e } 1 2 3 becauseitisintermsofthisbasisthatweusuallydealwithvectors.
FromProposition9.3.10, if [T] is this matrix, β   cosθ −sinθ 0   sinθ cosθ 0 0 0 1 ( ) ( ) −1 = u u u [T] u u u 1 2 3 β 1 2 3 and so you can solve for [T] if you know the u .
β i Recall why this is so.
R3 [T] R3 −−→γ q ↓ ◦ q ↓ γ γ R3 T R3 −−→ I ↑ ◦ I ↑ R3 [T] R3 −−→β Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation238 LINEAR TRANSFORMATIONS ( ) The map q is accomplished by a multiplication on the left by u u u .
Thus γ 1 2 3 ( ) ( ) [T] =q [T] q−1 = u u u [T] u u u −1.
β γ γ γ 1 2 3 γ 1 2 3 Suppose the unit vector u about which the counterclockwise rotation takes place is 3 (a,b,c).
ThenIobtainvectors,u andu suchthat{u ,u ,u }isarighthandedorthonor- 1 2 1 2 3 mal system with u = (a,b,c) and then use the above result.
It is of course somewhat 3 arbitrary how this is accomplished.
I will assume however, that |c|̸=1 since otherwise you are looking at either clockwise or counter clockwise rotation about the positive z axis and this is a problem which has been dealt with earlier.
(If c = −1, it amounts to clockwise rotation about the positive z axis while if c = 1, it is counter clockwise rotation about the positive z axis.)
Then let u = (a,b,c) and u ≡ √ 1 (b,−a,0).
This one is perpendicular to u .
If 3 2 a2+b2 3 {u ,u ,u } is to be a right hand system it is necessary to have 1 2 3 ( ) 1 u =u ×u = √ −ac,−bc,a2+b2 1 2 3 (a2+b2)(a2+b2+c2) Now recall that u is a unit vector and so the above equals 3 ( ) 1 √ −ac,−bc,a2+b2 (a2+b2) Then from the above, A is given by     √−ac √ b a   √−ac √ b a −1  (a2+b2) a2+b2  cosθ −sinθ 0  (a2+b2) a2+b2   √ −bc √−a b  sinθ cosθ 0  √ −bc √−a b  √(a2+b2) a2+b2 √(a2+b2) a2+b2 0 0 1 a2+b2 0 c a2+b2 0 c Of course the matrix is an orthogonal matrix so it is easy to take the inverse by simply taking the transpose.
Then doing the computation and then some simpliﬁcation yields  ( )  a2+ 1−a2 cosθ ab(1−(cosθ)−) csinθ ac(1−cosθ)+bsinθ = ab(1−cosθ)+csinθ b2+ 1−b2 cosθ bc(1−(cosθ)−) asinθ .
(9.6) ac(1−cosθ)−bsinθ bc(1−cosθ)+asinθ c2+ 1−c2 cosθ With this, it is clear how to rotate clockwise about the unit vector, (a,b,c).
Just rotate counterclockwisethroughanangleof−θ.Thusthematrixforthisclockwiserotationisjust  ( )  a2+ 1−a2 cosθ ab(1−(cosθ)+) csinθ ac(1−cosθ)−bsinθ = ab(1−cosθ)−csinθ b2+ 1−b2 cosθ bc(1−(cosθ)+) asinθ .
ac(1−cosθ)+bsinθ bc(1−cosθ)−asinθ c2+ 1−c2 cosθ In deriving (9.6) it was assumed that c ̸= ±1 but even in this case, it gives the correct answer.
Suppose for example that c = 1 so you are rotating in the counter clockwise direction about the positive z axis.
Then a,b are both equal to zero and (9.6) reduces to (9.5).
9.3.3 The Euler Angles An important application of the above theory is to the Euler angles, important in the mechanicsofrotatingbodies.
Lagrangestudiedthesethingsbackinthe1700’s.
Todescribe Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.3.
THE MATRIX OF A LINEAR TRANSFORMATION 239 theEuleranglesconsiderthefollowingpictureinwhichx ,x andx aretheusualcoordinate 1 2 3 axes ﬁxed in space and the axes labeled with a superscript denote other coordinate axes.
Here is the picture.
x1 3 x2 3 x =x1 θ x2 =x3 3 3 3 3 x2 2 θ x1 x1 2 x3 2 2 ϕ ψ x x2 2 2 ϕ x1 =x2 ψ 1 1 x x2 1 1 x1 x3 1 1 We obtain ϕ by rotating counter clockwise about the ﬁxed x axis.
Thus this rotation 3 has the matrix   cosϕ −sinϕ 0  sinϕ cosϕ 0 ≡M (ϕ) 1 0 0 1 Nextrotatecounterclockwiseaboutthex1 axiswhichresultsfromtheﬁrstrotationthrough 1 an angle of θ.
Thus it is desired to rotate counter clockwise through an angle θ about the unit vector      cosϕ −sinϕ 0 1 cosϕ      sinϕ cosϕ 0 0 = sinϕ .
0 0 1 0 0 Therefore, in (9.6), a=cosϕ,b=sinϕ, and c=0.
It follows the matrix of this transforma- tion with respect to the usual basis is   cos2ϕ+sin2ϕcosθ cosϕsinϕ(1−cosθ) sinϕsinθ  cosϕsinϕ(1−cosθ) sin2ϕ+cos2ϕcosθ −cosϕsinθ ≡M (ϕ,θ) 2 −sinϕsinθ cosϕsinθ cosθ Finally, we rotate counter clockwise about the positive x2 axis by ψ.
The vector in the 3 positive x1 axis is the same as the vector in the ﬁxed x axis.
Thus the unit vector in the 3 3 positive direction of the x2 axis is 3    cos2ϕ+sin2ϕcosθ cosϕsinϕ(1−cosθ) sinϕsinθ 1  cosϕsinϕ(1−cosθ) sin2ϕ+cos2ϕcosθ −cosϕsinθ  0  −sinϕsinθ cosϕsinθ cosθ 0     cos2ϕ+sin2ϕcosθ cos2ϕ+sin2ϕcosθ =  cosϕsinϕ(1−cosθ) = cosϕsinϕ(1−cosθ)  −sinϕsinθ −sinϕsinθ and it is desired to rotate counter clockwise through an angle of ψ about this vector.
Thus, in this case, a=cos2ϕ+sin2ϕcosθ,b=cosϕsinϕ(1−cosθ),c=−sinϕsinθ.
andyoucouldsubstituteintotheformulaofTheorem(9.6)andobtainamatrixwhichrep- resents the linear transformation obtained by rotating counter clockwise about the positive x2 axis,M (ϕ,θ,ψ).Thenwhatwouldbethematrixwithrespecttotheusualbasisforthe 3 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation240 LINEAR TRANSFORMATIONS linear transformation which is obtained as a composition of the three just described?
By Theorem 9.3.17, this matrix equals the product of these three, M (ϕ,θ,ψ)M (ϕ,θ)M (ϕ).
3 2 1 I leave the details to you.
There are procedures due to Lagrange which will allow you to write diﬀerential equations for the Euler angles in a rotating body.
To give an idea how these angles apply, consider the following picture.
x 3 x (t) 3 R ψ θ x 2 ϕ x 1 line of nodes This is as far as I will go on this topic.
The point is, it is possible to give a systematic description in terms of matrix multiplication of a very elaborate geometrical description of a composition of linear transformations.
You see from the picture it is possible to describe the motion of the spinning top shown in terms of these Euler angles.
9.4 Eigenvalues And Eigenvectors Of Linear Transfor- mations LetV beaﬁnitedimensionalvectorspace.
Forexample,itcouldbeasubspaceofCnorRn.
Also suppose A∈L(V,V).
Deﬁnition 9.4.1 The characteristic polynomial of A is deﬁned as q(λ) ≡ det(λI−A).
The zeros of q(λ) in C are called the eigenvalues of A. Lemma 9.4.2 When λ is an eigenvalue of A which is also in F, the ﬁeld of scalars, then there exists v ̸=0 such that Av =λv.
Proof: This follows from Theorem 9.3.16.
Since λ∈F, λI−A∈L(V,V) and since it has zero determinant, it is not one to one.
(cid:4) The following lemma gives the existence of something called the minimal polynomial.
Lemma 9.4.3 Let A∈L(V,V) where V is a ﬁnite dimensional vector space of dimension n with arbitrary ﬁeld of scalars.
Then there exists a unique polynomial of the form p(λ)=λm+cm−1λm−1+···+c1λ+c0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.4.
EIGENVALUES AND EIGENVECTORS OF LINEAR TRANSFORMATIONS 241 such that p(A)=0 and m is as small as possible for this to occur.
Proof: Considerthelineartransformations,I,A,A2,··· ,An2.Therearen2+1ofthese transformations and so by Theorem 9.2.3 the set is linearly dependent.
Thus there exist constants, c ∈F such that i ∑n2 c I+ c Ak =0.
0 k k=1 This implies there exists a polynomial, q(λ) which has the property that q(A)=0.
In fact, ∑ one example is q(λ) ≡ c + n2 c λk.
Dividing by the leading term, it can be assumed 0 k=1 k this polynomial is of the form λm+cm−1λm−1+···+c1λ+c0, a monic polynomial.
Now consider all such monic polynomials, q such that q(A)=0 and pick the one which has the smallestdegreem.
Thisiscalledtheminimalpolynomialandwillbedenotedherebyp(λ).
If there were two minimal polynomials, the one just found and another, λm+dm−1λm−1+···+d1λ+d0.
Then subtracting these would give the following polynomial, q′(λ)=(dm−1−cm−1)λm−1+···+(d1−c1)λ+d0−c0 Since q′(A) = 0, this requires each d = c since otherwise you could divide by d −c k k k k wherek isthelargestonewhichisnonzero.
Thusthechoiceofmwouldbecontradicted.
(cid:4) Theorem 9.4.4 Let V be a nonzero ﬁnite dimensional vector space of dimension n with the ﬁeld of scalars equal to F. Suppose A ∈ L(V,V) and for p(λ) the minimal polynomial deﬁned above, let µ ∈ F be a zero of this polynomial.
Then there exists v ̸= 0,v ∈ V such that Av =µv.
If F=C, then A always has an eigenvector and eigenvalue.
Furthermore, if {λ ,··· ,λ } 1 m are the zeros of p(λ) in F, these are exactly the eigenvalues of A for which there exists an eigenvector in V. Proof: Suppose ﬁrst µ is a zero of p(λ).
Since p(µ)=0, it follows p(λ)=(λ−µ)k(λ) where k(λ) is a polynomial having coeﬃcients in F. Since p has minimal degree, k(A)̸=0 and so there exists a vector, u̸=0 such that k(A)u≡v ̸=0.
But then (A−µI)v =(A−µI)k(A)(u)=0.
The next claim about the existence of an eigenvalue follows from the fundamental theo- rem of algebra and what was just shown.
It has been shown that every zero of p(λ) is an eigenvalue which has an eigenvector in V. NowsupposeµisaneigenvaluewhichhasaneigenvectorinV sothatAv =µv forsome v ∈V,v ̸=0.
Does it follow µ is a zero of p(λ)?
0=p(A)v =p(µ)v and so µ is indeed a zero of p(λ).
(cid:4) In summary, the theorem says that the eigenvalues which have eigenvectors in V are exactly the zeros of the minimal polynomial which are in the ﬁeld of scalars F. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation242 LINEAR TRANSFORMATIONS 9.5 Exercises 1.
If A,B, and C are each n×n matrices and ABC is invertible, why are each of A,B, and C invertible?
2.
Give an example of a 3×2 matrix with the property that the linear transformation determined by this matrix is one to one but not onto.
3.
Explain why Ax=0 always has a solution whenever A is a linear transformation.
4. Review problem: Suppose det(A−λI) = 0.
Show using Theorem 3.1.15 there exists x̸=0 such that (A−λI)x=0.
5.
How does the minimal polynomial of an algebraic number relate to the minimal poly- nomial of a linear transformation?
Can an algebraic number be thought of as a linear transformation?
How?
6.
Recall the fact from algebra that if p(λ) and q(λ) are polynomials, then there exists l(λ), a polynomial such that q(λ)=p(λ)l(λ)+r(λ) where the degree of r(λ) is less than the degree of p(λ) or else r(λ)=0.
With this in mind, why mustthe minimal polynomial always divide the characteristic polynomial?
That is, why does there always exist a polynomial l(λ) such that p(λ)l(λ) = q(λ)?
Canyougiveconditionswhichimplytheminimalpolynomialequalsthecharacteristic polynomial?
Go ahead and use the Cayley Hamilton theorem.
7.
In the following examples, a linear transformation, T is given by specifying its action on a basis β.
Find its matrix with respect to this basis.
( ) ( ) ( ) ( ) ( ) 1 1 −1 −1 −1 (a) T =2 +1 ,T = 2 2 1 1 1 ( ) ( ) ( ) ( ) ( ) 0 0 −1 −1 0 (b) T =2 +1 ,T = 1 1 1 1 1 ( ) ( ) ( ) ( ) ( ) ( ) 1 1 1 1 1 1 (c) T =2 +1 ,T =1 − 0 2 0 2 0 2 8.
Let β ={u ,··· ,u } be a basis for Fn and let T :Fn →Fn be deﬁned as follows.
1 n ( ) ∑n ∑n T a u = a b u k k k k k k=1 k=1 First show that T is a linear transformation.
Next show that the matrix of T with respect to this basis, [T] is β   b 1    ...  b n ShowthattheabovedeﬁnitionisequivalenttosimplyspecifyingT onthebasisvectors of β by T (u )=b u .
k k k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation9.5.
EXERCISES 243 9.
↑In the situation of the above problem, let γ ={e ,··· ,e } be the standard basis for 1 n Fn wheree isthevectorwhichhas1inthekth entryandzeroselsewhere.
Showthat k [T] = γ ( ) ( ) u ··· u [T] u ··· u −1 (9.7) 1 n β 1 n 10.
↑Generalize the above problem to the situation where T is given by specifying its action on the vectors of a basis β ={u ,··· ,u } as follows.
1 n ∑n Tu = a u .
k jk j j=1 LettingA=(a ),verifythatforγ ={e ,··· ,e },(9.7)stillholdsandthat[T] =A.
ij 1 n β 11.
Let P denote the set of real polynomials of degree no more than 3, deﬁned on an 3 interval [a,b].
Show that P3 is a subspace of th{e vector spa}ce of all functions deﬁned on this interval.
Show that a basis for P is 1,x,x2,x3 .
Now let D denote the 3 diﬀerentiation operator which sends a function to its derivative.
Show D is a linear transformation which sends P to P .
Find the matrix of this linear transformation 3 3 with respect to the given basis.
12.
Generalize the above problem to P , the space of polynomials of degree no more than n n with basis {1,x,··· ,xn}.
13.
In the situation of the above problem, let the linear transformation be T = D2 +1, deﬁned as Tf =f′′+f.
Find the matrix of this linear transformation with respect to the given basis {1,x,··· ,xn}.
Write it down for n=4.
14.
In calculus, the following situation is encountered.
There exists a vector valued func- tion f :U → Rm where U is an open subset of Rn.
Such a function is said to have a derivative or to be diﬀerentiable at x ∈ U if there exists a linear transformation T :Rn →Rm such that |f(x+v)−f(x)−Tv| lim =0.
v→0 |v| First show that this linear transformation, if it exists, must be unique.
Next show that for β ={e ,··· ,e },, the standard basis, the kth column of [T] is 1 n β ∂f (x).
∂x k Actually, the result of this problem is a well kept secret.
People typically don’t see this in calculus.
It is seen for the ﬁrst time in advanced calculus if then.
15.
Recall that A is similar to B if there exists a matrix P such that A=P−1BP.
Show that if A and B are similar, then they have the same determinant.
Give an example of two matrices which are not similar but have the same determinant.
16.
Suppose A∈L(V,W) where dim(V)>dim(W).
Show ker(A)̸={0}.
That is, show there exist nonzero vectors v∈V such that Av=0.
17.
A vector v is in the convex hull of a nonempty set S if there are ﬁnitely many vectors of S,{v ,··· ,v } and nonnegative scalars {t ,··· ,t } such that 1 m 1 m ∑m ∑m v= t v , t =1.
k k k k=1 k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation244 LINEAR TRANSFORMATIONS Such a linear combination is called a convex com∑bination.
Suppose now that S ⊆ V, a vector space of dimension n. Show that if v= m t v is a vector in the convex k=1 k k hull for m>n+1, then there exist other scalars {t′} such that k m∑−1 ′ v= t v .
k k k=1 Thus every vector in the convex hull of S can be obtained as a convex combination of at most n+1 points of S. This incredible result is in Rudin [23].
Hint: Consider L:Rm →V ×R deﬁned by ( ) ∑m ∑m L(a)≡ a v , a k k k k=1 k=1 Explain why ker(L) ̸= {0}.
Next, letting a ∈ ker(L)\{0} and λ ∈ R, note that λa∈ker(L).
Thus for all λ∈R, ∑m v= (t +λa )v .
k k k k=1 Now vary λ till some t +λa =0 for some a ̸=0.
k k k 18.
For those who know about compactness, use Problem 17 to show that if S ⊆Rn and S is compact, then so is its convex hull.
19.
Suppose Ax=b has a solution.
Explain why the solution is unique precisely when Ax=0 has only the trivial (zero) solution.
20.
Let A be an n×n matrix of elements of F. There are two cases.
In the ﬁrst case, F contains a splitting ﬁeld of p (λ) so that p(λ) factors into a product of linear A polynomials having coeﬃcients in F. It is the second case which is of interest here where p (λ) does not factor into linear factors having coeﬃcients in F. Let G be a A splitting ﬁeld of p (λ) and let q (λ) be the minimal polynomial of A with respect A A to the ﬁeld G. Explain why q (λ) must divide p (λ).
Now why must q (λ) factor A A A completely into linear factors?
21.
In Lemma 9.2.2 verify that L is linear.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationLinear Transformations Canonical Forms 10.1 A Theorem Of Sylvester, Direct Sums The notation is deﬁned as follows.
Deﬁnition 10.1.1 Let L∈L(V,W).
Then ker(L)≡{v ∈V :Lv =0}.
Lemma 10.1.2 Whenever L∈L(V,W), ker(L) is a subspace.
Proof: If a,b are scalars and v,w are in ker(L), then L(av+bw)=aL(v)+bL(w)=0+0=0 (cid:4) Suppose now that A ∈ L(V,W) and B ∈ L(W,U) where V,W,U are all ﬁnite dimen- sional vector spaces.
Then it is interesting to consider ker(BA).
The following theorem of Sylvester is a very useful and important result.
Theorem 10.1.3 Let A ∈ L(V,W) and B ∈ L(W,U) where V,W,U are all vector spaces over a ﬁeld F. Suppose also that ker(A) and A(ker(BA)) are ﬁnite dimensional subspaces.
Then dim(ker(BA))≤dim(ker(B))+dim(ker(A)).
Proof:Ifx∈ker(BA),thenAx∈ker(B)andsoA(ker(BA))⊆ker(B).Thefollowing picture may help.
ker(BA) ker(B) ker(A) A - A(ker(BA)) Now let {x1,··· ,xn} be a basis of ker(A) a∑nd let {Ay1,··· ,Aym} be a basis for A(ker(BA)).
Take any z ∈ker(BA).
Then Az = m a Ay and so i=1 i i ( ) ∑m A z− a y =0 i i i=1 ∑ which means z− m a y ∈ker(A) and so there are scalars b such that i=1 i i i ∑m ∑n z− a y = b x .
i i i i i=1 j=1 245 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation246 LINEAR TRANSFORMATIONS CANONICAL FORMS Itfollowsspan(x ,··· ,x ,y ,··· ,y )⊇ker(BA)andsobytheﬁrstpart,(Seethepicture.)
1 n 1 m dim(ker(BA))≤n+m≤dim(ker(A))+dim(ker(B)) (cid:4) Of course this result holds for any ﬁnite product of linear transformations by induc- tion.
One way t∏his is quite useful is in the case where you have a ﬁnite product of linear transformations l L all in L(V,V).
Then i=1 i ( ) ∏l ∑l dim ker L ≤ dim(kerL ) i i i=1 i=1 ( ) ∏ and so if you can ﬁnd a linearly independent set of vectors in ker l L of size i=1 i ∑l dim(kerL ), i i=1 ( ) ∏ then it must be a basis for ker l L .
This is discussed below.
i=1 i Deﬁnition 10.1.4 Let {V }r be subspaces of V. Then i i=1 ∑r V i i=1 ∑ denotes all sums of the form r v where v ∈V .
If whenever i=1 i i i ∑r v =0,v ∈V , (10.1) i i i i=1 ∑ it follows that v = 0 for each i, then a special notation is used to denote r V .
This i i=1 i notation is V ⊕···⊕V , 1 r and it is called a direct sum of subspaces.
{ } Lemma 10.1.5 If V = V ⊕···⊕V and if β = vi,··· ,vi is a basis for V , then a basis for V is {β ,··· ,β }1. r i 1 mi i 1 r ∑ ∑ Proof: Suppose r mi c vi =0.thensinceitisadirectsum, itfollowsforeachi, i=1 j=1 ij j ∑mi c vi =0 ij j j=1 { } and now since vi,··· ,vi is a basis, each c =0.
(cid:4) 1 mi ij Here is a useful lemma.
Lemma 10.1.6 Let L be in L(V,V) and suppose for i ̸= j,L L = L L and also L is i i j j i i one to one on ker(L ) whenever i̸=j.
Then j ( ) ∏p ker L =ker(L )⊕+···+⊕ker(L ) i 1 p i=1 ∏ ∏ Here p L is the product of all the linear transformations.
A symbol like L is the i=1 i j̸=i j product of all of them but L .
i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.1.
A THEOREM OF SYLVESTER, DIRECT SUMS 247 Proof: Note that since the operators commute, L :ker(L )→ker(L ).
Here is why.
If j i i L y =0 so that y ∈ker(L ), then i i L L y =L L y =L 0=0 i j j i j and so L :ker(L )→ker(L ).
Suppose j i i ∑p v =0, v ∈ker(L ), i i i i=1 ∏ butsomev ̸=0.Thendo L tobothsides.
Sincethelineartransformationscommute, i j̸=i j this results in ∏ L (v )=0 j i j̸=i which contradicts the assumption that these L are one to one and the observation that j they map ker(L ) to ker(L ).
Thus if i i ∑ v =0, v ∈ker(L ) i i i i then each vi =0.
{ } Suppose βi ={ v1i,··· ,v}mi i is a basis for ker(Li).
Then from what was just shown and Lemma 10.1.5, β ,··· ,β must be linearly independent and a basis for 1 p ker(L )⊕+···+⊕ker(L ).
1 p It is also clear that since these operators commute, ( ) ∏p ker(L )⊕+···+⊕ker(L )⊆ker L 1 p i i=1 Therefore, by Sylvester’s theorem and the above, ( ( )) ∏p ∑p dim ker L ≤ dim(ker(L )) i j i=1 j=1 ( ( )) ∏p =dim(ker(L )⊕+···+⊕ker(L ))≤dim ker L .
1 p i i=1 Now in general, if W is a subspace of V, a ﬁnite dimensional vector space and the two have thesamedimension, then W =V.
This isbecause W hasabasis andif v isnotin thespan ofthisbasis,thenv adjoinedtothebasisofW wouldbealinearlyindependentset,yielding a linearly independent set which has more vectors in it than a basis, a contradiction.
It follows that ( ) ∏p ker(L )⊕+···+⊕ker(L )=ker L (cid:4) 1 p i i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation248 LINEAR TRANSFORMATIONS CANONICAL FORMS 10.2 Direct Sums, Block Diagonal Matrices Let V be a ﬁnite dimensional vector space with ﬁeld of scalars F. Here I will make no assumption on F. Also suppose A∈L(V,V).
Recall Lemma 9.4.3 which gives the existence of the minimal polynomial for a linear transformation A.
This is the monic polynomial p which has smallest possible degree such that p(A)=0.
It is stated again for convenience.
Lemma 10.2.1 Let A∈L(V,V) where V is a ﬁnite dimensional vectorspaceof dimension n with ﬁeld of scalars F. Then there exists a unique monic polynomial of the form p(λ)=λm+cm−1λm−1+···+c1λ+c0 such that p(A)=0 and m is as small as possible for this to occur.
Now here is a useful lemma which will be used below.
Lemma 10.2.2 Let L ∈ L(V,V) where V is an n dimensional vector space.
Then if L is one to one, it follows that L is also onto.
In fact, if {v ,··· ,v } is a basis, then so is 1 n {Lv ,··· ,Lv }.
1 n Proof: Let {v ,··· ,v } be a basis for V. Then I claim that {Lv ,··· ,Lv } is also a 1 n 1 n basis for V. First of all, I show {Lv ,··· ,Lv } is linearly independent.
Suppose 1 n ∑n c Lv =0.
k k k=1 Then ( ) ∑n L c v =0 k k k=1 and since L is one to one, it follows ∑n c v =0 k k k=1 which implies each c = 0.
Therefore, {Lv ,··· ,Lv } is linearly independent.
If there k 1 n exists w not in the span of these vectors, then by Lemma 8.2.10, {Lv ,··· ,Lv ,w} would 1 n beindependentandthiscontradictstheexchangetheorem, Theorem8.2.4becauseitwould be a linearly independent set having more vectors than the spanning set {v ,··· ,v }.
(cid:4) 1 n Now it is time to consider the notion of a direct sum of subspaces.
Recall you can always assert the existence of a factorization of the minimal polynomial into a product of irreducible polynomials.
This fact will now be used to show how to obtain such a direct sum of subspaces.
Deﬁnition 10.2.3 For A ∈ L(V,V) where dim(V) = n, suppose the minimal polynomial is ∏q p(λ)= (ϕ (λ))rk k k=1 where the polynomials ϕ have coeﬃcients in F and are irreducible.
Now deﬁne the gener- k alized eigenspaces V ≡ker((ϕ (A))rk) k k Note that if one of these polynomials (ϕ (λ))rk is a monic linear polynomial, then the gen- k eralized eigenspace would be an eigenspace.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.2.
DIRECT SUMS, BLOCK DIAGONAL MATRICES 249 Theorem 10.2.4 In the context of Deﬁnition 10.2.3, V =V ⊕···⊕V (10.2) 1 q and each{Vk is A inv}ariant, meaning A(Vk)⊆{Vk.
ϕl(A) is o}ne to one on each Vk for k ̸=l.
If β = vi,··· ,vi is a basis for V , then β ,β ,··· ,β is a basis for V. i 1 mi i 1 2 q Proof: It is clear V is a subspace which is A invariant because A commutes with k ϕ (A)mk.
It is clear the operators ϕ (A)rk commute.
Thus if v ∈V , k k k ϕ (A)rkϕ (A)rlv =ϕ (A)rlϕ (A)rkv =ϕ (A)rl0=0 k l l k l and so ϕ (A)rl :V →V .
l k k I claim ϕ (A) is one to one on V whenever k ̸= l. The two polynomials ϕ (λ) and l k l ϕ (λ)rk are relatively prime so there exist polynomials m(λ),n(λ) such that k m(λ)ϕ (λ)+n(λ)ϕ (λ)rk =1 l k It follows that the sum of all coeﬃcients of λ raised to a positive power are zero and the constant term on the left is 1.
Therefore, using the convention A0 =I it follows m(A)ϕ (A)+n(A)ϕ (A)rk =I l k If v ∈V , then from the above, k m(A)ϕ (A)v+n(A)ϕ (A)rkv =v l k Since v is in V , it follows by deﬁnition, k m(A)ϕ (A)v =v l and so ϕl(A)v ̸= 0 unless v = 0.
Thus ϕl(A) and ∏hence ϕl(A)rl is one to one on Vk for everyk ̸=l.
ByLemma10.1.6andthefactthatker( q ϕ (λ)rk)=V,(10.2)isobtained.
k=1 k The claim about the bases follows from Lemma 10.1.5.
(cid:4) You could consider the restriction of A to V .
It turns out that this restriction has k minimal polynomial equal to ϕ (λ)mk.
k ∏ Corollary 10.2.5 Let the minimal polynomial of A be p(λ) = q ϕ (λ)mk where each k=1 k ϕ is irreducible.
Let V =ker(ϕ(A)mk).
Then k k V ⊕···⊕V =V 1 q and letting A denote the restriction of A to V , it follows the minimal polynomial of A is k k k ϕ (λ)mk.
k ∏ Proof: Recall the direct sum, V1⊕···⊕Vq =V where Vk =ker(ϕk(A)mk) for p(λ)= q ϕ (λ)mk theminimalpolynomialforAwheretheϕ (λ)areallirreducible.
Thuseach k=1 k k V is invariant with respect to A.
What is the minimal polynomial of A , the restriction of k k A to V ?
First note that ϕ (A )mk(V ) = {0} by deﬁnition.
Thus if η(λ) is the minimal k k k k polynomial for A then it must divide ϕ (λ)mk and so by Corollary 8.3.11 η(λ)=ϕ (λ)rk k k k where r ≤ m .
Could r < m ?
No, this is not possible because then p(λ) would fail k k k k to be the minimal polynomial for A.
You could substitute for the term ϕ (λ)mk in the k factorization of p(λ) with ϕ (λ)rk and the resulting polynomial p′ would satisfy p′(A)=0.
k Here is why.
From Theorem 10.2.4, a typical x∈V is of the form ∑q v , v ∈V i i i i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation250 LINEAR TRANSFORMATIONS CANONICAL FORMS Then since all the factors commute, ( ) ( ) ∑q ∏q ∑q p′(A) v = ϕ (A)miϕ (A)rk v i i k i i=1 i̸=k i=1 For j ̸=k ∏q ∏q ϕ (A)miϕ (A)rkv = ϕ (A)miϕ (A)rkϕ (A)mjv =0 i k j i k j j i̸=k i̸=k,j If j =k, ∏q ϕ (A)miϕ (A)rkv =0 i k k i̸=k whichshowsp′(λ)isamonicpolynomialhavingsmallerdegreethanp(λ)suchthatp′(A)= 0.
Thus the minimal polynomial for A is ϕ (λ)mk as claimed.
(cid:4) k k How does Theorem 10.2.4 relate to matrices?
Theorem 10.2.6 Suppose V is a vector space with ﬁeld of scalars F and A ∈ L(V,V).
Suppose also V =V ⊕···⊕V 1 q where each V is A invariant.
(AV ⊆ V ) Also let β be a basis for V and let A denote k k k k k k the restriction of A to Vk.
Letting Mk denote the{matrix of }Ak with respect to this basis, it follows the matrix of A with respect to the basis β ,··· ,β is 1 q   M1 0    ...  0 Mq Proof: Recall the matrix M of a linear transformation A is deﬁned such that the following diagram commutes.
A {v ,··· ,v } V → V {v ,··· ,v } 1 n 1 n q ↑ ◦ ↑q Fn → Fn M where ∑n q(x)≡ x v i i i=1 and {v1,··· ,vn} is a basis for V. Now when V =V1⊕···⊕Vq each {Vk being inva}riant with respect to the linear transformation A, and β a basis for V , β = vk,··· ,vk , one can k k k 1 mk considerthematrixMk ofA takenwithrespecttothebasisβ whereA istherestriction k k k of A to V .
Then the claim of the theorem is true because if M is given as described it k causes the diagram to commute.
To see this, let x∈Fmk.
     M1 0 0 0  ...  ...   ...  ∑ q Mk ...  x... =q M...kx ≡ ij Mikjxjvik 0 Mq 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.3.
CYCLIC SETS 251 while   0  .
  .
  .
 ∑ ∑ ∑ ∑ Aq x.
≡A xjvjk = xjAkvjk = xj Mikjvik  .
 j j j i .
0 ∑ because, asdiscussedearlier, Avk = Mkvk becauseMk isthematrixofA withrespect j i ij i k to the basis β .
(cid:4) k An examination of the proof of the above theorem yields the following corollary.
Corollary 10.2.7 If any β in the above consists of eigenvectors, then Mk is a diagonal k matrix having the corresponding eigenvalues down the diagonal.
It follows that it would be interesting to consider special bases for the vector spaces in the direct sum.
This leads to the Jordan form or more generally other canonical forms such as the rational canonical form.
10.3 Cyclic Sets It was shown above that for A ∈ L(V,V) for V a ﬁnite dimensional vector space over the ﬁeld of scalars F, there exists a direct sum decomposition V =V ⊕···⊕V 1 q where V =ker(ϕ (A)mk) k k and ϕ (λ) is an irreducible polynomial.
Here the minimal polynomial of A was k ∏q ϕ (λ)mk k k=1 Next I will consider the problem of ﬁnding a basis for V such that the matrix of A k restricted to V assumes various forms.
k { } Deﬁnition 10.3.1 Letting x̸=0 denote b(y βx the vector)s x,Ax,A2x,··· ,Am−1x where m is the smallest such that Amx ∈ span x,··· ,Am−1x .
This is called an A cyclic set.
The vectors which result are also called a Krylov sequence.
The following is the main idea.
To help organize the ideas in this lemma, here is a diagram.
ker(ϕ(A)m) U ⊆ ker(ϕ(A)) W v ,...,v 1 s x ,x ,...,x 1 2 p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation252 LINEAR TRANSFORMATIONS CANONICAL FORMS Lemma 10.3.2 Let W be an A invariant (AW ⊆W) subspace of ker(ϕ(A)m) for m a positive integer where ϕ(λ) is an irreducible monic polynomial of degree d. Then if η(λ) is a monic polynomial of smallest degree such that for x∈ker(ϕ(A)m)\{0}, η(A)x=0, then η(λ)=ϕ(λ)k for some positive integer k. Thus if r is the degree of η, then r =kd.
Also, for a cyclic set, { } β ≡ x,Ax,··· ,Ar−1x x is l{inearly independen}t. Recall that r is the smallest such that Arx is a linear combination of x,Ax,··· ,Ar−1x .
Now let U be an A invariant subspace of ker(ϕ(A)).
If {v ,··· ,v } is a basis for W then if x∈U \W, 1 s {v ,··· ,v ,β } 1 s x is linearly independent.
There exist vectors x ,··· ,x each in U such that 1 p { } v ,··· ,v ,β ,··· ,β 1 s x1 xp is a basis for U +W.
Proof: Consider the ﬁrst claim.
If η(A)x=0, then writing ϕ(λ)m =η(λ)g(λ)+r(λ) where either r(λ)=0 or the degree of r(λ) is less than that of η(λ), the latter possibility cannot occur because if it did, r(A)x=0 and this would contradict the deﬁnition of η(λ).
Therefore r(λ)=0 and so η(λ) divides ϕ(λ)m. From Corollary 8.3.11, η(λ)=ϕ(λ)k for some integer, k ≤ m. Since x ̸= 0, it follows k > 0.
In particular, the degree of η(λ) equals kd.
Now consider x ̸= 0,x ∈ ker(ϕ(A)m){and the vectors βx.
D}o these vectors yield a linearly independent set?
The vectors are x,Ax,A2x,··· ,Ar−1x where Arx is in ( ) span x,Ax,A2x,··· ,Ar−1x and r is as small as possible for this to happen.
Suppose then that there are scalars d , not j all zero such that ∑r−1 d Ajx=0, x̸=0.
(10.3) j j=0 Supposemisthelargestnonzeroscalarintheabovelinearcombination.
d ̸=0,m≤r−1.
m Then Amx is a linear combination of the preceeding vectors in the list, which contradicts the deﬁnition of r. Thus from the ﬁrst part, r =kd for some positive integer k. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.3.
CYCLIC SETS 253 Since β is independent for each x̸=0, it follows that whenever x̸=0, x { } x,Ax,A2x,··· ,Ad−1x is linearly independent because, the length of β is a multiple of d and is therefore, at least x as long as the above list.
However, if x ∈ ker(ϕ(A)), then β is equal to the above list.
x This is because ϕ(λ) is of degree d so β is no longer than the above list.
However, from x the ﬁrst part β has length equal to kd for some integer k so it is at least as long.
x Suppose now x∈U \W where U ⊆ker(ϕ(A)).
Consider { } v ,··· ,v ,x,Ax,A2x,··· ,Ad−1x .
1 s Is this set of vectors independent?
First note that ( ) span x,Ax,A2x,··· ,Ad−1x { } is A invariant because, from what was just shown, x,Ax,A2x,··· ,Ad−1x = β and so x Adx is a linear combination of the other vectors in the above list.
Suppose now that ∑s ∑d a v + d Aj−1x=0.
i i j i=1 j=1 ∑ ( ) Ifz ≡ d d Aj−1x,thenz ∈W∩span x,Ax,··· ,Ad−1x .Thenalsoforeachm≤d−1, j=1 j ( ) Amz ∈W ∩span x,Ax,··· ,Ad−1x ( ) because W,span x,Ax,··· ,Ad−1x are A invariant, and so ( ) ( ) span z,Az,··· ,Ad−1z ⊆ W ∩span x,Ax,··· ,Ad−1x ( ) ⊆ span x,Ax,··· ,Ad−1x (10.4) { } Suppose z ̸= 0.
Then from the above, z,Az,··· ,Ad−1z must be linearly independent.
Therefore, ( ( )) ( ( )) d=dim span z,Az,··· ,Ad−1z ≤dim W ∩span x,Ax,··· ,Ad−1x ( ( )) ≤dim span x,Ax,··· ,Ad−1x =d Thus ( ) ( ) span z,Az,··· ,Ad−1z ⊆span x,Ax,··· ,Ad−1x and both have the same dimension and so the two sets are equal.
It follows from (10.4) ( ) ( ) W ∩span x,Ax,··· ,Ad−1x =span x,Ax,··· ,Ad−1x which would require x∈W but this is assumed not to take place.
Hence z =0 and so the lin{earlyindependence}ofthe{v1,··· ,vs}implieseachai =0.Thenthelinearindependence of x,{Ax,··· ,Ad−1x whichfollow}sfrom the ﬁrst part of the argument showseachdj =0.
Thus v ,··· ,v ,x,Ax,··· ,Ad−1x is linearly independent as claimed.
1 s Let x ∈ U \W ⊆ ker(ϕ(A){).
Then it was just sho}wn that {v1,··· ,vs,βx} is linearly independent.
Recall that β = x,Ax,A2x,··· ,Ad−1x because x∈ker(ϕ(A)).
Also, if x ( ) y ∈span v ,··· ,v ,x,Ax,A2x,··· ,Ad−1x ≡W 1 s 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation254 LINEAR TRANSFORMATIONS CANONICAL FORMS ∑ then Ay ∈ W also.
This is because W is A invariant, and if you take A d−1a Aix, It 1 i=0 i must remain in ( ) span x,Ax,A2x,··· ,Ad−1x because Adx is in the above span, due to the assumption that ϕ(A)x = 0.
If W equals 1 U +W, then you are done.
If not, let W(1 play the role of W an)d pick x1 ∈ U \W1 and repeat the argument.
Continue till span v ,··· ,v ,β ,··· ,β = U +W.
The process stops because ker(ϕ(A)m) is ﬁnite dimensio1nal.
(cid:4)s x1 xn Now here is a simple lemma.
Lemma 10.3.3 Let V be a vector space and let B ∈L(V,V).
Then V =B(V)⊕ker(B) Proof: Let {Bv ,··· ,Bv } be a basis for B(V).
Now let {w ,··· ,w } be a basis for 1 r 1 s ker(B).
Then if v ∈V, there exist unique scalars c such that i ∑r Bv = c Bv i i i=1 ∑ and so B(v− r c v )=0 and so there exist unique scalars d such that i=1 i i i ∑r ∑s v− c v = d w .
i i j j i=1 j=1 Itremainstoverifythat{v ,··· ,v ,w ,··· ,w }islinearlyindependent.
Supposethenthat 1 r 1 s ∑ ∑ a v + b w =0 i i j j i j ∑ DoB tobothsides.
Thisyields a Bv =0andbyassumption, thisrequireseacha =0.
i i i i Then independence of the w yields each b =0.
(cid:4) i j With this preparation, here is the main result about a basis for ker(ϕ(A)m) for ϕ(λ) irreducible.
Formore on this theorem, including extra details, see [14].
See also Exercise 27 on Page 266.. Theorem 10.3.4 Let V = ker(ϕ(A)m) for m a positive integer and A ∈ L(Z,Z) where Z is some vector space containing V, and ϕ(λ) is an irreducible monic polynomial over the ﬁeld of scalars.
Then there exist vectors {v ,··· ,v } and A cyclic sets β such that { } 1 s vj β ,··· ,β is a basis for V. v1 vs Proof: First suppose m = 1.
Then in Lemma 10.3.2 you can let W{= {0} and U}= V =ker(ϕ(A)).
Then by this lemma, there exist v1,v2,··· ,vs such th(at βv1,··)· ,βvs is abasisforV.
SupposethenthatthetheoremistruewheneverV =ker ϕ(A)m−1 ,m≥2.
Suppose V =ker(ϕ(A)m).
Then ϕ(A)m−1 maps V to V and so by Lemma 10.3.3, ( ) V =ker ϕ(A)m−1 +ϕ(A)m−1(V) Clearly ϕ(A)m−1(V)⊆ker(ϕ(A)).
Is ϕ(A)m−1(V) also A invariant?
Yes, this is the case because if y ∈V =ker(ϕ(A)m), then ϕ(A)m−1y is a typical thing in ϕ(A)m−1(V).
But Aϕ(A)m−1(y)=ϕ(A)m−1(Ay)∈ϕ(A)m−1(V) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.4.
NILPOTENT TRANSFORMATIONS 255 ( ) By induction, there exists a basis for ker ϕ(A)m−1 which is of the form { } β ,··· ,β v1 vr and now, by Lemma 10.3.2, there exists a basis { } β ,··· ,β ,β ,··· ,β x1 xl v1 vr ( ) for V =ker ϕ(A)m−1 +ϕ(A)m−1(V).
(cid:4) 10.4 Nilpotent Transformations Deﬁnition 10.4.1 Let V be a vector space over the ﬁeld of scalars F. Then N ∈ L(V,V) is called nilpotent if for some m, it follows that Nm =0.
Thefollowinglemmacontainssomesigniﬁcantobservationsaboutnilpotenttransforma- tions.
{ } Lemma 10.4.2 Suppose Nkx ̸= 0.
Then x,Nx,··· ,Nkx is linearly independent.
Also, the minimal polynomial of N is λm where m is the ﬁrst such that Nm =0.
∑ Proof: Suppose k c Nix = 0.
There exists l such that k ≤ l < m and Nl+1x = 0 i=0 i but Nlx̸=0.
Then multiply both sides by Nl to conclude that c =0.
Next multiply both 0 sides by Nl−1 to conclude that c =0 and continue this way to obtain that all the c =0.
1 i Next consider the claim that λm is the minimal polynomial.
If p(λ) is the minimal polynomial, then p(λ)=λml(λ)+r(λ) where the degree of r(λ) is less than m or else r(λ)=0.
Suppose the degree of r(λ) is less than m. Then you would have 0=0+r(N).
If r(λ)=a +a λ+···+a λs for s≤m−1,a ̸=0, then for any x∈V, 0 1 s s 0=a x+a Nx+···+a Nsx 0 1 s If for some x,Nsx̸=0, then from the ﬁrst part of the argument, the above equation could not hold.
Hence Nsx = 0 for all x and so Ns = 0 for some s < m, a contradiction to the choice of m. It follows that r(λ)=0 and so p(λ) cannot be the minimal polynomial unless l(λ)=1.
Hence p(λ)=λm as claimed.
(cid:4) { } For such a nilpotent transformation, let β ,··· ,β be a basis for ker(Nm) = V x1 xq where these β are cyclic.
This basis exists thanks to Theorem 10.3.4.
Thus xi ( ) ( ) V =span β ⊕···⊕span β , x1 xq each of these subspaces in the above direct sum being N invariant.
For x one of the x , k consider β given by x x,Nx,N2x,··· ,Nr−1x where Nrx is in the span of the above vectors.
Then by the above lemma, Nrx=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation256 LINEAR TRANSFORMATIONS CANONICAL FORMS ByTheorem10.2.6,thematrixofN withrespecttotheabovebasisistheblockdiagonal matrix   M1 0    ...  0 Mq ( ) where Mk denotes the matrix of N restricted to span β .
In computing this matrix, I xk will order β as follows: xk (Nrk−1xk,··· ,xk) Also the cyclic sets β ,β ,··· ,β will be ordered according to length, the length of x1 x2 xq βxi being at least as large as the length of βxi+1.
Then since Nrkxk = 0, it is now easy to ﬁnd Mk.
Using the procedure mentioned above for determining the matrix of a linear transformation, ( ) 0 Nrk−1xk ··· Nxk =   0 1 0   ( Nrk−1xk Nrk−2xk ··· xk ) 0... 0... ...... 1  0 0 ··· 0 ThusthematrixM isther ×r matrixwhichhasonesdownthesuperdiagonalandzeros k k k elsewhere.
The following convenient notation will be used.
Deﬁnition 10.4.3 J (α) is a Jordan block if it is a k×k matrix of the form k   α 1 0    0 ... ...  Jk(α)= ... ... ... 1  0 ··· 0 α In words, there is an unbroken string of ones down the super diagonal and the number α ﬁlling every space on the main diagonal with zeros everywhere else.
Then with this deﬁnition and the above discussion, the following proposition has been proved.
Proposition 10.4.4 Let N ∈L(W,W) be nilpotent, Nm =0 for some m∈N.
Here W is a p dimensional vector space with ﬁeld of scalars F. Then there exists a basis for W such that the matrix of N with respect to this basis is of the form   J (0) 0  r1  J = Jr2(0) ...  0 J (0) rs ∑ where r ≥r ≥···≥r ≥1 and s r =p.
In the above, the J (0) is a Jordan block of 1 2 s i=1 i rj size r ×r with 0 down the main diagonal.
j j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.5.
THE JORDAN CANONICAL FORM 257 In fact, the matrix of the above proposition is unique.
Corollary 10.4.5 Let J,J′ both be matrices of the nilpotent linear transformation N ∈ L(W,W) which are of the form described in Proposition 10.4.4.
Then J = J′.
In fact, if the rank of Jk equals the rank of J′k for all nonnegative integers k, then J =J′.
Proof: Since J and J′ are similar, it follows that for each k an integer, Jk and J′k are similar.
Hence, for each k, these matrices have the same rank.
Now suppose J ̸= J′.
Note ﬁrst that J (0)r =0, J (0)r−1 ̸=0.
r r DJitrekfno(ol0lto)ew̸=tshJtehrbk′alt(o0ct)hk.seSotuwfpJopomassaeJttrrhikca(et0sr)Jkarn>kd−r1tk′ha.enBdbyloJbc′lrkoksc−ko1fmaJru′elatrisepsJlpircke′ac(tt0iio)vn.elLayneotdfktthhbeeeaftbohroemvﬁesrosbtsseurcvhattihoant,   M 0  r1   ...     Mrk   0     ...  0 0 and   Mr′ 0  1   ...     Mr′   k   0     ...  0 0 whereMrj =Mrj′ forj ≤k−1butMrk′ isazerork′ ×rk′ matrixwhileMrk isalargermatrix which is not equal to 0.
For example,   0 ··· 1 Mrk = ... ...  0 0 ThustherearemorepivotcolumnsinJrk−1 thanin(J′)rk−1,contradictingtherequirement that Jk and J′k have the same rank.
(cid:4) 10.5 The Jordan Canonical Form The Jordan canonical form has to do with the case where the minimal polynomial of A ∈ L(V,V) splits.
Thus there exist λ in the ﬁeld of scalars such that the minimal polynomial k of A is of the form ∏r p(λ)= (λ−λ )mk k k=1 Recall the following which follows from Theorem 9.4.4.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation258 LINEAR TRANSFORMATIONS CANONICAL FORMS Proposition 10.5.1 Let the minimal polynomial of A∈L(V,V) be given by ∏r p(λ)= (λ−λ )mk k k=1 Then the eigenvalues of A are {λ ,··· ,λ }.
1 r It follows from Corollary 10.2.4 that V = ker(A−λ I)m1 ⊕···⊕ker(A−λ I)mr 1 r ≡ V ⊕···⊕V 1 r where I denotes the identity linear transformation.
Without loss of generality, let the dimensions of the V be decreasing from left to right.
These V are called the generalized k k eigenspaces.
It follows from the deﬁnition of V that (A−λ I) is nilpotent on V and clearly each k k k V is A invariant.
Therefore from Proposition 10.4.4, and letting A denote the restriction k k of A to V , there exists an ordered basis for V ,β such that with respect to this basis, the k k k matrix of (A −λ I) is of the form given in that proposition, denoted here by Jk.
What is k k the matrix of A with respect to β ?
Letting {b ,··· ,b }=β , k k 1 r k ∑ ∑ ∑( ) A b =(A −λ I)b +λ Ib ≡ Jkb + λ δ b = Jk +λ δ b k j k k j k j sj s k sj s sj k sj s s s s and so the matrix of A with respect to this basis is k Jk+λ I k where I is the identity matrix.
Therefore, with respect to the ordered basis {β ,··· ,β } 1 r the matrix of A is in Jordan canonical form.
This means the matrix is of the form   J(λ ) 0 1    ...  (10.5) 0 J(λ ) r where J(λ ) is an m ×m matrix of the form k k k   J (λ ) 0  k1 k   Jk2(λk) ...  (10.6) 0 J (λ ) kr k ∑ where k ≥k ≥···≥k ≥1 and r k =m .
Here J (λ) is a k×k Jordan block of the 1 2 r i=1 i k k form   λ 1 0    0 λ ...    (10.7)  ... ... 1  0 0 λ This proves the existence part of the following fundamental theorem.
Note that if any of the β consists of eigenvectors, then the corresponding Jordan block k will consist of a diagonal matrix having λ down the main diagonal.
This corresponds to k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.5.
THE JORDAN CANONICAL FORM 259 m =1.
Thevectorswhichareinker(A−λ I)mk whicharenotinker(A−λ I)arecalled k k k generalized eigenvectors.
To illustrate the main idea used in proving uniqueness in this theorem, consider the following two matrices.
    0 1 0 0 0 1 0 0      0 0 1 0   0 0 0 0   ,  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 The ﬁrst has one 3×3 block and the second has two 2×2 blocks.
Initially both matrices have rank 2.
Now lets raise them to a power 2.
    2 0 1 0 0 0 0 1 0      0 0 1 0   0 0 0 0    =  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 which has rank 1 and     2 0 1 0 0 0 0 0 0      0 0 0 0   0 0 0 0    =  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 which has no rank.
You see, discrepancies occur in the rank upon raising to higher powers if the blocks are not the same.
Now with this preparation, here is the main theorem.
Theorem 10.5.2 Let V be an n dimensional vector space with ﬁeld of scalars C or some other ﬁeld such that the minimal polynomial of A∈L(V,V) completely factors into powers of linear factors.
Then there exists a unique Jordan canonical form for A as described in (10.5) - (10.7), where uniqueness is in the sense that any two have the same number and size of Jordan blocks.
Proof: Itonlyremainstoverifyuniqueness.
Supposetherearetwo,J andJ′.Thenthese arematricesofAwithrespecttopossiblydiﬀerentbasesandsotheyaresimilar.
Therefore, they have the same minimal polynomials and the generalized eigenspaces have the same dimension.
Thus the size of the matrices J(λ ) and J′(λ ) deﬁned by the dimension of k k these generalized eigenspaces, also corresponding to the algebraic multiplicity of λ , must k be the same.
Therefore, they comprise the same set of positive integers.
Thus listing the eigenvalues in the same order, corresponding blocks J(λ ),J′(λ ) are the same size.
k k It remains to show that J(λ ) and J′(λ ) are not just the same size but also are the k k same up to order of the Jordan blocks running down their respective diagonals.
It is only necessary to worry about the number and size of the Jordan blocks making up J(λ ) and k J′(λ ).SinceJ,J′ aresimilar,soareJ−λ I andJ′−λ I.Thusthefollowingtwomatrices k k k are similar   J(λ )−λ I 0 1 k    ...  A≡ J(λk)−λkI     ...  0 J(λ )−λ I r k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation260 LINEAR TRANSFORMATIONS CANONICAL FORMS   J′(λ )−λ I 0 1 k    ...  B ≡ J′(λk)−λkI     ...  0 J′(λ )−λ I r k ( ) ( ) and consequently, rank Ak = rank Bk for all k ∈ N. Also, both J(λ ) − λ I and j k J′(λ )−λ I areonetooneforeveryλ ̸=λ .
Sincealltheblocksinbothofthesematrices j k j k areonetooneexcepttheblocksJ′(λk)−λkI, J(λk)−λkI,i{tfollo(wsthatthisrequir)e}sthe two sequences of numbers {rank((J(λ )−λ I)m)}∞ and rank (J′(λ )−λ I)m ∞ k k m=1 k k m=1 must be the same.
Then   J (0) 0  k1  J(λk)−λkI ≡ Jk2(0) ...  0 J (0) kr and a similar formula holds for J′(λ ) k   J (0) 0  l1  J′(λk)−λkI ≡ Jl2(0) ...  0 J (0) lp and it is required to verify that p = r and that the same blocks occur in both.
Without loss of generality, let the blocks be arranged according to size with the largest on upper left corner falling to smallest in lower right.
Now the desired conclusion follows from Corollary 10.4.5.
(cid:4) Note that if any of the generalized eigenspaces ker(A−λ I)mk has a basis of eigen- k vectors, then it would be possible to use this basis and obtain a diagonal matrix in the blockcorrespondingtoλ .
Byuniqueness,thisisthe blockcorrespondingtotheeigenvalue k λ .
Thus when this happens, the block in the Jordan canonical form corresponding to λ k k is just the diagonal matrix having λ down the diagonal and there are no generalized k eigenvectors.
The Jordan canonical form is very signiﬁcant when you try to understand powers of a matrix.
There exists an n×n matrix S1 such that A=S−1JS.
Therefore, A2 =S−1JSS−1JS =S−1J2S and continuing this way, it follows Ak =S−1JkS.
where J is given in the above corollary.
Consider Jk.
By block multiplication,   Jk 0 1   Jk = ... .
0 Jk r 1TheS hereiswrittenasS(cid:0)1 inthecorollary.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.5.
THE JORDAN CANONICAL FORM 261 The matrix J is an m ×m matrix which is of the form s s s   α ··· ∗ Js = ... ... ...  (10.8) 0 ··· α which can be written in the form J =D+N s forD amultipleoftheidentityandN anuppertriangularmatrixwithzerosdownthemain diagonal.
Therefore, by the Cayley Hamilton theorem, Nms =0 because the characteristic equation for N is just λms =0.
(You could also verify this directly.)
Now since D is just a multiple of the identity, it follows that DN = ND.
Therefore, the usual binomial theorem may be applied and this yields the following equations for k ≥m .
s ( ) ∑k k Jk = (D+N)k = Dk−jNj s j j=0 ( ) ∑ms k = Dk−jNj, (10.9) j j=0 the third equation holding because Nms =0.
Thus Jk is of the form s   αk ··· ∗ Jsk = ... ... ... .
0 ··· αk Lemma 10.5.3 Suppose J is of the form J described above in (10.8) where the constant s α, on the main diagonal is less than one in absolute value.
Then ( ) lim Jk =0.
k→∞ ij Proof: From (10.9), it follows that for large k, and j ≤m , s ( ) k k(k−1)···(k−m +1) ≤ s .
j m !
s (cid:12) (cid:12) (cid:12)( ) (cid:12) Therefore, letting C be the largest value of (cid:12) Nj (cid:12) for 0≤j ≤m , pq s (cid:12) (cid:12) ( ) (cid:12)(cid:12)(Jk) (cid:12)(cid:12)≤m C k(k−1)···(k−ms+1) |α|k−ms pq s m !
s which converges to zero as k → ∞.
This is most easily seen by applying the ratio test to the series ( ) ∑∞ k(k−1)···(k−ms+1) |α|k−ms m !
s k=ms and then noting that if a series converges, then the kth term converges to zero.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation262 LINEAR TRANSFORMATIONS CANONICAL FORMS 10.6 Exercises 1.
InthediscussionofNilpotenttransformations,itwasassertedthatiftwon×nmatrices A,B are similar, then Ak is also similar to Bk.
Why is this so?
If two matrices are similar, why must they have the same rank?
2.
If A,B are both invertible, then they are both row equivalent to the identity matrix.
Are they necessarily similar?
Explain.
3.
Suppose you have two nilpotent matrices A,B and Ak and Bk both have the same rank for all k ≥1.
Does it follow that A,B are similar?
What if it is not known that A,B are nilpotent?
Does it follow then?
4.
When we say a polynomial equals zero, we mean that all the coeﬃcients equal 0.
If we assign a diﬀerent meaning to it which says that a polynomial ∑n p(λ)= a λk =0, k k=0 when the value of the polynomial equals zero whenever a particular value of λ ∈ F is placed in the formula for p(λ), can the same conclusion be drawn?
Is there any diﬀerence in the two deﬁnitions for ordinary ﬁelds like Q?
Hint: Consider Z , the 2 integers mod 2.
5.
Let A ∈ L(V,V) where V is a ﬁnite dimensional vector space with ﬁeld of scalars F. Letp(λ)betheminimalpolynomialandsupposeϕ(λ)isanynonzeropolynomialsuch that ϕ(A) is not one to one and ϕ(λ) has smallest possible degree such that ϕ(A) is nonzero and not one to one.
Show ϕ(λ) must divide p(λ).
6.
Let A ∈ L(V,V) where V is a ﬁnite dimensional vector space with ﬁeld of scalars F. Let p(λ) be the minimal polynomial and suppose ϕ(λ) is an irreducible polynomial with the property that ϕ(A)x = 0 for some speciﬁc x ̸= 0.
Show that ϕ(λ) must divide p(λ).
Hint: First write p(λ)=ϕ(λ)g(λ)+r(λ) where r(λ) is either 0 or has degree smaller than the degree of ϕ(λ).
If r(λ)=0 you are done.
Suppose it is not 0.
Let η(λ) be the monic polynomial of smallest degree with the property that η(A)x = 0.
Now use the Euclidean algorithm to divide ϕ(λ) by η(λ).
Contradict the irreducibility of ϕ(λ).
7.
Suppose A is a linear transformation and let the characteristic polynomial be ∏q det(λI−A)= ϕ (λ)nj j j=1 where the ϕ (λ) are irreducible.
Explain using Corollary 8.3.11 why the irreducible j factors of the minimal polynomial are ϕ (λ) and why the minimal polynomial is of j the form ∏q ϕ (λ)rj j j=1 where r ≤n .
You can use the Cayley Hamilton theorem if you like.
j j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.6.
EXERCISES 263 8.
Let   1 0 0 A= 0 0 −1  0 1 0 Find the minimal polynomial for A.
9.
Suppose{A is an n×n ma}trix and let v be a vector.
Consider the A cyclic set of vectors v,Av,··· ,Am−1v where this is an independent set of vectors but Amv is a linear combination of the preceding vectors in the list.
Show how to obtain a monic polynomial of smallest degree, m, ϕ (λ) such that v ϕ (A)v=0 v Now let {w ,··· ,w } be a basis and let ϕ(λ) be the least common multiple of the 1 n ϕ (λ).
Explain why this must be the minimal polynomial of A.
Give a reasonably wk easy algorithm for computing ϕ (λ).
v 10.
Here is a matrix.
  −7 −1 −1  −21 −3 −3  70 10 10 Using the process of Problem 9 ﬁnd the minimal polynomial of this matrix.
It turns out the characteristic polynomial is λ3.
11.
Find the minimal polynomial for   1 2 3   A= 2 1 4 −3 2 1 by the above technique.
Is what you found also the characteristic polynomial?
12.
Let A be an n×n matrix with ﬁeld of scalars C. Letting λ be an eigenvalue, show the dimension of the eigenspace equals the number of Jordan blocks in the Jordan canonical form which are associated with λ.
Recall the eigenspace is ker(λI−A).
13.
For any n×n matrix, why is the dimension of the eigenspace always less than or equal to the algebraic multiplicity of the eigenvalue as a root of the characteristic equation?
Hint: Note the algebraic multiplicity is the size of the appropriate block in the Jordan form.
14.
Give an example of two nilpotent matrices which are not similar but have the same minimal polynomial if possible.
15.
Use the existence of the Jordan canonical form for a linear transformation whose minimalpolynomialfactorscompletelytogiveaproofoftheCayleyHamiltontheorem which is valid for any ﬁeld of scalars.
Hint: First assume the minimal polynomial factorscompletelyintolinearfactors.
Ifthisdoesnothappen,considerasplittingﬁeld of the minimal polynomial.
Then consider the minimal polynomial with respect to thislargerﬁeld.
Howwillthetwominimalpolynomialsberelated?
Showtheminimal polynomial always divides the characteristic polynomial.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation264 LINEAR TRANSFORMATIONS CANONICAL FORMS 16.
Here is a matrix.
Find its Jordan canonical form by directly ﬁnding the eigenvectors andgeneralizedeigenvectorsbasedonthesetoﬁndabasiswhichwillyieldtheJordan form.
The eigenvalues are 1 and 2.
  −3 −2 5 3  −1 0 1 2   −4 −3 6 4  −1 −1 1 3 Why is it typically impossible to ﬁnd the Jordan canonical form?
17.
People like to consider the solutions of ﬁrst order linear systems of equations which are of the form ′ x (t)=Ax(t) where here A is an n×n matrix.
From the theorem on the Jordan canonical form, there exist S and S−1 such that A = SJS−1 where J is a Jordan form.
Deﬁne y(t)≡S−1x(t).Showy′ =Jy.
NowsupposeΨ(t)isann×nmatrixwhosecolumns are solutions of the above diﬀerential equation.
Thus ′ Ψ =AΨ Now let Φ be deﬁned by SΦS−1 =Ψ.
Show ′ Φ =JΦ.
18.
In the above Problem show that ′ det(Ψ) =trace(A)det(Ψ) and so det(Ψ(t))=Cetrace(A)t This is called Abel’s formula and det(Ψ(t)) is called the Wronskian.
Hint: Show it suﬃces to consider ′ Φ =JΦ and establish the formula for Φ.
Next let   ϕ 1  .
 Φ= .
 .
ϕ n where the ϕ are the rows of Φ.
Then explain why j ∑n ′ det(Φ) = det(Φ ) (10.10) i i=1 where Φ is the same as Φ except the ith row is replaced with ϕ′ instead of the row i i ϕ .
Now from the form of J, i ′ Φ =DΦ+NΦ where N has all nonzero entries above the main diagonal.
Explain why ′ ϕ (t)=λ ϕ (t)+a ϕ (t) i i i i i+1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.6.
EXERCISES 265 Now use this in the formula for the derivative of the Wronskian given in (10.10) and use properties of determinants to obtain ∑n ′ det(Φ) = λ det(Φ).
i i=1 Obtain Abel’s formula det(Φ)=Cetrace(A)t and so the Wronskian detΦ either vanishes identically or never.
19.
Let A be an n×n matrix and let J be its Jordan canonical form.
Recall J is a block diagonal matrix having blocks J (λ) down the diagonal.
Each of these blocks is of k the form   λ 1 0    λ ...  Jk(λ)= ... 1  0 λ Now for ε>0 given, let the diagonal matrix D be given by ε   1 0    ε  Dε = ...  0 εk−1 Show that D−1J (λ)D has the same form as J (λ) but instead of ones down the ε k ε k super diagonal, there is ε down the super diagonal.
That is J (λ) is replaced with k   λ ε 0    λ ...     ... ε  0 λ NowshowthatforAann×nmatrix, itissimilartoonewhichisjustliketheJordan canonical form except instead of the blocks having 1 down the super diagonal, it has ε.
20.
Let A be in L(V,V) and suppose that Apx̸=0 for some x̸=0.
Show that Ape ̸=0 k for some e ∈ {e ,··· ,e }, a basis for V. If you have a matrix which is nilpotent, k 1 n (Am =0 for some m) will it always be possible to ﬁnd its Jordan form?
Describe how to do it if this is the case.
Hint: First explain why all the eigenvalues are 0.
Then considerthewaytheJordanformfornilpotenttransformationswasconstructedinthe above.
21.
SupposeAisann×nmatrixandthatithasndistincteigenvalues.
Howdothemini- mal polynomial and characteristic polynomials compare?
Determine other conditions based on the Jordan Canonical form which will cause the minimal and characteristic polynomials to be diﬀerent.
22.
Suppose A is a 3×3 matrix and it has at least two distinct eigenvalues.
Is it possible that the minimal polynomial is diﬀerent than the characteristic polynomial?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation266 LINEAR TRANSFORMATIONS CANONICAL FORMS 23.
If A is an n×n matrix of entries from a ﬁeld of scalars and if the minimal polynomial of A splits over this ﬁeld of scalars, does it follow that the characteristic polynomial of A also splits?
Explain why or why not.
24.
In proving the uniqueness of the Jordan canonical form, it was asserted that if two n×nmatricesA,B aresimilar,thentheyhavethesameminimalpolynomialandalso that if this minimal polynomial is of the form ∏s p(λ)= ϕ (λ)ri i i=1 where the ϕ (λ) are irreducible, then ker(ϕ (A)ri) and ker(ϕ (B)ri) have the same i i i dimension.
Whyisthisso?
Thiswaswhatwasresponsiblefortheblockscorresponding to an eigenvalue being of the same size.
25.
Show that a given complex n×n matrix is non defective (diagonalizable) if and only if the minimal polynomial has no repeated roots.
26.
Describe a straight forward way to determine the minimal polynomial of an n×n matrix using row operations.
Next show that if p(λ) and p′(λ) are relatively prime, then p(λ) has no repeated roots.
With the above problem, explain how this gives a way to determine whether a matrix is non defective.
27.
In Theorem 10.3.4 show that the cyclic sets can be arranged in such a way that the length of β divides the length of β .
vi+1 vi 28.
ShowthatifAisacomplexn×nmatrix,thenAandAT aresimilar.
Hint: Consider a Jordan block.
Note that       0 0 1 λ 1 0 0 0 1 λ 0 0       0 1 0 0 λ 1 0 1 0 = 1 λ 0 1 0 0 0 0 λ 1 0 0 0 1 λ 29.
Let A be a linear transformation deﬁned on a ﬁnite dimensional vector space V. Let the minimal polynomial be ∏q ϕ (λ)mi i i=1 ( ) { } and let βi ,··· ,βi be the cyclic sets such that βi ,··· ,βi is a basis for v1i vrii ∑ ∑ v1i vrii ker(ϕ (A)mi).
Let v = vi.
Now let q(λ) be any polynomial and suppose that i i j j q(A)v =0 Show{that it follows q(A})=0.
Hint: First consider the special case where a basis for V is x,Ax,··· ,An−1x and q(A)x=0.
10.7 The Rational Canonical Form ∏ Hereonehastheminimalpolynomialintheform q ϕ(λ)mk whereϕ(λ)isanirreducible k=1 monic polynomial.
It is not necessarily the case that ϕ(λ) is a linear factor.
Thus this case iscompletelygeneralandincludesthesituationwheretheﬁeldisarbitrary.
Inparticular,it includes the case where the ﬁeld of scalars is, for example, the rational numbers.
This may Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.7.
THE RATIONAL CANONICAL FORM 267 be partly why it is called the rational canonical form.
As you know, the rational numbers are notorious for not having roots to polynomial equations which have integer or rational coeﬃcients.
ThiscanonicalformisduetoFrobenius.
Iamfollowingthepresentationgivenin[9]and there are more details given in this reference.
Another good source which has many of the same ideas is [14].
Here is a deﬁnition of the concept of a companion matrix.
Deﬁnition 10.7.1 Let q(λ)=a0+a1λ+···+an−1λn−1+λn be a monic polynomial.
The companion matrix of q(λ), denoted as C(q(λ)) is the matrix   0 ··· 0 −a 0  1 0 −a1   ... ... ...  0 1 −an−1 Proposition 10.7.2 Let q(λ) be a polynomial and let C(q(λ)) be its companion matrix.
Then q(C(q(λ)))=0.
Proof: Write C instead of C(q(λ)) for short.
Note that Ce1 =e2,Ce2 =e3,··· ,Cen−1 =en Thus e =Ck−1e , k =1,··· ,n (10.11) k 1 and so it follows { } e ,Ce ,C2e ,··· ,Cn−1e (10.12) 1 1 1 1 are linearly independent.
Hence these form a basis for Fn.
Now note that Ce is given by n Cen =−a0e1−a1e2−···−an−1en and from (10.11) this implies Cne1 =−a0e1−a1Ce1−···−an−1Cn−1e1 and so q(C)e =0.
1 Now since (10.12) is a basis, every vector of Fn is of the form k(C)e for some polynomial 1 k(λ).
Therefore, if v∈Fn, q(C)v=q(C)k(C)e =k(C)q(C)e =0 1 1 which shows q(C)=0.
(cid:4) The following theorem is on the existence of the rational canonical form.
Theorem 10.7.3 Let A ∈ L(V,V) where V is a vector space with ﬁeld of scalars F and minimal polynomial ∏q ϕ (λ)mi i i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation268 LINEAR TRANSFORMATIONS CANONICAL FORMS where each ϕ (λ) is irreducible.
Letting V ≡ker(ϕ (λ)mk), it follows i k k V =V ⊕···⊕V 1 q where each V is A invariant.
Letting B denote a basis for V and Mk the matrix of the k k k restriction of A to V , it follows that the matrix of A with respect to the basis {B ,··· ,B } k 1 q is the block diagonal matrix of the form   M1 0    ...  (10.13) 0 Mq { } If B is given as β ,··· ,β as described in Theorem 10.3.4 where each β is an A k v1 vs vj cyclic set of vectors, then the matrix Mk is of the form   C(ϕ (λ)r1) 0 k   Mk = ...  (10.14) 0 C(ϕ (λ)rs) k where the A cyclic sets of vectors may be arranged in order such that the positive integers r j satisfy r ≥···≥r and C(ϕ (λ)rj) is the companion matrix of the polynomial ϕ (λ)rj.
1 s k k Proof: By Theorem 10.2.6 the matrix of A with respect to {B ,··· ,B } is of the 1 q f{orm given in}(10.13).
Now by Theorem 10.3.4 the basis Bk may be chosen in the form β ,··· ,β where each β is an A cyclic set of vectors and also it can be assumed the v1 vs vk lengths of these β are decreasing.
Thus vk ( ) ( ) V =span β ⊕···⊕span β k v1 vs ( ) and it only remains to consider the matrix of A restricted to span β .
Then you can vk apply Theorem 10.2.6 to get the result in (10.14).
Say β =v ,Av ,··· ,Ad−1v vk k k k where η(A)v = 0 and the degree of η(λ) is d, the smallest degree such that this is so, η k being a monic polynomial.
Then by Corollary 8.3.11, η(λ)=ϕ (λ)rk where r ≤m .
Now k k k ( ( )) ( ) A span β ⊆span β ( vk ) vk because Advk is i(n sp)an vk,Avk,··· ,Ad−1vk .
It remains to consider the matrix of A restricted to span β .
Say vk η(λ)=ϕk(λ)rk =a0+a1λ+···+ad−1λd−1+λd Thus Advk =−a0vk−a1Avk−···−ad−1Ad−1vk Recall the formalism for ﬁnding the matrix of A restricted to this invariant subspace.
( ) Avk A2vk A3vk ··· −a0vk−a1Avk−···−ad−1Ad−1vk =   0 0 0 ··· −a 0  1 0 −a1  ( vk Avk A2vk ··· Ad−1vk ) 0 1 ... ...     ... ... 0 −ad−2  0 0 1 −ad−1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.8.
UNIQUENESS 269 Thus the matrix of the transformation is the above.
The is the companion matrix of ϕ (λ)rk = η(λ).
In other words, C = C(ϕ (λ)rk) and so Mk has the form claimed in k k the theorem.
(cid:4) 10.8 Uniqueness Given A ∈ L(V,V) where V is a vector space having ﬁeld of scalars F, the above shows thereexistsarationalcanonicalformforA.
CouldAhavemorethanonerationalcanonical form?
Recall the deﬁnition of an A cyclic set.
For convenience, here it is again.
{ } Deﬁnition 10.8.1 Letting x̸=0 denote b(y βx the vector)s x,Ax,A2x,··· ,Am−1x where m is the smallest such that Amx ∈ span x,··· ,Am−1x .
This is called an A cyclic set, denoted by β .
x The following proposition ties these A cyclic sets to polynomials.
It is just a review of ideas used above to prove existence.
{ } Proposition 10.8.2 Let x ̸= 0 and consider x,Ax,A2x,··· ,Am−1x .
Then this is an A cyclic set if and only if there exists a monic polynomial η(λ) such that η(A)x = 0 and among all such polynomials ψ(λ) satisfying ψ(A)x = 0, η(λ) has the smallest degree.
If V = ker(ϕ(λ)m) where ϕ(λ) is irreducible, then for some positive integer p ≤ m,η(λ) = ϕ(λ)p. Lemma 10.8.3 Let V be a vector space and A ∈ L(V,V) has minimal poly{nomial ϕ(λ)m} where ϕ(λ) is irreducible and has degree d. Let the basis for V consist of β ,··· ,β v1 vs where βvk is A cyclic as described above and th(cid:12)(cid:12)e ra(cid:12)(cid:12)tional canonical form for A is the matrix taken with respect to this basis.
Then letting βv(cid:12)(cid:12)k de(cid:12)(cid:12)note the number of vectors in βvk, it follows there is only one possible set of numbers β .
vk Proof: Say β is associated with the polynomial ϕ(λ)pj.
Thus, as described above (cid:12)(cid:12) (cid:12)(cid:12) vj (cid:12)β (cid:12) equals p d. Consider the following table which comes from the A cyclic set vj j { } v ,Av ,··· ,Ad−1v ,··· ,Apjd−1v j j j j αj αj αj ··· αj 0 1 2 d−1 v Av A2v ··· Ad−1v j j j j ϕ(A)v ϕ(A)Av ϕ(A)A2v ··· ϕ(A)Ad−1v j j j j .
.
.
.
.
.
.
.
.
.
.
.
ϕ(A)pj−1v ϕ(A)pj−1Av ϕ(A)pj−1A2v ··· ϕ(A)pj−1Ad−1v j j j j In the above, αj signiﬁes the vectors below it in the kth column.
None of these vectors k below the top row are equal to 0 because the degree of ϕ(λ)pj−1λd−1 is dp −1, which is j less than p d and the smallest degree of a nonzero polynomial sending v to 0 is p d. Also, j j j each of these vectors is in the span of β and there are dp of them, just as there are dp vj j j vectors in β .
vj { } Claim: The vectors αj,··· ,αj are linearly independent.
0 d−1 Proof of claim: Suppose ∑d−1p∑j−1 c ϕ(A)kAiv =0 ik j i=0 k=0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation270 LINEAR TRANSFORMATIONS CANONICAL FORMS Then multiplying both sides by ϕ(A)pj−1 this yields ∑d−1 c ϕ(A)pj−1Aiv =0 i0 j i=0 Now if any of the c is nonzero this would imply there exists a polynomial having degree i0 smaller than p d which sends v to 0.
Since this does not happen, it follows each c = 0. j j i0 Thus ∑d−1p∑j−1 c ϕ(A)kAiv =0 ik j i=0 k=1 Now multiply both sides by ϕ(A)pj−2 and do a similar argument to assert that c =0 for i1 each i.
Continuing thi{s way, all the c}ik =0 and this proves the claim.
(cid:12) (cid:12) (cid:12) (cid:12) Thus the vectors αj,··· ,αj are linearly independent and there are p d = (cid:12)β (cid:12) 0 d−1 ( ) j vj of them.
Therefore, they form a basis for span β .
Also note that if you list the vj c{olumns in rev}erse order starting from the bottom and going toward the top, the vectors αj,··· ,αj yield Jordan blocks in the matrix of ϕ(A).
Hence, considering all these 0 { d−1 } s vectors αj,··· ,αj listed in the reverse order, the matrix of ϕ(A) with respect to 0 d−1 j=1 this basis of V is in Jordan canonical form.
See Proposition 10.4.4 and Theorem 10.5.2 on existence and uniqueness f{or the Jordan}form.
This Jordan form is unique up to order of the blocks.
For a given j αj,··· ,αj yields d Jordan blocks of size p for ϕ(A).
The 0 d−1 j size and number of Jordan blocks of ϕ(A) depends only on ϕ(A), hence only on A.
Once A is determined, ϕ(A) is determined and hence the number and size of Jordan blocks is determined so the exponents p are determined and this shows the lengths of the β ,p d are also determined.
(cid:4) j vj j Note that if the p are known, then so is the rational canonical form because it comes j fromblockswhicharecompanionmatricesofthepolynomialsϕ(λ)pj.
Nowhereisthemain result.
Theorem 10.8.4 Let V be a vector space having ﬁeld of scalars F and let A ∈ L(V,V).
Then the rational canonical form of A is unique up to order of the blocks.
∏ Proof: LettheminimalpolynomialofAbe q ϕ (λ)mk.ThenrecallfromCorollary k=1 k 10.2.4 V =V ⊕···⊕V 1 q where V = ker(ϕ (A)mk).
Also recall from Corollary 10.2.5 that the minimal polynomial k k of the restriction of A to V is ϕ (λ)mk.
Now apply Lemma 10.8.3 to A restricted to V .
(cid:4) k k k In the case where two n×n matrices M,N are similar, recall this is equivalent to the two being matrices of the same linear transformation taken with respect to two diﬀerent bases.
Hence each are similar to the same rational canonical form.
Example 10.8.5 Here is a matrix.
  5 −2 1 A= 2 10 −2  9 0 9 Find a similarity transformation which will produce the rational canonical form for A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.8.
UNIQUENESS 271 The characteristic polynomial is λ3−24λ2+180λ−432.
This factors as (λ−6)2(λ−12) It turns out this is also the minimal polynomial.
You can see this by plugging in A where you see λ and observing things don’t work if you delete one of the λ−6 factors.
There is more on this in the exercises.
It turns ou(t you can c)ompute the minimal polynomial pretty easily.
Thus Q3 is the direct sum of ker (A−6I)2 and ker(A−12I).
Consider the ﬁrst of these.
You see easily that this is     1 −1 y 1 +z 0 ,y,z ∈Q.
0 1 What about the length of A cyclic sets?
It turns out it doesn’t matter much.
You can start with either of these and get a cycle of length 2.
Lets pick the second one.
This leads to the cycle           −1 −4 −1 −12 −1  0 , −4 =A 0 , −48 =A2 0  1 0 1 −36 1 where the last of the three is a linear combination of the ﬁrst two.
Take the ﬁrst two as the ﬁrst two columns of S. To get the third, you need a cycle of length 1 corresponding to ( ) ker(A−12I).
This yields the eigenvector 1 −2 3 T. Thus   −1 −4 1 S = 0 −4 −2  1 0 3 Now using Proposition 9.3.10, the Rational canonical form for A should be        −1 −4 1 −1 5 −2 1 −1 −4 1 0 −36 0  0 −4 −2   2 10 −2  0 −4 −2 = 1 12 0  1 0 3 9 0 9 1 0 3 0 0 12 Example 10.8.6 Here is a matrix.
  12 −3 −19 −14 8  −4 1 1 6 −4  A= 4 5 5 −2 4   0 −5 −5 2 0  −4 3 11 6 0 Find a basis such that if S is the matrix which has these vectors as columns S−1AS is in rational canonical form assuming the ﬁeld of scalars is Q. Firstitisnecessarytoﬁndtheminimalpolynomial.
Ofcourseyoucanﬁndthecharacter- istic polynomial and then take away factors till you ﬁnd the minimal polynomial.
However, there is a much better way which is described in the exercises.
Leaving out this detail, the minimal polynomial is λ3−12λ2+64λ−128 This polynomial factors as ( ) (λ−4) λ2−8λ+32 ≡ϕ (λ)ϕ (λ) 1 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation272 LINEAR TRANSFORMATIONS CANONICAL FORMS where the second factor is irreducible over Q.
Consider ϕ (λ) ﬁrst.
Messy computations 2 yield   −16 −16 −16 −16 −32    0 0 0 0 0    ϕ (A)= 0 0 0 0 0  2   0 0 0 0 0 16 16 16 16 32 and so         −1 −1 −1 −2          1   0   0   0          ker(ϕ (A))=a 0 +b 1 +c 0 +d 0 .
2         0 0 1 0 0 0 0 1 NowstartwithoneofthesebasisvectorsandlookforanAcycle.
Pickingtheﬁrstone, you obtain the cycle     −1 −15      1   5       0 , 1   0   −5  0 7 because the next vector involving A2 yields a vector which is in the span of the above two.
You check this by making the vectors the columns of a matrix and ﬁnding the row reduced echelonform.
Clearlythiscycledoesnotspanker(ϕ (A)),solookforanothercycle.
Begin 2 with a vector which is not in the span of these two.
The last one works well.
Thus another A cycle is     −2 −16      0   4   0 , −4      0 0 1 8 It follows a basis for ker(ϕ (A)) is 2          −02   −416   −11   −515   00 , −04 , 00 , −15  1 8 0 7 From the above theory, these vectors are linearly independent.
Finally consider a cycle coming from ker(ϕ (A)).
This amounts to nothing more than ﬁnding an eigenvector for A 1 ( ) correspondingtotheeigenvalue4.
Aneigenvectoris −1 0 0 0 1 T.Nowthedesired matrix for the similarity transformation is   −2 −16 −1 −15 −1    0 4 1 5 0  S ≡ 0 −4 0 1 0   0 0 0 −5 0  1 8 0 7 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation10.9.
EXERCISES 273 Then doing the computations, you get   0 −32 0 0 0    1 8 0 0 0  S−1AS = 0 0 0 −32 0    0 0 1 8 0 0 0 0 0 4 andyouseethisisinrationalcanonicalform,thetwo2×2blocksbeingcompanionmatrices forthepolynomialλ2−8λ+32andthe1×1blockbeingacompanionmatrixforλ−4.
Note that you could have written this without ﬁnding a similarity transformation to produce it.
This follows from the above theory which gave the existence of the rational canonical form.
Obviously there is a lot more which could be considered about rational canonical forms.
Just begin with a strange ﬁeld and start investigating what can be said.
It is as far as I feel like going on this subject at this time.
One can also derive more systematic methods for ﬁnding the rational canonical form.
The advantage of this is you don’t need to ﬁnd the eigenvalues in order to compute the rational canonical form and it can often be computed for this reason, unlike the Jordan form.
The uniqueness of this rational canonical form can be used to determine whether two matrices consisting of entries in some ﬁeld are similar.
10.9 Exercises 1.
Letting A be a complex n×n matrix, in obtaining the rational canonical form, one obtains the Cn as a direct sum of the form ( ) ( ) span β ⊕···⊕span β x1 xr where β is an ordered cyclic set of vectors, x,Ax,··· ,Am−1x such that Amx is in x thesp(anoftheprevious)vectors.
NowapplytheGramSchmidtprocesstotheordered basis β ,β ,··· ,β , thevectorsin eachβ listedaccording to increasing power of A, thux1s obxt2aining axnr ordered basis (q ,··· ,xqi ).
Letting Q be the unitary matrix 1 n whichhasthesevectorsascolumns,showthatQ∗AQequalsamatrixB whichsatisﬁes B = 0 if i−j ≥ 2.
Such a matrix is called an upper Hessenberg matrix and this ij shows that every n×n matrix is orthogonally similar to an upper Hessenberg matrix.
Thesearezerobelowthemainsubdiagonal,likecompanionmatricesdiscussedabove.
2.
In the argument for Theorem 10.2.4 it was shown that m(A)ϕ (A)v = v whenever l v ∈ ker(ϕ (A)rk).
Show that m(A) restricted to ker(ϕ (A)rk) is the inverse of the k k linear transformation ϕ (A) on ker(ϕ (A)rk).
l k 3.
Suppose A is a linear transformation and let the characteristic polynomial be ∏q det(λI−A)= ϕ (λ)nj j j=1 where the ϕ (λ) are irreducible.
Explain using Corollary 8.3.11 why the irreducible j factors of the minimal polynomial are ϕ (λ) and why the minimal polynomial is of j the form ∏q ϕ (λ)rj j j=1 where r ≤n .
You can use the Cayley Hamilton theorem if you like.
j j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation274 LINEAR TRANSFORMATIONS CANONICAL FORMS 4.
Find the minimal polynomial for   1 2 3   A= 2 1 4 −3 2 1 by the above technique assuming the ﬁeld of scalars is the rational numbers.
Is what you found also the characteristic polynomial?
5.
Show, using the rational root theorem, the minimal polynomial for A in the above problem is irreducible with respect to Q.
Letting the ﬁeld of scalars be Q ﬁnd the rational canonical form and a similarity transformation which will produce it.
6.
Find the rational canonical form for the matrix   1 2 1 −1    2 3 0 2    1 3 2 4 1 2 1 2 ( ) 7.
LetA:Q3 →Q3 belinear.
Supposetheminimalpolynomialis(λ−2) λ2+2λ+7 .
Find the rational canonical form.
Can you give generalizations of this rather simple problem to other situations?
8.
Find the rational canonical form with respect to the ﬁeld of scalars equal to Q for the matrix   0 0 1 A= 1 0 −1  0 1 1 Observe that this particular matrix is already a companion matrix of λ3−λ2+λ−1.
Then ﬁnd the rational canonical form if the ﬁeld of scalars equals C or Q+iQ.
9.
Let q(λ) be a polynomial and C its companion matrix.
Show the characteristic and minimal polynomial of C are the same and both equal q(λ).
10.
↑UsetheexistenceoftherationalcanonicalformtogiveaproofoftheCayleyHamilton theoremvalidforanyﬁeld,evenﬁeldsliketheintegersmodpforpaprime.
Theearlier proofbasedondeterminantswasﬁneforﬁeldslikeQorRwhereyoucouldletλ→∞ but it is not clear the same result holds in general.
11.
Supposeyouhavetwon×nmatricesA,B whoseentriesareinaﬁeldFandsupposeG is an extension of F. For example, you could have F=Q and G=C.
Suppose A and B are similar with respect to the ﬁeld G. Can it be concluded that they are similar with respect to the ﬁeld F?
Hint: First show that the two have the same minimal polynomialoverF.
NextconsidertheproofofLemma10.8.3andshowthattheyhave the same rational canonical form with respect to F. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationMarkov Chains And Migration Processes 11.1 Regular Markov Matrices The existence of the Jordan form is the basis for the proof of limit theorems for certain kinds of matrices called Markov matrices.
Deﬁnition 11.1.1 An n×n matrix A = (a ), is a Markov matrix if a ≥ 0 for all i,j ij ij and ∑ a =1.
ij i It may also be called a stochastic matrix.
A matrix which has nonnegative entries such that ∑ a =1 ij j will also be called a stochastic matrix.
A Markov or stochastic matrix is called regular if some power of A has all entries strictly positive.
A vector, v ∈ Rn, is a steady state if Av=v.
Lemma 11.1.2 The property of being a stochastic matrix is preserved by taking products.
Proof: Suppose the sum over a row equals 1 for A and B.
Then letting the entries be denoted by (a ) and (b ) respectively, ij ij ( ) ∑∑ ∑ ∑ ∑ a b = a b = b =1.
ik kj ik kj kj i k k i k A similar argument yields the same result in the case where it is the sum over a column which is equal to 1.
It is obvious that when the product is taken, if each a ,b ≥ 0, then ij ij the same will be true of sums of products of these numbers.
The following theorem is convenient for showing the existence of limits.
Theorem 11.1.3 Let A be a real p×p matrix having the properties 1. a ≥0 ij ∑ ∑ 2.
Either p a =1 or p a =1.
i=1 ij j=1 ij 3.
The distinct eigenvalues of A are {1,λ ,...,λ } where each |λ |<1.
2 m j 275 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation276 MARKOV CHAINS AND MIGRATION PROCESSES Then limn→∞An = A∞ exists in the sense that limn→∞ anij = a∞ij, the ijth entry A∞.
Here an denotes the ijth entry of An.
Also, if λ = 1 has algebraic multiplicity r, then ij the Jordan block corresponding to λ=1 is just the r×r identity.
Proof.
BytheexistenceoftheJordanformforA,itfollowsthatthereexistsaninvertible matrix P such that   I+N   P−1AP = Jr2(λ2) ... =J J (λ ) rm m where I is r×r for r the multiplicity of the eigenvalue 1 and N is a nilpotent matrix for which Nr =0.
I will show that because of Condition 2, N =0.
First of all, J (λ )=λ I+N ri i i i where N satisﬁes Nri =0 for some r >0.
It is clear that N (λ I)=(λ I)N and so i i i i i i ( ) ( ) ∑n ∑r n n (J (λ ))n = Nkλn−k = Nkλn−k ri i k i k i k=0 k=0 which converges to 0 due to the assumption that |λ | < 1.
There are ﬁnitely many terms i and a typical one is a matrix whose entries are no larger than an expression of the form |λ |n−kC n(n−1)···(n−k+1)≤C |λ |n−knk i k k i ∑ which converges to 0 because, by the root test, the series ∞ |λ |n−knk converges.
Thus n=1 i for each i=2,...,p, lim (J (λ ))n =0.
n→∞ ri i By Condition 2, if an denotes the ijth entry of An, then either ij ∑p ∑p an =1 or an =1, an ≥0.
ij ij ij i=1 j=1 This follows from Lemma 11.1.2.
It is obvious each an ≥ 0, and so the entries of An must ij be bounded independent of n. It follows easily from z nt}im|es { P−1APP−1APP−1AP ···P−1AP =P−1AnP that P−1AnP =Jn (11.1) Hence Jn must also have bounded entries as n → ∞.
However, this requirement is incom- patible with an assumption that N ̸=0.
If N ̸=0, then Ns ̸=0 but Ns+1 =0 for some 1≤s≤r.
Then ( ) ∑s n (I+N)n =I+ Nk k k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation11.1.
REGULAR MARKOV MATRICES 277 One of the entries of Ns is nonzero by the deﬁnition of s. Let this entry be ns .
Then this ( ) ij implies that one of the entries of (I+N)n is of the form n ns .
This entry dominates the ( ) s ij ijth entries of n Nk for all k <s because k ( ) ( ) n n lim / =∞ n→∞ s k Therefore, the entries of (I+N)n cannot all be bounded.
From block multiplication,   (I+N)n P−1AnP = (Jr2(λ2))n ...  (J (λ ))n rm m and this is a contradiction because entries are bounded on the left and unbounded on the right.
Since N =0, the above equation implies limn→∞An exists and equals   I    0  P  ... P−1 (cid:4) 0 Are there examples which will cause the eigenvalue condition of this theorem to hold?
The following lemma gives such a condition.
It turns out that if a >0, not just ≥0, then ij the eigenvalue condition of the above theorem is valid.
Lemma 11.1.4 Suppose A=(a ) is a stochastic matrix.
Then λ=1 is an eigenvalue.
If ij a > 0 for all i,j, then if µ is an eigenvalue of A, either |µ| < 1 or µ = 1.
In addition to ij this, if Av=v for a nonzero vector, v∈Rn, then v v ≥0 for all i,j so the components of j i v have the same sign.
Proof: Suppose the matrix satisﬁes ∑ a =1.
ij j ( ) Then if v = 1 ··· 1 T , it is obvious that Av=v.
Therefore, this matrix has λ = 1 as an eigenvalue.
Suppose then that µ is an eigenvalue.
Is |µ| < 1 or µ = 1?
Let v be an eigenvector and let |v | be the largest of the |v |.
i j ∑ µv = a v i ij j j and now multiply both sides by µv to obtain i ∑ ∑ |µ|2|v |2 = a v v µ= a Re(v v µ) i ij j i ij j i ∑j j ≤ a |µ||v |2 =|µ||v |2 ij i i j Therefore, |µ| ≤ 1.
If |µ| = 1, then equality must hold in the above, and so v v µ must be j i real and nonnegative for each j.
In particular, this holds for j =1 which shows µ and hence µ are real.
Thu∑s, in this case, µ=1.
The only other case is where |µ|<1.
If instead, a = 1, consider AT.
Both A and AT have the same characteristic poly- i ij nomial and so their eigenvalues are exactly the same.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation278 MARKOV CHAINS AND MIGRATION PROCESSES Lemma 11.1.5 L∑etAbeanyMarkovmatrixandletv beavectorhavingallitsc∑omponents non negative with v =c.
Then if w=Av, it follows that w ≥0 for all i and w =c.
i i i i i Proof: From the deﬁnition of w, ∑ w ≡ a v ≥0.
i ij j j Also ∑ ∑∑ ∑∑ ∑ w = a v = a v = v =c.
i ij j ij j j i i j j i j The following theorem about limits is now easy to obtain.
Theorem 11.1.6 Suppose A is a Markov matrix (The sum over a column equals 1) in which a >0 for all i,j and suppose w is a vector.
Then for each i, ij ( ) lim Akw =v k→∞ i i where Av=v.
In words, Akw always con∑verges to a steady state.
In addition to this, if the vector, w satisﬁ∑es wi ≥0 for all i and iwi =c, then the vector v will also satisfy the conditions, v ≥0, v =c.
i i i Proof:ByLemma11.1.4,sinceeacha >0,theeigenvaluesareeither1orhaveabsolute ij valuelessthan1.
Therefore,theclaimedlimitexistsbyTheorem11.1.3.
Theassertionthat the components are nonnegative and sum to c follows from Lemma 11.1.5.
That Av=v follows from v= lim Anw= lim An+1w=A lim Anw=Av.
(cid:4) n→∞ n→∞ n→∞ It is not hard to generalize the conclusion of this theorem to regular Markov processes.
Corollary 11.1.7 Suppose A is a regular Markov matrix, on for which the entries of Ak are all positive for some k, and suppose w is a vector.
Then for each i, lim (Anw) =v n→∞ i i where Av=v.
In words, Anw always co∑nverges to a steady state.
In addition to this, if the vector w satisﬁ∑es wi ≥0 for all i and iwi =c, Then the vector v will also satisfy the conditions v ≥0, v =c.
i i i Proof: Let the entries of Ak be all positive.
Now suppose that a ≥ 0 for all i,j and ij A = (a ) is a transition matrix.
Then if B = (b ) is a transition matrix with b > 0 for ij ij ij all ij, it follows that BA is a transition matrix which has strictly positive entries.
The ijth entry of BA is ∑ b a >0, ik kj k Thus, from Lemma 11.1.4, Ak has an eigenvalue equal to 1 for all k suﬃciently large, and all the other eigenvalues have absolute value strictly less than 1.
The same must be true of A, for if λ is an eigenvalue of A with |λ| = 1, then λk is an eigenvalue for Ak and so, for all k large enough, λk =1 which is absurd unless λ=1.
By Theorem 11.1.3, limn→∞Anw exists.
The rest follows as in Theorem 11.1.6.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation11.2.
MIGRATION MATRICES 279 11.2 Migration Matrices Deﬁnition 11.2.1 Let n locations be denoted by the numbers 1,2, ··· ,n. Also suppose it is the case that each year a denotes the proportion of residents in location j which move to ij locationi.Alsosuppo∑senooneescapesoremigratesfromwithoutthesenlocations.
Thislast assumption requires a = 1.
Thus (a ) is a Markov matrix referred to as a migration i ij ij matrix.
Ifv=(x1,··· ,xn)T wherexiisthepopulationoflocati∑oniatagiveninstant,youobtain the population of location i one year later by computing a x =(Av) .
Therefore, the ( ) j ij j i population of location i after k years is Akv .
Furthermore, Corollary 11.1.7 can be used i topredictinthecasewhereAisregularwhatthelongtimepopulationwillbeforthegiven locations.
As an example of the above, consider the case where n=3 and the migration matrix is of the form   .6 0 .1   .2 .8 0 .
.2 .2 .9 Now     2 .6 0 .1 .38 .02 .15     .2 .8 0 = .28 .64 .02 .2 .2 .9 .34 .34 .83 ( ) and so the Markov matrix is regular.
Therefore, Akv will converge to the ith component i of a steady state.
It follows the steady state can be obtained from solving the system .6x+.1z =x .2x+.8y =y .2x+.2y+.9z =z alongwiththestipulationthatthesumofx,y,andz mustequaltheconstantvaluepresent at the beginning of the process.
The solution to this system is {y =x,z =4x,x=x}.
If the total population at the beginning is 150,000, then you solve the following system y =x z =4x x+y+z =150000 whose solution is easily seen to be {x=25000,z =100000,y =25000}.
Thus, after a long time there would be about four times as many people in the third location as in either of the other two.
11.3 Markov Chains A random variable is just a function which can have certain values which have probabilities associated with them.
Thus it makes sense to consider the probability that the random variable has a certain value or is in some set.
The idea of a Markov chain is a sequence of random variables, {X } which can be in any of a collection of states which can be labeled n with nonnegative integers.
Thus you can speak of the probability the random variable, X n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation280 MARKOV CHAINS AND MIGRATION PROCESSES is in state i.
The probability that X is in state j given that X is in state i is called n+1 n a one step transition probability.
When this probability does not depend on n it is called stationary and this is the case of interest here.
Since this probability does not depend on n it can be denoted by p .
Here is a simple example called a random walk.
ij Example 11.3.1 Let there be n points, x , and consider a process of something moving i randomly from one point to another.
Suppose X is a sequence of random variables which n has values {1,2,··· ,n} where X =i indicates the process has arrived at the ith point.
Let n pij be the probability that Xn+1 has the value j∑given that Xn has the value i.
Since Xn+1 must have some value, it must be the case that a =1.
Note this says that the sum over j ij a row equals 1 and so the situation is a little diﬀerent than the above in which the sum was over a column.
As an example, let x ,x ,x ,x be four points taken in order on R and suppose x 1 2 3 4 1 and x are absorbing.
This means that p = 0 for all k ̸= 4 and p = 0 for all k ̸= 1.
4 4k 1k Otherwise, you can move either to the left or to the right with probability 1.
The Markov 2 matrix associated with this situation is   1 0 0 0    .5 0 .5 0   .
0 .5 0 .5 0 0 0 1 Deﬁnition 11.3.2 Let the stationary transition probabilities, p be deﬁned above.
The ij resulting matrix having p as its ijth entry is called the matrix of transition probabilities.
ij The sequence of random variables for which these p are the transition probabilities is called ij a Markov chain.
The matrix of transition probabilities is called a stochastic matrix.
The next proposition is fundamental and shows the signiﬁcance of the powers of the matrix of transition probabilities.
Proposition 11.3.3 Let pn denote the probability that X is in state j given that X was ij n 0 in state i.
Then pn is the ijth entry of the matrix Pn where P =(p ).
ij ij Proof: This is clearly true if n = 1 and follows from the deﬁnition of the pij.∑Suppose true for n. Then the probability that X is at j given that X was at i equals pnp n+1 0 k ik kj because X must have some value, k, and so this represents all possible ways to go from i n to j.
You can go from i to 1 in n steps with probability p and then from 1 to j in one step i1 with probability p and so the probability of this is pnp but you can also go from i to 2 1j i1 1j and then from 2 to j and from i to 3 and then from 3 to j etc.
Thus the sum of these is just what is given and represents the probability of X having the value j given X has n+1 0 the value i.
(cid:4) Intheaboverandomwalkexample,letstakeapowerofthetransitionprobabilitymatrix to determine what happens.
Rounding oﬀ to two decimal places,     20 1 0 0 0 1 0 0 0  .5 0 .5 0   .67 9.5×10−7 0 .33   0 .5 0 .5  = .33 0 9.5×10−7 .67 .
0 0 0 1 0 0 0 1 Thus p is about 2/3 while p is about 1/3 and terms like p are verysmall.
Yousee this 21 32 22 seems to be converging to the matrix   1 0 0 0  2 0 0 1   31 0 0 23 .
3 3 0 0 0 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation11.3.
MARKOV CHAINS 281 After many iterations of the process, if you start at 2 you will end up at 1 with probability 2/3andat4withprobability1/3.Thismakesgoodintuitivesensebecauseitistwiceasfar from 2 to 4 as it is from 2 to 1.
Theorem 11.3.4 The eigenvalues of   0 p 0 ··· 0  q 0 p ··· 0   0 q 0 ... ...   ... 0 ... ... p  .
.
0 .
0 q 0 have absolute value less than 1.
Here p+q =1 and both p,q >0.
Proof: By Gerschgorin’s theorem, if λ is an eigenvalue, then |λ|≤ 1.
Now suppose v is an eigenvector for λ.
Then     pv v 2 1      qv1+pv3   v2   .
  .
 Av= .
=λ .
  .
  .
     qvn−2+pvn vn−1 qvn−1 vn Suppose |λ| = 1.
Then the top row shows p|v | = |v | so |v | < |v |.
Suppose |v | < |v | < 2 1 1 2 1 2 ···<|v | for some k <n.
Then k |λvk|=|vk|≤q|vk−1|+p|vk+1|<q|vk|+p|vk+1| and so subtracting q|v | from both sides, k p|v |<p|v | k k+1 showing {|v |}n is an increasing sequence.
Now a contradiction results on the last line k k=1 which requires |vn−1|>|vn|.
Therefore, |λ|<1 for any eigenvalue of the above matrix.
(cid:4) Corollary 11.3.5 Let p,q be positive numbers and let p+q =1.
The eigenvalues of   a p 0 ··· 0  q a p ··· 0   0 q a ... ...   ... 0 ... ... p  .
.
0 .
0 q a are all strictly closer than 1 to a.
That is, whenever λ is an eigenvalue, |λ−a|<1 have absolute value less than 1.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation282 MARKOV CHAINS AND MIGRATION PROCESSES Proof: Let A be the above matrix and suppose Ax=λx.
Then letting A′ denote   0 p 0 ··· 0  q 0 p ··· 0   0 q 0 ... ... ,  ... 0 ... ... p  .
.
0 .
0 q 0 it follows A′x=(λ−a)x and so from the above theorem, |λ−a|<1.
(cid:4) Example 11.3.6 In the gambler’s ruin problem a gambler plays a game with someone, say a casino, until he either wins all the other’s money or loses all of his own.
A simple version of this is as follows.
Let X denote the amount of money the gambler has.
Each time the k game is played he wins with probability p ∈ (0,1) or loses with probability (1−p) ≡ q.
In case he wins, his money increases to X +1 and if he loses, his money decreases to X −1.
k k The transition probability matrix P, describing this situation is as follows.
  1 0 0 0 ··· 0 0  q 0 p 0 ··· 0 0   .
  0 q 0 p ··· 0 ..  P = 0 0 q 0 ... ... 0  (11.2)  ... ... 0 ... ... p 0     ..  0 0 .
0 q 0 p 0 0 0 0 0 0 1 Here the matrix is b+1×b+1 because the possible values of X are all integers from 0 up k to b.
The 1 in the upper left corner corresponds to the gambler’s ruin.
It involves X = 0 k so he has no money left.
Once this state has been reached, it is not possible to ever leave it.
This is indicated by the row of zeros to the right of this entry the kth of which gives the probability of going from state 1 corresponding to no money to state k1.
In this case 1 is a repeated root of the characteristic equation of multiplicity 2 and all the other eigenvalues have absolute value less than 1.
To see that this is the case, note that the characteristic polynomial is of the form   −λ p 0 ··· 0  q −λ p ··· 0  (1−λ)2det 0 q −λ ... ...   ... 0 ... ... p  .
0 .. 0 q −λ 1Noonewillgivethegamblermoney.
Thisiswhytheonlyreasonablenumberforentriesinthisrowto therightof1is0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation11.3.
MARKOV CHAINS 283 and the factor after (1−λ)2 has zeros which are in absolute value less than 1.
Its zeros are the eigenvalues of the matrix   0 p 0 ··· 0  q 0 p ··· 0  A≡ 0 q 0 ... ...   ... 0 ... ... p  .
.
0 .
0 q 0 and by Corollary 11.3.5 these all have absolute value less than 1.
Therefore, byTheorem11.1.3limn→∞Pn exists.
Thecaseoflimn→∞pnj0 isparticularly interesting because it gives the probability that, starting with an amount j, the gambler eventually ends up at 0 and is ruined.
From the matrix, it follows pn = qpn−1 +ppn−1 for j ∈[1,b−1], j0 (j−1)0 (j+1)0 pn = 1, and pn =0.
00 b0 To simplify the notation, deﬁne Pj ≡limn→∞pnj0 as the probability of ruin given the initial fortune of the gambler equals j.
Then the above simpliﬁes to Pj = qPj−1+pPj+1 for j ∈[1,b−1], (11.3) P = 1, and P =0.
0 b Now, knowing that P exists, it is not too hard to ﬁnd it from (11.3).
This equation is j called a diﬀerence equation and there is a standard procedure for ﬁnding solutions of these.
You try a solution of the form P = xj and then try to ﬁnd x such that things work out.
j Therefore, substitute this in to the ﬁrst equation of (11.3) and obtain xj =qxj−1+pxj+1.
Therefore, px2−x+q =0 and so in case p̸=q, you can use the fact that p+q =1 to obtain ( √ ) ( √ ) 1 1 x = 1+ (1−4pq) or 1− (1−4pq) 2p 2p ( √ ) ( √ ) 1 1 = 1+ (1−4p(1−p)) or 1− (1−4p(1−p)) 2p 2p q = 1 or .
p ( ) j Now it follows that both P = 1 and P = q satisfy the diﬀerence equation (11.3).
j j p Therefore, anything of the form ( ) q j α+β (11.4) p willsatisfythisequation.
Finda,bsuchthatthisalsosatisﬁesthesecondequationof(11.3).
Thus it is required that ( ) q b α+β =1, α+β =0 p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation284 MARKOV CHAINS AND MIGRATION PROCESSES and so α+β =1 ( ) b α+β q =0 p { } (q)b Solutionis: β =− 1 ,α= p .Substitutingthisinto(11.4)andsimplifying, −1+(q)b −1+(q)b p p yields the following in the case that p̸=q.
pb−jqj −qb P = (11.5) j pb−qb Note that pb−jqj −qb b−j lim = .
p→q pb−qb b Thusasthegamebecomesmorefairinthesensetheprobabilitiesofwinningbecomecloser to 1/2, the probability of ruin given an initial amount j is b−j.
b Alternatively, you could consider the diﬀerence equation directly in the case where p= q =1/2.
In this case, you can see that two solutions to the diﬀerence equation 1 1 Pj = 2Pj−1+ 2Pj+1 for j ∈[1,b−1], (11.6) P = 1, and P =0.
0 b are P =1 and P =j.
This leads to a solution to the above of j j b−j P = .
(11.7) j b This last case is pretty interesting because it shows, for example that if the gambler starts with a fortune of 1 so that he starts at state j =1, then his probability of losing all is b−1 which might be quite large, especially if the other player has a lot of money to begin b with.
Asthegamblerstartswithmoreandmoremoney,hisprobabilityoflosingeverything does decrease.
11.4 Exercises 1.
Suppose the migration matrix for three locations is   .5 0 .3   .3 .8 0 .
.2 .2 .7 Find a comparison for the populations in the three locations after a long time.
∑ 2.
Show that if a = 1, then if A = (a ), then the sum of the entries of Av equals i ij ij the sum of the entries of v. Thus it does not matter whether a ≥0 for this to be so.
ij 3.
IfAsatisﬁestheconditionsoftheaboveproblem,canitbeconcludedthatlimn→∞An exists?
4.
Give an example of a non regular Markov matrix which has an eigenvalue equal to −1.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation11.4.
EXERCISES 285 5.
ShowthatwhenaMarkovmatrixisnondefective,alloftheabovetheorycanbeproved very easily.
In particular, prove the theorem about the existence of limn→∞An if the eigenvalues are either 1 or have absolute value less than 1.
6.
Find a formula for An where   5 −1 0 −1  52 02 0 −4  A= 7 −1 1 −5  2 2 2 2 7 −1 0 −2 2 2 Doeslimn→∞An exist?
Notethatalltherowssumto1.
Hint: Thismatrixissimilar to a diagonal matrix.
The eigenvalues are 1,−1,1,1.
2 2 7.
Find a formula for An where   2 −1 1 −1  4 02 12 −4  A= 5 −1 1 −2  2 2 3 −1 1 −2 2 2 Note that the rows sum to 1 in this matrix also.
Hint: This matrix is not similar to a diagonal matrix but you can ﬁnd the Jordan form and consider this in order to obtain a formula for this product.
The eigenvalues are 1,−1,1,1.
2 2 8.
Find limn→∞An if it exists for the matrix   1 −1 −1 0  −21 12 −21 0  A= 12 21 32 0  2 2 2 3 3 3 1 2 2 2 The eigenvalues are 1,1,1,1.
2 9.
Give an example of a matrix A which has eigenvalues which are either equal to 1,−1, orhaveabsolutevaluestrictlylessthan1butwhichhasthepropertythatlimn→∞An does not exist.
10.
If A is an n×n matrix such that all the eigenvalues have absolute value less than 1, show limn→∞An =0.
11.
Find an example of a 3 × 3 matrix A such that limn→∞An does not exist but limr→∞A5r does exist.
12.
If A is a Markov matrix and B is similar to A, does it follow that B is also a Markov matrix?
13.
∑In Theorem 11∑.1.3 suppose everything is unchanged except that you assume either a ≤ 1 or a ≤ 1.
Would the same conclusion be valid?
What if you don’t j ij i ij insist that each a ≥0?
Would the conclusion hold in this case?
ij 14.
Let V be an n dimensional vector space and let x ∈ V and x̸=0.
Consider β ≡ x x,Ax,··· ,Am−1x where ( ) Amx∈span x,Ax,··· ,Am−1x Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation286 MARKOV CHAINS AND MIGRATION PROCESSES and m{ is the smallest s}uch that the above inclusion in the span takes place.
Show that x,Ax,··· ,Am−1x must be linearly independent.
Next suppose {v ,··· ,v } 1 n ilsinaeabrlaysicsofmorbiVna.tiCoonnosfidver,Aβvvi,a·s··ju,Astmd−i1svcusfsoerdm, haasvisnmgallelnagsthpomssii.blTe.hLusetApmiv(λi)isbae i i i vi the monic polynomial which expresses this linear combination.
Thus p (A)v = 0 vi i and the degree of p (λ) is as small as possible for this to take place.
Show that the vi minimal polynomial for A must be the monic polynomial which is the least common multiple of these polynomials p (λ).
vi 15.
IfA isacomplex Hermitian n×n matrixwhichhas alleigenvaluesnonnegative, show that there exists a complex Hermitian matrix B such that BB =A.
16.
↑Suppose A,B are n×n real Hermitian matrices and they both have all nonnegative eigenvalues.
Showthatdet(A+B)≥det(A)+det(B).
Hint: Usetheaboveproblem and the Cauchy Binet theorem.
Let P2 = A,Q2 = B where P,Q are Hermitian and nonnegative.
Then ( ) ( ) P A+B = P Q .
Q ( ) α c∗ 17.
SupposeB = isan(n+1)×(n+1)Hermitiannonnegativematrixwhere b A α is a scalar and A is n×n.
Show that α must be real, c=b, and A = A∗,A is nonnegative, and that if α=0, then b=0.
Otherwise, α>0.
18.
↑If A is an n×n complex Hermitian and nonnegative matrix, show that there exists an upper triangular matrix B such that B∗B =A.
Hint: Prove this by induction.
It isobviouslytrueifn=1.Nowifyouhavean(n+1)×((n+1)Hermi)tiannonnegative α2 αb∗ matrix, then from the above problem, it is of the form ,α real.
αb A 19.
↑ Suppose A is a nonnegative Hermitian matrix which is partitioned as ( ) A A A= 11 12 A A 21 22 where A ,A are square matrices.
Show that det(A) ≤ det(A )det(A ).
Hint: 11 22 11 22 Use the above problem to factor A getting ( )( ) B∗ 0∗ B B A= 11 11 12 B∗ B∗ 0 B 12 22 22 Next argue that A =B∗ B ,A =B∗ B +B∗ B .
Use the Cauchy Binet theo- 11 11 11 22 12 12 22 22 rem to argue that det(A ) = det(B∗ B +B∗ B ) ≥ det(B∗ B ).
Then explain 22 12 12 22 22 22 22 why ∗ ∗ det(A) = det(B )det(B )det(B )det(B ) 11 22 11 22 ∗ ∗ = det(B B )det(B B ) 11 11 22 22 20.
↑ProvetheinequalityofHadamard.
IfAisaHermitianmatrixwhichisnonnegative, then ∏ det(A)≤ A ii i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationInner Product Spaces 12.1 General Theory It is assumed here that the ﬁeld of scalars is either R or C. The usual example of an inner productspaceisCn orRn asdescribedearlier.
However,therearemanyotherinnerproduct spaces and the topic is of such importance that it seems appropriate to discuss the general theory of these spaces.
Deﬁnition 12.1.1 A vector space X is said to be a normed linear space if there exists a function, denoted by |·| : X →[0,∞) which satisﬁes the following axioms.
1.
|x|≥0 for all x∈X, and |x|=0 if and only if x=0.
2.
|ax|=|a||x| for all a∈F.
3.
|x+y|≤|x|+|y|.
This function |·| is called a norm.
The notation ||x|| is also often used.
Not all norms are created equal.
There are many geometric properties which they may or may not possess.
There is also a concept called an inner product which is discussed next.
It turns out that the best norms come from an inner product.
Deﬁnition 12.1.2 A mapping (·,·) : V ×V → F is called an inner product if it satisﬁes the following axioms.
1.
(x,y)=(y,x).
2.
(x,x)≥0 for all x∈V and equals zero if and only if x=0.
3.
(ax+by,z)=a(x,z)+b(y,z) whenever a,b∈F.
Note that 2 and 3 imply (x,ay+bz)=a(x,y)+b(x,z).
Then a norm is given by (x,x)1/2 ≡|x|.
It remains to verify this really is a norm.
Deﬁnition 12.1.3 A normed linear space in which the norm comes from an inner product as just described is called an inner product space.
287 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation288 INNER PRODUCT SPACES Example 12.1.4 Let V =Cn with the inner product given by ∑n (x,y)≡ x y .
k k k=1 This is an example of a complex inner product space already discussed.
Example 12.1.5 Let V = Rn, ∑n (x,y)=x·y≡ x y .
j j j=1 This is an example of a real inner product space.
Example 12.1.6 Let V be any ﬁnite dimensional vector space and let {v ,··· ,v } be a 1 n basis.
Decree that { 1 if i=j (v ,v )≡δ ≡ i j ij 0 if i̸=j and deﬁne the inner product by ∑n (x,y)≡ xiyi i=1 where ∑n ∑n x= xiv , y = yiv .
i i i=1 i=1 The above is well deﬁned because {v ,··· ,v } is a basis.
Thus the components x 1 n i associated with any given x∈V are uniquely determined.
Thisexampleshowsthereisnolossofgeneralitywhenstudyingﬁnitedimensionalvector spaceswithﬁeldofscalarsRorCinassumingthevectorspaceisactuallyaninnerproduct space.
The following theorem was presented earlier with slightly diﬀerent notation.
Theorem 12.1.7 (Cauchy Schwarz) In any inner product space |(x,y)|≤|x||y|.
where |x|≡(x,x)1/2.
Proof: Let ω ∈C,|ω|=1, and ω(x,y)=|(x,y)|=Re(x,yω).
Let F(t)=(x+tyω,x+tωy).
Then from the axioms of the inner product, F(t)=|x|2+2tRe(x,ωy)+t2|y|2 ≥0.
This yields |x|2+2t|(x,y)|+t2|y|2 ≥0.
If |y|=0, then the inequality requires that |(x,y)|=0 since otherwise, you could pick large negative t and contradict the inequality.
If |y| > 0, it follows from the quadratic formula that 4|(x,y)|2−4|x|2|y|2 ≤0.
(cid:4) Earlier it was claimed that the inner product deﬁnes a norm.
In this next proposition this claim is proved.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.2.
THE GRAM SCHMIDT PROCESS 289 Proposition 12.1.8 For an inner product space, |x|≡(x,x)1/2 does specify a norm.
Proof: All the axioms are obvious except the triangle inequality.
To verify this, |x+y|2 ≡ (x+y,x+y)≡|x|2+|y|2+2Re(x,y) ≤ |x|2+|y|2+2|(x,y)| ≤ |x|2+|y|2+2|x||y|=(|x|+|y|)2.
(cid:4) The best norms of all are those which come from an inner product because of the following identity which is known as the parallelogram identity.
Proposition 12.1.9 If (V,(·,·)) is an inner product space then for |x| ≡ (x,x)1/2, the following identity holds.
|x+y|2+|x−y|2 =2|x|2+2|y|2.
It turns out that the validity of this identity is equivalent to the existence of an inner product which determines the norm as described above.
These sorts of considerations are topics for more advanced courses on functional analysis.
Deﬁnition 12.1.10 A basis for an inner product space, {u ,··· ,u } is an orthonormal 1 n basis if { 1 if k =j (u ,u )=δ ≡ .
k j kj 0 if k ̸=j Note that if a list of vectors satisﬁes the above condition for being an orthonormal set, then the list of vectors is automatically linearly independent.
To see this, suppose ∑n cju =0 j j=1 Then taking the inner product of both sides with u , k ∑n ∑n 0= cj(u ,u )= cjδ =ck.
j k jk j=1 j=1 12.2 The Gram Schmidt Process Lemma 12.2.1 Let X be a ﬁnite dimensional inner product space of dimension n whose basis is {x ,··· ,x }.
Then there exists an orthonormal basis for X, {u ,··· ,u } which has 1 n 1 n the property that for each k ≤n, span(x ,··· ,x )=span(u ,··· ,u ).
1 k 1 k Proof: Let{x ,··· ,x }beabasisforX.Letu ≡x /|x |.Thusfork =1,span(u )= 1 n 1 1 1 1 span(x ) and {u } is an orthonormal set.
Now suppose for some k < n, u , ···, u have 1 1 1 k been chosen such that (u ,u )=δ and span(x ,··· ,x )=span(u ,··· ,u ).
Then deﬁne j l jl 1 k 1 k ∑ x − k (x ,u )u uk+1 ≡ (cid:12)(cid:12) k+1 ∑j=1 k+1 j j(cid:12)(cid:12), (12.1) (cid:12)x − k (x ,u )u (cid:12) k+1 j=1 k+1 j j where the denominator is not equal to zero because the x form a basis and so j x ∈/ span(x ,··· ,x )=span(u ,··· ,u ) k+1 1 k 1 k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation290 INNER PRODUCT SPACES Thus by induction, u ∈span(u ,··· ,u ,x )=span(x ,··· ,x ,x ).
k+1 1 k k+1 1 k k+1 Also, x ∈ span(u ,··· ,u ,u ) which is seen easily by solving (12.1) for x and it k+1 1 k k+1 k+1 follows span(x ,··· ,x ,x )=span(u ,··· ,u ,u ).
1 k k+1 1 k k+1 If l≤k,   ∑k (u ,u ) = C(x ,u )− (x ,u )(u ,u ) k+1 l k+1 l k+1 j j l j=1   ∑k = C(x ,u )− (x ,u )δ  k+1 l k+1 j lj j=1 = C((x ,u )−(x ,u ))=0.
k+1 l k+1 l The vectors, {u }n , generated in this way are therefore an orthonormal basis because j j=1 each vector has unit length.
(cid:4) The process by which these vectors were generated is called the Gram Schmidt process.
The following corollary is obtained from the above process.
Corollary 12.2.2 Let X be a ﬁnite dimensional inner product space of dimension n whose basis is {u ,··· ,u ,x ,··· ,x }.
Then if {u ,··· ,u } is orthonormal, then the Gram 1 k k+1 n 1 k Schmidt process applied to the given list of vectors in order leaves {u ,··· ,u } unchanged.
1 k Lemma 12.2.3 Suppose {u }n is an orthonormal basis for an inner product space X. j j=1 Then for all x∈X, ∑n x= (x,u )u .
j j j=1 Proof: By assumption that this is an orthonormal basis, ∑n z δ}j|l { (x,u )(u ,u )=(x,u ).
j j l l j=1 ∑ Letting y = n (x,u )u , it follows k=1 k k ∑n (x−y,u ) = (x,u )− (x,u )(u ,u ) j j k k j k=1 = (x,u )−(x,u )=0 j j for all j.
Hence, for any choice of scalars c1, ···, cn,   ∑n x−y, cju =0 j j=1 and so (x−y,z)=0 for all z ∈X.
Thus this holds in particular for z =x−y.
Therefore, x = y.
(cid:4) The following theorem is of fundamental importance.
First note that a subspace of an inner product space is also an inner product space because you can use the same inner product.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.2.
THE GRAM SCHMIDT PROCESS 291 Theorem 12.2.4 Let M be a subspace of X, a ﬁnite dimensional inner product space and let {x }m be an orthonormal basis for M. Then if y ∈X and w ∈M, i i=1 { } |y−w|2 =inf |y−z|2 :z ∈M (12.2) if and only if (y−w,z)=0 (12.3) for all z ∈M.
Furthermore, ∑m w = (y,x )x (12.4) i i i=1 is the unique element of M which has this property.
It is called the orthogonal projection.
Proof: Let t∈R.
Then from the properties of the inner product, |y−(w+t(z−w))|2 =|y−w|2+2tRe(y−w,w−z)+t2|z−w|2.
(12.5) If (y−w,z)=0 for all z ∈M, then letting t=1, the middle term in the above expression vanishes and so |y−z|2 is minimized when z =w.
Conversely, if (12.2) holds, then the middle term of (12.5) must also vanish since other- wise, you could choose small real t such that |y−w|2 >|y−(w+t(z−w))|2.
Here is why.
If Re(y−w,w−z) < 0, then let t be very small and positive.
The middle term in (12.5) will then be more negative than the last term is positive and the right side of this formula will then be less than |y−w|2.
If Re(y−w,w−z)>0 then choose t small and negative to achieve the same result.
It follows, letting z =w−z that 1 Re(y−w,z )=0 1 for all z ∈M.
Now letting ω ∈C be such that ω(y−w,z )=|(y−w,z )|, 1 1 1 |(y−w,z )|=(y−w,ωz )=Re(y−w,ωz )=0, 1 1 1 which proves the ﬁrst part of the theorem since z is arbitrary.
1 It only remains to verify that w given in (12.4) satisﬁes (12.3) and is the only point of M which does so.
To do this, note that if c ,d are scalars, then the properties of the inner i i product and the fact the {x } are orthonormal implies i   ∑m ∑m ∑   c x , d x = c d .
i i j j i i i=1 j=1 i By Lemma 12.2.3, ∑ z = (z,x )x i i i and so ( ) ( ) ∑m ∑m ∑m y− (y,x )x ,z = y− (y,x )x , (z,x )x i i i i i i i=1 i=1 i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation292 INNER PRODUCT SPACES   ∑m ∑m ∑m = (z,x )(y,x )− (y,x )x , (z,x )x  i i i i j j i=1 i=1 j=1 ∑m ∑m = (z,x )(y,x )− (y,x )(z,x )=0.
i i i i i=1 i=1 This shows w given in (12.4) does minimize the function, z → |y−z|2 for z ∈ M. It only remains to verify uniqueness.
Suppose than that w ,i = 1,2 minimizes this function of z i for z ∈M.
Then from what was shown above, |y−w |2 = |y−w +w −w |2 1 2 2 1 = |y−w |2+2Re(y−w ,w −w )+|w −w |2 2 2 2 1 2 1 = |y−w |2+|w −w |2 ≤|y−w |2, 2 2 1 2 thelastequalsignholdingbecausew isaminimizerandthelastinequalityholdingbecause 2 w minimizes.
(cid:4) 1 12.3 Riesz Representation Theorem Thenexttheoremisoneofthemostimportantresultsinthetheoryofinnerproductspaces.
It is called the Riesz representation theorem.
Theorem 12.3.1 Let f ∈ L(X,F) where X is an inner product space of dimension n. Then there exists a unique z ∈X such that for all x∈X, f(x)=(x,z).
Proof: First I will verify uniqueness.
Suppose z works for j =1,2.
Then for all x∈X, j 0=f(x)−f(x)=(x,z −z ) 1 2 and so z =z .
1 2 It remains to verify existence.
By Lemma 12.2.1, there exists an orthonormal basis, {u }n .
Deﬁne j j=1 ∑n z ≡ f(u )u .
j j j=1 Then using Lemma 12.2.3,   ∑n ∑n   (x,z) = x, f(u )u = f(u )(x,u ) j j j j j=1 j=1   ∑n = f (x,u )u =f(x).
(cid:4) j j j=1 Corollary 12.3.2 Let A∈L(X,Y) where X and Y are two inner product spaces of ﬁnite dimension.
Then there exists a unique A∗ ∈L(Y,X) such that ∗ (Ax,y) =(x,A y) (12.6) Y X for all x∈X and y ∈Y.
The following formula holds ∗ ∗ ∗ (αA+βB) =αA +βB Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.3.
RIESZ REPRESENTATION THEOREM 293 Proof: Let f ∈L(X,F) be deﬁned as y f (x)≡(Ax,y) .
y Y Then by the Riesz representation theorem, there exists a unique element of X, A∗(y) such that ∗ (Ax,y) =(x,A (y)) .
Y X It only remains to verify that A∗ is linear.
Let a and b be scalars.
Then for all x∈X, (x,A∗(ay +by )) ≡(Ax,(ay +by )) 1 2 X 1 2 Y ≡a(Ax,y )+b(Ax,y )≡ 1 2 ∗ ∗ ∗ ∗ a(x,A (y ))+b(x,A (y ))=(x,aA (y )+bA (y )).
1 2 1 2 Since this holds for every x, it follows ∗ ∗ ∗ A (ay +by )=aA (y )+bA (y ) 1 2 1 2 which shows A∗ is linear as claimed.
Consider the last assertion that ∗ is conjugate linear.
( ) x,(αA+βB)∗y ≡((αA+βB)x,y) ∗ ∗ = α(Ax,y)+β(Bx,y)=α(x,A y)+β(x,B y) ( ) ( ( ) ) ∗ ∗ ∗ ∗ = (x,αA y)+ x,βA y = x, αA +βA y .
Since x is arbitrary, ( ) ∗ ∗ ∗ (αA+βB) y = αA +βA y and since this is true for all y, (αA+βB)∗ =αA∗+βA∗.
(cid:4) Deﬁnition 12.3.3 Thelinearmap, A∗ iscalledtheadjointofA.InthecasewhenA:X → X and A=A∗, A is called a self adjoint map.
Such a map is also called Hermitian.
( ) Theorem 12.3.4 Let M be an m×n matrix.
Then M∗ = M T in words, the transpose of the conjugate of M is equal to the adjoint.
Proof: Using the deﬁnition of the inner product in Cn, ∑ ∑ ∑ (Mx,y)=(x,M∗y)≡ x (M∗) y = (M∗) y x .
i ij j ij j i i j i,j Also ∑∑ (Mx,y)= M y x .
ji j i j i Since x,y are arbitrary vectors, it follows that M = (M∗) and so, taking conjugates of ji ij both sides, ∗ M =M ij ji which gives the conclusion of the theorem.
The next theorem is interesting.
You have a p dimensional subspace of Fn where F=R or C. Of course this might be “slanted”.
However, there is a linear transformation Q which preserves distances which maps this subspace to Fp.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation294 INNER PRODUCT SPACES Theorem 12.3.5 SupposeV isasubspaceofFn havingdimensionp≤n.Thenthereexists a Q∈L(Fn,Fn) such that QV ⊆span(e ,··· ,e ) 1 p and |Qx|=|x| for all x.
Also ∗ ∗ Q Q=QQ =I.
Proof: ByLemma 12.2.1thereexists anorthonormal basisfor V,{v }p .
Byusing the i i=1 Gram Schmidt process this may be extended to an orthonormal basis of the whole space, Fn, {v ,··· ,v ,v ,··· ,v }.
1 p p+1 n ∑ Now deﬁne Q∈L(Fn,Fn) by Q(v )≡e and extend linearly.
If n x v is an arbitrary i i i=1 i i element of Fn, (cid:12) ( )(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:12) ∑n (cid:12)(cid:12)2 (cid:12)(cid:12)∑n (cid:12)(cid:12)2 ∑n (cid:12)(cid:12)∑n (cid:12)(cid:12)2 (cid:12)Q x v (cid:12) =(cid:12) x e (cid:12) = |x |2 =(cid:12) x v (cid:12) .
(cid:12) i i (cid:12) (cid:12) i i(cid:12) i (cid:12) i i(cid:12) i=1 i=1 i=1 i=1 It remains to verify that Q∗Q=QQ∗ =I.
To do so, let x,y∈Fn.
Then (Q(x+y),Q(x+y))=(x+y,x+y).
Thus |Qx|2+|Qy|2+2Re(Qx,Qy)=|x|2+|y|2+2Re(x,y) and since Q preserves norms, it follows that for all x,y∈Fn, ∗ Re(Qx,Qy)=Re(x,Q Qy)=Re(x,y).
Thus Re(x,Q∗Qy−y)=0 (12.7) for all x,y.
Let ω be a complex number such that |ω|=1 and ω(x,Q∗Qy−y)=|(x,Q∗Qy−y)|.
Then from (12.7), 0 = Re(ωx,Q∗Qy−y)=Reω(x,Q∗Qy−y) = |(x,Q∗Qy−y)| and since x is arbitrary, it follows that for all y, Q∗Qy−y=0 Thus ∗ I =Q Q.
Similarly QQ∗ =I.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.4.
THE TENSOR PRODUCT OF TWO VECTORS 295 12.4 The Tensor Product Of Two Vectors Deﬁnition 12.4.1 Let X and Y be inner product spaces and let x∈X and y ∈Y.
Deﬁne the tensor product of these two vectors, y⊗x, an element of L(X,Y) by y⊗x(u)≡y(u,x) .
X This is also called a rank one transformation because the image of this transformation is contained in the span of the vector, y.
The veriﬁcation that this is a linear map is left to you.
Be sure to verify this!
The following lemma has some of the most important properties of this linear transformation.
Lemma 12.4.2 Let X,Y,Z be inner product spaces.
Then for α a scalar, (α(y⊗x))∗ =αx⊗y (12.8) (z⊗y )(y ⊗x)=(y ,y )z⊗x (12.9) 1 2 2 1 Proof: Let u∈X and v ∈Y.
Then (α(y⊗x)u,v)=(α(u,x)y,v)=α(u,x)(y,v) and (u,αx⊗y(v))=(u,α(v,y)x)=α(y,v)(u,x).
Therefore, this veriﬁes (12.8).
To verify (12.9), let u∈X.
(z⊗y )(y ⊗x)(u)=(u,x)(z⊗y )(y )=(u,x)(y ,y )z 1 2 1 2 2 1 and (y ,y )z⊗x(u)=(y ,y )(u,x)z.
2 1 2 1 Since the two linear transformations on both sides of (12.9) give the same answer for every u∈X, it follows the two transformations are the same.
(cid:4) Deﬁnition 12.4.3 Let X,Y be two vector spaces.
Then deﬁne for A,B ∈ L(X,Y) and α∈F, new elements of L(X,Y) denoted by A+B and αA as follows.
(A+B)(x)≡Ax+Bx, (αA)x≡α(Ax).
Theorem 12.4.4 Let X and Y be ﬁnite dimensional inner product spaces.
Then L(X,Y) is a vector space with the above deﬁnition of what it means to multiply by a scalar and add.
Let {v ,··· ,v } be an orthonormal basis for X and {w ,··· ,w } be an orthonormal basis 1 n 1 m for Y.
Then a basis for L(X,Y) is {w ⊗v :i=1,··· ,n, j =1,··· ,m}.
j i Proof: It is obvious that L(X,Y) is a vector space.
It remains to verify the given set is a basis.
Consider the following:    ∑ A− (Av ,w )w ⊗v v ,w =(Av ,w )− k l l k p r p r k,l Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation296 INNER PRODUCT SPACES ∑ (Av ,w )(v ,v )(w ,w ) k l p k l r k,l ∑ =(Av ,w )− (Av ,w )δ δ p r k l pk rl k,l =(Av ,w )−(Av ,w )=0.
p r p r ∑ Letting A− (Av ,w )w ⊗v = B, this shows that Bv = 0 since w is an arbitrary k,l k l l k p r element of the basis for Y.
Since v is an arbitrary element of the basis for X, it follows p B =0 as hoped.
This has shown {w ⊗v :i=1,··· ,n, j =1,··· ,m} spans L(X,Y).
j i It only remains to verify the w ⊗v are linearly independent.
Suppose then that j i ∑ c w ⊗v =0 ij j i i,j Then do both sides to v .
By deﬁnition this gives s ∑ ∑ ∑ 0= c w (v ,v )= c w δ = c w ij j s i ij j si sj j i,j i,j j Now the vectors {w ,··· ,w } are independent because it is an orthonormal set and so the 1 m aboverequiresc =0foreachj.Sinceswasarbitrary,thisshowsthelineartransformations, sj {w ⊗v } form a linearly independent set.
(cid:4) j i Note this shows the dimension of L(X,Y) = nm.
The theorem is also of enormous importance because it shows you can always consider an arbitrary linear transformation as a sum of rank one transformations whose properties are easily understood.
The following theorem is also of great interest.
∑ Theorem 12.4.5 Let A= c w ⊗v ∈L(X,Y) where as before, the vectors, {w } are i,j ij i j i an orthonormal basis for Y and the vectors, {v } are an orthonormal basis for X.
Then if j the matrix of A has entries M , it follows that M =c .
ij ij ij Proof: Recall ∑ Av ≡ M w i ki k k Also ∑ ∑ Av = c w ⊗v (v )= c w (v ,v ) i kj k j i kj k i j ∑k,j ∑ k,j = c w δ = c w kj k ij ki k k,j k Therefore, ∑ ∑ M w = c w ki k ki k k k and so M =c for all k. This happens for each i.
(cid:4) ki ki 12.5 Least Squares A common problem in experimental work is to ﬁnd a straight line which approximates as well as possible a collection of points in the plane {(x ,y )}p .
The usual way of dealing i i i=1 with these problems is by the method of least squares and it turns out that all these sorts of approximation problems can be reduced to Ax=b where the problem is to ﬁnd the best x for solving this equation even when there is no solution.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.5.
LEAST SQUARES 297 Lemma 12.5.1 LetV andW beﬁnitedimensionalinnerproductspacesandletA:V →W be linear.
For each y ∈W there exists x∈V such that |Ax−y|≤|Ax −y| 1 for all x ∈ V. Also, x ∈ V is a solution to this minimization problem if and only if x is a 1 solution to the equation, A∗Ax=A∗y.
Proof: By Theorem 12.2.4 on Page 291 there exists a point, Ax , in the ﬁnite dimen- 0 sional subspace, A(V), of W such that for all x ∈ V,|Ax−y|2 ≥ |Ax −y|2.
Also, from 0 this theorem, this happens if and only if Ax −y is perpendicular to every Ax ∈ A(V).
0 Therefore, the solution is characterized by (Ax −y,Ax) = 0 for all x ∈ V which is the 0 same as saying (A∗Ax −A∗y,x)=0 for all x∈V.
In other words the solution is obtained 0 by solving A∗Ax =A∗y for x .
(cid:4) 0 0 Consider the problem of ﬁnding the least squares regression line in statistics.
Suppose you have given points in the plane, {(x ,y )}n and you would like to ﬁnd constants m i i i=1 and b such that the line y = mx+b goes through all these points.
Of course this will be impossible ingeneral.
Therefore, tryto ﬁndm,b suchthatyoudothe bestyoucanto solve the system     y x 1 ( ) 1 1  .
  .
.
 m  .
= .
.
 .
.
.
b y x 1 n n (cid:12)  (cid:12) (cid:12)(cid:12) ( ) y1 (cid:12)(cid:12)2 which is of the form y = Ax.
In other words try to make (cid:12)(cid:12)A m − .. (cid:12)(cid:12) as small (cid:12) b .
(cid:12) (cid:12) (cid:12) y n aspossible.
Accordingtowhatwasjustshown,itisdesiredtosolvethefollowingformand b.
  ( ) y 1 A∗A m =A∗ .. .
b .
y n Since A∗ =AT in this case, ( ∑ ∑ )( ) ( ∑ ) ∑ni=1x2i ni=1xi m = ∑ni=1xiyi n x n b n y i=1 i i=1 i Solving this system of equations for m and b, ∑ ∑ ∑ −( n x )( n y )+( n x y )n m= i=∑1 i i=1 i ∑ i=1 i i ( n x2)n−( n x )2 i=1 i i=1 i and ∑ ∑ ∑ ∑ −( n x ) n x y +( n y ) n x2 b= i=1 ∑i i=1 i i ∑ i=1 i i=1 i.
( n x2)n−( n x )2 i=1 i i=1 i One could clearly do a least squares ﬁt for curves of the form y = ax2+bx+c in the same way.
In this case you solve as well as possible for a,b, and c the system      x2 x 1 y 1 1 a 1  .. .. ..  b = ..  .
.
.
.
x2 x 1 c y n n n using the same techniques.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation298 INNER PRODUCT SPACES 12.6 Fredholm Alternative Again The best context in which to study the Fredholm alternative is in inner product spaces.
This is done here.
Deﬁnition 12.6.1 Let S be a subset of an inner product space, X. Deﬁne S⊥ ≡{x∈X :(x,s)=0 for all s∈S}.
The following theorem also follows from the above lemma.
It is sometimes called the Fredholm alternative.
Theorem 12.6.2 LetA:V →W whereAislinearandV andW areinnerproductspaces.
Then A(V)=ker(A∗)⊥.
Proof: Let y =Ax so y ∈A(V).
Then if A∗z =0, ∗ (y,z)=(Ax,z)=(x,A z)=0 showing that y ∈ker(A∗)⊥.
Thus A(V)⊆ker(A∗)⊥.
Now suppose y ∈ ker(A∗)⊥.
Does there exists x such that Ax = y?
Since this might not be immediately clear, take the least squares solution to the problem.
Thus let x be a solution to A∗Ax=A∗y.
It followsA∗(y−Ax)=0 and so y−Ax∈ker(A∗) which implies from the assumption about y that (y−Ax,y) = 0.
Also, since Ax is the closest point to y in A(V), Theorem 12.2.4 on Page 291 implies that (y−Ax,Ax ) = 0 for all x ∈ V. 1 1 z =}|0 { In particular this is true for x = x and so 0 = (y−Ax,y)−(y−Ax,Ax) = |y−Ax|2, 1 showing that y =Ax.
Thus A(V)⊇ker(A∗)⊥.
(cid:4) Corollary 12.6.3 Let A,V, and W be as described above.
If the only solution to A∗y = 0 is y =0, then A is onto W. Proof: IftheonlysolutiontoA∗y =0is y =0,thenker(A∗)={0}andsoeveryvector from W is contained in ker(A∗)⊥ and by the above theorem, this shows A(V)=W.
(cid:4) 12.7 Exercises 1.
Find the best solution to the system x+2y =6 2x−y =5 3x+2y =0 2.
Find an orthonormal basis for R3, {w ,w ,w } given that w is a multiple of the 1 2 3 1 vector (1,1,2).
3.
Suppose A=AT is a symmetric real n×n matrix which has all positive eigenvalues.
Deﬁne (x,y)≡(Ax,y).
Show this is an inner product on Rn.
What does the Cauchy Schwarz inequality say in this case?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.7.
EXERCISES 299 4.
Let ||x|| ≡max{|x |:j =1,2,··· ,n}.
∞ j ( ) Show this is a norm on Cn.
Here x= x ··· x T. Show 1 n ||x|| ≤|x|≡(x,x)1/2 ∞ where the above is the usual inner product on Cn.
5.
Let ∑n ||x|| ≡ |x |.
1 j j=1 ( ) Show this is a norm on Cn.
Here x= x ··· x T. Show 1 n ||x|| ≥|x|≡(x,x)1/2 1 where the above is the usual inner product on Cn.
Show there cannot exist an inner product such that this norm comes from the inner product as described above for inner product spaces.
6.
Show that if ||·|| is any norm on any vector space, then |||x||−||y|||≤||x−y||.
7.
Relax the assumptions in the axioms for the inner product.
Change the axiom about (x,x)≥0andequals0ifandonlyifx=0tosimplyread(x,x)≥0.ShowtheCauchy Schwarz inequality still holds in the following form.
|(x,y)|≤(x,x)1/2(y,y)1/2.
8.
Let H be an inner product space and let {u }n be an orthonormal basis for H. k k=1 Show ∑n (x,y)= (x,u )(y,u ).
k k k=1 9.
Let the vector space V consist of real polynomials of degree no larger than 3.
Thus a typical vector is a polynomial of the form a+bx+cx2+dx3.
For p,q ∈V deﬁne the inner product, ∫ 1 (p,q)≡ p(x)q(x)dx.
0 Show this is indeed an inner product{.
Then sta}te the Cauchy Schwarz inequality in terms of this inner product.
Show 1,x,x2,x3 is a basis for V. Finally, ﬁnd an orthonormal basis for V. This is an example of some orthonormal polynomials.
10.
Let P denote the polynomials of degree no larger than n−1 which are deﬁned on an n interval [a,b].
Let {x ,··· ,x } be n distinct points in [a,b].
Now deﬁne for p,q ∈P , 1 n n ∑n (p,q)≡ p(x )q(x ) j j j=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation300 INNER PRODUCT SPACES Show this yields an inner product on P .
Hint: Most of the axioms are obvious.
The n onewhichsays(p,p)=0ifandonlyifp=0istheonlyinterestingone.
Toverifythis one, note that a nonzero polynomial of degree no more than n−1 has at most n−1 zeros.
11.
Let C([0,1]) denote the vector space of continuous real valued functions deﬁned on [0,1].
Let the inner product be given as ∫ 1 (f,g)≡ f(x)g(x)dx 0 Show this is an inner product.
Also let V be the subspace described in Problem 9.
Using the result of this problem, ﬁnd the vector in V which is closest to x4.
12.
A regular Sturm Liouville problem involves the diﬀerential equation, for an un- known function of x which is denoted here by y, (p(x)y′)′+(λq(x)+r(x))y =0, x∈[a,b] and it is assumed that p(t),q(t) > 0 for any t ∈ [a,b] and also there are boundary conditions, ′ C y(a)+C y (a) = 0 1 2 ′ C y(b)+C y (b) = 0 3 4 where C2+C2 >0, and C2+C2 >0.
1 2 3 4 There is an immense theory connected to these important problems.
The constant, λ iscalledaneigenvalue.
Showthatifyisasolutiontotheaboveproblemcorresponding to λ= λ and if z is a solution corresponding to λ= λ ̸=λ , then 1 2 1 ∫ b q(x)y(x)z(x)dx=0.
(12.10) a and this deﬁnes an inner product.
Hint: Do something like this: ′ ′ (p(x)y ) z+(λ q(x)+r(x))yz = 0, 1 ′ ′ (p(x)z ) y+(λ q(x)+r(x))zy = 0.
2 Now subtract and either use integration by parts or show (p(x)y′)′z−(p(x)z′)′y =((p(x)y′)z−(p(x)z′)y)′ andthenintegrate.
Usetheboundaryconditionstoshowthaty′(a)z(a)−z′(a)y(a)= 0 and y′(b)z(b)−z′(b)y(b)=0.
The formula, (12.10) is called an orthogonality rela- tion.
It turns out there are typically inﬁnitely many eigenvalues and it is interesting to write given functions as an inﬁnite series of these “eigenfunctions”.
13.
Consider the continuous functions deﬁned on [0,π], C([0,π]).
Show ∫ π (f,g)≡ fgdx 0 {√ } ∞ is an inner product on this vector space.
Show the functions 2 sin(nx) are π n=1 an orthonormal set.
What does this mean about the dimension of the vector space Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.7.
EXERCISES 301 (√ √ ) C([0,π])?
Now let V =span 2 sin(x),··· , 2 sin(Nx) .
For f ∈C([0,π]) ﬁnd N π π aformulaforthevectorinV whichisclosesttof withrespecttothenormdetermined N from the above inner product.
This is called the Nth partial sum of the Fourier series of f. An important problem is to determine whether and in what way this Fourier series converges to the function f. The norm which comes from this inner product is sometimes called the mean square norm.
14.
Consider the subspace V ≡ ker(A) where   1 4 −1 −1    2 1 2 3  A=  4 9 0 1 5 6 3 4 Find an orthonormal basis for V. Hint: You might ﬁrst ﬁnd a basis and then use the Gram Schmidt procedure.
15.
The Gram Schmidt process starts with a basis for a subspace {v ,··· ,v } and pro- 1 n duces an orthonormal basis for the same subspace {u ,··· ,u } such that 1 n span(v ,··· ,v )=span(u ,··· ,u ) 1 k 1 k for each k. Show that in the case of Rm the QR factorization does the same thing.
More speciﬁcally, if ( ) A= v ··· v 1 n and if ( ) A=QR≡ q ··· q R 1 n then the vectors {q ,··· ,q } is an orthonormal set of vectors and for each k, 1 n span(q ,··· ,q )=span(v ,··· ,v ) 1 k 1 k 16.
Verify the parallelogram identify for any inner product space, |x+y|2+|x−y|2 =2|x|2+2|y|2.
Why is it called the parallelogram identity?
17.
Let H be an inner product space and let K ⊆H be a nonempty convex subset.
This means that if k ,k ∈K, then the line segment consisting of points of the form 1 2 tk +(1−t)k for t∈[0,1] 1 2 is also contained in K. Suppose for each x∈H, there exists Px deﬁned to be a point ofK closesttox.ShowthatPxisuniquesothatP actuallyisamap.
Hint: Suppose z andz bothworkasclosestpoints.
Considerthemidpoint,(z +z )/2andusethe 1 2 1 2 parallelogram identity of Problem 16 in an auspicious manner.
18.
In the situation of Problem 17 suppose K is a closed convex subset and that H is complete.
This means every Cauchy sequence converges.
Recall from calculus a sequence {k } is a Cauchy sequence if for every ε > 0 there exists N such that n ε whenever m,n>N , it follows |k −k |<ε.
Let {k } be a sequence of points of K ε m n n such that lim |x−k |=inf{|x−k|:k ∈K} n n→∞ Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation302 INNER PRODUCT SPACES This is called a minimizing sequence.
Show there exists a unique k ∈ K such that limn→∞|kn−k| and that k =Px.
That is, there exists a well deﬁned projection map onto the convex subset of H. Hint: Use the parallelogram identity in an auspicious manner to show {k } is a Cauchy sequence which must therefore converge.
Since K n is closed it follows this will converge to something in K which is the desired vector.
19.
LetH beaninnerproductspacewhichisalsocompleteandletP denotetheprojection map onto a convex closed subset, K. Show this projection map is characterized by the inequality Re(k−Px,x−Px)≤0 for all k ∈ K. That is, a point z ∈ K equals Px if and only if the above variational inequality holds.
This is what that inequality is called.
This is because k is allowed to vary and the inequality continues to hold for all k ∈K.
20.
Using Problem 19 and Problems 17 - 18 show the projection map, P onto a closed convex subset is Lipschitz continuous with Lipschitz constant 1.
That is |Px−Py|≤|x−y| 21.
Give an example of two vectors in R4 x,y and a subspace V such that x·y = 0 but Px·Py̸=0 where P denotes the projection map which sends x to its closest point on V. 22.
Suppose you are given the data, (1,2),(2,4),(3,8),(0,0).
Find the linear regression line using the formulas derived above.
Then graph the given data along with your regression line.
23.
Generalizethe least squares procedure to the situation in whichdata is given and you desiretoﬁtitwithanexpressionoftheformy =af(x)+bg(x)+cwheretheproblem would be to ﬁnd a,b and c in order to minimize the error.
Could this be generalized to higher dimensions?
How about more functions?
24.
LetA∈L(X,Y)whereX andY areﬁnitedimensionalvectorspaceswiththedimen- sion of X equal to n. Deﬁne rank(A) ≡ dim(A(X)) and nullity(A) ≡ dim(ker(A)).
Show that nullity(A)+rank(A)=dim(X).
Hint: Let {x }r be a basis for ker(A) i i=1 and let {x }r ∪ {y }n−r be a basis for X.
Then show that {Ay }n−r is linearly i i=1 i i=1 i i=1 independent and spans AX.
25.
LetAbeanm×nmatrix.
ShowthecolumnrankofAequalsthecolumnrankofA∗A.
Next verify column rank of A∗A is no larger than column rank of A∗.
Next justify the following inequality to conclude the column rank of A equals the column rank of A∗.
rank (A)=rank (A∗A)≤rank (A∗)≤ =rank (AA∗)≤rank (A).
Hint: Start with an orthonormal basis, {Ax }r of A(Fn) and verify {A∗Ax }r j j=1 j j=1 is a basis for A∗A(Fn).
26.
Let A be a real m × n matrix and let A = QR be the QR factorization with Q orthogonalandRuppertriangular.
Showthatthereexistsasolutionxtotheequation RTRx=RTQTb andthatthissolutionisalsoaleastsquaressolutiondeﬁnedabovesuchthatATAx= ATb.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.8.
THE DETERMINANT AND VOLUME 303 12.8 The Determinant And Volume The determinant is the essential algebraic tool which provides a way to give a uniﬁed treat- mentoftheconceptofpdimensionalvolumeofaparallelepipedinRM.
Hereisthedeﬁnition of what is meant by such a thing.
Deﬁnition 12.8.1 Let u ,··· ,u be vectors in RM,M ≥p.
The parallelepiped determined 1 p by these vectors will be denoted by P (u ,··· ,u ) and it is deﬁned as 1 p   ∑p  P (u ,··· ,u )≡ s u :s ∈[0,1] .
1 p  j j j  j=1 The volume of this parallelepiped is deﬁned as volume of P (u ,··· ,u )≡v(P (u ,··· ,u ))≡(det(u ·u ))1/2.
1 p 1 p i j If the vectors are dependent, this deﬁnition will give the volume to be 0.
∑ First lets observe the last assertion is true.
Say u = α u .
Then the ith row is i j̸=i j j a linear combination of the other rows and so from the properties of the determinant, the determinant of this matrix is indeed zero as it should be.
A parallelepiped is a sort of a squashed box.
Here is a picture which shows the relation- ship between P (u1,··· ,up−1) and P (u1,··· ,up).
6 N (cid:30) u p 3 θ P(u1,··· ,up−1) - In a sense, we can deﬁne the volume any way we want but if it is to be reasonable, the followingrelationshipmusthold.
TheappropriatedeﬁnitionofthevolumeofP (u ,··· ,u ) 1 p in terms of P (u1,··· ,up−1) is v(P (u1,··· ,up))=|up||cos(θ)|v(P (u1,··· ,up−1)) (12.11) In the case where p = 1, the parallelepiped P (v) consists of the single vector and the one ( ) dimensionalvolumeshouldbe|v|= vTv 1/2.Nowhavingmadethisdeﬁnition,Iwillshow that this is the appropriate deﬁnition of p dimensional volume for every p. Deﬁnition 12.8.2 Let {u ,··· ,u } be vectors.
Then 1 p v(P (u ,··· ,u ))≡ 1 p    uT 1/2 1 ≡det u..T2 ( u1 u2 ··· up ) .
uT p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation304 INNER PRODUCT SPACES As just pointed out, this is the only reasonable deﬁnition of volume in the case of one vector.
The next theorem shows that it is the only reasonable deﬁnition of volume of a parallelepiped in the case of p vectors because (12.11) holds.
Theorem 12.8.3 With the above deﬁnition of volume, (12.11) holds.
Proof: Tocheckwhetherthisisso,itisnecessarytoﬁnd|cos(θ)|.Thisinvolvesﬁnding the vector perpendicular to P (u1,··· ,up−1).
Let {w1,··· ,wp} be an orthonormal basis for span(u ,··· ,u ) such that span(w ,··· ,w )=span(u ,··· ,u ) for each k ≤p.
Such 1 p 1 k 1 k an orthonormal basis exists because of the Gram Schmidt procedure.
First note that since {w } is an orthonormal basis for span(u ,··· ,u ), k 1 p ∑p u = (u ·w )w j j k k k=1 and if i,j ≤k ∑k u ·u = (u ·w )(u ·w ) j i j k i k k=1 Therefore, for each k ≤p    uT 1 det u..T2 ( u1 u2 ··· uk ) .
uT k is the determinant of a matrix whose ijth entry is ∑k uTu =u ·u = (u ·w )(w ·u ) i j i j i r r j r=1 Thus this matrix is the product of the two k×k matrices, one which is the transpose of the other.
  (u ·w ) (u ·w ) ··· (u ·w ) 1 1 1 2 1 k  (u2·w1) (u2·w2) ··· (u2·wk)   .
.
.
·  .
.
.
 .
.
.
(u ·w ) (u ·w ) ··· (u ·w )  k 1 k 2 k k  (u ·w ) (u ·w ) ··· (u ·w ) 1 1 2 1 k 1  (u1·w2) (u2·w2) ··· (uk·w2)   .
.
.
  .
.
.
 .
.
.
(u ·w ) (u ·w ) ··· (u ·w ) 1 k 2 k k k It follows    uT 1 det u..T2 ( u1 u2 ··· uk ) .
uT k    (u ·w ) (u ·w ) ··· (u ·w ) 2 1 1 1 2 1 k   (u2·w1) (u2·w2) ··· (u2·wk)  =det .
.
.
   .
.
.
 .
.
.
(u ·w ) (u ·w ) ··· (u ·w ) k 1 k 2 k k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation12.8.
THE DETERMINANT AND VOLUME 305 and so from the deﬁnition, v(P (u ,··· ,u ))= 1 k (cid:12)  (cid:12) (cid:12)(cid:12) (u1·w1) (u1·w2) ··· (u1·wk) (cid:12)(cid:12) (cid:12)(cid:12)  (u2·w1) (u2·w2) ··· (u2·wk) (cid:12)(cid:12) (cid:12)det .
.
.
(cid:12) (cid:12)  .
.
.
(cid:12) (cid:12) .
.
.
(cid:12) (cid:12) (u ·w ) (u ·w ) ··· (u ·w ) (cid:12) k 1 k 2 k k Now consider the vector   w w ··· w 1 2 p  (u1·w1) (u1·w2) ··· (u1·wp)  N≡det .
.
.
  .
.
.
 .
.
.
(up−1·w1) (up−1·w2) ··· (up−1·wp) which results from formally expanding along the top row.
Note that from what was just discussed, v(P (u1,··· ,up−1))=±A1p Now it follows from the formula for expansion of a determinant along the top row that for each j ≤p−1 ∑p ∑p N·u = (u ·w )(N·w )= (u ·w )A j j k k j k 1k k=1 k=1 where A is the 1kth cofactor of the above matrix.
Thus if j ≤p−1 1k   (u ·w ) (u ·w ) ··· (u ·w ) j 1 j 2 j p  (u1·w1) (u1·w2) ··· (u1·wp)  N·uj =det .. .. .. =0 .
.
.
(up−1·w1) (up−1·w2) ··· (up−1·wp) because the matrix has two equal rows while if j = p, the above discussion shows N·u p equals±v(P (u ,··· ,u )).
Therefore, Npointsinthedirectionofthenormalvectorinthe 1 p above picture or else it points in the opposite direction to this vector.
From the geometric description of the dot product, |N·u | |cos(θ)|= p |u ||N| p and it follows |N·u | |up||cos(θ)|v(P (u1,··· ,up−1))=|up||u ||Np|v(P (u1,··· ,up−1)) p v(P (u ,··· ,u )) = 1|N| p v(P (u1,··· ,up−1)) Now at this point, note that from the construction, w ·u =0 whenever k ≤p−1 because p k uk ∈span(w1,··· ,wp−1).
Therefore, |N|=|A1p|=v(P (u1,··· ,up−1)) and so the above reduces to |up||cos(θ)|v(P (u1,··· ,up−1))=v(P (u1,··· ,up)).
(cid:4) The theorem shows that the only reasonable deﬁnition of p dimensional volume of a parallelepiped is the one given in the above deﬁnition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation306 INNER PRODUCT SPACES 12.9 Exercises 1.
Here are three vectors in R4 : (1,2,0,3)T ,(2,1,−3,2)T ,(0,0,1,2)T. Find the three dimensional volume of the parallelepiped determined by these three vectors.
2.
Here are two vectors in R4 :(1,2,0,3)T ,(2,1,−3,2)T. Find the volume of the paral- lelepiped determined by these two vectors.
3.
Here are three vectors in R2 : (1,2)T ,(2,1)T ,(0,1)T. Find the three dimensional volume of the parallelepiped determined by these three vectors.
Recall that from the above theorem, this should equal 0.
4.
Find the equation of the plane through the three points (1,2,3),(2,−3,1),(1,1,7).
5.
Let T map a vector space V to itself.
Explain why T is one to one if and only if T is onto.
It is in the text, but do it again in your own words.
6.
↑LetallmatricesbecomplexwithcomplexﬁeldofscalarsandletAbeann×nmatrix and B a m×m matrix while X will be an n×m matrix.
The problem is to consider solutions to Sylvester’s equation.
Solve the following equation for X AX−XB =C whereC isanarbitraryn×mmatrix.
Showthereexistsauniquesolutionifandonly if σ(A)∩σ(B)=∅.
Hint: If q(λ) is a polynomial, show ﬁrst that if AX−XB =0, then q(A)X −Xq(B) = 0.
Next deﬁne the linear map T which maps the n×m matrices to the n×m matrices as follows.
TX ≡AX−XB Show that the only solution to TX =0 is X =0 so that T is one to one if and only if σ(A)∩σ(B)=∅.Dothisbyusingtheﬁrstpartforq(λ)thecharacteristicpolynomial for B and then use the Cayley Hamilton theorem.
Explain why q(A)−1 exists if and only if the condition σ(A)∩σ(B)=∅.
7.
Compare Deﬁnition 12.8.2 with the Binet Cauchy theorem, Theorem 3.3.14.
What is the geometric meaning of the Binet Cauchy theorem in this context?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationSelf Adjoint Operators 13.1 Simultaneous Diagonalization Recall the following deﬁnition of what it means for a matrix to be diagonalizable.
Deﬁnition 13.1.1 Let A be an n×n matrix.
It is said to be diagonalizable if there exists an invertible matrix S such that S−1AS =D where D is a diagonal matrix.
Also, here is a useful observation.
Observation 13.1.2 If A is an n×n matrix and AS =SD for D a diagonal matrix, then each column of S is an eigenvector or else it is the zero vector.
This follows from observing that for s the kth column of S and from the way we multiply matrices, k As =λ s k k k It is sometimes interesting to consider the problem of ﬁnding a single similarity trans- formation which will diagonalize all the matrices in some set.
Lemma 13.1.3 Let A be an n×n matrix and let B be an m×m matrix.
Denote by C the matrix ( ) A 0 C ≡ .
0 B Then C is diagonalizable if and only if both A and B are diagonalizable.
Proof: Suppose SA−1ASA = DA and SB−1BSB = DB where(DA and DB)are diagonal S 0 matrices.
You should use block multiplication to verify that S ≡ A is such that 0 S B S−1CS =D , a diagonal matrix.
C Conversely, suppose C is diagonalized by S = (s ,··· ,s ).
Thus S has columns s .
1 n+m i For each of these columns, write in the form ( ) x s = i i y i where x ∈ Fn and where y ∈Fm.
The result is i i ( ) S S S = 11 12 S S 21 22 307 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation308 SELF ADJOINT OPERATORS where S is an n×n matrix and S is an m×m matrix.
Then there is a diagonal matrix 11 22 ( ) D 0 D =diag(λ ,··· ,λ )= 1 1 n+m 0 D 2 such that ( )( ) A 0 S S 11 12 0 B S S ( )(21 22 ) S S D 0 = 11 12 1 S S 0 D 21 22 2 Hence by block multiplication AS =S D , BS =S D 11 11 1 22 22 2 BS =S D , AS =S D 21 21 1 12 12 2 Itfollowseachof the x isan eigenvectorofA orelseis thezero vectorand thateachofthe i y is an eigenvector of B or is the zero vector.
If there are n linearly independent x , then i i A is diagonalizable by Theorem 9.3.12 on Page 9.3.12.
The row rank of the matrix (x ,··· ,x ) must be n because if this is not so, the rank 1 n+m of S would be less than n+m which would mean S−1 does not exist.
Therefore, since the column rank equals the row rank, this matrix has column rank equal to n and this means therearenlinearlyindependenteigenvectorsofAimplyingthatAisdiagonalizable.
Similar reasoning applies to B.
(cid:4) The following corollary follows from the same type of argument as the above.
Corollary 13.1.4 Let A be an n ×n matrix and let C denote the block diagonal k k k ( ) ( ) ∑r ∑r n × n k k k=1 k=1 matrix given below.
  A 0 1   C ≡ ... .
0 A r Then C is diagonalizable if and only if each A is diagonalizable.
k Deﬁnition 13.1.5 A set, F of n×n matrices is said to be simultaneously diagonalizable if andonlyifthereexistsasingleinvertiblematrixS suchthatforeveryA∈F, S−1AS =D A where D is a diagonal matrix.
A Lemma 13.1.6 If F is a set of n×n matrices which is simultaneously diagonalizable, then F is a commuting family of matrices.
Proof: Let A,B ∈ F and let S be a matrix which has the property that S−1AS is a diagonal matrix for all A ∈ F. Then S−1AS = D and S−1BS = D where D and D A B A B are diagonal matrices.
Since diagonal matrices commute, AB = SD S−1SD S−1 =SD D S−1 A B A B = SD D S−1 =SD S−1SD S−1 =BA.
B A B A Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.1.
SIMULTANEOUS DIAGONALIZATION 309 Lemma 13.1.7 Let D be a diagonal matrix of the form   λ I 0 ··· 0 D ≡ 10...n1 λ2.I..n2 ...... 0... , (13.1) 0 ··· 0 λ I r nr where I denotes the n ×n identity matrix and λ ̸= λ for i ̸= j and suppose B is a ni i i i j matrix which commutes with D. Then B is a block diagonal matrix of the form   B 0 ··· 0 1 B = 0... B..2.
...... 0...  (13.2) 0 ··· 0 B r where B is an n ×n matrix.
i i i Proof: Let B =(B ) where B =B a block matrix as above in (13.2).
ij ii i   B B ··· B 11 12 1r    B21 B22 ... B2r   ... ... ... ...  B B ··· B r1 r2 rr Then by block multiplication, since B is given to commute with D, λ B =λ B j ij i ij Therefore, if i̸=j,B =0.
(cid:4) ij Lemma 13.1.8 Let F denote a commuting family of n×n matrices such that each A∈F is diagonalizable.
Then F is simultaneously diagonalizable.
Proof: First note that if every matrix in F has only one eigenvalue, there is nothing to prove.
This is because for A such a matrix, S−1AS =λI and so A=λI Thus all the matrices in F are diagonal matrices and you could pick any S to diagonalize themall.
Therefore,withoutlossofgenerality,assumesomematrixinF hasmorethanone eigenvalue.
The signiﬁcant part of the lemma is proved by induction on n. If n=1, there is nothing to prove because all the 1×1 matrices are already diagonal matrices.
Suppose then that the theorem is true for all k ≤ n−1 where n ≥ 2 and let F be a commuting family of diagonalizable n×n matrices.
Pick A ∈ F which has more than one eigenvalue and let S be an invertible matrix such that S−1AS = D where D is of the form given in (13.1).
By permuting the columns of S there is no l{oss of generality i}n assuming D has this form.
Now denote by Fe the collection of matrices, S−1CS :C ∈F .
Note Fe features the single matrix S. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation310 SELF ADJOINT OPERATORS It follows easily that Fe is also a commuting family of diagonalizable matrices.
By Lemma 13.1.7 every B ∈ Fe is of the form given in (13.2) because each of these commutes with D described above as S−1AS and so by block multiplication, the diagonal blocks B i corresponding to diﬀerent B ∈ Fe commute.
ByCorollary13.1.4eachoftheseblocksisdiagonalizable.
ThisisbecauseB isknownto beso.
Therefore,byinduction,sincealltheblocksarenolargerthann−1×n−1thanksto theassumptionthatAhasmorethanoneeigenvalue,thereexistinvertiblen ×n matrices, i i T such that T−1B T is a diagonal matrix whenever B is one of the matrices making up i i i i i the block diagonal of any B ∈F.
It follows that for T deﬁned by   T 0 ··· 0 1 T ≡ 0... .T.2.
...... 0... , 0 ··· 0 T r then T−1BT = a diagonal matrix for every B ∈ Fe including D. Consider ST.
It follows that for all C ∈F, somzeth}i|ngi{nFe T−1 S−1CS T =(ST)−1C(ST)= a diagonal matrix.
(cid:4) Theorem 13.1.9 Let F denote a family of matrices which are diagonalizable.
Then F is simultaneously diagonalizable if and only if F is a commuting family.
Proof:IfF isacommutingfamily,itfollowsfromLemma13.1.8thatitissimultaneously diagonalizable.
Ifitissimultaneouslydiagonalizable,thenitfollowsfromLemma13.1.6that it is a commuting family.
(cid:4) 13.2 Schur’s Theorem Recallthatforalineartransformation,L∈L(V,V)forV aﬁnitedimensionalinnerproduct space, it could be represented in the form ∑ L= l v ⊗v ij i j ij where {v ,··· ,v } is an orthonormal basis.
Of course diﬀerent bases will yield diﬀerent 1 n matrices,(l ).Schur’stheoremgivestheexistenceofabasisinaninnerproductspacesuch ij that (l ) is particularly simple.
ij Deﬁnition 13.2.1 Let L∈L(V,V) where V is vector space.
Then a subspace U of V is L invariant if L(U)⊆U.
In what follows, F will be the ﬁeld of scalars, usually C but maybe something else.
Theorem 13.2.2 Let L ∈ L(H,H) for H a ﬁnite dimensional inner product space such that the restriction of L∗to every L invariant subspace has its eigenvalues in F. Then there exist constants, c for i≤j and an orthonormal basis, {w }n such that ij i i=1 ∑n ∑j L= c w ⊗w ij i j j=1i=1 The constants, c are the eigenvalues of L. ii Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.2.
SCHUR’S THEOREM 311 Proof: If dim(H) = 1, let H = span(w) where |w| = 1.
Then Lw = kw for some k. Then L=kw⊗w because by deﬁnition, w⊗w(w)=w.
Therefore, the theorem holds if H is 1 dimensional.
Now suppose the theorem holds for n−1=dim(H).
Let w be an eigenvector for L∗.
n Dividing by its length, it can be assumed |w | = 1.
Say L∗w = µw .
Using the Gram n n n Schmidt process, there exists an orthonormal basis for H of the form {v1,··· ,vn−1,wn}.
Then ∗ (Lv ,w )=(v ,L w )=(v ,µw )=0, k n k n k n which shows L:H1 ≡span(v1,··· ,vn−1)→span(v1,··· ,vn−1).
Denote by L the restriction of L to H .
Since H has dimension n − 1, the induction 1 1 1 hypothesis yields an orthonormal basis, {w1,··· ,wn−1} for H1 such that n∑−1∑j L = c w ⊗w .
(13.3) 1 ij i j j=1 i=1 Then {w ,··· ,w } is an orthonormal basis for H because every vector in 1 n span(v1,··· ,vn−1) has the property that its inner product with w is 0 so in particular, this is true for the n vectors {w1,··· ,wn−1}.
Now deﬁne cin to be the scalars satisfying ∑n Lw ≡ c w (13.4) n in i i=1 and let ∑n ∑j B ≡ c w ⊗w .
ij i j j=1i=1 Then by (13.4), ∑n ∑j ∑n Bw = c w δ = c w =Lw .
n ij i nj in i n j=1i=1 j=1 If 1≤k ≤n−1, ∑n ∑j ∑k Bw = c w δ = c w k ij i kj ik i j=1i=1 i=1 while from (13.3), n∑−1∑j ∑k Lw =L w = c w δ = c w .
k 1 k ij i jk ik i j=1 i=1 i=1 Since L=B on the basis {w ,··· ,w }, it follows L=B.
1 n Itremainstoverifytheconstants,c aretheeigenvaluesofL,solutionsoftheequation, kk det(λI −L)=0.
However, the deﬁnition of det(λI−L) is the same as det(λI−C) where C is the upper triangular matrix which has c for i ≤ j and zeros elsewhere.
This ij equals 0 if and only if λ is one of the diagonal entries, one of the c .
(cid:4) kk Now with the above Schur’s theorem, the following diagonalization theorem comes very easily.
Recall the following deﬁnition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation312 SELF ADJOINT OPERATORS Deﬁnition 13.2.3 Let L∈L(H,H) where H is a ﬁnite dimensional inner product space.
Then L is Hermitian if L∗ =L.
Theorem 13.2.4 Let L∈L(H,H) where H is an n dimensional inner product space.
If L is Hermitian, then all of its eigenvalues λ are real and there exists an orthonormal basis k of eigenvectors {wk} such that ∑ L= λ w ⊗w .
k k k k Proof: By Schur’s theorem, Theorem 13.2.2, there exist l ∈F such that ij ∑n ∑j L= l w ⊗w ij i j j=1i=1 Then by Lemma 12.4.2, ∑n ∑j ∑n ∑j l w ⊗w = L=L∗ = (l w ⊗w )∗ ij i j ij i j j=1i=1 j=1i=1 ∑n ∑j ∑n ∑i = l w ⊗w = l w ⊗w ij j i ji i j j=1i=1 i=1j=1 By independence, if i=j, l =l ii ii and so these are all real.
If i<j, it follows from independence again that l =0 ij because the coeﬃcients corresponding to i<j are all 0 on the right side.
Similarly if i>j, it follows l =0.
Letting λ =l , this shows ij k kk ∑ L= λ w ⊗w k k k k That each of these w is an eigenvector corresponding to λ is obvious from the deﬁnition k k of the tensor product.
(cid:4) 13.3 Spectral Theory Of Self Adjoint Operators The following theorem is about the eigenvectors and eigenvalues of a self adjoint operator.
Such operators are also called Hermitian as in the case of matrices.
The proof given gen- eralizes to the situation of a compact self adjoint operator on a Hilbert space and leads to many very useful results.
It is also a very elementary proof because it does not use the fundamental theorem of algebra and it contains a way, very important in applications, of ﬁnding the eigenvalues.
This proof depends more directly on the methods of analysis than theprecedingmaterial.
TheﬁeldofscalarswillbeRorC.
Thefollowingisusefulnotation.
Deﬁnition 13.3.1 Let X be an inner product space and let S ⊆X.
Then S⊥ ≡{x∈X :(x,s)=0 for all s∈S}.
Note that even if S is not a subspace, S⊥ is.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.3.
SPECTRAL THEORY OF SELF ADJOINT OPERATORS 313 Deﬁnition 13.3.2 A Hilbert space is a complete inner product space.
Recall this means that every Cauchy sequence,{x }, one which satisﬁes n lim |x −x |=0, n m n,m→∞ converges.
It can be shown, although I will not do so here, that for the ﬁeld of scalars either R or C, any ﬁnite dimensional inner product space is automatically complete.
Theorem 13.3.3 Let A∈L(X,X) be self adjoint (Hermitian) where X is a ﬁnite dimen- sional Hilbert space.
Thus A=A∗.
Then there exists an orthonormal basis of eigenvectors, {u }n .
j j=1 Proof: Consider (Ax,x).
This quantity is always a real number because ∗ (Ax,x)=(x,Ax)=(x,A x)=(Ax,x) thanks to the assumption that A is self adjoint.
Now deﬁne λ ≡inf{(Ax,x):|x|=1,x∈X ≡X}.
1 1 Claim: λ is ﬁnite and there exists v ∈X with |v |=1 such that (Av ,v )=λ .
1 1 1 1 1 1 Proof of claim: Let{u }n beanorthonormalbasisforX andforx∈X,let(x ,···, j j=1 1 x ) be deﬁned as the components of the vector x.
Thus, n ∑n x= x u .
j j j=1 Since this is an orthonormal basis, it follows from the axioms of the inner product that ∑n |x|2 = |x |2.
j j=1 Thus   ∑n ∑ ∑   (Ax,x)= x Au , x u = x x (Au ,u ), k k j j k j k j k=1 j=1 k,j a real valued continuous function of (x , ···, x ) which is deﬁned on the compact set 1 n ∑n K ≡{(x ,··· ,x )∈Fn : |x |2 =1}.
1 n j j=1 Therefore, it achieves its minimum from the extreme value theorem.
Then deﬁne ∑n v ≡ x u 1 j j j=1 where (x ,··· ,x ) is the point of K at which the above function achieves its minimum.
1 n This proves the claim.
Continuing with the proof of the theorem, let X ≡{v }⊥.
This is a closed subspace of 2 1 X.
Let λ ≡inf{(Ax,x):|x|=1,x∈X } 2 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation314 SELF ADJOINT OPERATORS As before, there exists v ∈ X such that (Av ,v ) = λ ,λ ≤ λ .
Now let X ≡ {v ,v }⊥ 2 2 2 2 2 1 2 3 1 2 andcontinueinthisway.
Thisleadstoanincreasingsequenceofrealnumbers,{λ }n and k k=1 an orthonormal set of vectors, {v , ···, v }.
It only remains to show these are eigenvectors 1 n and that the λ are eigenvalues.
j Considertheﬁrstofthesevectors.
Lettingw ∈X ≡X,thefunctionoftherealvariable, 1 t, given by (A(v +tw),v +tw) f(t)≡ 1 1 |v +tw|2 1 (Av ,v )+2tRe(Av ,w)+t2(Aw,w) = 1 1 1 |v |2+2tRe(v ,w)+t2|w|2 1 1 achieves its minimum when t = 0.
Therefore, the derivative of this function evaluated at t=0 must equal zero.
Using the quotient rule, this implies, since |v |=1 that 1 2Re(Av ,w)|v |2−2Re(v ,w)(Av ,v ) 1 1 1 1 1 =2(Re(Av ,w)−Re(v ,w)λ )=0.
1 1 1 ThusRe(Av −λ v ,w)=0forallw ∈X.
ThisimpliesAv =λ v .
Toseethis, let w ∈X 1 1 1 1 1 1 be arbitrary and let θ be a complex number with |θ|=1 and |(Av −λ v ,w)|=θ(Av −λ v ,w).
1 1 1 1 1 1 Then ( ) |(Av −λ v ,w)|=Re Av −λ v ,θw =0.
1 1 1 1 1 1 Since this holds for all w, Av =λ v .
1 1 1 Now suppose Av =λ v for all k <m.
Observe that A:X →X because if y ∈X k k k m m m and k <m, (Ay,v )=(y,Av )=(y,λ v )=0, k k k k showing that Ay ∈{v1,··· ,vm−1}⊥ ≡Xm.Thus the same argument just given shows that for all w ∈X , m (Av −λ v ,w)=0.
(13.5) m m m Since Av ∈ X , I can let w = Av −λ v in the above and thereby conclude Av = m m m m m m λ v .
(cid:4) m m Contained in the proof of this theorem is the following important corollary.
Corollary 13.3.4 Let A∈L(X,X) be self adjoint where X is a ﬁnite dimensional Hilbert space.
Then all the eigenvalues are real and for λ ≤ λ ≤ ··· ≤ λ the eigenvalues of A, 1 2 n there exists an orthonormal set of vectors {u ,··· ,u } for which 1 n Au =λ u .
k k k Furthermore, λ ≡inf{(Ax,x):|x|=1,x∈X } k k where Xk ≡{u1,··· ,uk−1}⊥,X1 ≡X.
Corollary 13.3.5 Let A∈L(X,X) be self adjoint (Hermitian) where X is a ﬁnite dimen- sional Hilbert space.
Then the largest eigenvalue of A is given by max{(Ax,x):|x|=1} (13.6) and the minimum eigenvalue of A is given by min{(Ax,x):|x|=1}.
(13.7) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.3.
SPECTRAL THEORY OF SELF ADJOINT OPERATORS 315 Proof: The proof of this is just like the proof of Theorem 13.3.3.
Simply replace inf with sup and obtain a decreasing list of eigenvalues.
This establishes (13.6).
The claim (13.7) follows from Theorem 13.3.3.
Another important observation is found in the following corollary.
∑ Corollary 13.3.6 Let A∈L(X,X) where A is self adjoint.
Then A= λ v ⊗v where i i i i Av =λ v and {v }n is an orthonormal basis.
i i i i i=1 Proof : If v is one of the orthonormal basis vectors, Av =λ v .
Also, k k k k ∑ ∑ λ v ⊗v (v ) = λ v (v ,v ) i i i k i i k i i ∑i = λ δ v =λ v .
i ik i k k i Since the two linear transformations agree on a basis, it follows they must coincide.
(cid:4) By Theorem 12.4.5 this says the matrix of A with respect to this basis {v }n is the i i=1 diagonal matrix having the eigenvalues λ ,··· ,λ down the main diagonal.
1 n The result of Courant and Fischer which follows resembles Corollary 13.3.4 but is more useful because it does not depend on a knowledge of the eigenvectors.
Theorem 13.3.7 Let A∈L(X,X) be self adjoint where X is a ﬁnite dimensional Hilbert space.
Then for λ ≤ λ ≤ ··· ≤ λ the eigenvalues of A, there exist orthonormal vectors 1 2 n {u ,··· ,u } for which 1 n Au =λ u .
k k k Furthermore, { { }} λk ≡ max min (Ax,x):|x|=1,x∈{w1,··· ,wk−1}⊥ (13.8) w1,···,wk−1 where if k =1,{w1,··· ,wk−1}⊥ ≡X.
Proof: FromTheorem13.3.3,thereexisteigenvaluesandeigenvectorswith{u ,··· ,u } 1 n orthonormal and λ ≤λ .
Therefore, by Corollary 13.3.6 i i+1 ∑n A= λ u ⊗u j j j j=1 Fix {w1,··· ,wk−1}.
∑n ∑n (Ax,x)= λ (x,u )(u ,x)= λ |(x,u )|2 j j j j j j=1 j=1 Then let Y ={w1,··· ,wk−1}⊥   ∑n  inf{(Ax,x):|x|=1,x∈Y}=inf λ |(x,u )|2 :|x|=1,x∈Y  j j  j=1   ∑k  ≤inf λ |(x,u )|2 :|x|=1,(x,u )=0 for j >k, and x∈Y .
(13.9)  j j j  j=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation316 SELF ADJOINT OPERATORS Thereasonthisissoisthattheinﬁmumistakenoverasmallerset.
Therefore,theinﬁmum gets larger.
Now (13.9) is no larger than    ∑k  inf λ |(x,u )|2 :|x|=1,(x,u )=0 for j >k, and x∈Y =λ  k j j  k j=1 ∑ because since {u ,··· ,u } is an orthonormal basis, |x|2 = n |(x,u )|2.
It follows since 1 n j=1 j {w1,··· ,wk−1} is arbitrary, { { }} sup inf (Ax,x):|x|=1,x∈{w1,··· ,wk−1}⊥ ≤λk.
(13.10) w1,···,wk−1 However, for each w1,··· ,wk−1, the inﬁmum is achieved so you can replace the inf in the above with min.
In addition to this, it follows from Corollary 13.3.4 that there exists a set, {w1,··· ,wk−1} for which { } inf (Ax,x):|x|=1,x∈{w1,··· ,wk−1}⊥ =λk.
Pick{w1,··· ,wk−1}={u1,··· ,uk−1}.Therefore,thesupin(13.10)isachievedandequals λ and (13.8) follows.
(cid:4) k The following corollary is immediate.
Corollary 13.3.8 Let A∈L(X,X) be self adjoint where X is a ﬁnite dimensional Hilbert space.
Then for λ ≤ λ ≤ ··· ≤ λ the eigenvalues of A, there exist orthonormal vectors 1 2 n {u ,··· ,u } for which 1 n Au =λ u .
k k k Furthermore, { { }} λk ≡w1,m···a,wxk−1 min (A|xx|,2x) :x̸=0,x∈{w1,··· ,wk−1}⊥ (13.11) where if k =1,{w1,··· ,wk−1}⊥ ≡X.
Here is a version of this for which the roles of max and min are reversed.
Corollary 13.3.9 Let A∈L(X,X) be self adjoint where X is a ﬁnite dimensional Hilbert space.
Then for λ ≤ λ ≤ ··· ≤ λ the eigenvalues of A, there exist orthonormal vectors 1 2 n {u ,··· ,u } for which 1 n Au =λ u .
k k k Furthermore, { { }} λk ≡w1,·m··,iwnn−k max (A|xx|,2x) :x̸=0,x∈{w1,··· ,wn−k}⊥ (13.12) where if k =n,{w1,··· ,wn−k}⊥ ≡X.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.4.
POSITIVE AND NEGATIVE LINEAR TRANSFORMATIONS 317 13.4 Positive And Negative Linear Transformations Thenotionofapositivedeﬁniteornegativedeﬁnitelineartransformationisveryimportant in many applications.
In particular it is used in versions of the second derivative test for functions of many variables.
Here the main interest is the case of a linear transformation whichisann×nmatrixbutthetheoremisstatedandprovedusingamoregeneralnotation becausealltheseissuesdiscussedherehaveinterestinggeneralizationstofunctionalanalysis.
Lemma 13.4.1 Let X be a ﬁnite dimensional Hilbert space and let A ∈ L(X,X).
Then if {v ,··· ,v } is an orthonormal basis for X and M(A) denotes the matrix of the linear 1 n transformation A then M(A∗) = (M(A))∗.
In particular, A is self adjoint, if and only if M(A) is.
Proof: Consider the following picture A X → X q ↑ ◦ ↑q Fn → Fn M(A) ∑ where q is the coordinate map which satisﬁes q(x)≡ x v .
Therefore, since {v ,··· ,v } i i i 1 n is orthonormal, it is clear that |x|=|q(x)|.
Therefore, |x|2+|y|2+2Re(x,y) = |x+y|2 =|q(x+y)|2 = |q(x)|2+|q(y)|2+2Re(q(x),q(y)) (13.13) Now in any inner product space, (x,iy)=Re(x,iy)+iIm(x,iy).
Also (x,iy)=(−i)(x,y)=(−i)Re(x,y)+Im(x,y).
Therefore, equating the real parts, Im(x,y)=Re(x,iy) and so (x,y)=Re(x,y)+iRe(x,iy) (13.14) Now from (13.13), since q preserves distances, .Re(q(x),q(y)) = Re(x,y) which implies from (13.14) that (x,y)=(q(x),q(y)).
(13.15) Now consulting the diagram which gives the meaning for the matrix of a linear transforma- tion, observe that q◦M(A)=A◦q and q◦M(A∗)=A∗◦q.
Therefore, from (13.15) ∗ ∗ ∗ (A(q(x)),q(y))=(q(x),A q(y))=(q(x),q(M(A )(y)))=(x,M(A )(y)) but also ( ) ∗ (A(q(x)),q(y))=(q(M(A)(x)),q(y))=(M(A)(x),y)= x,M(A) (y) .
Sincex,yarearbitrary,thisshowsthatM(A∗)=M(A)∗ asclaimed.
Therefore,ifAisself adjoint, M(A) = M(A∗) = M(A)∗ and so M(A) is also self adjoint.
If M(A) = M(A)∗ then M(A)=M(A∗) and so A=A∗.
(cid:4) The following corollary is one of the items in the above proof.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation318 SELF ADJOINT OPERATORS Corollary 13.4.2 Let X be a ﬁnite dimensional Hilbert space and let {v ,··· ,v } be an 1 n orthonormal b∑asis for X.
Also, let q be the coordinate map associated with this basis satis- fying q(x) ≡ x v .
Then (x,y) = (q(x),q(y)) .
Also, if A ∈ L(X,X), and M(A) i i i Fn X is the matrix of A with respect to this basis, (Aq(x),q(y)) =(M(A)x,y) .
X Fn Deﬁnition 13.4.3 A self adjoint A ∈ L(X,X), is positive deﬁnite if whenever x̸=0, (Ax,x) > 0 and A is negative deﬁnite if for all x̸=0, (Ax,x) < 0.
A is positive semidef- inite or just nonnegative for short if for all x, (Ax,x) ≥ 0.
A is negative semideﬁnite or nonpositive for short if for all x, (Ax,x)≤0.
The following lemma is of fundamental importance in determining which linear trans- formations are positive or negative deﬁnite.
Lemma 13.4.4 Let X be a ﬁnite dimensional Hilbert space.
A self adjoint A ∈ L(X,X) is positive deﬁnite if and only if all its eigenvalues are positive and negative deﬁnite if and only if all its eigenvalues are negative.
It is positive semideﬁnite if all the eigenvalues are nonnegative and it is negative semideﬁnite if all the eigenvalues are nonpositive.
Proof: Suppose ﬁrst that A is positive deﬁnite and let λ be an eigenvalue.
Then for x an eigenvector corresponding to λ, λ(x,x) = (λx,x) = (Ax,x) > 0.
Therefore, λ > 0 as claimed.
Now supp∑ose all the eigenvalues of A are positive.
From Theorem 13.3.3 and Corollary 13.3.6, A = n λ u ⊗u where the λ are the positive eigenvalues and {u } are an i=1 i i i i i orthonormal set of eigenvectors.
Therefore, letting x̸=0, (( ) ) ( ) ∑n ∑n (Ax,x) = λ u ⊗u x,x = λ u (x,u ),x i i i i i i ( i=1 ) i=1 ∑n ∑n = λ (x,u )(u ,x) = λ |(u ,x)|2 >0 i i i i i i=1 i=1 ∑ because, since {u } is an orthonormal basis, |x|2 = n |(u ,x)|2.
i i=1 i To establish the claim about negative deﬁnite, it suﬃces to note that A is negative deﬁnite if and only if −A is positive deﬁnite and the eigenvalues of A are (−1) times the eigenvalues of −A.
The claims about positive semideﬁnite and negative semideﬁnite are obtained similarly.
(cid:4) The next theorem is about a way to recognize whether a self adjoint A ∈ L(X,X) is positive or negative deﬁnite without having to ﬁnd the eigenvalues.
In order to state this theorem, here is some notation.
Deﬁnition 13.4.5 Let A be an n×n matrix.
Denote by A the k×k matrix obtained by k deleting the k+1,··· ,n columns and the k+1,··· ,n rows from A.
Thus A =A and A n k is the k×k submatrix of A which occupies the upper left corner of A.
The determinants of these submatrices are called the principle minors.
The following theorem is proved in [8] Theorem 13.4.6 Let X be a ﬁnite dimensional Hilbert space and let A∈L(X,X) be self adjoint.
Then A is positive deﬁnite if and only if det(M(A) )>0 for every k =1,··· ,n. k Here M(A) denotes the matrix of A with respect to some ﬁxed orthonormal basis of X. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.5.
FRACTIONAL POWERS 319 Proof:Thistheoremisprovedbyinductiononn.Itisclearlytrueifn=1.Supposethen thatitistrueforn−1wheren≥2.
Sincedet(M(A))>0,itfollowsthatalltheeigenvalues are nonzero.
Are they all positive?
Suppose not.
Then there is some even number of them which are negative, even because the product of all the eigenvalues is known to be positive, equalingdet(M(A)).
Picktwo,λ andλ andletM(A)u =λ u whereu ̸=0fori=1,2 1 2 i i i i and (u ,u ) = 0.
Now if y ≡ α u +α u is an element of span(u ,u ), then since these 1 2 1 1 2 2 1 2 are eigenvalues and (u ,u )=0, a short computation shows 1 2 (M(A)(α u +α u ),α u +α u ) 1 1 2 2 1 1 2 2 =|α |2λ |u |2+|α |2λ |u |2 <0.
1 1 1 2 2 2 Now letting x∈Cn−1, the induction hypothesis implies ( ) x ∗ ∗ (x ,0)M(A) =x M(A) x=(M(A)x,x)>0.
0 n−1 Nowthedimensionof{z∈Cn :z =0}isn−1andthedimensionofspan(u ,u )=2and n 1 2 so there must be some nonzero x∈Cn which is in both of these subspaces of Cn.
However, the ﬁrst computation would require that (M(A)x,x) < 0 while the second would require that (M(A)x,x) > 0.
This contradiction shows that all the eigenvalues must be positive.
This proves the if part of the theorem.
The only if part is left to the reader.
Corollary 13.4.7 Let X be a ﬁnite dimensional Hilbert space and let A ∈ L(X,X) be self adjoint.
Then A is negative deﬁnite if and only if det(M(A) )(−1)k > 0 for every k k = 1,··· ,n. Here M(A) denotes the matrix of A with respect to some ﬁxed orthonormal basis of X.
Proof: This is immediate from the above theorem by noting that, as in the proof of Lemma 13.4.4, A is negative deﬁnite if and only if −A is positive deﬁnite.
Therefore, if det(−M(A) ) > 0 for all k = 1,··· ,n, it follows that A is negative deﬁnite.
However, k det(−M(A) )=(−1)kdet(M(A) ).
(cid:4) k k 13.5 Fractional Powers Withtheabovetheory,itispossibletotakefractionalpowersofcertainelementsofL(X,X) where X is a ﬁnite dimensional Hilbert space.
To begin with, consider the square root of a nonnegativeselfadjointoperator.
Thisiseasierthanthegeneraltheoryanditisthesquare root which is of most importance.
Theorem 13.5.1 Let A ∈ L(X,X) be self adjoint and nonnegative.
Then there exists a unique self adjoint nonnegative B ∈L(X,X) such that B2 =A and B commutes with every element of L(X,X) which commutes with A.
Proof: By Theorem 13.3.3, there exists an orthonormal basis o∑f eigenvectors of A, say {v }n such that Av = λ v .
Therefore, by Theorem 13.2.4, A = λ v ⊗v where each i i=1 i i i i i i i λ ≥0.
i Now by Lemma 13.4.4, each λ ≥0.
Therefore, it makes sense to deﬁne i ∑ B ≡ λ1/2v ⊗v .
i i i i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation320 SELF ADJOINT OPERATORS It is easy to verify that { 0 if i̸=j (v ⊗v )(v ⊗v )= .
i i j j v ⊗v if i=j i i ∑ Therefore, a short computation veriﬁes that B2 = λ v ⊗v = A.
If C commutes with i i i i A, then for some cij, ∑ C = c v ⊗v ij i j ij and so since they commute, ∑ ∑ ∑ c v ⊗v λ v ⊗v = c λ δ v ⊗v = c λ v ⊗v ij i j k k k ij k jk i k ik k i k i,j,k i,j,k i,k ∑ ∑ ∑ = c λ v ⊗v v ⊗v = c λ δ v ⊗v = c λ v ⊗v ij k k k i j ij k ki k j kj k k j i∑,j,k i,j,k j,k = c λ v ⊗v ik i i k k,i Then by independence, c λ =c λ ik i ik k Therefore, c λ1/2 = c λ1/2 which amounts to saying that B also commutes with C. It is ik i ik k clear that this operator is self adjoint.
This proves existence.
SupposeB isanothersquarerootwhichisselfadjoint,nonnegativeandcommuteswith 1 every matrix which commutes with A.
Since both B,B are nonnegative, 1 (B(B−B )x,(B−B )x)≥0, 1 1 (B (B−B )x,(B−B )x)≥0 (13.16) 1 1 1 Now, adding these together, and using the fact that the two commute, (( ) ) B2−B2 x,(B−B )x =((A−A)x,(B−B )x)=0.
1 1 1 It follows that both inner products in (13.16) equal 0.
N√ext u√se the existence part of this to take the square root of B and B which is denoted by B, B respectively.
Then 1 1 (√ √ ) 0 = B(B−B )x, B(B−B )x 1 1 (√ √ ) 0 = B (B−B )x, B (B−B )x 1 1 1 1 √ √ which implies B(B−B )x= B (B−B )x=0.
Thus also, 1 1 1 B(B−B )x=B (B−B )x=0 1 1 1 Hence 0=(B(B−B )x−B (B−B )x,x)=((B−B )x,(B−B )x) 1 1 1 1 1 and so, since x is arbitrary, B =B.
(cid:4) 1 The main result is the following theorem.
Theorem 13.5.2 Let A∈L(X,X) be self adjoint and nonnegative and let k be a positive integer.
ThenthereexistsauniqueselfadjointnonnegativeB ∈L(X,X)suchthatBk =A.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.5.
FRACTIONAL POWERS 321 Proof: By Theorem 13.3.3, there exists an orthonormal basis of eigenvectors of A, say {∑vi}ni=1 such that Avi = λivi.
Therefore, by Corollary 13.3.6 or Theorem 13.2.4, A = λ v ⊗v where each λ ≥0.
i i i i i Now by Lemma 13.4.4, each λ ≥0.
Therefore, it makes sense to deﬁne i ∑ B ≡ λ1/kv ⊗v .
i i i i It is easy to verify that { 0 if i̸=j (v ⊗v )(v ⊗v )= .
i i j j v ⊗v if i=j i i ∑ Therefore, a short computation veriﬁes that Bk = λ v ⊗v =A.
This proves existence.
i i i i In order to prove uniqueness, let p(t) be a polynomial which has(the prop)erty that p(λ )=λ1/k for each i.
In other words, goes through the ordered pairs λ ,λ1/k .
Then a i i i i similar short computation shows ∑ ∑ p(A)= p(λ )v ⊗v = λ1/kv ⊗v =B.
i i i i i i i i Now suppose Ck =A where C ∈L(X,X) is self adjoint and nonnegative.
Then ( ) ( ) CB =Cp(A)=Cp Ck =p Ck C =p(A)C =BC.
Therefore, {B,C} is a commuting family of linear transformations which are both self adjoint.
Letting M(B) and M(C) denote matrices of these linear transformations taken withrespecttosomeﬁxedorthonormalbasis,{v ,··· ,v },itfollowsthatM(B)andM(C) 1 n commute and that both can be diagonalized (Lemma 13.4.1).
See the diagram for a short veriﬁcation of the claim the two matrices commute.. B C X → X → X q ↑ ◦ ↑q ◦ ↑q Fn → Fn → Fn M(B) M(C) Therefore,byTheorem13.1.9,thesetwomatricescanbesimultaneouslydiagonalized.
Thus U−1M(B)U =D , U−1M(C)U =D (13.17) 1 2 where the D is a diagonal matrix consisting of the eigenvalues of B or C. Also it is clear i that M(C)k =M(A) because M(C)k is given by z kt}im|es { q−1Cqq−1Cq···q−1Cq =q−1Ckq =q−1Aq =M(A) and similarly M(B)k =M(A).
Then raising these to powers, U−1M(A)U =U−1M(B)kU =Dk 1 and U−1M(A)U =U−1M(C)kU =Dk.
2 Therefore, Dk =Dk andsincethediagonalentriesofD arenonnegative, thisrequiresthat 1 2 i D =D .
Therefore, from (13.17), M(B)=M(C) and so B =C.
(cid:4) 1 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation322 SELF ADJOINT OPERATORS 13.6 Polar Decompositions An application of Theorem 13.3.3, is the following fundamental result, important in geo- metric measure theory and continuum mechanics.
It is sometimes called the right polar decomposition.
The notation used is that which is seen in continuum mechanics, see for example Gurtin [11].
Don’t confuse the U in this theorem with a unitary transformation.
It is not so.
When the following theorem is applied in continuum mechanics, F is normally the deformation gradient, the derivative of a nonlinear map from some subset of three di- mensional space to three dimensional space.
In this context, U is called the right Cauchy Greenstraintensor.
Itisameasureofhowabodyisstretchedindependentofrigidmotions.
First, here is a simple lemma.
Lemma 13.6.1 Suppose R∈L(X,Y) where X,Y are Hilbert spaces and R preserves dis- tances.
Then R∗R=I.
Proof: Since R preserves distances, |Rx| = |x| for every x.
Therefore from the axioms of the inner product, |x|2+|y|2+(x,y)+(y,x)=|x+y|2 =(R(x+y),R(x+y)) =(Rx,Rx)+(Ry,Ry)+(Rx,Ry)+(Ry,Rx) =|x|2+|y|2+(R∗Rx,y)+(y,R∗Rx) and so for all x,y, (R∗Rx−x,y)+(y,R∗Rx−x)=0 Hence for all x,y, Re(R∗Rx−x,y)=0 Now for x,y given, choose α∈C such that α(R∗Rx−x,y)=|(R∗Rx−x,y)| Then 0 = Re(R∗Rx−x,αy)=Reα(R∗Rx−x,y) = |(R∗Rx−x,y)| Thus |(R∗Rx−x,y)| = 0 for all x,y because the given x,y were arbitrary.
Let y = R∗Rx−x to conclude that for all x, R∗Rx−x=0 which says R∗R=I since x is arbitrary.
(cid:4) The decomposition in the following is called the right polar decomposition.
Theorem 13.6.2 Let X be a Hilbert space of dimension n and let Y be a Hilbert space of dimension m ≥n and let F ∈L(X,Y).
Then there exists R∈L(X,Y) and U ∈L(X,X) such that ∗ F =RU, U =U ,(U is Hermitian), all eigenvalues of U are non negative, U2 =F∗F,R∗R=I, and |Rx|=|x|.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.6.
POLAR DECOMPOSITIONS 323 Proof: (F∗F)∗ = F∗F and so by Theorem 13.3.3, there is an orthonormal basis of eigenvectors, {v ,··· ,v } such that 1 n ∑n F∗Fv =λ v , F∗F = λ v ⊗v .
i i i i i i i=1 It is also clear that λ ≥0 because i λ (v ,v )=(F∗Fv ,v )=(Fv ,Fv )≥0.
i i i i i i i Let ∑n U ≡ λ1/2v ⊗v .
i i i i=1 { } n Then U2 =F∗F, U =U∗, and the eigenvalues of U, λ1/2 are all non negative.
i i=1 Let{Ux ,··· ,Ux }beanorthonormalbasisforU(X).BytheGramSchmidtprocedure 1 r there exists an extension to an orthonormal basis for X, {Ux ,··· ,Ux ,y ,··· ,y }.
1 r r+1 n Next note that {Fx ,··· ,Fx } is also an orthonormal set of vectors in Y because 1 r ( ) (Fx ,Fx )=(F∗Fx ,x )= U2x ,x =(Ux ,Ux )=δ .
k j k j k j k j jk By the Gram Schmidt procedure, there exists an extension of {Fx ,··· ,Fx } to an or- 1 r thonormal basis for Y, {Fx ,··· ,Fx ,z ,··· ,z }.
1 r r+1 m Since m≥n, there are at least as many z as there are y .
Now for x∈X, since k k {Ux ,··· ,Ux ,y ,··· ,y } 1 r r+1 n is an orthonormal basis for X, there exist unique scalars c ,··· ,c ,d ,··· ,d 1 r r+1 n such that ∑r ∑n x= c Ux + d y k k k k k=1 k=r+1 Deﬁne ∑r ∑n Rx≡ c Fx + d z (13.18) k k k k k=1 k=r+1 Thus ∑r ∑n |Rx|2 = |c |2+ |d |2 =|x|2.
k k k=1 k=r+1 Therefore, by Lemma 13.6.1 R∗R=I.
Then also there exist scalars b such that k ∑r Ux= b Ux (13.19) k k k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation324 SELF ADJOINT OPERATORS and so from (13.18), ( ) ∑r ∑r RUx= b Fx =F b x k k k k k=1 k=1 ∑ Is F ( r b x )=F (x)?
k=1 k k ( ( ) ( ) ) ∑r ∑r F b x −F (x),F b x −F (x) k k k k k=1 k=1 ( ( ) ( )) ∑r ∑r = (F∗F) b x −x , b x −x k k k k ( ( k=1 ) ( k=1 )) ∑r ∑r = U2 b x −x , b x −x k k k k ( ( k=1 ) (k=1 )) ∑r ∑r = U b x −x ,U b x −x k k k k ( k=1 k=1 ) ∑r ∑r = b Ux −Ux, b Ux −Ux =0 k k k k k=1 k=1 ∑ ∑ Because from (13.19), Ux= r b Ux .
Therefore, RUx=F ( r b x )=F (x).
(cid:4) k=1 k k k=1 k k The following corollary follows as a simple consequence of this theorem.
It is called the left polar decomposition.
Corollary 13.6.3 Let F ∈ L(X,Y) and suppose n ≥ m where X is a Hilbert space of dimension n and Y is a Hilbert space of dimension m. Then there exists a Hermitian U ∈ L(X,X), and an element of L(X,Y), R, such that ∗ F =UR, RR =I.
Proof: Recall that L∗∗ = L and (ML)∗ = L∗M∗.
Now apply Theorem 13.6.2 to F∗ ∈L(Y,X).
Thus, ∗ ∗ F =R U where R∗ and U satisfy the conditions of that theorem.
Then F =UR and RR∗ =R∗∗R∗ =I.
(cid:4) The following existence theorem for the polar decomposition of an element of L(X,X) is a corollary.
Corollary 13.6.4 Let F ∈ L(X,X).
Then there exists a Hermitian W ∈ L(X,X), and a unitary matrix Q such that F = WQ, and there exists a Hermitian U ∈ L(X,X) and a unitary R, such that F =RU.
This corollary has a fascinating relation to the question whether a given linear transfor- mationisnormal.
Recallthatann×nmatrixA,isnormalifAA∗ =A∗A.Retainthesame deﬁnition for an element of L(X,X).
Theorem 13.6.5 Let F ∈ L(X,X).
Then F is normal if and only if in Corollary 13.6.4 RU =UR and QW =WQ.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.7.
AN APPLICATION TO STATISTICS 325 Proof: I will prove the statement about RU = UR and leave the other part as an exercise.
First suppose that RU =UR and show F is normal.
To begin with, ∗ ∗ ∗ ∗ UR =(RU) =(UR) =R U.
Therefore, F∗F = UR∗RU =U2 FF∗ = RUUR∗ =URR∗U =U2 which shows F is normal.
Now suppose F is normal.
Is RU =UR?
Since F is normal, FF∗ =RUUR∗ =RU2R∗ and F∗F =UR∗RU =U2.
Therefore, RU2R∗ =U2, and both are nonnegative and self adjoint.
Therefore, the square rootsofbothsidesmustbeequalbytheuniquenesspartofthetheoremonfractionalpowers.
It follows that the square root of the ﬁrst, RUR∗ must equal the square root of the second, U.Therefore,RUR∗ =U andsoRU =UR.Thisprovesthetheoreminonecase.
Theother case in which W and Q commute is left as an exercise.
(cid:4) 13.7 An Application To Statistics A random vector is a function X : Ω → Rp where Ω is a probability space.
This means thatthereexistsaσ algebraofmeasurablesetsF andaprobabilitymeasureP :F →[0,1].
In practice, people often don’t worry too much about the underlying probability space and instead pay more attention to the distribution measure of the random variable.
For E a suitable subset of Rp, this measure gives the probability that X has values in E. There are often excellent reasons for believing that a random vector is normally distributed.
This means that the probability that X has values in a set E is given by ∫ ( ) 1 exp −1(x−m)∗Σ−1(x−m) dx E (2π)p/2det(Σ)1/2 2 The expression in the integral is called the normal probability density function.
There are twoparameters,mandΣwheremiscalledthemeanandΣiscalledthecovariancematrix.
It is a symmetric matrix which has all real eigenvalues which are all positive.
While it may be reasonable to assume this is the distribution, in general, you won’t know m and Σ and in order to use this formula to predict anything, you would need to know these quantities.
What people do to estimate these is to take n independent observations x ,··· ,x and 1 n try to predict what m and Σ should be based on these observations.
One criterion used for making this determination is the method of maximum likelihood.
In this method, you seek to choose the two parameters in such a way as to maximize the likelihood which is given as ( ) ∏n 1 exp −1(x −m)∗Σ−1(x −m) .
det(Σ)1/2 2 i i i=1 For convenience the term (2π)p/2 was ignored.
This leads to the estimate for m as ∑n 1 m= x ≡x.
n i i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation326 SELF ADJOINT OPERATORS Thispartfollowsfairlyeasilyfromtakingthelnandthensettingpartialderivativesequalto 0.
The estimation of Σ is harder.
However, it is not too hard using the theorems presented above.
I am following a nice discussion given in Wikipedia.
It will make use of Theorem 7.5.3 on the trace as well as the theorem about the square root of a linear transformation given above.
First note that by Theorem 7.5.3, ( ) (x −m)∗Σ−1(x −m) = trace (x −m)∗Σ−1(x −m) i i ( i i ) = trace (x −m)(x −m)∗Σ−1 i i Therefore, the thing to maximize is ( ) ∏n ( ) 1 exp −1trace (x −m)(x −m)∗Σ−1 det(Σ)1/2 2 i i i=1 ( ) ( ) ∑n = det Σ−1 n/2exp −1trace (x −m)(x −m)∗Σ−1 2 i i i=1   z }S| { = det(Σ−1)n/2exp−1trace∑n (x −m)(x −m)∗Σ−1  2 i i  i=1 ( ) ( ) ( ) 1 ≡ det Σ−1 n/2exp − trace SΣ−1 2 whereS isthep×pmatrixindicatedabove.
NowS issymmetricandhaseigenvalueswhich are all nonnegative because (Sy,y)≥0.
Therefore, S has a unique self adjoint square root.
Using Theorem 7.5.3 again, the above equals ( ) ( ( )) 1 det Σ−1 n/2exp − trace S1/2Σ−1S1/2 2 Let B = S1/2Σ−1S1/2 and assume det(S) ̸= 0.
Then Σ−1 = S−1/2BS−1/2.
The above equals ( ) ( ) 1 det S−1 det(B)n/2exp − trace(B) 2 ( ) Of course the thing to estimate is only found in B.
Therefore, det S−1 can be discarded in trying to maximize things.
Since B is symmetric, it is similar to a diagonal matrix D which has λ ,··· ,λ down the diagonal.
Thus it is desired to maximize 1 n ( ) ( ) ∏p n/2 ∑p 1 λ exp − λ i 2 i i=1 i=1 Taking ln it follows that it suﬃces to maximize ∑p ∑p n 1 lnλ − λ 2 i 2 i i=1 i=1 Taking the derivative with respect to λ , i n 1 1 − =0 2λ 2 i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.8.
THE SINGULAR VALUE DECOMPOSITION 327 and so λ =n.
It follows from the above that i Σ=S1/2B−1S1/2 where B−1 has only the eigenvalues 1/n.
It follows B−1 must equal the diagonal matrix which has 1/n down the diagonal.
The reason for this is that B is similar to a diagonal matrix because it is symmetric.
Thus B = P−11IP = 1I because the identity commutes n n with every matrix.
But now it follows that 1 Σ= S n Of course this is just an estimate and so we write Σˆ instead of Σ.
This has shown that the maximum likelihood estimate for Σ is ∑n Σˆ = 1 (x −m)(x −m)∗ n i i i=1 13.8 The Singular Value Decomposition In this section, A will be an m×n matrix.
To begin with, here is a simple lemma.
Lemma 13.8.1 Let A be an m×n matrix.
Then A∗A is self adjoint and all its eigenvalues are nonnegative.
Proof: It is obvious that A∗A is self adjoint.
Suppose A∗Ax = λx.
Then λ|x|2 = (λx,x)=(A∗Ax,x)=(Ax,Ax)≥0.
(cid:4) Deﬁnition 13.8.2 Let A be an m×n matrix.
The singular values of A are the square roots of the positive eigenvalues of A∗A.
With this deﬁnition and lemma here is the main theorem on the singular value decom- position.
In all that follows, I will write the following partitioned matrix ( ) σ 0 0 0 where σ denotes an r×r diagonal matrix of the form   σ 0 1    ...  0 σ k and the bottom row of zero matrices in the partitioned matrix, as well as the right columns of zero matrices are each of the right size so that the resulting matrix is m×n.
Either could vanish completely.
However, I will write it in the above form.
It is easy to make the necessary adjustments in the other two cases.
Theorem 13.8.3 Let A be an m×n matrix.
Then there exist unitary matrices, U and V of the appropriate size such that ( ) σ 0 ∗ U AV = 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation328 SELF ADJOINT OPERATORS where σ is of the form   σ 0 1   σ = ...  0 σ k for the σ the singular values of A, arranged in order of decreasing size.
i Proof: By the above lemma and Theorem 13.3.3 there exists an orthonormal basis, {v }n such that A∗Av =σ2v where σ2 >0 for i=1,··· ,k,(σ >0), and equals zero if i i=1 i i i i i i>k.
Thus for i>k, Av =0 because i ∗ (Av ,Av )=(A Av ,v )=(0,v )=0.
i i i i i For i=1,··· ,k, deﬁne u ∈Fm by i u ≡σ−1Av .
i i i Thus Av =σ u .
Now i i i ( ) ( ) (u ,u ) = σ−1Av ,σ−1Av = σ−1v ,σ−1A∗Av i j i i j j i i j j ( ) σ = σ−1v ,σ−1σ2v = j (v ,v )=δ .
i i j j j σ i j ij i Thus {u }k is an orthonormal set of vectors in Fm.
Also, i i=1 AA∗u =AA∗σ−1Av =σ−1AA∗Av =σ−1Aσ2v =σ2u .
i i i i i i i i i i Now extend {u }k to an orthonormal basis for all of Fm,{u }m and let i i=1 i i=1 ( ) U ≡ u ··· u 1 m while ( ) V ≡ v ··· v .
1 n ThusU isthematrixwhichhastheu ascolumnsandV isdeﬁnedasthematrixwhichhas i the v as columns.
Then i   u∗ 1  .
  .
  .
 ( ) U∗AV = u.∗k A v1 ··· vn  .
 .
u∗ m   u∗ 1  .
  .
 ( )  .
( ) = u.∗k  σ1u1 ··· σkuk 0 ··· 0 = σ0 00  .
 .
u∗ m where σ is given in the statement of the theorem.
(cid:4) Thesingularvaluedecompositionhasasanimmediatecorollarythefollowinginteresting result.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.9.
APPROXIMATION IN THE FROBENIUS NORM 329 Corollary 13.8.4 Let A be an m×n matrix.
Then the rank of A and A∗equals the number of singular values.
Proof: Since V and U are unitary, they are each one to one and onto and so it follows that ( ) σ 0 ∗ rank(A)=rank(U AV)=rank =number of singular values.
0 0 Also since U,V are unitary, ( ) ∗ ∗ ∗ ∗ ∗ rank(A )=rank(V A U)=rank (U AV) (( ) ) ∗ σ 0 =rank =number of singular values.
(cid:4) 0 0 13.9 Approximation In The Frobenius Norm The Frobenius norm is one of many norms for a matrix.
It is arguably the most obvious of all norms.
Here is its deﬁnition.
Deﬁnition 13.9.1 Let A be a complex m×n matrix.
Then ||A|| ≡(trace(AA∗))1/2 F Also this norm comes from the inner product (A,B) ≡trace(AB∗) F ∑ Thus ||A||2 is easily seen to equal |a |2 so essentially, it treats the matrix as a vector F ij ij in Fm×n.
Lemma 13.9.2 Let A be an m×n complex matrix with singular matrix ( ) σ 0 Σ= 0 0 with σ as deﬁned above.
Then ||Σ||2 =||A||2 (13.20) F F and the following hold for the Frobenius norm.
If U,V are unitary and of the right size, ||UA|| =||A|| , ||UAV|| =||A|| .
(13.21) F F F F Proof: From the deﬁnition and letting U,V be unitary and of the right size, ||UA||2 ≡trace(UAA∗U∗)=trace(AA∗)=||A||2 F F Also, ||AV||2 ≡trace(AVV∗A∗)=trace(AA∗)=||A||2 .
F F It follows ||UAV||2 =||AV||2 =||A||2 .
F F F Now consider (13.20).
From what was just shown, ||A||2 =||UΣV∗||2 =||Σ||2 .
(cid:4) F F F Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation330 SELF ADJOINT OPERATORS Of course, this shows that ∑ ||A||2 = σ2, F i i the sum of the squares of the singular values of A.
Why is the singular value decomposition important?
It implies ( ) σ 0 ∗ A=U V 0 0 whereσisthediagonalmatrixhavingthesingularvaluesdownthediagonal.
Nowsometimes A is a huge matrix, 1000×2000 or something like that.
This happens in applications to situations where the entries of A describe a picture.
What also happens is that most of the singular values are very small.
What if you deleted those which were very small, say for all i≥l and got a new matrix ( ) σ′ 0 A′ ≡U V∗?
0 0 Then the entries of A′ would end up being close to the entries of A but there is much less information to keep track of.
This turns out to be very useful.
More precisely, letting   σ 0 ( ) 1 σ = ... , U∗AV = σ 0 , 0 0 0 σ r (cid:12)(cid:12) ( ) (cid:12)(cid:12) ||A−A′||2 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)U σ−σ′ 0 V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 = ∑r σ2 F 0 0 k F k=l+1 Thus A is approximated by A′ where A′ has rank l <r.
In fact, it is also true that out ofallmatricesofrankl,thisA′ istheonewhichisclosesttoAintheFrobeniusnorm.
Here is why.
Let B be a matrix which has rank l. Then from Lemma 13.9.2 (cid:12)(cid:12)( ) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)2 ||A−B||2 =||U∗(A−B)V||2 =||U∗AV −U∗BV||2 =(cid:12)(cid:12)(cid:12)(cid:12) σ 0 −U∗BV(cid:12)(cid:12)(cid:12)(cid:12) F F F 0 0 F and since the singular values of A decrease from the upper left to the lower right, it follows that for B to be closest as possible to A in the Frobenius norm, ( ) σ′ 0 ∗ U BV = 0 0 which implies B =A′ above.
This is really obvious if you look at a simple example.
Say   ( ) 3 0 0 0 σ 0   = 0 2 0 0 0 0 0 0 0 0 for example.
Then what rank 1 matrix would be closest to this one in the Frobenius norm?
Obviously   3 0 0 0   0 0 0 0 0 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.10.
LEAST SQUARES AND SINGULAR VALUE DECOMPOSITION 331 13.10 Least Squares And Singular Value Decomposition The singular value decomposition also has a very interesting connection to the problem of leastsquaressolutions.
Recallthatitwasdesiredtoﬁndxsuchthat|Ax−y|isassmallas possible.
Lemma12.5.1showsthatthereisasolutiontothisproblemwhichcanbefoundby solving the system A∗Ax = A∗y.
Each x which solves this system solves the minimization problem as was shown in the lemma just mentioned.
Now consider this equation for the solutions of the minimization problem in terms of the singular value decomposition.
z A}|∗ {z }A| { z A}|∗ { ( ) ( ) ( ) σ 0 σ 0 σ 0 ∗ ∗ ∗ V U U V x=V U y.
0 0 0 0 0 0 Therefore, this yields the following upon using block multiplication and multiplying on the left by V∗.
( ) ( ) σ2 0 σ 0 ∗ ∗ V x= U y.
(13.22) 0 0 0 0 One solution to this equation which is very easy to spot is ( ) σ−1 0 ∗ x=V U y.
(13.23) 0 0 13.11 The Moore Penrose Inverse The particular solution of the least squares problem given in (13.23) is important enough that it motivates the following deﬁnition.
Deﬁnition 13.11.1 Let A be an m×n matrix.
Then the Moore Penrose inverse of A, denoted by A+ is deﬁned as ( ) σ−1 0 A+ ≡V U∗.
0 0 Here ( ) σ 0 ∗ U AV = 0 0 as above.
ThusA+yisasolutiontotheminimizationproblemtoﬁndxwhichminimizes|Ax−y|.
In fact, one can say more about this.
In the following picture M denotes the set of least y squares solutions x such that A∗Ax=A∗y.
A+(y) (cid:9) (cid:14)I M y x (cid:27) ker(A∗A) Then A+(y) is as given in the picture.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation332 SELF ADJOINT OPERATORS Proposition 13.11.2 A+y is the solution to the problem of minimizing |Ax−y| for all x which has smallest norm.
Thus (cid:12) (cid:12) (cid:12)AA+y−y(cid:12)≤|Ax−y| for all x and if x satisﬁes |Ax −y|≤|Ax−y| for all x, then |A+y|≤|x |.
1 1 1 Proof: Consider x satisfying (13.22), equivalently A∗Ax=A∗y, ( ) ( ) σ2 0 σ 0 ∗ ∗ V x= U y 0 0 0 0 which has smallest norm.
This is equivalent to making |V∗x| as small as possible because V∗ is unitary and so it preserves norms.
For z a vector, denote by (z) the vector in Fk k which consists of the ﬁrst k entries of z.
Then if x is a solution to (13.22) ( ) ( ) σ2(V∗x) σ(U∗y) k = k 0 0 and so (V∗x) = σ−1(U∗y) .
Thus the ﬁrst k entries of V∗x are determined.
In order to k k make |V∗x| as small as possible, the remaining n−k entries should equal zero.
Therefore, ( ) ( ) ( ) (V∗x) σ−1(U∗y) σ−1 0 V∗x= k = k = U∗y 0 0 0 0 and so ( ) σ−1 0 x=V U∗y≡A+y (cid:4) 0 0 Lemma 13.11.3 The matrix A+ satisﬁes the following conditions.
AA+A=A, A+AA+ =A+, A+A and AA+ are Hermitian.
(13.24) Proof: This is routine.
Recall ( ) σ 0 ∗ A=U V 0 0 and ( ) σ−1 0 A+ =V U∗ 0 0 so you just plug in and verify it works.
(cid:4) A much more interesting observation is that A+ is characterized as being the unique matrixwhichsatisﬁes(13.24).
ThisisthecontentofthefollowingTheorem.
Theconditions are sometimes called the Penrose conditions.
Theorem 13.11.4 Let A be an m×n matrix.
Then a matrix A , is the Moore Penrose 0 inverse of A if and only if A satisﬁes 0 AA A=A, A AA =A , A A and AA are Hermitian.
(13.25) 0 0 0 0 0 0 Proof: From the above lemma, the Moore Penrose inverse satisﬁes (13.25).
Suppose then that A satisﬁes (13.25).
It is necessary to verify that A =A+.
Recall that from the 0 0 singular value decomposition, there exist unitary matrices, U and V such that ( ) σ 0 U∗AV =Σ≡ , A=UΣV∗.
0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.11.
THE MOORE PENROSE INVERSE 333 Let ( ) P Q ∗ V A U = (13.26) 0 R S where P is k×k.
Next use the ﬁrst equation of (13.25) to write z }A| {z ( A}|0 ) {z }A| { z }A| { P Q ∗ ∗ ∗ ∗ UΣV V U UΣV =UΣV .
R S Then multiplying both sides on the left by U∗ and on the right by V, ( )( )( ) ( ) σ 0 P Q σ 0 σ 0 = 0 0 R S 0 0 0 0 Now this requires ( ) ( ) σPσ 0 σ 0 = .
(13.27) 0 0 0 0 Therefore, P =σ−1.
From the requirement that AA is Hermitian, 0 z }A| {z ( A}|0 ) { ( )( ) P Q σ 0 P Q ∗ ∗ ∗ UΣV V U =U U R S 0 0 R S must be Hermitian.
Therefore, it is necessary that ( )( ) ( ) σ 0 P Q σP σQ = 0 0 R S 0 0 ( ) I σQ = 0 0 is Hermitian.
Then ( ) ( ) I σQ I 0 = 0 0 Q∗σ 0 Thus ∗ Q σ =0 and so multiplying both sides on the right by σ−1, it follows Q∗ =0 and so Q=0.
From the requirement that A A is Hermitian, it is necessary that 0 z ( A}|0 ) {z }A| { ( ) P Q Pσ 0 ∗ ∗ ∗ V U UΣV = V V R S Rσ 0 ( ) I 0 ∗ = V V Rσ 0 is Hermitian.
Therefore, also ( ) I 0 Rσ 0 is Hermitian.
Thus R=0 because this equals ( ) ( ) I 0 ∗ I σ∗R∗ = Rσ 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation334 SELF ADJOINT OPERATORS which requires Rσ =0.
Now multiply on right by σ−1 to ﬁnd that R=0.
Use (13.26) and the second equation of (13.25) to write z ( A}|0 ) {z }A| {z ( A}|0 ) { z ( A}|0 ) { P Q P Q P Q ∗ ∗ ∗ ∗ V U UΣV V U =V U .
R S R S R S which implies ( )( )( ) ( ) P Q σ 0 P Q P Q = .
R S 0 0 R S R S This yields from the above in which is was shown that R,Q are both 0 ( )( )( ) ( ) σ−1 0 σ 0 σ−1 0 σ−1 0 = (13.28) 0 S 0 0 0 S 0 0 ( ) σ−1 0 = .
(13.29) 0 S Therefore, S =0 also and so ( ) ( ) P Q σ−1 0 V∗A U ≡ = 0 R S 0 0 which says ( ) σ−1 0 A =V U∗ ≡A+.
(cid:4) 0 0 0 The theorem is signiﬁcant because there is no mention of eigenvalues or eigenvectors in thecharacterizationoftheMoorePenroseinversegivenin(13.25).
Italsoshowsimmediately that the Moore Penrose inverse is a generalization of the usual inverse.
See Problem 3.
13.12 Exercises 1.
Show (A∗)∗ =A and (AB)∗ =B∗A∗.
2.
Prove Corollary 13.3.9.
3.
Show that if A is an n×n matrix which has an inverse then A+ =A−1.
4.
Usingthesingularvalue decomposition,showthatforanysquarematrixA,itfollows that A∗A is unitarily similar to AA∗.
5.
Let A,B be a m×n matrices.
Deﬁne an inner product on the set of m×n matrices by (A,B) ≡trace(AB∗).
F Showthisisaninnerprodu∑ctsatisfyingalltheinnerproductaxioms.
RecallforM an n×nmatrix,trace(M)≡ n M .Theresultingnorm,||·|| iscalledtheFrobenius i=1 ii F norm and it can be used to measure the distance between two matrices.
∑ 6.
Let A be an m×n matrix.
Show ||A||2 ≡ (A,A) = σ2 where the σ are the F F j j j singular values of A.
7.
If A is a general n×n matrix having possibly repeated eigenvalues, show there is a sequence {A } of n×n matrices having distinct eigenvalues which has the property k thattheijth entryofA convergestotheijth entryofAforallij.
Hint: UseSchur’s k theorem.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation13.12.
EXERCISES 335 8.
Prove the Cayley Hamilton theorem as follows.
First suppose A has a basis of eigen- vectors {v }n ,Av = λ v .
Let p(λ) be the characteristic polynomial.
Show k k=1 k k k p(A)v = p(λ )v = 0.
Then since {v } is a basis, it follows p(A)x=0 for all k k k k xandsop(A)=0.Nextinthegeneralcase,useProblem7toobtainasequence{A } k of matrices whose entries converge to the entries of A such that A has n distinct k eigenvalues and therefore by Theorem 7.1.7 A has a basis of eigenvectors.
There- k fore, from the ﬁrst part and for p (λ) the characteristic polynomial for A , it follows k k pk(Ak)=0.
Now explain why and the sense in which limk→∞pk(Ak)=p(A).
9.
Prove that Theorem 13.4.6 and Corollary 13.4.7 can be strengthened so that the condit(ion o)n the Ak is necessary as well as suﬃcient.
Hint: Consider vectors of the x form where x∈Fk.
0 10.
Show directly that if A is an n×n matrix and A=A∗ (A is Hermitian) then all the eigenvalues are real and eigenvectors can be assumed to be real and that eigenvectors associated with distinct eigenvalues are orthogonal, (their inner product is zero).
11.
Let v ,··· ,v be an orthonormal basis for Fn.
Let Q be a matrix whose ith column 1 n is v .
Show i ∗ ∗ Q Q=QQ =I.
12.
Show that an n×n matrix Q is unitary if and only if it preserves distances.
This means |Qv|=|v|.
This was done in the text but you should try to do it for yourself.
13.
Suppose {v ,··· ,v } and {w ,··· ,w } are two orthonormal bases for Fn and sup- 1 n 1 n pose Q is an n×n matrix satisfying Qv = w .
Then show Q is unitary.
If |v| = 1, i i show there is a unitary transformation which maps v to e .
1 14.
Finish the proof of Theorem 13.6.5.
15.
Let A be a Hermitian matrix so A = A∗ and suppose all eigenvalues of A are larger than δ2.
Show (Av,v)≥δ2|v|2 ∑ Where here, the inner product is (v,u)≡ n v u .
j=1 j j 16.
Suppose A+A∗ has all negative eigenvalues.
Then show that the eigenvalues of A have all negative real parts.
17.
The discrete Fourier transform maps Cn →Cn as follows.
n∑−1 1 F (x)=z where zk = √n e−i2n(cid:25)jkxj.
j=0 Show that F−1 exists and is given by the formula n∑−1 1 F−1(z)=x where xj = √n ei2n(cid:25)jkzk j=0 Here is one way to approach this problem.
Note z=Ux where   e−i2n(cid:25)0·0 e−i2n(cid:25)1·0 e−i2n(cid:25)2·0 ··· e−i2n(cid:25)(n−1)·0    e−i2n(cid:25)0·1 e−i2n(cid:25)1·1 e−i2n(cid:25)2·1 ··· e−i2n(cid:25)(n−1)·1  U = √1  e−i2n(cid:25)0·2 e−i2n(cid:25)1·2 e−i2n(cid:25)2·2 ··· e−i2n(cid:25)(n−1)·2  n .
.
.
.
  .. .. .. ..  e−i2n(cid:25)0·(n−1) e−i2n(cid:25)1·(n−1) e−i2n(cid:25)2·(n−1) ··· e−i2n(cid:25)(n−1)·(n−1) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation336 SELF ADJOINT OPERATORS Now argue U is unitary and use this to establish the result.
To show this verify each row has length 1 and the inner product of two diﬀerent rows gives 0.
Now Ukj =e−i2n(cid:25)jk and so (U∗)kj =ei2n(cid:25)jk.
18.
Letf beaperiodicfunctionhavingperiod2π.
TheFourierseriesoff isanexpression of the form ∑∞ ∑n c eikx ≡ lim c eikx k k n→∞ k=−∞ k=−n and the idea is to ﬁnd c such that the above sequence converges in some way to f. If k ∑∞ f(x)= c eikx k k=−∞ and you formally multiply both sides by e−imx and then integrate from 0 to 2π, interchanging the integral with the sum without any concern for whether this makes sense, show it is reasonable from this to expect ∫ 1 2π c = f(x)e−imxdx.
m 2π 0 Now suppose you only know f(x) at equally spaced points 2πj/n for j =0,1,··· ,n. Consider the Riemann sum for this integral ob{taine}d from using the left endpoint of thesubintervalsdeterminedfromthepartition 2πj n .
Howdoesthiscomparewith n j=0 the discrete Fourier transform?
What happens as n→∞ to this approximation?
19.
Suppose A is a real 3×3 orthogonal matrix (Recall this means AAT = ATA = I. )
having determinant 1.
Show it must have an eigenvalue equal to 1.
Note this shows there exists a vector x̸=0 such that Ax=x.
Hint: Show ﬁrst or recall that any orthogonal matrix must preserve lengths.
That is, |Ax|=|x|.
20.
LetAbeacomplexm×nmatrix.
UsingthedescriptionoftheMoorePenroseinverse in terms of the singular value decomposition, show that lim (A∗A+δI)−1A∗ =A+ δ→0+ where the convergence happens in the Frobenius norm.
Also verify, using the singular value decomposition, that the inverse exists in the above formula.
21.
Show that A+ =(A∗A)+A∗.
Hint: You might use the description of A+ in terms of the singular value decomposition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationNorms For Finite Dimensional Vector Spaces In this chapter, X and Y are ﬁnite dimensional vector spaces which have a norm.
The following is a deﬁnition.
Deﬁnition 14.0.1 A linear space X is a normed linear space if there is a norm deﬁned on X, ||·|| satisfying ||x||≥0, ||x||=0 if and only if x=0, ||x+y||≤||x||+||y||, ||cx||=|c|||x|| whenever c is a scalar.
A set, U ⊆ X, a normed linear space is open if for every p ∈ U, there exists δ >0 such that B(p,δ)≡{x:||x−p||<δ}⊆U.
Thus, a set is open if every point of the set is an interior point.
To begin with recall the Cauchy Schwarz inequality which is stated here for convenience in terms of the inner product space, Cn.
Theorem 14.0.2 The following inequality holds for a and b ∈C.
i i (cid:12) (cid:12) ( ) ( ) (cid:12)(cid:12)∑n (cid:12)(cid:12) ∑n 1/2 ∑n 1/2 (cid:12) a b (cid:12)≤ |a |2 |b |2 .
(14.1) (cid:12) i i(cid:12) i i i=1 i=1 i=1 Deﬁnition 14.0.3 Let (X,||·||) be a normed linear space and let {x }∞ be a sequence of n n=1 vectors.
Then this is called a Cauchy sequence if for all ε > 0 there exists N such that if m,n≥N, then ||x −x ||<ε.
n m This is written more brieﬂy as lim ||x −x ||=0.
n m m,n→∞ Deﬁnition 14.0.4 A normed linear space, (X,||·||) is called a Banach space if it is com- plete.
This means that, whenever, {x } is a Cauchy sequence there exists a unique x ∈ X n such that limn→∞||x−xn||=0.
337 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation338 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Let X be a ﬁnite dimensional normed linear space with norm ||·|| where the ﬁeld of scalars is denoted by F and is understood to be either R or C. Let {v ,···,v } be a basis 1 n for X.
If x∈X, denote by x the ith component of x with respect to this basis.
Thus i ∑n x= x v .
i i i=1 Deﬁnition 14.0.5 For x∈X and {v ,··· ,v } a basis, deﬁne a new norm by 1 n ( ) ∑n 1/2 |x|≡ |x |2 .
i i=1 where ∑n x= x v .
i i i=1 Similarly, for y ∈ Y with basis {w ,··· ,w }, and y its components with respect to this 1 m i basis, ( ) ∑m 1/2 |y|≡ |y |2 i i=1 For A∈L(X,Y), the space of linear mappings from X to Y, ||A||≡sup{|Ax|:|x|≤1}.
(14.2) The ﬁrst thing to show is that the two norms, ||·|| and |·|, are equivalent.
This means the conclusion of the following theorem holds.
Theorem 14.0.6 Let (X,||·||) be a ﬁnite dimensional normed linear space and let |·| be described above relative to a given basis, {v ,··· ,v }.
Then |·| is a norm and there exist 1 n constants δ,∆>0 independent of x such that δ||x||≤|x|≤∆||x||.
(14.3) Proof: Alloftheabovepropertiesofanormareobviousexceptthesecond, thetriangle inequality.
To establish this inequality, use the Cauchy Schwarz inequality to write ∑n ∑n ∑n ∑n |x+y|2 ≡ |x +y |2 ≤ |x |2+ |y |2+2Re x y i i i i i i i=1 i=1 i=1 i=1 ( ) ( ) ∑n 1/2 ∑n 1/2 ≤ |x|2+|y|2+2 |x |2 |y |2 i i i=1 i=1 = |x|2+|y|2+2|x||y|=(|x|+|y|)2 and this proves the second property above.
Itremainstoshowtheequivalenceofthetwonorms.
BytheCauchySchwarzinequality again, (cid:12)(cid:12) (cid:12)(cid:12) ( ) (cid:12)(cid:12)(cid:12)(cid:12)∑n (cid:12)(cid:12)(cid:12)(cid:12) ∑n ∑n 1/2 ||x|| ≡ (cid:12)(cid:12) x v (cid:12)(cid:12)≤ |x |||v ||≤|x| ||v ||2 (cid:12)(cid:12) i i(cid:12)(cid:12) i i i i=1 i=1 i=1 ≡ δ−1|x|.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation339 This proves the ﬁrst half of the inequality.
Suppose the second half of the inequality is not valid.
Then there exists a sequence xk ∈X such that (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)xk(cid:12)>k(cid:12)(cid:12)xk(cid:12)(cid:12), k =1,2,··· .
Then deﬁne xk yk ≡ .
|xk| It follows (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)yk(cid:12)=1, (cid:12)yk(cid:12)>k(cid:12)(cid:12)yk(cid:12)(cid:12).
(14.4) Letting yk be the components of yk with respect to the given basis, it follows the vector i ( ) yk,··· ,yk 1 n isaunitvectorinFn.BytheHeineBoreltheorem, thereexistsasubsequence, stilldenoted by k such that ( ) yk,··· ,yk →(y ,··· ,y ).
1 n 1 n It follows from (14.4) and this that for ∑n y= y v , i i i=1 (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) 0= lim (cid:12)(cid:12)(cid:12)(cid:12)yk(cid:12)(cid:12)(cid:12)(cid:12)= lim (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑n ykv (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑n y v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k→∞ k→∞(cid:12)(cid:12) i i(cid:12)(cid:12) (cid:12)(cid:12) i i(cid:12)(cid:12) i=1 i=1 but not all the y equal zero.
This contradicts the assumption that {v ,··· ,v } is a basis i 1 n and proves the second half of the inequality.
(cid:4) Corollary 14.0.7 If (X,||·||) is a ﬁnite dimensional normed linear space with the ﬁeld of scalars F=C or R, then X is complete.
Proof: Let{xk}beaCauchysequence.
Thenlettingthecomponentsofxk withrespect to the given basis be xk,··· ,xk, 1 n it follows from Theorem 14.0.6, that ( ) xk,··· ,xk 1 n is a Cauchy sequence in Fn and so ( ) xk,··· ,xk →(x ,··· ,x )∈Fn.
1 n 1 n Thus, ∑n ∑n xk = xkv → x v ∈X.
(cid:4) i i i i i=1 i=1 Corollary 14.0.8 Suppose X is a ﬁnite dimensional linear space with the ﬁeld of scalars either C or R and ||·|| and |||·||| are two norms on X.
Then there exist positive constants, δ and ∆, independent of x∈X such that δ|||x|||≤||x||≤∆|||x|||.
Thus any two norms are equivalent.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation340 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES This is very important because it shows that all questions of convergence can be consid- ered relative to any norm with the same outcome.
Proof: Let {v ,··· ,v } be a basis for X and let |·| be the norm taken with respect to 1 n thisbasiswhichwasdescribedearlier.
ThenbyTheorem14.0.6,therearepositiveconstants δ ,∆ ,δ ,∆ , all independent of x∈X such that 1 1 2 2 δ |||x|||≤|x|≤∆ |||x|||, 2 2 δ ||x||≤|x|≤∆ ||x||.
1 1 Then ∆ ∆ ∆ δ |||x|||≤|x|≤∆ ||x||≤ 1 |x|≤ 1 2 |||x||| 2 1 δ δ 1 1 and so δ ∆ 2 |||x|||≤||x||≤ 2 |||x||| (cid:4) ∆ δ 1 1 Deﬁnition 14.0.9 Let X and Y be normed linear spaces with norms ||·|| and ||·|| re- X Y spectively.
Then L(X,Y) denotes the space of linear transformations, called bounded linear transformations, mapping X to Y which have the property that ||A||≡sup{||Ax|| :||x|| ≤1}<∞.
Y X Then ||A|| is referred to as the operator norm of the bounded linear transformation A.
It is an easy exercise to verify that ||·|| is a norm on L(X,Y) and it is always the case that ||Ax|| ≤||A||||x|| .
Y X Furthermore, you should verify that you can replace ≤1 with =1 in the deﬁnition.
Thus ||A||≡sup{||Ax|| :||x|| =1}.
Y X Theorem 14.0.10 Let X and Y be ﬁnite dimensional normed linear spaces of dimension n and m respectively and denote by ||·|| the norm on either X or Y.
Then if A is any linear function mapping X to Y, then A ∈ L(X,Y) and (L(X,Y),||·||) is a complete normed linear space of dimension nm with ||Ax||≤||A||||x||.
Proof: It is necessary to show the norm deﬁned on linear transformations really is a norm.
Againtheﬁrstandthirdpropertieslistedabovefornormsareobvious.
Itremainsto show the second and verify ||A||<∞.
Letting {v ,··· ,v } be a basis and |·| deﬁned with 1 n respect to this basis as above, there exist constants δ,∆>0 such that δ||x||≤|x|≤∆||x||.
Then, ||A+B|| ≡ sup{||(A+B)(x)||:||x||≤1} ≤ sup{||Ax||:||x||≤1}+sup{||Bx||:||x||≤1} ≡ ||A||+||B||.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation341 Next consider the claim that ||A||<∞.
This follows from (cid:12)(cid:12) ( )(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) ∑n (cid:12)(cid:12)(cid:12)(cid:12) ∑n ||A(x)||=(cid:12)(cid:12)A x v (cid:12)(cid:12)≤ |x |||A(v )|| (cid:12)(cid:12) i i (cid:12)(cid:12) i i i=1 i=1 ( ) ( ) ∑n 1/2 ∑n 1/2 ≤|x| ||A(v )||2 ≤∆||x|| ||A(v )||2 <∞.
i i i=1 i=1 ( ) ∑ 1/2 Thus ||A||≤∆ n ||A(v )||2 .
i=1 i Next consider the assertion about the dimension of L(X,Y).
It follows from Theorem 9.2.3.
By Corollary 14.0.7 (L(X,Y),||·||) is complete.
If x̸=0, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) ||Ax|| 1 =(cid:12)(cid:12)(cid:12)(cid:12)A x (cid:12)(cid:12)(cid:12)(cid:12)≤||A|| (cid:4) ||x|| ||x|| NotebyCorollary14.0.8youcandeﬁneanormanywaydesiredonanyﬁnitedimensional linear space which has the ﬁeld of scalars R or C and any other way of deﬁning a norm on this space yields an equivalent norm.
Thus, it doesn’t much matter as far as notions of convergence are concerned which norm is used for a ﬁnite dimensional space.
In particular in the space of m×n matrices, you can use the operator norm deﬁned above, or some other way of giving this space a norm.
A popular choice for a norm is the Frobenius norm discussed earlier but reviewed here.
Deﬁnition 14.0.11 Make the space of m×n matrices into a Hilbert space by deﬁning (A,B)≡tr(AB∗).
Another way of describing a norm for an n×n matrix is as follows.
Deﬁnition 14.0.12 Let A be an m×n matrix.
Deﬁne the spectral norm of A, written as ||A||2 to be { } max λ1/2 :λ is an eigenvalue of A∗A .
Thatis, thelargestsingularvalueofA.
(NotetheeigenvaluesofA∗Aareallpositivebecause if A∗Ax=λx, then λ(x,x)=(A∗Ax,x)=(Ax,Ax)≥0.)
Actually, this is nothing new.
It turns out that ||·|| is nothing more than the operator 2 norm for A taken with respect to the usual Euclidean norm, ( ) ∑n 1/2 |x|= |x |2 .
k k=1 Proposition 14.0.13 The following holds.
||A|| =sup{|Ax|:|x|=1}≡||A||.
2 Proof: Note that A∗A is Hermitian and so by Corollary 13.3.5, { } ||A|| = max (A∗Ax,x)1/2 :|x|=1 2 { } = max (Ax,Ax)1/2 :|x|=1 = max{|Ax|:|x|=1}=||A||.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation342 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Here is another proof of t(his prop)osition.
Recall there are unitary matrices of the right σ 0 size U,V such that A = U V∗ where the matrix on the inside is as described 0 0 in the section on the singular value decomposition.
Then since unitary matrices preserve norms, (cid:12) ( ) (cid:12) (cid:12) ( ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ||A|| = sup (cid:12)(cid:12)U σ 0 V∗x(cid:12)(cid:12)= sup (cid:12)(cid:12)U σ 0 V∗x(cid:12)(cid:12) 0 0 0 0 |x|≤1(cid:12) ( ) (cid:12) |V∗(cid:12)(x|≤1 ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = sup (cid:12)(cid:12)U σ0 00 y(cid:12)(cid:12)= sup (cid:12)(cid:12) σ0 00 y(cid:12)(cid:12)=σ1 ≡||A||2 |y|≤1 |y|≤1 This completes the alternate proof.
From now on, ||A|| will mean either the operator norm of A taken with respect to the 2 usual Euclidean norm or the largest singular value of A, whichever is most convenient.
An interesting application of the notion of equivalent norms on Rn is the process of giving a norm on a ﬁnite Cartesian product of normed linear spaces.
Deﬁnition 14.0.14 Let X , i=1,··· ,n be normed linear spaces with norms, ||·|| .
For i i ∏n x≡(x ,··· ,x )∈ X 1 n i i=1 ∏ deﬁne θ : n X →Rn by i=1 i θ(x)≡(||x || ,··· ,||x || ) 1 1 n n ∏ Then if ||·|| is any norm on Rn, deﬁne a norm on n X , also denoted by ||·|| by i=1 i ||x||≡||θx||.
The following theorem follows immediately from Corollary 14.0.8.
The∏orem 14.0.15 Let Xi and ||·||i be given in the above deﬁnition and consider the norms o∏n ni=1Xi described there in terms of norms on Rn.
Then any two of these norms on n X obtained in this way are equivalent.
i=1 i For example, deﬁne ∑n ||x|| ≡ |x |, 1 i i=1 ||x|| ≡max{|x |,i=1,··· ,n}, ∞ i or ( ) ∑n 1/2 ||x|| = |x |2 2 i i=1 ∏ and all three are equivalent norms on n X .
i=1 i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.1.
THE P NORMS 343 14.1 The p Norms In addition to ||·|| and ||·|| mentioned above, it is common to consider the so called p 1 ∞ norms for x∈Cn.
Deﬁnition 14.1.1 Let x∈Cn.
Then deﬁne for p≥1, ( ) ∑n 1/p ||x|| ≡ |x |p p i i=1 The following inequality is called Holder’s inequality.
Proposition 14.1.2 For x,y∈Cn, ( ) ( ) ∑n ∑n 1/p ∑n 1/p′ |x ||y |≤ |x |p |y |p′ i i i i i=1 i=1 i=1 The proof will depend on the following lemma.
Lemma 14.1.3 If a,b≥0 and p′ is deﬁned by 1 + 1 =1, then p p′ ap bp′ ab≤ + .
p p′ Proof of the Proposition: If x or y equals the zero vector there is nothing to ∑ prove.
Therefore, assume they are both nonzero.
Let A = ( n |x |p)1/p and B = ( ) i=1 i ∑n |y |p′ 1/p′.
Then using Lemma 14.1.3, i=1 i [ ( ) ( ) ] ∑n |x ||y | ∑n 1 |x | p 1 |y | p′ i i ≤ i + i A B p A p′ B i=1 i=1 ∑n ∑n = 1 1 |x |p+ 1 1 |y |p′ pAp i p′Bp i i=1 i=1 1 1 = + =1 p p′ and so ( ) ( ) ∑n ∑n 1/p ∑n 1/p′ |x ||y |≤AB = |x |p |y |p′ .
(cid:4) i i i i i=1 i=1 i=1 Theorem 14.1.4 The p norms do indeed satisfy the axioms of a norm.
Proof: It is obvious that ||·|| does indeed satisfy most of the norm axioms.
The only p one that is not clear is the triangle inequality.
To save notation write ||·|| in place of ||·|| p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation344 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES in what follows.
Note also that p =p−1.
Then using the Holder inequality, p′ ∑n ||x+y||p = |x +y |p i i i=1 ∑n ∑n ≤ |x +y |p−1|x |+ |x +y |p−1|y | i i i i i i i=1 i=1 ∑n ∑n = |xi+yi|pp′ |xi|+ |xi+yi|pp′ |yi| (i=1 ) i=(1 ) ( )  ∑n 1/p′ ∑n 1/p ∑n 1/p ≤ |x +y |p  |x |p + |y |p  i i i i i=1 i=1 i=1 ( ) = ||x+y||p/p′ ||x|| +||y|| p p so dividing by ||x+y||p/p′, it follows ||x+y||p||x+y||−p/p′ =||x+y||≤||x|| +||y|| p p ( ( ) ) p− p =p 1− 1 =p1 =1.
.
(cid:4) p′ p′ p It only remains to prove Lemma 14.1.3.
Proof of the lemma: Letp′ =q tosaveonnotationandconsiderthefollowingpicture: x b x=tp−1 t=xq−1 t a ∫ ∫ a b ap bq ab≤ tp−1dt+ xq−1dx= + .
p q 0 0 Note equality occurs when ap =bq.
Alternate proof of the lemma: Let ( ) 1 1 b q f(t)≡ (at)p+ , t>0 p q t You see right away it is decreasing for a while, having an asymptote at t = 0 and then reaches a minimum and increases from then on.
Take its derivative.
( ) ( ) f′(t)=(at)p−1a+ b q−1 −b t t2 Set it equal to 0.
This happens when bq tp+q = .
(14.5) ap Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.2.
THE CONDITION NUMBER 345 Thus bq/(p+q) t= ap/(p+q) and so at this value of t, ( ) b at=(ab)q/(p+q), =(ab)p/(p+q).
t Thus the minimum of f is ( ) ( ) 1 p 1 q (ab)q/(p+q) + (ab)p/(p+q) =(ab)pq/(p+q) p q but recall 1/p+1/q =1 and so pq/(p+q)=1.
Thus the minimum value of f is ab.
Letting t=1, this shows ap bq ab≤ + .
p q Note that equality occurs when the minimum value happens for t = 1 and this indicates from (14.5) that ap =bq.
(cid:4) Now ||A|| may be considered as the operator norm of A taken with respect to ||·|| .
In p p the case when p = 2, this is just the spectral norm.
There is an easy estimate for ||A|| in p terms of the entries of A. Theorem 14.1.5 The following holds.
    q/p 1/q ∑ ∑   ||A|| ≤  |A |p  p jk k j Proof: Let||x|| ≤1andletA=(a ,··· ,a )wherethea arethecolumnsofA.Then p 1 n k ( ) ∑ Ax= x a k k k and so by Holder’s inequality, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)∑ (cid:12)(cid:12) ∑ (cid:12)(cid:12) (cid:12)(cid:12) ||Ax|| ≡ (cid:12)(cid:12) x a (cid:12)(cid:12) ≤ |x |||a || p (cid:12)(cid:12) k k(cid:12)(cid:12) k k p k p k ( ) ( ) ∑ 1/p ∑ 1/q ≤ |x |p ||a ||q k k p k k     q/p 1/q ∑ ∑   ≤   |A |p  (cid:4) jk k j 14.2 The Condition Number Let A ∈ L(X,X) be a linear transformation where X is a ﬁnite dimensional vector space and consider the problem Ax = b where it is assumed there is a unique solution to this problem.
How does the solution change if A is changed a little bit and if b is changed a Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation346 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES little bit?
This is clearly an interesting question because you often do not know A and b exactly.
If a small change in these quantities results in a large change in the solution, x, then it seems clear this would be undesirable.
In what follows ||·|| when applied to a linear transformation will always refer to the operator norm.
Lemma 14.2.1 Let A,B ∈L(X,X) where X is a normed vector space as above.
Then for ||·|| denoting the operator norm, ||AB||≤||A||||B||.
Proof: This follows from the deﬁnition.
Letting ||x|| ≤ 1, it follows from Theorem 14.0.10 ||ABx||≤||A||||Bx||≤||A||||B||||x||≤||A||||B|| and so ||AB||≡ sup ||ABx||≤||A||||B||.
(cid:4) ||x||≤1 (cid:12)(cid:12) (cid:12)(cid:12) Lemma 14.2.2 Let A,B ∈ L(X,X),A−1 ∈ L(X,X), and suppose ||B|| < 1/(cid:12)(cid:12)A−1(cid:12)(cid:12).
Then (A+B)−1 exists and (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A+B)−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤(cid:12)(cid:12)(cid:12)(cid:12)A−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1 (cid:12)(cid:12)(cid:12)(cid:12).
1−||A−1B|| (cid:12)(cid:12) (cid:12)(cid:12) The above formula makes sense because (cid:12)(cid:12)A−1B(cid:12)(cid:12)<1.
Proof: By Lemma 14.2.1, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)A−1B(cid:12)(cid:12)≤(cid:12)(cid:12)A−1(cid:12)(cid:12)||B||<(cid:12)(cid:12)A−1(cid:12)(cid:12) 1 =1 ||A−1|| ( ) ( Suppose) (A+B)x = 0.
Then 0 = A I+A−1B x and so since A is one to one, I+A−1B x=0.
Therefore, (cid:12)(cid:12)( ) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) 0 = (cid:12)(cid:12) I+A−1B x(cid:12)(cid:12)≥||x||−(cid:12)(cid:12)A−1Bx(cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) ( (cid:12)(cid:12) (cid:12)(cid:12)) ≥ ||x||−(cid:12)(cid:12)A−1B(cid:12)(cid:12)||x||= 1−(cid:12)(cid:12)A−1B(cid:12)(cid:12) ||x||>0 ( ) acontradiction.
Thisalsoshows I+A−1B isonetoone.
Therefore, both(A+B)−1 and ( ) I+A−1B −1 are in L(X,X).
Hence ( ( )) ( ) (A+B)−1 = A I+A−1B −1 = I+A−1B −1A−1 Now if ( ) x= I+A−1B −1y for ||y||≤1, then ( ) I+A−1B x=y and so ( (cid:12)(cid:12) (cid:12)(cid:12)) (cid:12)(cid:12) (cid:12)(cid:12) ||x|| 1−(cid:12)(cid:12)A−1B(cid:12)(cid:12) ≤(cid:12)(cid:12)x+A−1Bx(cid:12)(cid:12)≤||y||=1 and so (cid:12)(cid:12) (cid:12)(cid:12) ||x||=(cid:12)(cid:12)(cid:12)(cid:12)(I+A−1B)−1y(cid:12)(cid:12)(cid:12)(cid:12)≤ 1 1−||A−1B|| Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.2.
THE CONDITION NUMBER 347 Since ||y||≤1 is arbitrary, this shows (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(I+A−1B)−1(cid:12)(cid:12)(cid:12)(cid:12)≤ 1 1−||A−1B|| Therefore, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(A+B)−1(cid:12)(cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)(cid:12)(cid:12)(I+A−1B)−1A−1(cid:12)(cid:12)(cid:12)(cid:12) ≤ (cid:12)(cid:12)(cid:12)(cid:12)A−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(I+A−1B)−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤(cid:12)(cid:12)(cid:12)(cid:12)A−1(cid:12)(cid:12)(cid:12)(cid:12) 1 (cid:4) 1−||A−1B|| Proposition 1(cid:12)4(cid:12).2.3(cid:12)(cid:12)Suppose A is invertible, b ̸= 0, Ax = b, and A1x1 = b1 where ||A−A ||<1/(cid:12)(cid:12)A−1(cid:12)(cid:12).
Then 1 ( ) ||x1−x|| ≤ 1 ||A||(cid:12)(cid:12)(cid:12)(cid:12)A−1(cid:12)(cid:12)(cid:12)(cid:12) ||A1−A|| + ||b−b1|| .
(14.6) ||x|| (1−||A−1(A −A)||) ||A|| ||b|| 1 Proof: It follows from the assumptions that Ax−A x+A x−A x =b−b .
1 1 1 1 1 Hence A (x−x )=(A −A)x+b−b .
1 1 1 1 Now A =(A+(A −A)) and so by the above lemma, A−1 exists and so 1 1 1 (x−x )=A−1(A −A)x+A−1(b−b ) 1 1 1 1 1 =(A+(A −A))−1(A −A)x+(A+(A −A))−1(b−b ).
1 1 1 1 By the estimate in Lemma 14.2.2, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)A−1(cid:12)(cid:12) ||x−x ||≤ (||A −A||||x||+||b−b ||).
1 1−||A−1(A −A)|| 1 1 1 Dividing by ||x||, (cid:12)(cid:12) (cid:12)(cid:12) ( ) ||x−x || (cid:12)(cid:12)A−1(cid:12)(cid:12) ||b−b || 1 ≤ ||A −A||+ 1 (14.7) ||x|| 1−||A−1(A −A)|| 1 ||x|| 1 ( ) (cid:12)(cid:12) (cid:12)(cid:12) Now b=Ax=A A−1b and so ||b||≤||A||(cid:12)(cid:12)A−1b(cid:12)(cid:12) and so (cid:12)(cid:12) (cid:12)(cid:12) ||x||=(cid:12)(cid:12)A−1b(cid:12)(cid:12)≥||b||/||A||.
Therefore, from (14.7), (cid:12)(cid:12) (cid:12)(cid:12) ( ) ||x−x || (cid:12)(cid:12)A−1(cid:12)(cid:12) ||A||||A −A|| ||A||||b−b || 1 ≤ 1 + 1 ||x|| 1−||A−1(A −A)|| ||A|| ||b|| (cid:12)(cid:12) (cid:12)(cid:12) 1 ( ) (cid:12)(cid:12)A−1(cid:12)(cid:12)||A|| ||A −A|| ||b−b || ≤ 1 + 1 1−||A−1(A −A)|| ||A|| ||b|| 1 which proves the proposition.
(cid:4) (cid:12)(cid:12) (cid:12)(cid:12) This shows that the number, (cid:12)(cid:12)A−1(cid:12)(cid:12)||A||, controls how sensitive the relative change in the solution of Ax = b is to small changes in A and b.
This number is called the condition number.
It is bad when it is large because a small relative change in b, for example could yield a large relative change in x. RecallthatforAann×nmatrix,||A|| =σ whereσ isthelargestsingularvalue.
The 2 1 1 largestsingularvalueofA−1 istherefore, 1/σ whereσ isthesmallestsingularvalueofA.
n n Therefore, the condition number reduces to σ /σ , the ratio of the largest to the smallest 1 n singular value of A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation348 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES 14.3 The Spectral Radius EventhoughitisingeneralimpracticaltocomputetheJordanform,itsexistenceisallthat is needed in order to prove an important theorem about something which is relatively easy to compute.
This is the spectral radius of a matrix.
Deﬁnition 14.3.1 Deﬁne σ(A) to be the eigenvalues of A.
Also, ρ(A)≡max(|λ|:λ∈σ(A)) The number, ρ(A) is known as the spectral radius of A.
Recall the following symbols and their meaning.
lim sup a , lim inf a n n n→∞ n→∞ They are respectively the largest and smallest limit points of the sequence {a } where ±∞ n is allowed in the case where the sequence is unbounded.
They are also deﬁned as lim sup a ≡ lim (sup{a :k ≥n}), n k n→∞ n→∞ lim inf a ≡ lim (inf{a :k ≥n}).
n k n→∞ n→∞ Thus, the limit of the sequence exists if and only if these are both equal to the same real number.
Lemma 14.3.2 Let J be a p×p Jordan matrix   J 1   J = ...  J s where each J is of the form k J =λ I+N k k k in which N is a nilpotent matrix having zeros down the main diagonal and ones down the k super diagonal.
Then lim ||Jn||1/n =ρ n→∞ where ρ=max{|λ |,k =1,...,n}.
Here the norm is deﬁned to equal k ||B||=max{|B |,i,j}.
ij Proof: Supposeﬁrstthatρ̸=0.Firstnotethatforthisnorm,ifB,C arep×pmatrices, ||BC||≤p||B||||C|| which follows from a simple computation.
Now (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) (λ1I+N1)n (cid:12)(cid:12)(cid:12)(cid:12)1/n (cid:12)(cid:12) (cid:12)(cid:12) ||Jn||1/n =(cid:12)(cid:12)(cid:12)(cid:12) ... (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (λ I+N )n (cid:12)(cid:12) s s Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.3.
THE SPECTRAL RADIUS 349 (cid:12)(cid:12) ( ) (cid:12)(cid:12) (cid:12)(cid:12) n (cid:12)(cid:12)1/n (cid:12)(cid:12)(cid:12)(cid:12) λρ1I+ ρ1N1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) =ρ(cid:12)(cid:12)(cid:12)(cid:12) ... ( ) (cid:12)(cid:12)(cid:12)(cid:12) (14.8) (cid:12)(cid:12) n (cid:12)(cid:12) (cid:12)(cid:12) λρ2I+ ρ1N2 (cid:12)(cid:12) From the deﬁnition of ρ, at least one of the λ /ρ has absolute value equal to 1.
Therefore, k (cid:12)(cid:12) ( ) (cid:12)(cid:12) (cid:12)(cid:12) n (cid:12)(cid:12)1/n (cid:12)(cid:12)(cid:12)(cid:12) λρ1I+ ρ1N1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) ... ( ) (cid:12)(cid:12)(cid:12)(cid:12) −1≡en ≥0 (cid:12)(cid:12) n (cid:12)(cid:12) (cid:12)(cid:12) λρ2I+ ρ1N2 (cid:12)(cid:12) because each N has only zero terms on the main diagonal.
Therefore, some term in the k matrix has absolute value at least as large as 1.
Now also, since Np = 0, the norm of k the matrix in the above is dominated by an expression of the form Cnp where C is some constant which does not depend on n. This is because a typical block in the above matrix is of the form ( )( ) ∑p n λ n−i k Ni i ρ k i=1 and each |λ |≤ρ.
k It follows that for n>p+1, ( ) n Cnp ≥(1+e )n ≥ ep+1 n p+1 n and so ( ) 1/(p+1) Cnp ( ) ≥e ≥0 n n p+1 Therefore, limn→∞en = 0.
It follows from (14.8) that the expression in the norms in this equation converges to 1 and so lim ||Jn||1/n =ρ.
n→∞ In case ρ = 0 so that all the eigenvalues equal zero, it follows that Jn = 0 for all n > p. Therefore, the limit still exists and equals ρ.
(cid:4) The following theorem is due to Gelfand around 1941.
Theorem 14.3.3 (Gelfand) Let A be a complex p×p matrix.
Then if ρ is the absolute value of its largest eigenvalue, lim ||An||1/n =ρ.
n→∞ Here ||·|| is any norm on L(Cn,Cn).
Proof: First assume ||·|| is the special norm of the above lemma.
Then letting J denote the Jordan form of A,S−1AS =J, it follows from Lemma 14.3.2 (cid:12)(cid:12) (cid:12)(cid:12) lim sup ||An||1/n = lim sup (cid:12)(cid:12)SJnS−1(cid:12)(cid:12)1/n n→∞ n→∞ (( ) (cid:12)(cid:12) (cid:12)(cid:12)) ≤ lim sup p2 ||S||(cid:12)(cid:12)S−1(cid:12)(cid:12) 1/n||Jn||1/n =ρ n→∞ Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation350 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES (cid:12)(cid:12) (cid:12)(cid:12) = lim inf ||Jn||1/n =lim inf (cid:12)(cid:12)S−1AnS(cid:12)(cid:12)n n→∞ n→∞ (( ) (cid:12)(cid:12) (cid:12)(cid:12)) = lim inf p2 ||S||(cid:12)(cid:12)S−1(cid:12)(cid:12) 1/n||An||1/n =lim inf ||An||1/n n→∞ n→∞ If follows that liminfn→∞||An||1/n =limsupn→∞||An||1/n =limn→∞||An||1/n =ρ.
Now by equivalence of norms, if |||·||| is any other norm for the set of complex p×p matrices, there exist constants δ,∆ such that δ||An||≤|||An|||≤∆||An|| Then raising to the 1/n power and taking a limit, ρ≤lim inf |||An|||1/n ≤lim sup |||An|||1/n ≤ρ (cid:4) n→∞ n→∞   9 −1 2 Example 14.3.4 Consider  −2 8 4 .
Estimate the absolute value of the largest 1 1 8 eigenvalue.
A laborious computation reveals the eigenvalues are 5, and 10.
Therefore, the right (cid:12)(cid:12) (cid:12)(cid:12) answer in this case is 10.
Consider (cid:12)(cid:12)A7(cid:12)(cid:12)1/7 where the norm is obtained by taking the maximum of all the absolute values of the entries.
Thus     9 −1 2 7 8015625 −1984375 3968750  −2 8 4  = −3968750 6031250 7937500  1 1 8 1984375 1984375 6031250 and taking the seventh root of the largest entry gives ρ(A)≈80156251/7 =9.68895123671.
Ofcoursetheinterestliesprimarilyinmatricesforwhichtheexactrootstothecharacteristic equation are not known and in the theoretical signiﬁcance.
14.4 Series And Sequences Of Linear Operators Before beginning this discussion, it is necessary to deﬁne what is meant by convergence in L(X,Y).
Deﬁnition 14.4.1 Let {A }∞ be a sequence in L(X,Y) where X,Y are ﬁnite dimen- k k=1 sional normed linear spaces.
Then limn→∞Ak = A if for every ε > 0 there exists N such that if n>N, then ||A−A ||<ε.
n Here the norm refers to any of the norms deﬁned on L(X,Y).
By Corollary 14.0.8 and Theorem 9.2.3 it doesn’t matter which one is used.
Deﬁne the symbol for an inﬁnite sum in the usual way.
Thus ∑∞ ∑n A ≡ lim A k k n→∞ k=1 k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.4.
SERIES AND SEQUENCES OF LINEAR OPERATORS 351 Lemma 14.4.2 Suppose {A }∞ is a sequence in L(X,Y) where X,Y are ﬁnite dimen- k k=1 sional normed linear spaces.
Then if ∑∞ ||A ||<∞, k k=1 It follows that ∑∞ A (14.9) k k=1 exists.
In words, absolute convergence implies convergence.
Proof: For p≤m≤n, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)∑n ∑m (cid:12)(cid:12)(cid:12)(cid:12) ∑∞ (cid:12)(cid:12) A − A (cid:12)(cid:12)≤ ||A || (cid:12)(cid:12) k k(cid:12)(cid:12) k k=1 k=1 k=p andsoforplargeenough, thistermontherightintheaboveinequalityislessthanε.Since ε is arbitrary, this shows the partial sums of (14.9) are a Cauchy sequence.
Therefore by Corollary 14.0.7 it follows that these partial sums converge.
(cid:4) As a special case, suppose λ∈C and consider ∑∞ tkλk k!
k=0 where t∈R.
In this case, A = tkλk and you can think of it as being in L(C,C).
Then the k k!
following corollary is of great interest.
Corollary 14.4.3 Let ∑∞ tkλk ∑∞ tkλk f(t)≡ ≡1+ k!
k!
k=0 k=1 Then this function is a well deﬁned complex valued function and furthermore, it satisﬁes the initial value problem, ′ y =λy, y(0)=1 Furthermore, if λ=a+ib, |f|(t)=eat.
Proof: That f(t) makes sense follows right away from Lemma 14.4.2.
(cid:12) (cid:12) ∑∞ (cid:12)(cid:12)tkλk(cid:12)(cid:12) ∑∞ |t|k|λ|k (cid:12) (cid:12)= =e|t||λ| (cid:12) k!
(cid:12) k!
k=0 k=0 It only remains to verify f satisﬁes the diﬀerential equation because it is obvious from the series that f(0)=1.
( ) f(t+h)−f(t) 1 ∑∞ (t+h)k−tk λk = h h k!
k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation352 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES and by the mean value theorem this equals an expression of the following form where θ is k a number between 0 and 1.
∑∞ k(t+θ h)k−1λk ∑∞ (t+θ h)k−1λk k = k k!
(k−1)!
k=1 k=1 ∑∞ (t+θ h)kλk = λ k k!
k=0 It only remains to verify this converges to ∑∞ tkλk λ =λf(t) k!
k=0 as h→0.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑∞ (t+θkkh!
)kλk −∑∞ tkkλ!k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑∞ ((t+θkhk)k!
−tk)λk(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k=0 k=0 k=0 and by the mean value theorem again and the triangle inequality (cid:12) (cid:12) (cid:12)(cid:12)∑∞ k|(t+η )|k−1|h||λ|k(cid:12)(cid:12) ∑∞ k|(t+η )|k−1|λ|k ≤(cid:12) k (cid:12)≤|h| k (cid:12) k!
(cid:12) k!
k=0 k=0 where η is between 0 and 1.
Thus k ∑∞ k(|t|+1)k−1|λ|k ≤|h| =|h|C(t) k!
k=0 It follows f′(t)=λf(t).
This proves the ﬁrst part.
Next note that for f(t)=u(t)+iv(t), both u,v are diﬀerentiable.
This is because f +f f −f u= , v = .
2 2i Then from the diﬀerential equation, ′ ′ (a+ib)(u+iv)=u +iv and equating real and imaginary parts, u′ =au−bv, v′ =av+bu.
Then a short computation shows ( ) ( ) ( ) ′ u2+v2 =2a u2+v2 , u2+v2 (0)=1.
Now in general, if ′ y =cy, y(0)=1, with c real it follows y(t)=ect.
To see this, y′−cy =0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.4.
SERIES AND SEQUENCES OF LINEAR OPERATORS 353 and so, multiplying both sides by e−ct you get ( ) d ye−ct =0 dt and so ye−ct equals a constant which must be 1 because of the initial condition y(0) = 1.
Thus ( ) u2+v2 (t)=e2at and taking square roots yields the desired conclusion.
(cid:4) Deﬁnition 14.4.4 The function in Corollary 14.4.3 given by that power series is denoted as exp(λt) or eλt.
The next lemma is normally discussed in advanced calculus courses but is proved here for the convenience of the reader.
It is known as the root test.
Deﬁnition 14.4.5 For {a } any sequence of real numbers n lim sup a ≡ lim (sup{a :k ≥n}) n k n→∞ n→∞ Similarly lim inf a ≡ lim (inf{a :k ≥n}) n k n→∞ n→∞ In case A is an increasing (decreasing) sequence which is unbounded above (below) then it n is understood that limn→∞An =∞(−∞) respectively.
Thus either of limsup or liminf can equal +∞ or −∞.
However, the important thing about these is that unlike the limit, these always exist.
It is convenient to think of these as the largest point which is the limit of some sub- sequence of {a } and the smallest point which is the limit of some subsequence of {a } n n respectively.
Thus limn→∞an exists and equals some point of [−∞,∞] if and only if the two are equal.
Lemma 14.4.6 Let {a } be a sequence of nonnegative terms and let p r =lim sup a1/p.
p p→∞ ∑ ∞ Thenifr <1,itfollowstheseries, a convergesandifr >1,thena failstoconverge k=1 k p to 0 so the series diverges.
If A is an n×n matrix and 1<lim sup ||Ap||1/p, (14.10) p→∞ ∑ then ∞ Ak fails to converge.
k=0 Proof: Suppose r <1.
Then there exists N such that if p>N, a1/p <R p where r < R <∑1.
Therefore, fo∑r all such p, ap < Rp and so by comparison with the geometric series, Rp, it follows ∞ a converges.
p=1 p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation354 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Next suppose r >1.
Then letting 1 <R <r, it follows there are inﬁnitely many values of p at which R<a1/p p which implies Rp < a , showing that a cannot converge to 0 and so the series cannot p p converge either.
To see the last claim, if {(1∑4.10) ho}lds, then from the ﬁrst part of this lem∑ma, ||Ap|| fails to converge to 0 and so m Ak ∞ is not a Cauchy sequence.
Hence ∞ Ak ≡ ∑ k=0 m=0 k=0 limm→∞ mk=0Ak cannot exist.
(cid:4) Now denote by σ(A)p the collection of all numbers of the form λp where λ∈σ(A).
Lemma 14.4.7 σ(Ap)=σ(A)p Proof: Indealingwithσ(Ap),issuﬃcestodealwithσ(Jp)whereJ istheJordanform of A because Jp and Ap are similar.
Thus if λ ∈ σ(Ap), then λ ∈ σ(Jp) and so λ = α where α is one of the entries on the main diagonal of Jp.
These entries are of the form λp where λ∈σ(A).
Thus λ∈σ(A)p and this shows σ(Ap)⊆σ(A)p. Now take α∈σ(A) and consider αp.
( ) αpI−Ap = αp−1I+···+αAp−2+Ap−1 (αI −A) and so αpI −Ap fails to be one to one which shows that αp ∈ σ(Ap) which shows that σ(A)p ⊆σ(Ap).
(cid:4) 14.5 Iterative Methods For Linear Systems Consider the problem of solving the equation Ax=b (14.11) where A is an n×n matrix.
In many applications, the matrix A is huge and composed mainly of zeros.
For such matrices, the method of Gauss elimination (row operations) is not a good way to solve the system because the row operations can destroy the zeros and storing all those zeros takes a lot of room in a computer.
These systems are called sparse.
To solve them, it is common to use an iterative technique.
I am following the treatment given to this subject by Nobel and Daniel [20].
Deﬁnition 14.5.1 The Jacobi iterative technique, also called the method of simultaneous corrections is deﬁned as follows.
Let x1 be an initial vector, say the zero vector or some other vector.
The method generates a succession of vectors, x2,x3,x4,··· and hopefully this sequenceofvectorswillconvergetothesolutionto(14.11).
Thevectorsinthislistarecalled iterates and they are obtained according to the following procedure.
Letting A=(a ), ij ∑ a xr+1 =− a xr+b .
(14.12) ii i ij j i j̸=i In terms of matrices, letting   ∗ ··· ∗ A= ... ... ...  ∗ ··· ∗ Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.5.
ITERATIVE METHODS FOR LINEAR SYSTEMS 355 The iterates are deﬁned as    ∗ 0 ··· 0 xr+1  0... .∗.. ...... 0...  x1r2...+1  0 ··· 0 ∗ xrn+1      0 ∗ ··· ∗ xr b = − ∗... .0.. ...... ∗...  x...1r2 + b...12  (14.13) ∗ ··· ∗ 0 xrn bn The matrix on the left in (14.13) is obtained by retaining the main diagonal of A and setting every other entry equal to zero.
The matrix on the right in (14.13) is obtained from Abysettingeverydiagonalentryequaltozeroandretainingalltheotherentriesunchanged.
Example 14.5.2 Use the Jacobi method to solve the system      3 1 0 0 x 1 1       1 4 1 0  x2 = 2  0 2 5 1 x 3 3 0 0 2 4 x 4 4 Of course this is solved most easily using row reductions.
The Jacobi method is useful when the matrix is 1000×1000 or larger.
This example is just to illustrate how the method works.
First lets solve it using row operations.
The augmented matrix is   3 1 0 0 1    1 4 1 0 2    0 2 5 1 3 0 0 2 4 4 The row reduced echelon form is   1 0 0 0 6  0 1 0 0 1219   0 0 1 0 289  29 0 0 0 1 25 29 which in terms of decimals is approximately equal to   1.0 0 0 0 .206    0 1.0 0 0 .379   .
0 0 1.0 0 .275 0 0 0 1.0 .862 In terms of the matrices, the Jacobi iteration is of the form         3 0 0 0 xr+1 0 1 0 0 xr 1  00 40 05 00  xx1r2r++11 =− 10 02 10 01  xx1r2r + 23 .
3 3 0 0 0 4 xr+1 0 0 2 0 xr 4 4 4 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation356 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Multiplying by the inverse of the matrix on the left, 1this iteration reduces to        xr+1 0 1 0 0 xr 1  xx1r2r++11 =− 014 032 014 01  xx1r2r + 3123 .
(14.14) 3 5 5 3 5 xr+1 0 0 1 0 xr 1 4 2 4 Now iterate this starting with   0   x1 ≡ 0 .
0 0 Thus        0 1 0 0 0 1 1 x2 =− 014 032 014 01  00 + 3123 = 3123  5 5 5 5 0 0 1 0 0 1 1 2 Then  z }x|2 {     0 1 0 0 1 1 .166 x3 =− 014 032 014 01  3123 + 3123 = ..226  5 5 5 5 0 0 1 0 1 1 .7 2 Continuing this way one ﬁnally gets z }x|5 {        0 1 0 0 .197 1 .216 x6 =− 014 032 014 01  ..2355616 + 3123 = ..328965 .
5 5 5 0 0 1 0 .822 1 .871 2 You can keep going like this.
Recall the solution is approximately equal to   .206    .379    .275 .862 so you see that with no care at all and only 6 iterations, an approximate solution has been obtained which is not too far oﬀ from the actual solution.
It is important to realize that a computer would use (14.12) directly.
Indeed, writing theproblemintermsofmatricesasIhavedoneabovedestroyseverybeneﬁtofthemethod.
However, it makes it a little easier to see what is happening and so this is why I have presented it in this way.
Deﬁnition 14.5.3 The Gauss Seidel method, also called the method of successive correc- tions is given as follows.
For A = (a ), the iterates for the problem Ax=b are obtained ij according to the formula ∑i ∑n a xr+1 =− a xr+b .
(14.15) ij j ij j i j=1 j=i+1 1Youcertainlywouldnotcomputetheinveseinsolvingalargesystem.
Thisisjusttoshowyouhowthe methodworksforthissimpleexample.
Youwouldusetheﬁrstdescriptionintermsofindices.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.5.
ITERATIVE METHODS FOR LINEAR SYSTEMS 357 In terms of matrices, letting   ∗ ··· ∗ A= ... ... ...  ∗ ··· ∗ The iterates are deﬁned as    ∗ 0 ··· 0 xr+1  ∗... .∗.. ...... 0...  x1r2...+1  ∗ ··· ∗ ∗ xr+1 n      0 ∗ ··· ∗ xr b = − 0... .0.. ...... ∗...  x...1r2 + b...12  (14.16) 0 ··· 0 0 xrn bn In words, you set every entry in the original matrix which is strictly above the main diagonal equal to zero to obtain the matrix on the left.
To get the matrix on the right, you set every entry of A which is on or below the main diagonal equal to zero.
Using the iterationprocedureof(14.15)directly,theGaussSeidelmethodmakesuseoftheverylatest information which is available at that stage of the computation.
The following example is the same as the example used to illustrate the Jacobi method.
Example 14.5.4 Use the Gauss Seidel method to solve the system      3 1 0 0 x 1 1       1 4 1 0  x2 = 2  0 2 5 1 x 3 3 0 0 2 4 x 4 4 In terms of matrices, this procedure is         3 0 0 0 xr+1 0 1 0 0 xr 1  10 42 05 00  xx1r2r++11 =− 00 00 10 01  xx1r2r + 23 .
3 3 0 0 2 4 xr+1 0 0 0 0 xr 4 4 4 Multiplying by the inverse of the matrix on the left2 this yields        xr+1 0 1 0 0 xr 1  xx1r2r++11 =− 00 −31112 −141 01  xx1r2r + 113523  3 30 10 5 3 30 xr+1 0 − 1 1 − 1 xr 47 4 60 20 10 4 60 As before, I will be totally unoriginal in the choice of x1.
Let it equal the zero vector.
Therefore,   1  35  x2 = 1123 .
30 47 60 2As in the case of the Jacobi iteration, the computer would not do this.
It would use the iteration procedureintermsoftheentriesofthematrixdirectly.
Otherwiseallbeneﬁttousingthismethodislost.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation358 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Now z }x|2 {        0 1 0 0 1 1 .194 x3 =− 00 −31112 −141 01  113523 + 113523 = ..334036 .
30 10 5 30 30 0 − 1 1 − 1 47 47 .846 60 20 10 60 60 It follows        0 1 0 0 .194 1 .219 x4 =− 00 −31112 −141 01  ..334036 + 113523 = ..326883735  30 10 5 30 0 − 1 1 − 1 .846 47 .85835 60 20 10 60 and so        0 1 0 0 .219 1 .21042 x5 =− 00 −31112 −141 01  ..326883735 + 113523 = ..327767577 .
30 10 5 30 0 − 1 1 − 1 .85835 47 .86115 60 20 10 60 Recall the answer is   .206    .379    .275 .862 sotheiteratesarealreadyprettyclosetotheanswer.
Youcouldcontinuedoingtheseiterates and it appears they converge to the solution.
Now consider the following example.
Example 14.5.5 Use the Gauss Seidel method to solve the system      1 4 0 0 x 1 1       1 4 1 0  x2 = 2  0 2 5 1 x 3 3 0 0 2 4 x 4 4 The exact solution is given by doing row operations on the augmented matrix.
When this is done the row echelon form is   1 0 0 0 6  0 1 0 0 −5   4  0 0 1 0 1 0 0 0 1 1 2 and so the solution is approximately     6 6.0  −5   −1.25   4 =  1 1.0 1 .5 2 The Gauss Seidel iterations are of the form         1 0 0 0 xr+1 0 4 0 0 xr 1  10 42 05 00  xx1r2r++11 =− 00 00 10 01  xx1r2r + 23  3 3 0 0 2 4 xr+1 0 0 0 0 xr 4 4 4 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.5.
ITERATIVE METHODS FOR LINEAR SYSTEMS 359 and so, multiplying by the inverse of the matrix on the left, the iteration reduces to the following in terms of matrix multiplication.
    0 4 0 0 1 xr+1 =− 00 −21 −141 01 xr+ 141 .
5 10 5 2 0 −1 1 − 1 3 5 20 10 4 This time, I will pick an initial vector close to the answer.
Let   6 x1 = −1  1 1 2 This is very close to the answer.
Now lets see what the Gauss Seidel iteration does to it.
       0 4 0 0 6 1 5.0 x2 =− 00 −21 −141 01  −11 + 141 = −.19.0  5 10 5 2 0 −1 1 − 1 1 3 .55 5 20 10 2 4 You can’t expect to be real close after only one iteration.
Lets do another.
       0 4 0 0 5.0 1 5.0 x3 =− 00 −21 −141 01  −.19.0 + 141 = −..89875  5 10 5 2 0 −1 1 − 1 .55 3 .56 5 20 10 4        0 4 0 0 5.0 1 4.9 x4 =− 00 −21 −141 01  −..89875 + 141 = −..896465  5 10 5 2 0 −1 1 − 1 .56 3 .567 5 20 10 4 The iterates seem to be getting farther from the actual solution.
Why is the process which worked so well in the other examples not working here?
A better question might be: Why does either process ever work at all?
Both iterative procedures for solving Ax=b (14.17) are of the form Bxr+1 =−Cxr+b where A = B + C. In the Jacobi procedure, the matrix C was obtained by setting the diagonal of A equal to zero and leaving all other entries the same while the matrix B was obtainedbymakingeveryentryofAequaltozerootherthanthediagonalentrieswhichare leftunchanged.
IntheGaussSeidelprocedure,thematrixBwasobtainedfromAbymaking everyentrystrictlyabovethemaindiagonalequaltozeroandleavingtheothersunchanged and C was obtained from A by making every entry on or below the main diagonal equal to zeroandleavingtheothersunchanged.
ThusintheJacobiprocedure,B isadiagonalmatrix whileintheGaussSeidelprocedure,B islowertriangular.
Usingmatricestoexplicitlysolve for the iterates, yields xr+1 =−B−1Cxr+B−1b.
(14.18) Thisiswhatyouwouldneverhavethecomputerdobutthisiswhatwillallowthestatement ofatheoremwhichgivestheconditionforconvergenceoftheseandallothersimilarmethods.
Recall the deﬁnition of the spectral radius of M,ρ(M), in Deﬁnition 14.3.1 on Page 348.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation360 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES ( ) Theorem 14.5.6 Suppose ρ B−1C < 1.
Then the iterates in (14.18) converge to the unique solution of (14.17).
Iwillprovethistheoreminthenextsection.
Theproofdependsonanalysiswhichshould not be surprising because it involves a statement about convergence of sequences.
Whatisaneasytoverifysuﬃcientconditionwhichwillimplytheaboveholds?
Itiseasy togiveoneinth∑ecaseoftheJacobimethod.
SupposethematrixAisdiagonallydominant.
That is |a | > |a |.
Then B would be the diagonal matrix consisting of the entries ii j̸=i ij |aii|.
You can see(cid:12)(cid:12)then t(cid:12)h(cid:12)at every entry of B−1C has absolute value less than 1.
Thus if you let the norm (cid:12)(cid:12)B−1C(cid:12)(cid:12) be given by the maximum of the absolute values of the entries (cid:12)(cid:12) ∞(cid:12)(cid:12) of the matrix, then (cid:12)(cid:12)B−1C(cid:12)(cid:12) =r <1.
Also, by equivalence of norms it follows there exist ∞ positive constants δ,∆ such that δ||·||≤||·|| ≤∆||·|| ∞ ( ) where here ||·|| is an operator norm.
It follows that if |λ|≥1, then λI−B−1C −1 exists.
In fact it equals ( ) ∑∞ B−1C k λ−1 , λ k=0 the series converging because (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)∑n (B−1C)k(cid:12)(cid:12)(cid:12)(cid:12) ∑∞ (cid:12)(cid:12)(cid:12)(cid:12)(B−1C)k(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) ≤ (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) λ (cid:12)(cid:12) (cid:12)(cid:12) λ (cid:12)(cid:12) k=m ∞ k=m ∞ ≤ ∑∞ ∆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(B−λ1C)k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ∑∞ ∆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(B−λ1C)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k k=m ∞ k=m (cid:12)(cid:12)( )(cid:12)(cid:12) ( ) ≤ ∑∞ ∆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) B−1C (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k ≤ ∆ ∑∞ rk ≤ ∆ rm δ λ δ δ 1−r ∞ k=m k=m ( ) which shows the partial sums form a Cauchy sequence.
Therefore, ρ B−1C < 1 in this case.
You might try a similar argument in the case of the Gauss Seidel method.
14.6 Theory Of Convergence Deﬁnition 14.6.1 A normed vector space, E with norm ||·|| is called a Banach space if it is also complete.
This means that every Cauchy sequence converges.
Recall that a sequence {x }∞ isaCauchysequenceifforeveryε>0thereexistsN suchthatwheneverm,n>N, n n=1 ||x −x ||<ε.
n m Thus whenever {x } is a Cauchy sequence, there exists x such that n lim ||x−x ||=0.
n n→∞ Example 14.6.2 Let Ω be a nonempty subset of a normed linear space, F. Denote by BC(Ω;E) the set of bounded continuous functions having values in E where E is a Banach space.
Then deﬁne the norm on BC(Ω;E) by ||f||≡sup{||f(x)|| :x∈Ω}.
E Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.6.
THEORY OF CONVERGENCE 361 Lemma 14.6.3 The space BC(Ω;E) with the given norm is a Banach space.
Proof: It is obvious ||·|| is a norm.
It only remains to verify BC(Ω;E) is complete.
Let {f } be a Cauchy sequence.
Then pick x∈Ω.
n ||f (x)−f (x)|| ≤||f −f ||<ε n m E n m whenever m,n are large enough.
Thus, for each x,{f (x)} is a Cauchy sequence in E. n Since E is complete, it follows there exists a function, f deﬁned on Ω such that f(x) = limn→∞fn(x).
It remains to verify that f ∈BC(Ω;E) and that ||f −f ||→0.
I will ﬁrst show that n ( ) lim sup{||f(x)−f (x)|| } =0.
(14.19) n→∞ x∈Ω n E From this it will follow that f is bounded.
Then I will show that f is continuous and ||f −f ||→0.
Let ε>0 be given and let N be such that for m,n>N n ||f −f ||<ε/3.
n m Then it follows that for all x, ||f(x)−f (x)|| = lim ||f (x)−f (x)|| ≤ε/3 m E n→∞ n m E Therefore, for m>N, ε sup{||f(x)−f (x)|| }≤ <ε.
x∈Ω m E 3 This proves (14.19).
Then by the triangle inequality and letting N be as just described, pick m>N.
Then for any x∈Ω ||f(x)|| ≤||f (x)|| +ε≤||f ||+ε.
E m E m Hence f is bounded.
Now pick x∈Ω and let ε>0 be given and N be as above.
Then ||f(x)−f(y)|| ≤ ||f(x)−f (x)|| +||f (x)−f (y)|| +||f (y)−f(y)|| E m E m m E m E ε ε ≤ +||f (x)−f (y)|| + .
3 m m E 3 Now by continuity of f , the middle term is less than ε/3 whenever ||x−y|| is suﬃciently m small.
Therefore, f is also continuous.
Finally, from the above, ε ||f −f ||≤ n 3 whenever n>N and so limn→∞||f −fn||=0 as claimed.
(cid:4) The most familiar example of a Banach space is Fn.
The following lemma is of great importance so it is stated in general.
Lemma 14.6.4 SupposeT :E →E whereE isaBanachspacewithnorm|·|.
Alsosuppose |Tx−Ty|≤r|x−y| (14.20) for some r ∈(0,1).
Then there exists a unique ﬁxed point, x∈E such that Tx=x.
(14.21) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation362 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Letting x1 ∈E, this ﬁxed point, x, is the limit of the sequence of iterates, x1,Tx1,T2x1,··· .
(14.22) In addition to this, there is a nice estimate which tells how close x1 is to x in terms of things which can be computed.
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)x1−x(cid:12)≤ 1 (cid:12)x1−Tx1(cid:12).
(14.23) 1−r { } ∞ Proof: This follows easily when it is shown that the above sequence, Tkx1 is a k=1 Cauchy sequence.
Note that (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)T2x1−Tx1(cid:12)≤r(cid:12)Tx1−x1(cid:12).
Suppose (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Tkx1−Tk−1x1(cid:12)≤rk−1(cid:12)Tx1−x1(cid:12).
(14.24) Then (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Tk+1x1−Tkx1(cid:12) ≤ r(cid:12)Tkx1−Tk−1x1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ rrk−1(cid:12)Tx1−x1(cid:12)=rk(cid:12)Tx1−x1(cid:12).
By induction, this shows that for all k ≥2, (14.24) is valid.
Now let k >l≥N.
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)k∑−1( )(cid:12) k∑−1(cid:12) (cid:12) (cid:12)Tkx1−Tlx1(cid:12) = (cid:12)(cid:12) Tj+1x1−Tjx1 (cid:12)(cid:12)≤ (cid:12)Tj+1x1−Tjx1(cid:12) (cid:12) (cid:12) j=l j=l ≤ k∑−1rj(cid:12)(cid:12)Tx1−x1(cid:12)(cid:12)≤(cid:12)(cid:12)Tx1−x1(cid:12)(cid:12) rN 1−r j=N which converges to 0 as N →∞.
Therefore, this is a Cauchy sequence so it must converge to x∈E.
Then x= lim Tkx1 = lim Tk+1x1 =T lim Tkx1 =Tx.
k→∞ k→∞ k→∞ This shows the existence of the ﬁxed point.
To show it is unique, suppose there were another one, y.
Then |x−y|=|Tx−Ty|≤r|x−y| and so x=y.
It remains to verify the estimate.
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)x1−x(cid:12) ≤ (cid:12)x1−Tx1(cid:12)+(cid:12)Tx1−x(cid:12)=(cid:12)x1−Tx1(cid:12)+(cid:12)Tx1−Tx(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ (cid:12)x1−Tx1(cid:12)+r(cid:12)x1−x(cid:12) (cid:12) (cid:12) and solving the inequality for (cid:12)x1−x(cid:12) gives the estimate desired.
(cid:4) The following corollary is what will be used to prove the convergence condition for the various iterative procedures.
Corollary 14.6.5 Suppose T :E →E, for some constant C |Tx−Ty|≤C|x−y|, for all x,y∈E, and for some N ∈N, (cid:12) (cid:12) (cid:12)TNx−TNy(cid:12)≤r|x−y|, for all x,y∈E where r ∈{(0,1).
}Then there exists a unique ﬁxed point for T and it is still the limit of the sequence, Tkx1 for any choice of x1.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.7.
EXERCISES 363 Proof: From Lemma 14.6.4 there exists a unique ﬁxed point for TN denoted here as x.
Therefore, TNx=x.
Now doing T to both sides, TNTx=Tx.
By uniqueness, Tx=x because the above equation shows Tx is a ﬁxed point of TN and there is only one ﬁxed point of TN.
In fact, there is only one ﬁxed point of T because a ﬁxed point of T is automatically a ﬁxed point of TN.
It remains to show Tkx1 → x, the unique ﬁxed point of TN.
If this does not happen, there exists ε>0 and a subsequence, still denoted by Tk such that (cid:12) (cid:12) (cid:12)Tkx1−x(cid:12)≥ε Now k = j N + r where r ∈ {0,··· ,N −1} and j is a positive integer such that k k k k limk→∞jk = ∞.
Then there exists a single r ∈ {0,··· ,N −1} such that for inﬁnitely many k,r =r.
Taking a further subsequence, still denoted by Tk it follows k (cid:12) (cid:12) (cid:12)TjkN+rx1−x(cid:12)≥ε (14.25) However, TjkN+rx1 =TrTjkNx1 →Trx=x and this contradicts (14.25).
(cid:4) ( ) Theorem 14.6.6 Suppose ρ B−1C < 1.
Then the iterates in (14.18) converge to the unique solution of (14.17).
Proof: Consider the iterates in (14.18).
Let Tx=B−1Cx+b.
Then (cid:12)(cid:12)Tkx−Tky(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(B−1C)kx−(B−1C)ky(cid:12)(cid:12)(cid:12)≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(B−1C)k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|x−y|.
Here ||·|| refers to any of the operator norms.
It doesn’t matter which one you pick because they are all equivalent.
I am writing the(proof t)o indicate the operator norm taken with respect to the usual norm on E. Since ρ B−1C < 1, it follows from Gelfand’s theorem, Theorem 14.3.3 on Page 349, there exists N such that if k ≥N, then for some r1/k <1, (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(B−1C)k(cid:12)(cid:12)(cid:12)(cid:12)1/k <r1/k <1.
Consequently, (cid:12) (cid:12) (cid:12)TNx−TNy(cid:12)≤r|x−y|.
(cid:12)(cid:12) (cid:12)(cid:12) Also |Tx−Ty|≤(cid:12)(cid:12)B−1C(cid:12)(cid:12)|x−y| and so Corollary 14.6.5 applies and gives the conclusion of this theorem.
(cid:4) 14.7 Exercises 1.
Solve the system      4 1 1 x 1      1 5 2 y = 2 0 2 6 z 3 using the Gauss Seidel method and the Jacobi method.
Check your answer by also solving it using row operations.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation364 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES 2.
Solve the system      4 1 1 x 1      1 7 2 y = 2 0 2 4 z 3 using the Gauss Seidel method and the Jacobi method.
Check your answer by also solving it using row operations.
3.
Solve the system      5 1 1 x 1      1 7 2 y = 2 0 2 4 z 3 using the Gauss Seidel method and the Jacobi method.
Check your answer by also solving it using row operations.
4.
IfyouareconsideringasystemoftheformAx=bandA−1 doesnotexist,willeither the Gauss Seidel or Jacobi methods work?
Explain.
What does this indicate about ﬁnding eigenvectors for a given eigenvalue?
5.
For ||x|| ≡ max{|x |:j =1,2,··· ,n}, the parallelogram identity does not hold.
∞ j Explain.
6.
A norm ||·|| is said to be strictly convex if whenever ||x||=||y||,x̸=y, it follows (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)x+y(cid:12)(cid:12)(cid:12)(cid:12)<||x||=||y||.
2 Show the norm |·| which comes from an inner product is strictly convex.
7.
A norm ||·|| is said to be uniformly convex if whenever ||x ||,||y || are equal to 1 for n n all n ∈ N and limn→∞||xn+yn|| = 2, it follows limn→∞||xn−yn|| = 0.
Show the norm |·| coming from an inner product is always uniformly convex.
Also show that uniform convexity implies strict convexity which is deﬁned in Problem 6.
8.
Suppose A:Cn →Cn is a one to one and onto matrix.
Deﬁne ||x||≡|Ax|.
Show this is a norm.
9.
If X is a ﬁnite dimensional normed v(cid:12)(cid:12)ector s(cid:12)p(cid:12)ace and A,B ∈ L(X,X) such that ||B||<||A||, can it be concluded that (cid:12)(cid:12)A−1B(cid:12)(cid:12)<1?
10.
Let X be a vector space with a norm ||·|| and let V = span(v ,··· ,v ) be a ﬁnite 1 m dimensional subspace of X such that {v ,··· ,v } is a basis for V. Show V is a closed 1 m subspace of X.
This means that if w →w and each w ∈V, then so is w. Next show n n that if w ∈/ V, dist(w,V)≡inf{||w−v||:v ∈V}>0 is a continuous function of w and |dist(w,V)−dist(w ,V)|≤∥w −w∥ 1 1 Next show that if w ∈/ V, there exists z such that ||z|| = 1 and dist(z,V) > 1/2.
For those who know some advanced calculus, show that if X is an inﬁnite dimensional vector space having norm ||·||, then the closed unit ball in X cannot be compact.
Thus closed and bounded is never compact in an inﬁnite dimensional normed vector space.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.7.
EXERCISES 365 11.
Suppose ρ(A) < 1 for A ∈ L(V,V) where V is a p dimensional vector space having a norm ||·||.
You can use Rp or Cp if you like.
Show there exists a new norm |||·||| such that with respect to this new norm, |||A|||<1 where |||A||| denotes the operator norm of A taken with respect to this new norm on V, |||A|||≡sup{|||Ax|||:|||x|||≤1} Hint: You know from Gelfand’s theorem that ||An||1/n <r <1 provided n is large enough, this operator norm taken with respect to ||·||.
Show there exists 0<λ<1 such that ( ) A ρ <1.
λ YoucandothisbyarguingtheeigenvaluesofA/λarethescalarsµ/λwhereµ∈σ(A).
Now let Z denote the nonnegative integers.
+ (cid:12)(cid:12) (cid:12)(cid:12) |||x|||≡ sup (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Anx(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n∈Z+ λn First show this is actually a norm.
Next explain why (cid:12)(cid:12) (cid:12)(cid:12) |||Ax|||≡λ sup (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)An+1x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤λ|||x|||.
n∈Z+ λn+1 12.
Establish a similar result to Problem 11 without using Gelfand’s theorem.
Use an argument which depends directly on the Jordan form or a modiﬁcation of it.
13.
UsingProblem11giveaneasierproofofTheorem14.6.6withouthavingtouseCorol- lary14.6.5.
Itwouldsuﬃcetouseadiﬀerentnormofthisproblemandthecontraction mapping principle of Lemma 14.6.4.
∑ 14.
A matrix A is diagonally dominant if |a | > |a |.
Show that the Gauss Seidel ii j̸=i ij method converges if A is diagonally dominant.
∑ 15.
Suppose f(λ) = ∞ a λn converges if |λ| < R. Show that if ρ(A) < R where A is k=0 n an n×n matrix, then ∑∞ f(A)≡ a An n k−0 converges in L(Fn,Fn).
Hint: Use Gelfand’s theorem and the root test.
16.
Referring to Corollary 14.4.3, for λ=a+ib show exp(λt)=eat(cos(bt)+isin(bt)).
Hint: Let y(t)=exp(λt) and let z(t)=e−aty(t).
Show z′′+b2z =0, z(0)=1,z′(0)=ib.
Now letting z =u+iv where u,v are real valued, show u′′+b2u = 0, u(0)=1,u′(0)=0 v′′+b2v = 0, v(0)=0,v′(0)=b.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation366 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES Next show u(t) = cos(bt) and v(t) = sin(bt) work in the above and that there is at most one solution to w′′+b2w =0w(0)=α,w′(0)=β.
Thus z(t) = cos(bt)+isin(bt) and so y(t) = eat(cos(bt)+isin(bt)).
To show there is at most one solution to the above problem, suppose you have two, w ,w .
Subtract 1 2 them.
Let f =w −w .
Thus 1 2 f′′+b2f =0 and f is real valued.
Multiply both sides by f′ and conclude ( ) d (f′)2 f2 +b2 =0 dt 2 2 Thus the expression in parenthesis is constant.
Explain why this constant must equal 0.
17.
Let A∈L(Rn,Rn).
Show the following power series converges in L(Rn,Rn).
∑∞ tkAk k!
k=0 YoumightwanttouseLemma14.4.2.
Thisishowyoucandeﬁneexp(tA).
Nextshow using arguments like those of Corollary 14.4.3 d exp(tA)=Aexp(tA) dt sothatthisisamatrixvaluedsolutiontothediﬀerentialequationandinitialcondition ′ Ψ (t)=AΨ(t), Ψ(0)=I.
ThisΨ(t)iscalledafundamentalmatrix forthediﬀerentialequationy′ =Ay.
Show t→Ψ(t)y gives a solution to the initial value problem 0 ′ y =Ay, y(0)=y .
0 18.
InProblem17Ψ(t)isdeﬁnedbythegivenseries.
Denotebyexp(tσ(A))thenumbers exp(tλ) where λ ∈ σ(A).
Show exp(tσ(A)) = σ(Ψ(t)).
This is like Lemma 14.4.7.
Letting J be the Jordan canonical form for A, explain why ∑∞ ∑∞ tkAk tkJk Ψ(t)≡ =S S−1 k!
k!
k=0 k=0 and you note that in Jk, the diagonal entries are of the form λk for λ an eigenvalue of A.
Also J =D+N where N is nilpotent and commutes with D. Argue then that ∑∞ tkJk k!
k=0 is an upper triangular matrix which has on the diagonal the expressions eλt where λ∈σ(A).
Thus conclude σ(Ψ(t))⊆exp(tσ(A)) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.7.
EXERCISES 367 Next take etλ ∈ exp(tσ(A)) and argue it must be in σ(Ψ(t)).
You can do this as follows: ∑∞ tkAk ∑∞ tkλk ∑∞ tk ( ) Ψ(t)−etλI = − I = Ak−λkI k!
k!
k!
k=0 k=0  k=0 = ∑∞ tk k∑−1Ak−jλj(A−λI) k!
k=0 j=1 Now you need to argue ∑∞ tk k∑−1 Ak−jλj k!
k=0 j=1 convergestosomethinginL(Rn,Rn).
Todothis,usetheratiotestandLemma14.4.2 after ﬁrst using the triangle inequality.
Since λ∈σ(A), Ψ(t)−etλI is not one to one and so this establishes the other inclusion.
You ﬁll in the details.
This theorem is a special case of theorems which go by the name “spectral mapping theorem”.
19.
Suppose Ψ(t)∈L(V,W) where V,W are ﬁnite dimensional inner product spaces and t→Ψ(t)iscontinuousfort∈[a,b]: Foreveryε>0therethereexistsδ >0suchthat if |s−t|<δ then ||Ψ(t)−Ψ(s)||<ε.
Show t→(Ψ(t)v,w) is continuous.
Here it is the inner product in W. Also deﬁne what it means for t → Ψ(t)v to be continuous and show this is continuous.
Do it all for diﬀerentiable in place of continuous.
Next show t→||Ψ(t)|| is continuous.
20.
If z(t)∈W, a ﬁnite dimensional inner product space, what does it mean for t→z(t) to be continuous or diﬀerentiable?
If z is continuous, deﬁne ∫ b z(t)dt∈W a as follows.
( ∫ ) ∫ b b w, z(t)dt ≡ (w,z(t))dt.
a a Show that this deﬁnition is well deﬁned and furthermore the triangle inequality, (cid:12) (cid:12) (cid:12)∫ (cid:12) ∫ (cid:12) b (cid:12) b (cid:12) z(t)dt(cid:12)≤ |z(t)|dt, (cid:12) (cid:12) a a and fundamental theorem of calculus, (∫ ) d t z(s)ds =z(t) dt a hold along with any other interesting properties of integrals which are true.
21.
For V,W two inner product spaces, deﬁne ∫ b Ψ(t)dt∈L(V,W) a as follows.
( ∫ ) ∫ b b w, Ψ(t)dt(v) ≡ (w,Ψ(t)v)dt.
a a Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation368 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES ∫ Show this is well deﬁned and does indeed give bΨ(t)dt ∈ L(V,W).
Also show the a triangle inequality (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)∫ (cid:12)(cid:12) ∫ (cid:12)(cid:12) b (cid:12)(cid:12) b (cid:12)(cid:12) Ψ(t)dt(cid:12)(cid:12)≤ ||Ψ(t)||dt (cid:12)(cid:12) (cid:12)(cid:12) a a where ||·|| is the operator norm and verify the fundamental theorem of calculus holds.
(∫ ) ′ t Ψ(s)ds =Ψ(t).
a Also verify the usual properties of integrals continue to hold such as the fact the integral is linear and ∫ ∫ ∫ b c c Ψ(t)dt+ Ψ(t)dt= Ψ(t)dt a b a and similar things.
Hint: On showing the triangle inequality, it will help if you use the fact that |w| = sup |(w,v)|.
W |v|≤1 You should show this also.
22.
Prove Gronwall’s inequality.
Suppose u(t)≥0 and for all t∈[0,T], ∫ t u(t)≤u + Ku(s)ds.
0 0 where K is some nonnegative constant.
Then u(t)≤u eKt.
0 ∫ t Hint: w(t) = u(s)ds.
Then using the fundamental theorem of calculus, w(t) 0 satisﬁes the following.
u(t)−Kw(t)=w′(t)−Kw(t)≤u , w(0)=0.
0 Now use the usual techniques you saw in an introductory diﬀerential equations class.
Multiply both sides of the above inequality by e−Kt and note the resulting left side is now a total derivative.
Integrate both sides from 0 to t and see what you have got.
If youhaveproblems,lookaheadinthebook.
ThisinequalityisprovedlaterinTheorem C.4.3.
23.
With Gronwall’s inequality and the integral deﬁned in Problem 21 with its properties listed there, prove there is at most one solution to the initial value problem ′ y =Ay, y(0)=y .
0 Hint: If there are two solutions, subtract them and call the result z.
Then ′ z =Az, z(0)=0.
It follows ∫ t z(t)=0+ Az(s)ds 0 and so ∫ t ||z(t)||≤ ∥A∥||z(s)||ds 0 Now consider Gronwall’s inequality of Problem 22.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation14.7.
EXERCISES 369 24.
Suppose A is a matrix which has the property that whenever µ ∈ σ(A), Reµ < 0.
Consider the initial value problem ′ y =Ay,y(0)=y .
0 Theexistenceanduniquenessofasolutiontothisequationhasbeenestablishedabove in preceding problems, Problem 17 to 23.
Show that in this case where the real parts of the eigenvalues are all negative, the solution to the initial value problem satisﬁes lim y(t)=0.
t→∞ Hint: A nice way to approach this problem is to show you can reduce it to the consideration of the initial value problem ′ z =J z, z(0)=z ε 0 where J is the modiﬁed Jordan canonical form where instead of ones down the main ε diagonal, there are ε down the main diagonal (Problem 19).
Then ′ z =Dz+N z ε whereDisthediagonalmatrixobtainedfromtheeigenvaluesofAandN isanilpotent ε matrix commuting with D which is very small provided ε is chosen very small.
Now let Ψ(t) be the solution of Ψ′ =−DΨ, Ψ(0)=I described earlier as ∑∞ (−1)ktkDk .
k!
k=0 Thus Ψ(t) commutes with D and N .
Tell why.
Next argue ε ′ (Ψ(t)z) =Ψ(t)N z(t) ε and integrate from 0 to t. Then ∫ t Ψ(t)z(t)−z = Ψ(s)N z(s)ds.
0 ε 0 It follows ∫ t ||Ψ(t)z(t)||≤||z ||+ ||N ||||Ψ(s)z(s)||ds.
0 ε 0 It follows from Gronwall’s inequality ||Ψ(t)z(t)||≤||z ||e||N"||t 0 Now look closely at the form of Ψ(t) to get an estimate which is interesting.
Explain why   eµ1t 0   Ψ(t)= ...  0 eµnt andnowobservethatifεischosensmallenough,||N ||issosmallthateachcomponent ε of z(t) converges to 0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation370 NORMS FOR FINITE DIMENSIONAL VECTOR SPACES 25.
Using Problem 24 show that if A is a matrix having the real parts of all eigenvalues less than 0 then if ′ Ψ (t)=AΨ(t), Ψ(0)=I it follows lim Ψ(t)=0.
t→∞ Hint: Consider the columns of Ψ(t)?
26.
Let Ψ(t) be a fundamental matrix satisfying ′ Ψ (t)=AΨ(t), Ψ(0)=I.
ShowΨ(t)n =Ψ(nt).Hint: SubtractandshowthediﬀerencesatisﬁesΦ′ =AΦ, Φ(0)= 0.
Use uniqueness.
27.
If the real parts of the eigenvalues of A are all negative, show that for every positive t, lim Ψ(nt)=0.
n→∞ Hint: Pick Re(σ(A)) < −λ < 0 and use Problem 18 about the spectrum of Ψ(t) a(cid:12)(cid:12)nd Gelfand’s (cid:12)t(cid:12)heorem for the spectral radius along with Problem 26 to argue that (cid:12)(cid:12)Ψ(nt)/e−λnt(cid:12)(cid:12)<1 for all n large enough.
∑ 28.
Let H be a Hermitian matrix.
(H =H∗).
Show that eiH ≡ ∞ (iH)n is unitary.
n=0 n!
29.
Show the converse of the above exercise.
If V is unitary, then V = eiH for some H Hermitian.
30.
If U is unitary and does not have −1 as an eigenvalue so that (I+U)−1 exists, show that H =i(I−U)(I+U)−1 is Hermitian.
Then, verify that U =(I +iH)(I −iH)−1.
31.
Suppose that A ∈ L(V,V) where V is a normed linear space.
Also suppose that ∥A∥<1 where this refers to the operator norm on A. Verify that ∑∞ (I−A)−1 = Ai i=0 This is called the Neumann series.
Suppose now that you only know the algebraic conditionρ(A)<1.IsitstillthecasethattheNeumannseriesconvergesto(I−A)−1?
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationNumerical Methods For Finding Eigenvalues 15.1 The Power Method For Eigenvalues This chapter discusses numerical methods for ﬁnding eigenvalues.
However, to do this correctly, you must include numerical analysis considerations which are distinct from linear algebra.
The purpose of this chapter is to give an introduction to some numerical methods without leaving the context of linear algebra.
In addition, some examples are given which make use of computer algebra systems.
For a more thorough discussion, you should see books on numerical methods in linear algebra like some listed in the references.
Let A be a complex p×p matrix and suppose that it has distinct eigenvalues {λ ,··· ,λ } 1 m and that |λ |>|λ | for all k. Also let the Jordan form of A be 1 k   J 1   J = ...  J m with J =λ I +N k k k k where Nrk ̸=0 but Nrk+1 =0.
Also let k k P−1AP =J, A=PJP−1.
Now ﬁx x ∈ Fp.
Take Ax and let s be the entry of the vector Ax which has largest 1 absolute value.
Thus Ax/s is a vector y which has a component of 1 and every other 1 1 entry of this vector has magnitude no larger than 1.
If the scalars {s1,··· ,sn−1} and vectors {y1,··· ,yn−1} have been obtained, let y ≡ Ayn−1 n s n where sn is the entry of Ayn−1 which has largest absolute value.
Thus y = AAyn−2 ···= Anx (15.1) n snsn−1 snsn−1···s1 371 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation372 NUMERICAL METHODS FOR FINDING EIGENVALUES Consider one of the blocks in the Jordan form.
( ) ∑rk n λn−i Jn =λn k Ni ≡λnK(k,n) k 1 i λn k 1 i=0 1 Then from the above,   K(1,n) snsn−A1n···s1 =Psnsn−λ1n1···s1  ... P−1 K(m,n) Consider one of the terms in the sum for K(k,n) for k > 1.
Letting the norm of a matrix be the maximum of the absolute values of its entries, (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(ni)λλnkn−iNki(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤nrk(cid:12)(cid:12)(cid:12)(cid:12)λλk(cid:12)(cid:12)(cid:12)(cid:12)nprkC 1 1 where C depends on the eigenvalues but is independent of n. Then this converges to 0 because the inﬁnite sum of these converges due to the root test.
Thus each of the matrices K(k,n) converges to 0 for each k >1 as n→∞.
Now what about K(1,n)?
It equals ( ) (( ) ( )) n ∑r1 n n / λ−iNi r i r 1 1 1 1 i=0 ( ) ( ) n = λ−r1Nr1 +m(n) r 1 1 1 where limn→∞m(n)=0.
This follows from (( ) ( )) n n lim / =0, i<r n→∞ i r1 1 It follows that (15.1) is of the form ( ) ( ( ) ) y = λn1 n P λ−1r1N1r1 +m(n) 0 P−1x= Ayn−1 n snsn−1···s1 r1 0 En sn ( ) where the entries of E converge to 0 as n → ∞.
Now denote by P−1x the ﬁrst m entries of P−1x wherenit is assumed that λ has multiplicity m .
Assume thma1t 1 1 1 ( ) P−1x ∈/ kerNr1 m1 1 This will be the case unless you have made an extremely unfortunate choice of x.
Then y n is of the form ( ) ( ( )( ) ) λn n λ−r1Nr1 +m(n) P−1x yn = snsn−11···s1 r1 P 1 1 zn m1 (15.2) ( ) where n z →0.
Also, from the construction, there is a single entry of y equal to 1 and r1 n n all other entries of the above vector have absolute value no larger than 1.
It follows that ( ) λn n 1 snsn−1···s1 r1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 373 must be bounded independent of n. Then it follows from this observation, that for large n, the above vector y is approxi- n mately equal to ( ) ( ( ) ) λn n λ−r1Nr1 P−1x snsn−11···s1 r1 P 1 1 0 m1 ( ( ) ) = snsn−11···s1P λn1−r10rn1 N1r1 00 P−1x (15.3) ( ) If P−1x ∈/ ker(Nr1),thentheabovevectorisalsonotequalto0.
Whathappenswhen it is multipml1ied on the1left by A−λ I =P (J −λ I)P−1?
This results in 1 1 ( ( ) ) snsn−11···s1P λn1−r1N10 rn1 N1r1 00 P−1x=0 becauseNr1+1 =0.
Therefore,thevectorin(15.3)isaneigenvectorandy isapproximately 1 n equal to this eigenvector.
With this preparation, here is a theorem.
Theorem 15.1.1 Let A be a complex p×p matrix such that the eigenvalues are {λ ,λ ,··· ,λ } 1 2 r with |λ |>|λ | for all j ̸=1.
Then for x a given vector, let 1 j Ax y = 1 s 1 wheres1 isanentryofAxwhichhasthelargestabsolutevalue.
Ifthescalars{s1,··· ,sn−1} and vectors {y1,··· ,yn−1} have been obtained, let y ≡ Ayn−1 n s n where sn is the entry of Ayn−1 which has largest absolute value.
Then it is probably the case that {s } will converge to λ and {y } will converge to an eigenvector associated with λ .
n 1 n 1 Proof: Consider the claim about s .
It was shown above that n+1 ( ( ) ) λ−r1Nr1 P−1x z≡P 1 1 m1 0 is an eigenvector for λ .
Let z be the entry of z which has largest absolute value.
Then for 1 l large n, it will probably be the case that the entry of y which has largest absolute value n will also be in the lth slot.
This follows from (15.2) because for large n,z will be very n small, smaller than the largest entry of the top part of the vector in that expression.
Then, since m(n) is very small, the result follows if z has a well deﬁned entry which has largest absolute value.
Now from the above construction, ( ) λn+1 n s y ≡Ay ≈ 1 z n+1 n+1 n s ···s r n 1 1 Applying a similar formula to s and the above observation, about the largest entry, it n follows that for large n ( ) ( ) λn+1 n λn n−1 s ≈ 1 z , s ≈ 1 z n+1 sn···s1 r1 l n sn−1···s1 r1 l Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation374 NUMERICAL METHODS FOR FINDING EIGENVALUES Therefore, for large n, s λ n···(n−r +1) λ n+1 ≈ 1 1 ≈ 1 s s (n−1)···(n−r ) s n n 1 n which shows that s ≈λ .
n+1 1 Now from the construction and the formula in (15.2), for large n ( ) ( ( )( ) ) λn+1 n+1 λ−r1Nr1 +m(n) P−1x yn+1 = sn+1sn1−1···s1 (r1 P) (1( 1 zn )( m1) ) λ λn n+1 λ−r1Nr1 +m(n) P−1x = s(n+11)snsn−11···s1 ( r1) (P ( 1 1 zn)( ) m)1 n+1 λn n λ−r1Nr1 +m(n) P−1x ≈ ((rrn11))snsn−11···s1 r1 P 1 1 zn m1 n+1 = (r1) y ≈y n n n r1 Thus{y }isaCauchysequenceandmustconvergetoavectorv.Nowfromtheconstruction, n λ v= lim s y = lim Ay =Av.
(cid:4) 1 n+1 n+1 n n→∞ n→∞ In summary, here is the procedure.
Finding the largest eigenvalue with its eigenvector.
1.
Start with a vector, u which you hope is not unlucky.
1 2.
If u is known, k Au u = k k+1 s k+1 where s is the entry of Au which has largest absolute value.
k+1 k 3.
Whenthescalingfactorss arenotchangingmuch,s willbeclosetotheeigenvalue k k+1 and u will be close to an eigenvector.
k+1 4.
Check your answer to see if it worked well.
  5 −14 11 Example 15.1.2 Find the largest eigenvalue of A=  −4 4 −4 .
3 6 −3 You can begin with u =(1,··· ,1)T and apply the above procedure.
However, you can 1 accelerate the process if you begin with Anu and then divide by the largest entry to get 1 the ﬁrst approximate eigenvector.
Thus       5 −14 11 20 1 2.5558×1021  −4 4 −4   1 = −1.2779×1021  3 6 −3 1 −3.6562×1015 Divide by the largest entry to obtain a good aproximation.
    2.5558×1021 1.0  −1.2779×1021  1 = −0.5  −3.6562×1015 2.5558×1021 −1.4306×10−6 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 375 Now begin with this one.
     5 −14 11 1.0 12.000  −4 4 −4  −0.5 = −6.0000  3 6 −3 −1.4306×10−6 4.2918×10−6 Divide by 12 to get the next iterate.
    12.000 1.0  −6.0000  1 = −0.5  4.2918×10−6 12 3.5765×10−7 Another iteration will reveal that the scaling factor is still 12.
Thus this is an approximate eigenvalue.
In fact, it is the largest eigenvalue and the corresponding eigenvector is   1.0  −0.5  0 The process has worked very well.
15.1.1 The Shifted Inverse Power Method This method can ﬁnd various eigenvalues and eigenvectors.
It is a signiﬁcant generalization oftheabovesimpleprocedureandyieldsverygoodresults.
Onecanﬁndcomplexeigenvalues using this method.
The situation is this: You have a number, α which is close to λ, some eigenvalue of an n×n matrix A.
You don’t know λ but you know that α is closer to λ thantoanyothereigenvalue.
Yourproblemistoﬁndbothλandaneigenvectorwhichgoes with λ.
Another way to look at this is to start with α and seek the eigenvalue λ, which is closest to α along with an eigenvector associated with λ.
If α is an eigenvalue of A, then you have what you want.
Therefore, I will always assume α is not an eigenvalue of A and so (A−αI)−1 exists.
The method is based on the following lemma.
Lemma 15.1.3 Let {λ }n be the eigenvalues of A.
If x is an eigenvector of A for the k k=1 k eigenvalue λ , then x is an eigenvector for (A−αI)−1 corresponding to the eigenvalue k k 1 .
Conversely, if λk−α (A−αI)−1y= 1 y (15.4) λ−α and y̸=0, then Ay=λy.
Proof: Let λ and x be as described in the statement of the lemma.
Then k k (A−αI)x =(λ −α)x k k k and so 1 x =(A−αI)−1x .
λ −α k k k Suppose (15.4).
Then y= 1 [Ay−αy].
Solving for Ay leads to Ay=λy.
(cid:4) λ−α Now assume α is closer to λ than to any other eigenvalue.
Then the magnitude of 1 λ−α is greater than the magnitude of all the other eigenvalues of (A−αI)−1.
Therefore, the power method applied to (A−αI)−1 will yield 1 .
You end up with s ≈ 1 and λ−α n+1 λ−α solve for λ. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation376 NUMERICAL METHODS FOR FINDING EIGENVALUES 15.1.2 The Explicit Description Of The Method Here is how you use this method to ﬁnd the eigenvalue and eigenvector closest to α.
1.
Find (A−αI)−1.
2.
Pick u .
If you are not phenomenally unlucky, the iterations will converge.
1 3.
If u has been obtained, k (A−αI)−1u u = k k+1 s k+1 where s is the entry of (A−αI)−1u which has largest absolute value.
k+1 k 4.
Whenthescalingfactors,s arenotchangingmuchandtheu arenotchangingmuch, k k ﬁnd the approximation to the eigenvalue by solving 1 s = k+1 λ−α for λ.
The eigenvector is approximated by u .
k+1 5.
Checkyourworkbymultiplyingbytheoriginalmatrixtoseehowwellwhatyouhave found works.
Thus this amounts to the power method for the matrix (A−αI)−1.
  5 −14 11 Example 15.1.4 Find the eigenvalue of A=  −4 4 −4  which is closest to −7.
3 6 −3 Also ﬁnd an eigenvector which goes with this eigenvalue.
In this case the eigenvalues are −6,0, and 12 so the correct answer is −6 for the eigen- value.
Then from the above procedure, I will start with an initial vector,   1 u ≡ 1 .
1 1 Then I must solve the following equation.
       5 −14 11 1 0 0 x 1  −4 4 −4 +7 0 1 0  y = 1  3 6 −3 0 0 1 z 1 Simplifying the matrix on the left, I must solve      12 −14 11 x 1  −4 11 −4  y = 1  3 6 4 z 1 and then divide by the entry which has largest absolute value to obtain   1.0   u = .184 2 −.76 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 377 Now solve      12 −14 11 x 1.0  −4 11 −4  y = .184  3 6 4 z −.76 and divide by the largest entry, 1.0515 to get   1.0   u = .0266 3 −.97061 Solve      12 −14 11 x 1.0  −4 11 −4  y = .0266  3 6 4 z −.97061 and divide by the largest entry, 1.01 to get   1.0 u = 3.8454×10−3 .
4 −.99604 These scaling factors are pretty close after these few iterations.
Therefore, the predicted eigenvalue is obtained by solving the following for λ.
1 =1.01 λ+7 which gives λ = −6.01.
You see this is pretty close.
In this case the eigenvalue closest to −7 was −6.
How would you know what to start with for an initial guess?
You might apply Ger- schgorin’s theorem.
  1 2 3   Example 15.1.5 Consider the symmetric matrix A = 2 1 4 .
Find the middle 3 4 2 eigenvalue and an eigenvector which goes with it.
Since A is symmetric, it follows it has three real eigenvalues which are solutions to      1 0 0 1 2 3 p(λ) = detλ 0 1 0 − 2 1 4  0 0 1 3 4 2 = λ3−4λ2−24λ−17=0 Ifyouuseyourgraphingcalculatortographthispolynomial,youﬁndthereisaneigenvalue somewherebetween−.9and−.8andthatthisisthemiddleeigenvalue.
Ofcourseyoucould zoom in and ﬁnd it very accurately without much trouble but what about the eigenvector which goes with it?
If you try to solve         1 0 0 1 2 3 x 0 (−.8) 0 1 0 − 2 1 4  y = 0  0 0 1 3 4 2 z 0 there will be only the zero solution because the matrix on the left will be invertible and the same will be true if youreplace −.8 with a better approximationlike−.86or −.855.
Thisis Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation378 NUMERICAL METHODS FOR FINDING EIGENVALUES because all these are only approximations to the eigenvalue and so the matrix in the above is nonsingular for all of these.
Therefore, you will only get the zero solution and Eigenvectors are never equal to zero!
However,thereexistssuchaneigenvectorandyoucanﬁnditusingtheshiftedinversepower method.
Pick α=−.855.
Then you solve        1 2 3 1 0 0 x 1        2 1 4 +.855 0 1 0 y = 1 3 4 2 0 0 1 z 1 or in other words,      1.855 2.0 3.0 x 1      2.0 1.855 4.0 y = 1 3.0 4.0 2.855 z 1 and after ﬁnding the solution, divide by the largest entry −67.944, to obtain   1.0 u = −.58921  2 −.23044 After a couple more iterations, you obtain   1.0 u = −.58777  (15.5) 3 −.22714 Then doing it again, the scaling factor is −513.42 and the next iterate is   1.0 u = −.58778  4 −.22714 Clearly the u are not changing much.
This suggests an approximate eigenvector for this k eigenvalue which is close to −.855 is the above u and an eigenvalue is obtained by solving 3 1 =−514.01, λ+.855 which yields λ=−.8569 Lets check this.
     1 2 3 1.0 −.85696  2 1 4  −.58777 = .50367 .
3 4 2 −.22714 .19464     1.0 −.8569 −.8569 −.58777 = .5037  −.22714 .1946 Thus the vector of (15.5) is very close to the desired eigenvector, just as −.8569 is very close to the desired eigenvalue.
For practical purposes, I have found both the eigenvector and the eigenvalue.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 379   2 1 3   Example 15.1.6 Find the eigenvalues and eigenvectors of the matrix A= 2 1 1 .
3 2 1 This is only a 3×3 matrix and so it is not hard to estimate the eigenvalues.
Just get the characteristic equation, graph it using a calculator and zoom in to ﬁnd the eigenvalues.
If you do this, you ﬁnd there is an eigenvalue near −1.2, one near −.4, and one near 5.5.
(The characteristic equation is 2+8λ+4λ2−λ3 = 0.)
Of course I have no idea what the eigenvectors are.
Lets ﬁrst try to ﬁnd the eigenvector and a better approximation for the eigenvalue near −1.2.
In this case, let α=−1.2.
Then   −25.357143 −33.928571 50.0 (A−αI)−1 = 12.5 17.5 −25.0 .
23.214286 30.357143 −45.0 As before, it helps to get things started if you raise to a power and then go from the approximate eigenvector obtained.
      −25.357143 −33.928571 50.0 7 1 −2.2956×1011  12.5 17.5 −25.0   1 = 1.1291×1011  23.214286 30.357143 −45.0 1 2.0865×1011 Then the next iterate will be     −2.2956×1011 1.0  1.1291×1011  1 = −0.49185  −2.2956×1011 2.0865×1011 −0.90891 Next iterate:      −25.357143 −33.928571 50.0 1.0 −54.115  12.5 17.5 −25.0  −0.49185 = 26.615  23.214286 30.357143 −45.0 −0.90891 49.184 Divide by largest entry     −54.115 1.0  26.615  1 = −0.49182  −54.115 49.184 −0.90888 You can see the vector didn’t change much and so the next scaling factor will not be much diﬀerent than this one.
Hence you need to solve for λ 1 =−54.115 λ+1.2 Then λ= −1.2185 is an approximate eigenvalue and   1.0  −0.49182  −0.90888 is an approximate eigenvector.
How well does it work?
     2 1 3 1.0 −1.2185  2 1 1  −0.49182  =  0.5993  3 2 1 −0.90888 1.1075     1.0 −1.2185 (−1.2185) −0.49182  =  0.59928  −0.90888 1.1075 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation380 NUMERICAL METHODS FOR FINDING EIGENVALUES You can see that for practical purposes, this has found the eigenvalue closest to −1.2185 and the corresponding eigenvector.
The other eigenvectors and eigenvalues can be found similarly.
In the case of −.4, you could let α=−.4 and then   8.0645161×10−2 −9.2741935 6.4516129 (A−αI)−1 = −.40322581 11.370968 −7.2580645 .
.40322581 3.6290323 −2.7419355 Following the procedure of the power method, you ﬁnd that after about 5 iterations, the scaling factor is 9.7573139, they are not changing much, and   −.7812248   u = 1.0 .
5 .26493688 Thus the approximate eigenvalue is 1 =9.7573139 λ+.4 whichshowsλ=−.29751278isanapproximationtotheeigenvaluenear.4.
Howwelldoes it work?
     2 1 3 −.7812248 .23236104  2 1 1  1.0 = −.29751272 .
3 2 1 .26493688 −.07873752     −.7812248 .23242436 −.29751278 1.0 = −.29751278 .
.26493688 −7.8822108×10−2 It works pretty well.
For practical purposes, the eigenvalue and eigenvector have now been found.
If you want better accuracy, you could just continue iterating.
Next I will ﬁnd the eigenvalue and eigenvector for the eigenvalue near 5.5.
In this case,   29.2 16.8 23.2 (A−αI)−1 = 19.2 10.8 15.2 .
28.0 16.0 22.0 As before, I have no idea what the eigenvector is but I am tired of always using (1,1,1)T and I don’t want to give the impression that you always need to start with this vector.
Therefore, I shall let u =(1,2,3)T .
Also, I will begin by raising the matrix to a power.
1       29.2 16.8 23.2 9 1 3.009×1016  19.2 10.8 15.2   2 = 1.9682×1016 .
28.0 16.0 22.0 3 2.8706×1016 Divide by largest entry to get the next iterate.
    3.009×1016 1.0  1.9682×1016  1 = 0.6541  3.009×1016 2.8706×1016 0.954 Now      29.2 16.8 23.2 1.0 62.322      19.2 10.8 15.2 0.6541 = 40.765 28.0 16.0 22.0 0.954 59.454 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 381 Then the next iterate is     62.322 1.0   1   40.765 = 0.6541 62.322 59.454 0.95398 This is very close to the eigenvector given above and so the next scaling factor will also be close to 62.322.
Thus the approximate eigenvalue is obtained by solving 1 =62.322 λ−5.5 An approximate eigenvalue is λ = 5.516 and an approximate eigenvector is the above vector.
How well does it work?
     2 1 3 1.0 5.516      2 1 1 0.6541 = 3.6081 3 2 1 0.95398 5.2622     1.0 5.516     5.516 0.6541 = 3.608 0.95398 5.2622 It appears this is very close.
15.1.3 Complex Eigenvalues What about complex eigenvalues?
If your matrix is real, you won’t see these by graphing the characteristic equation on your calculator.
Will the shifted inverse power method ﬁnd these eigenvalues and their associated eigenvectors?
The answer is yes.
However, for a real matrix, you must pick α to be complex.
This is because the eigenvalues occur in conjugate pairs so if you don’t pick it complex, it will be the same distance between any conjugate pairofcomplexnumbersandsonothing intheaboveargumentforconvergenceimpliesyou will get convergence to a complex number.
Also, the process of iteration will yield only real vectors and scalars.
Example 15.1.7 Find the complex eigenvalues and corresponding eigenvectors for the ma- trix   5 −8 6   1 0 0 .
0 1 0 Here the characteristic equation is λ3 −5λ2 +8λ−6 = 0.
One solution is λ = 3.
The other two are 1+i and 1−i.
I will apply the process to α=i to ﬁnd the eigenvalue closest to i.
  −.02−.14i 1.24+.68i −.84+.12i (A−αI)−1 = −.14+.02i .68−.24i .12+.84i  .02+.14i −.24−.68i .84+.88i Then let u =(1,1,1)T for lack of any insight into anything better.
1      −.02−.14i 1.24+.68i −.84+.12i 1 .38+.66i  −.14+.02i .68−.24i .12+.84i  1 = .66+.62i  .02+.14i −.24−.68i .84+.88i 1 .62+.34i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation382 NUMERICAL METHODS FOR FINDING EIGENVALUES s2 = .66+.62i.
  .80487805+.24390244i   u = 1.0 2 .75609756−.19512195i   −.02−.14i 1.24+.68i −.84+.12i  −.14+.02i .68−.24i .12+.84i · .02+.14i −.24−.68i .84+.88i   .80487805+.24390244i   1.0 .75609756−.19512195i   .64634146+.81707317i   = .81707317+.35365854i .54878049−6.0975609×10−2i s =.64634146+.81707317i.Aftermoreiterations,ofthissort,youﬁnds =1.0027485+ 3 9 2.1376217×10−4i and   1.0 u = .50151417−.49980733i .
9 1.5620881×10−3−.49977855i Then   −.02−.14i 1.24+.68i −.84+.12i  −.14+.02i .68−.24i .12+.84i · .02+.14i −.24−.68i .84+.88i   1.0  .50151417−.49980733i  1.5620881×10−3−.49977855i   1.0004078+1.269979×10−3i =  .50107731−.49889366i  8.848928×10−4−.49951522i s =1.0004078+1.269979×10−3i.
10   1.0 u = .50023918−.49932533i  10 2.5067492×10−4−.49931192i Thescalingfactorsarenotchangingmuchatthispoint.
Thusyouwouldsolvethefollowing for λ.
1 1.0004078+1.269979×10−3i= λ−i The approximate eigenvalue is then λ = .99959076+.99873106i.
This is pretty close to 1+i.
How well does the eigenvector work?
   5 −8 6 1.0  1 0 0  .50023918−.49932533i  0 1 0 2.5067492×10−4−.49931192i   .99959061+.99873112i   = 1.0 .50023918−.49932533i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 383   1.0 (.99959076+.99873106i) .50023918−.49932533i  2.5067492×10−4−.49931192i   .99959076+.99873106i =  .99872618+4.8342039×10−4i  .4989289−.49885722i It took more iterations than before because α was not very close to 1+i.
This illustrates an interesting topic which leads to many related topics.
If you have a polynomial, x4+ax3+bx2+cx+d, you can consider it as the characteristic polynomial of a certain matrix, called a companion matrix.
In this case,   −a −b −c −d    1 0 0 0   .
0 1 0 0 0 0 1 0 The above example was just a companion matrix for λ3 −5λ2 +8λ−6.
You can see the patternwhichwillenableyoutoobtainacompanionmatrixforanypolynomialoftheform λn +a1λn−1 +···+an−1λ+an.
This illustrates that one way to ﬁnd the complex zeros of a polynomial is to use the shifted inverse power method on a companion matrix for the polynomial.
Doubtless there are better ways but this does illustrate how impressive this procedure is.
Do you have a better way?
Notethattheshiftedinversepowermethodisawayyoucanbeginwithsomethingclose but not equal to an eigenvalue and end up with something close to an eigenvector.
15.1.4 Rayleigh Quotients And Estimates for Eigenvalues TherearemanyspecializedresultsconcerningtheeigenvaluesandeigenvectorsforHermitian matrices.
Recall a matrix A is Hermitian if A=A∗ where A∗ means to take the transpose of the conjugate of A.
In the case of a real matrix, Hermitian reduces to symmetric.
Recall also that for x∈Fn, ∑n |x|2 =x∗x= |x |2.
j j=1 Recall the following corollary found on Page 179 which is stated here for convenience.
Corollary 15.1.8 If A is Hermitian, then all the eigenvalues of A are real and there exists an orthonormal basis of eigenvectors.
Thus for {x }n this orthonormal basis, k k=1 { 1 if i=j x∗x =δ ≡ i j ij 0 if i̸=j For x∈Fn, x̸=0, the Rayleigh quotient is deﬁned by x∗Ax .
|x|2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation384 NUMERICAL METHODS FOR FINDING EIGENVALUES Now let the eigenvalues of A be λ ≤ λ ≤ ··· ≤ λ and Ax = λ x where {x }n is 1 2 n k k k k k=1 the above orthonormal basis of eigenvectors mentioned in the corollary.
Then if x is an arbitrary vector, there exist constants, a such that i ∑n x= a x .
i i i=1 Also, ∑n ∑n |x|2 = a x∗ a x i i j j i=1 j=1 ∑ ∑ ∑n = a a x∗x = a a δ = |a |2.
i j i j i j ij i ij ij i=1 Therefore, ( ) ∑ ∑ x∗Ax ( ni=1aix∗i) nj=1ajλjxj = ∑ |x|2 n |a |2 ∑ i=1 i ∑ a a λ x∗x a a λ δ = i∑j i j j i j = ∑ij i j j ij n |a |2 n |a |2 ∑ i=1 i i=1 i n |a |2λ = ∑i=1 i i ∈[λ ,λ ].
n |a |2 1 n i=1 i Inotherwords,theRayleighquotientisalwaysbetweenthelargestandthesmallesteigenval- uesofA.Whenx=x ,theRayleighquotientequalsthelargesteigenvalueandwhenx=x n 1 theRayleighquotientequalsthesmallesteigenvalue.
SupposeyoucalculateaRayleighquo- tient.
How close is it to some eigenvalue?
Theorem 15.1.9 Let x̸=0 and form the Rayleigh quotient, x∗Ax ≡q.
|x|2 Then there exists an eigenvalue of A, denoted here by λ such that q |Ax−qx| |λ −q|≤ .
(15.6) q |x| ∑ Proof: Let x= n a x where {x }n is the orthonormal basis of eigenvectors.
k=1 k k k k=1 |Ax−qx|2 = (Ax−qx)∗(Ax−qx) ( ) ( ) ∑n ∗ ∑n = a λ x −qa x a λ x −qa x k k k k k k k k k k k=1 ( k=1 ) ∑n ∑n =  (λ −q)a x∗ (λ −q)a x j j j k k k j=1 k=1 ∑ = (λ −q)a (λ −q)a x∗x j j k k j k j,k ∑n = |a |2(λ −q)2 k k k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.1.
THE POWER METHOD FOR EIGENVALUES 385 Now pick the eigenvalue λ which is closest to q.
Then q ∑n ∑n |Ax−qx|2 = |a |2(λ −q)2 ≥(λ −q)2 |a |2 =(λ −q)2|x|2 k k q k q k=1 k=1 which implies (15.6).
(cid:4)   1 2 3 Example 15.1.10 Consider the symmetric matrix A= 2 2 1 .
Let x=(1,1,1)T .
3 1 4 How close is the Rayleigh quotient to some eigenvalue of A?
Find the eigenvector and eigen- value to several decimal places.
Everything is real and so there is no need to worry about taking conjugates.
Therefore, the Rayleigh quotient is    ( ) 1 2 3 1    1 1 1 2 2 1 1 3 1 4 1 19 = 3 3 According to the above theorem, there is some eigenvalue of this matrix λ such that q (cid:12)    (cid:12) (cid:12) (cid:12) (cid:12) 1 2 3 1 1 (cid:12) (cid:12)(cid:12)(cid:12)(cid:12)λq− 139(cid:12)(cid:12)(cid:12)(cid:12) ≤ (cid:12)(cid:12)(cid:12) 23 21 14 √113 − 139 11 (cid:12)(cid:12)(cid:12)   −1 = √1  −34  3 53 √ 3 ( ) ( ) 1 + 4 2+ 5 2 = 9 √3 3 =1.2472 3 Could you ﬁnd this eigenvalue and associated eigenvector?
Of course you could.
This is what the shifted inverse power method is all about.
Solve        1 2 3 1 0 0 x 1  2 2 1 − 19 0 1 0  y = 1  3 3 1 4 0 0 1 z 1 In other words solve      −16 2 3 x 1  23 −13 1  y = 1  3 3 1 −7 z 1 3 and divide by the entry which is largest, 3.8707, to get   .69925   u = .49389 2 1.0 Now solve      −16 2 3 x .69925  23 −13 1  y = .49389  3 3 1 −7 z 1.0 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation386 NUMERICAL METHODS FOR FINDING EIGENVALUES and divide by the largest entry, 2.9979 to get   .71473   u = .52263 3 1.0 Now solve      −16 2 3 x .71473  23 −13 1  y = .52263  3 3 1 −7 z 1.0 3 and divide by the largest entry, 3.0454, to get   .7137   u = .52056 4 1.0 Solve      −16 2 3 x .7137  23 −13 1  y = .52056  3 3 1 −7 z 1.0 3 and divide by the largest entry, 3.0421 to get   .71378   u = .52073 5 1.0 You can see these scaling factors are not changing much.
The predicted eigenvalue is then about 1 19 + =6.6621.
3.0421 3 How close is this?
     1 2 3 .71378 4.7552      2 2 1 .52073 = 3.469 3 1 4 1.0 6.6621 while     .71378 4.7553     6.6621 .52073 = 3.4692 .
1.0 6.6621 You see that for practical purposes, this has found the eigenvalue and an eigenvector.
15.2 The QR Algorithm 15.2.1 Basic Properties And Deﬁnition RecallthetheoremabouttheQRfactorizationinTheorem5.7.5.
Itsaysthatgivenann×n realmatrixA,thereexistsarealorthogonalmatrixQandanuppertriangularmatrixRsuch that A = QR and that this factorization can be accomplished by a systematic procedure.
One such procedure was given in proving this theorem.
There is also a way to generalize the QR factorization to the case where A is just a complexn×nmatrixandQisunit(arywhileRisup)pertriangularwithnonnegativeentries onthemaindiagonal.
LettingA= a ··· a bethematrixwiththea thecolumns, 1 n j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 387 each a vector in Cn, let Q be a unitary matrix which maps a to |a |e in the case that 1 1 1 1 a ̸=0.
If a =0, let Q =I.
Why does such a unitary matrix exist?
Let 1 1 1 {a /|a |,u ,··· ,u } 1 1 2 n ( ) beanorthonormalbasisandletQ1 |aa11| =e1,Q1(u2)=e2 etc.
ExtendQ1 linearly.
Then Q preserves lengths so it is unitary by Lemma 13.6.1.
Now 1 ( ) Q A = Q a Q a ··· Q a 1 ( 1 1 1 2 1 n ) = |a |e Q a ··· Q a 1 1 1 2 1 n which is a matrix of the form ( ) |a | b 1 0 A 1 Now do the same thing for A obtaining an n−1×n−1 unitary matrix Q′ which when 1 2 multiplied on the left of A yields something of the form 1 ( ) a b 1 0 A 2 Then multiplying A on the left by the product ( ) 1 0 Q ≡Q Q 0 Q′ 1 2 1 2 yields a matrix which is upper triangular with respect to the ﬁrst two columns.
Continuing this way QnQn−1···Q1A=R where R is upper triangular having all positive entries on the main diagonal.
Then the desired unitary matrix is Q=(QnQn−1···Q1)∗ I I The QR algorithm is described in the following deﬁnition.
Deﬁnition 15.2.1 The QR algorithm is the following.
In the description of this algorithm, Q is unitary and R is upper triangular having nonnegative entries on the main diagonal.
Starting with A an n×n matrix, form A ≡A=Q R (15.7) 0 1 1 Then A ≡R Q .
(15.8) 1 1 1 In general given A =R Q , (15.9) k k k obtain A by k+1 A =Q R , A =R Q (15.10) k k+1 k+1 k+1 k+1 k+1 This algorithm was proposed by Francis in 1961.
The sequence {A } is the desired k sequenceofiterates.
Nowwiththeabovedeﬁnitionofthealgorithm,hereareitsproperties.
ThenextlemmashowseachoftheA isunitarilysimilartoAandtheamazingthingabout k this algorithm is that often it becomes increasingly easy to ﬁnd the eigenvalues of the A .
k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation388 NUMERICAL METHODS FOR FINDING EIGENVALUES Lemma 15.2.2 LetA bean n×n matrixand let the Q and R beas describedinthe algo- k k rithm.
Then each A is unitarily similar to A and denoting by Q(k) the product Q Q ···Q k 1 2 k and R(k) the product RkRk−1···R1, it follows that Ak =Q(k)R(k) (The matrix on the left is A raised to the kth power.)
A=Q(k)A Q(k)∗, A =Q(k)∗AQ(k).
k k Proof: From the algorithm, R =A Q∗ and so k+1 k+1 k+1 ∗ A =Q R =Q A Q k k+1 k+1 k+1 k+1 k+1 Now iterating this, it follows ∗ ∗ ∗ Ak−1 =QkAkQk =QkQk+1Ak+1Qk+1Qk ∗ ∗ ∗ ∗ Ak−2 =Qk−1Ak−1Qk−1 =Qk−1QkQk+1Ak+1Qk+1QkQk−1 etc.
Thus, after k−2 more iterations, A=Q(k+1)A Q(k+1)∗ k+1 The product of unitary matrices is unitary and so this proves the ﬁrst claim of the lemma.
Now consider the part about Ak.
From the algorithm, this is clearly true for k = 1.
(A1 =QR) Suppose then that Ak =Q1Q2···QkRkRk−1···R1 What was just shown indicated A=Q Q ···Q A Q∗ Q∗···Q∗ 1 2 k+1 k+1 k+1 k 1 and now from the algorithm, A =R Q and so k+1 k+1 k+1 A=Q Q ···Q R Q Q∗ Q∗···Q∗ 1 2 k+1 k+1 k+1 k+1 k 1 Then Ak+1 =AAk = z }A| { Q1Q2···Qk+1Rk+1Qk+1Q∗k+1Q∗k···Q∗1Q1···QkRkRk−1···R1 =Q1Q2···Qk+1Rk+1RkRk−1···R1 ≡Q(k+1)R(k+1) (cid:4) Here is another very interesting lemma.
Lemma 15.2.3 Suppose Q(k),Q are unitary and R is upper triangular such that the di- k agonal entries on R are all positive and k Q= lim Q(k)R k k→∞ Then lim Q(k) =Q, lim R =I.
k k→∞ k→∞ Also the QR factorization of A is unique whenever A−1 exists.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 389 Proof: Let ( ) Q=(q ,··· ,q ), Q(k) = qk,··· ,qk 1 n 1 n where the q are the columns.
Also denote by rk the ijth entry of R .
Thus ij k   rk ∗ ( ) 11  Q(k)Rk = qk1,··· ,qkn  ...  0 rk nn It follows rk qk →q 11 1 1 and so (cid:12) (cid:12) rk =(cid:12)rk qk(cid:12)→1 11 11 1 Therefore, qk →q .
1 1 Next consider the second column.
rk qk+rk qk →q 12 1 22 2 2 Taking the inner product of both sides with qk it follows 1 ( ) lim rk = lim q ·qk =(q ·q )=0.
k→∞ 12 k→∞ 2 1 2 1 Therefore, lim rk qk =q k→∞ 22 2 2 and since rk >0, it follows as in the ﬁrst part that rk →1.
Hence 22 22 lim qk =q .
k→∞ 2 2 Continuing this way, it follows lim rk =0 k→∞ ij for all i̸=j and lim rk =1, lim qk =q .
k→∞ jj k→∞ j j Thus R →I and Q(k) →Q.
This proves the ﬁrst part of the lemma.
k The second part follows immediately.
If QR=Q′R′ =A where A−1 exists, then ∗ ′ ′ −1 Q Q =R(R) and I need to show both sides of the above are equal to I.
The left side of the above is unitary and the right side is upper triangular having positive entries on the diagonal.
This is because the inverse of such an upper triangular matrix having positive entries on the main diagonal is still upper triangular having positive entries on the main diagonal and the product of two such upper triangular matrices gives another of the same form having positive entries on the main diagonal.
Suppose then that Q=R where Q is unitary and R is upper triangular having positive entries on the main diagonal.
Let Q =Q and R =R.
k k It follows IR →R=Q k and so from the ﬁrst part, R → I but R = R and so R = I.
Thus applying this to k k Q∗Q′ =R(R′)−1 yields both sides equal I.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation390 NUMERICAL METHODS FOR FINDING EIGENVALUES A case of all this is of grea(t interest.
Suppose A) has a largest eigenvalue λ which is real.
Then An is of the form An−1a ,··· ,An−1a and so likely each of these columns 1 n will be pointing roughly in the direction of an eigenvector of A which corresponds to this eigenvalue.
Then when you do the QR factorization of this, it follows from the fact that R is upper triangular, that the ﬁrst column of Q will be a multiple of An−1a and so will end 1 up being roughly parallel to the eigenvector desired.
Also this will require the entries below thetopintheﬁrstcolumnofA =QTAQwillallbesmallbecausetheywillbeoftheform n qTAq ≈ λqTq =0.
Therefore, A will be of the form i 1 i 1 n ( ) ′ λ a e B ′ whereeissmall.
Itfollowsthatλ willbeclosetoλandq willbeclosetoaneigenvectorfor 1 λ. Thenifyoulike,youcoulddothesamethingwiththematrixBtoobtainapproximations for the other eigenvalues.
Finally, you could use the shifted inverse power method to get more exact solutions.
15.2.2 The Case Of Real Eigenvalues Withtheselemmas,itispossibletoprovethatfortheQRalgorithmandcertainconditions, the sequence A converges pointwise to an upper triangular matrix having the eigenvalues k of A down the diagonal.
I will assume all the matrices are real here.
( ) 0 1 This convergence won’t always happen.
Consider for example the matrix .
1 0 You can verify quickly that the algorithm will return this matrix for each k. The problem hereisthat,althoughthematrixhasthetwoeigenvalues−1,1,theyhavethesameabsolute value.
TheQRalgorithmworksinsomewhatthesamewayasthepowermethod,exploiting diﬀerences in the size of the eigenvalues.
If A has all real eigenvalues and you are interested in ﬁnding these eigenvalues along with the corresponding eigenvectors, you could always consider A+λI instead where λ is suﬃcientlylargeandpositivethatA+λI hasallpositiveeigenvalues.
(RecallGerschgorin’s theorem.)
Then if µ is an eigenvalue of A+λI with (A+λI)x=µx then Ax=(µ−λ)x so to ﬁnd the eigenvalues of A you just subtract λ from the eigenvalues of A+λI.
Thus there is no loss of generality in assuming at the outset that the eigenvalues of A are all positive.
Here is the theorem.
It involves a technical condition which will often hold.
The proof presented here follows [26] and is a special case of that presented in this reference.
Before giving the proof, note that the product of upper triangular matrices is upper triangular.
If they both have positive entries on the main diagonal so will the product.
Furthermore, the inverse of an upper triangular matrix is upper triangular.
I will use these simple facts without much comment whenever convenient.
Theorem 15.2.4 Let A be a real matrix having eigenvalues λ >λ >···>λ >0 1 2 n and let A=SDS−1 (15.11) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 391 where   λ 0 1   D = ...  0 λ n and suppose S−1 has an LU factorization.
Then the matrices A in the QR algorithm k described above converge to an upper triangular matrix T′ having the eigenvalues of A, λ ,··· ,λ descending on the main diagonal.
The matrices Q(k) converge to Q′, an orthog- 1 n onal matrix which equals Q except for possibly having some columns multiplied by −1 for Q the unitary part of the QR factorization of S, S =QR, and lim A =T′ =Q′TAQ′ k k→∞ Proof: From Lemma 15.2.2 Ak =Q(k)R(k) =SDkS−1 (15.12) LetS =QRwherethisisjustaQRfactorizationwhichisknowntoexistandletS−1 =LU which is assumed to exist.
Thus Q(k)R(k) =QRDkLU (15.13) and so Q(k)R(k) =QRDkLU =QRDkLD−kDkU That matrix in the middle, DkLD−k satisﬁes ( ) DkLD−k =λkL λ−k for j ≤i, 0 if j >i.
ij i ij j Thus for j < i the expression converges to 0 because λ > λ when this happens.
When j i i=j it reduces to 1.
Thus the matrix in the middle is of the form I+E k where E →0.
Then it follows k Ak =Q(k)R(k) =QR(I+E )DkU k ( ) =Q I+RE R−1 RDkU ≡Q(I+F )RDkU k k where F → 0.
Then let I +F = Q R where this is another QR factorization.
Then it k k k k reduces to Q(k)R(k) =QQ R RDkU k k This looks really interesting because by Lemma 15.2.3 Q → I and R → I because k k Q R =(I +F )→I.
So it follows QQ is an orthogonal matrix converging to Q while k k k k ( ) −1 R RDkU R(k) k isuppertriangular, beingtheproductofuppertriangularmatrices.
Unfortunately, itisnot known that the diagonal entries of this matrix are nonnegative because of the U.
Let Λ be just like the identity matrix but having some of the ones replaced with −1 in such a way Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation392 NUMERICAL METHODS FOR FINDING EIGENVALUES that ΛU is an upper triangular matrix having positive diagonal entries.
Note Λ2 = I and also Λ commutes with a diagonal matrix.
Thus Q(k)R(k) =QQ R RDkΛ2U =QQ R RΛDk(ΛU) k k k k At this point, one does some inspired massaging to write the above in the form ( )[( ) ] QQ ΛDk ΛDk −1R RΛDk (ΛU) k k [( ) ] = Q(Q Λ)Dk ΛDk −1R RΛDk (ΛU) k k z [( ) ≡}G|k ] { = Q(Q Λ)Dk ΛDk −1R RΛDk (ΛU) k k Now I claim the middle matrix in [·] is upper triangular and has all positive entries on the diagonal.
This is because it is an upper triangular matrix which is similar to the upper triangular matrix RkR[a(nd so)it has the sa]me eigenvalues (diagonal entries) as RkR.
Thus the matrix G ≡ Dk ΛDk −1R RΛDk (ΛU) is upper triangular and has all positive k k entries on the diagonal.
Multiply on the right by G−1 to get k Q(k)R(k)G−1 =QQ Λ→Q′ k k where Q′ is essentially equal to Q but might have some of the columns multiplied by −1.
This is because Q →I and so Q Λ→Λ.
Now by Lemma 15.2.3, it follows k k Q(k) →Q′, R(k)G−1 →I.
k ItremainstoverifyA convergestoanuppertriangularmatrix.
Recallthatfrom(15.12) k and the deﬁnition below this (S =QR) A=SDS−1 =(QR)D(QR)−1 =QRDR−1QT =QTQT WhereT isanuppertriangularmatrix.
Thisisbecauseitistheproductofuppertriangular matrices R,D,R−1.
Thus QTAQ=T.
IfyoureplaceQwithQ′ intheabove,itstillresultsinanuppertriangularmatrixT′ having the same diagonal entries as T. This is because T =QTAQ=(Q′Λ)T A(Q′Λ)=ΛQ′TAQ′Λ and considering the iith entry yields ( ) ∑ ( ) ( ) ( ) QTAQ ≡ Λ Q′TAQ′ Λ =Λ Λ Q′TAQ′ = Q′TAQ′ ii ij jk ki ii ii ii ii j,k Recall from Lemma 15.2.2, A =Q(k)TAQ(k) k Thus taking a limit and using the ﬁrst part, A =Q(k)TAQ(k) →Q′TAQ′ =T′.
(cid:4) k An easy case is for A symmetric.
Recall Corollary 7.4.13.
By this corollary, there exists an orthogonal (real unitary) matrix Q such that QTAQ=D whereD isdiagonalhavingtheeigenvaluesonthemaindiagonaldecreasinginsizefromthe upper left corner to the lower right.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 393 Corollary 15.2.5 Let A be a real symmetric n×n matrix having eigenvalues λ >λ >···>λ >0 1 2 n and let Q be deﬁned by QDQT =A, D =QTAQ, (15.14) where Q is orthogonal and D is a diagonal matrix having the eigenvalues on the main diagonal decreasing in size from the upper left corner to the lower right.
Let QT have an LU factorization.
Then in the QR algorithm, the matrices Q(k) converge to Q′ where Q′ is the same as Q except having some columns multiplied by (−1).
Thus the columns of Q′ are eigenvectors of A.
The matrices A converge to D. k Proof: This follows from Theorem 15.2.4.
Here S =Q,S−1 =QT.
Thus Q=S =QR and R=I.
By Theorem 15.2.4 and Lemma 15.2.2, A =Q(k)TAQ(k) →Q′TAQ′ =QTAQ=D.
k because formula (15.14) is unaﬀected by replacing Q with Q′.
(cid:4) When using the QR algorithm, it is not necessary to check technical condition about S−1 having an LU factorization.
The algorithm delivers a sequence of matrices which are similar to the original one.
If that sequence converges to an upper triangular matrix, then the algorithm worked.
Furthermore, the technical condition is suﬃcient but not necessary.
The algorithm will work even without the technical condition.
Example 15.2.6 Find the eigenvalues and eigenvectors of the matrix   5 1 1   A= 1 3 2 1 2 1 It is a symmetric matrix but other than that, I just pulled it out of the air.
By Lemma 15.2.2 it follows A = Q(k)TAQ(k).
And so to get to the answer quickly I could have the k computer raise A to a power and then take the QR factorization of what results to get the kth iteration using the above formula.
Lets pick k =10.
    5 1 1 10 4.2273×107 2.5959×107 1.8611×107  1 3 2  = 2.5959×107 1.6072×107 1.1506×107  1 2 1 1.8611×107 1.1506×107 8.2396×106 Now take QR factorization of this.
The computer will do that also.
This yields   .79785 −.59912 −6.6943×10−2  .48995 .70912 −.50706 · .35126 .37176 .85931   5.2983×107 3.2627×107 2.338×107  0 1.2172×105 71946.
 0 0 277.03 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation394 NUMERICAL METHODS FOR FINDING EIGENVALUES Next it follows   .79785 −.59912 −6.6943×10−2 T A =  .48995 .70912 −.50706  · 10 .35126 .37176 .85931    5 1 1 .79785 −.59912 −6.6943×10−2  1 3 2  .48995 .70912 −.50706  1 2 1 .35126 .37176 .85931 and this equals   6.0571 3.698×10−3 3.4346×10−5  3.698×10−3 3.2008 −4.0643×10−4  3.4346×10−5 −4.0643×10−4 −.2579 By Gerschgorin’s theorem, the eigenvalues are pretty close to the diagonal entries of the abovematrix.
NoteIdidn’tusethetheorem,justLemma15.2.2andGerschgorin’stheorem to verify the eigenvalues are close to the above numbers.
The eigenvectors are close to       .79785 −.59912 −6.6943×10−2  .48995 , .70912 , −.50706  .35126 .37176 .85931 Lets check one of these.
     5 1 1 1 0 0 .79785  1 3 2 −6.0571 0 1 0  .48995  1 2 1 0 0 1 .35126     −2.1972×10−3 0 =  2.5439×10−3 ≈ 0  1.3931×10−3 0 Now lets see how well the smallest approximate eigenvalue and eigenvector works.
     5 1 1 1 0 0 −6.6943×10−2  1 3 2 −(−.2579) 0 1 0  −.50706  1 2 1 0 0 1 .85931     2.704×10−4 0 = −2.7377×10−4 ≈ 0  −1.3695×10−4 0 For practical purposes, this has found the eigenvalues and eigenvectors.
15.2.3 The QR Algorithm In The General Case InthecasewhereAhasdistinctpositiveeigenvaluesitwasshownabovethatunderreason- able conditions related to a certain matrix having an LU factorization the QR algorithm producesasequenceofmatrices{A }whichconvergestoanuppertriangularmatrix.
What k ifAisjustann×nmatrixhavingpossiblycomplexeigenvaluesbutAisnondefective?What happens with the QR algorithm in this case?
The short answer to this question is that the A of the algorithm typically cannot converge.
However, this does not mean the algo- k rithm is not useful in ﬁnding eigenvalues.
It turns out the sequence of matrices {A } have k the appearance of a block upper triangular matrix for large k in the sense that the entries Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 395 below the blocks on the main diagonal are small.
Then looking at these blocks gives a way to approximate the eigenvalues.
An important example of the concept of a block triangular matrix is the real Schur form for a matrix discussed in Theorem 7.4.6 but the concept as described here allows for any size block centered on the diagonal.
First it is important to note a simple fact about unitary diagonal matrices.
In what follows Λ will denote a unitary matrix which is also a diagonal matrix.
These matrices are just the identity matrix with some of the ones replaced with a number of the form eiθ for some θ.
The important property of multiplication of any matrix by Λ on either side is that it leaves all the zero entries the same and also preserves the absolute values of the other entries.
Thus a block triangular matrix multiplied by Λ on either side is still block triangular.
If the matrix is close to being block triangular this property of being close to a blocktriangularmatrixisalsopreservedbymultiplyingoneithersidebyΛ.
Otherpatterns depending only on the size of the absolute value occurring in the matrix are also preserved by multiplying on either side by Λ.
In other words, in looking for a pattern in a matrix, multiplication by Λ is irrelevant.
Now let A be an n×n matrix having real or complex entries.
By Lemma 15.2.2 and the assumption that A is nondefective, there exists an invertible S, Ak =Q(k)R(k) =SDkS−1 (15.15) where   λ 0 1   D = ...  0 λ n and by rearranging the columns of S, D can be made such that |λ |≥|λ |≥···≥|λ |.
1 2 n Assume S−1 has an LU factorization.
Then Ak =SDkLU =SDkLD−kDkU.
Consider the matrix in the middle, DkLD−k.
The ijth entry is of the form  ( )  λkiLijλ−jk if j <i DkLD−k = 1 if i=j ij  0 if j >i and these all converge to 0 whenever |λ |<|λ |.
Thus i j DkLD−k =(L +E ) k k where L is a lower triangular matrix which has all ones down the diagonal and some k subdiagonal terms of the form λkL λ−k (15.16) i ij j for which |λ |=|λ | while E →0.
(Note the entries of L are all bounded independent of i j k k k but some may fail to converge.)
Then Q(k)R(k) =S(L +E )DkU k k Let SL =Q R (15.17) k k k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation396 NUMERICAL METHODS FOR FINDING EIGENVALUES where this is the QR factorization of SL .
Then k Q(k)R(k) = (Q R +SE )DkU k( k k ) = Q I+Q∗SE R−1 R DkU k k k k k = Q (I+F )R DkU k k k where F →0.
Let I+F =Q′R′.
Then k k k k Q(k)R(k) =Q Q′R′R DkU k k k k By Lemma 15.2.3 Q′ →I and R′ →I.
(15.18) k k Now let Λ be a diagonal unitary matrix which has the property that k Λ∗DkU k is an upper triangular matrix which has all the diagonal entries positive.
Then Q(k)R(k) =Q Q′Λ (Λ∗R′R Λ )Λ∗DkU k k k k k k k k That matrix in the middle has all positive diagonal entries because it is itself an upper triangular matrix, being the product of such, and is similar to the matrix R′R which is k k uppertriangularwithpositivediagonalentries.
ByLemma15.2.3again,thistimeusingthe uniqueness assertion, Q(k) =Q Q′Λ , R(k) =(Λ∗R′R Λ )Λ∗DkU k k k k k k k k Note the term Q Q′Λ must be real because the algorithm gives all Q(k) as real matrices.
k k k By (15.18) it follows that for k large enough Q(k) ≈Q Λ k k where ≈ means the two matrices are close.
Recall A =Q(k)TAQ(k) k and so for large k, A ≈(Q Λ )∗A(Q Λ )=Λ∗Q∗AQ Λ k k k k k k k k k As noted above, the form of Λ∗Q∗AQ Λ in terms of which entries are large and small is k k k k not aﬀected by the presence of Λ and Λ∗.
Thus, in considering what form this is in, it k k suﬃces to consider Q∗AQ .
k k This could get pretty complicated but I will consider the case where if |λ |=|λ |, then |λ |<|λ |.
(15.19) i i+1 i+2 i+1 ThisistypicalofthesituationwheretheeigenvaluesarealldistinctandthematrixAisreal so the eigenvalues occur as conjugate pairs.
Then in this case, L above is lower triangular k withsomenonzerotermsonthediagonalrightbelowthemaindiagonalbutzeroseverywhere else.
Thus maybe (L ) ̸=0 k s+1,s Recall (15.17) which implies Q =SL R−1 (15.20) k k k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 397 where R−1 is upper triangular.
Also recall that from the deﬁnition of S in (15.15), k S−1AS =D and so the columns of S are eigenvectors of A, the ith being an eigenvector for λ .
Now i fromtheformofL ,itfollowsL R−1 isablockuppertriangularmatrixdenotedbyT and k k k B so Q = ST .
It follows from the above construction in (15.16) and the given assumption k B on the sizes of the eigenvalues, there are ﬁnitely many 2×2 blocks centered on the main diagonal along with possibly some diagonal entries.
Therefore, for large k the matrix A =Q(k)TAQ(k) k is approximately of the same form as that of Q∗AQ =T−1S−1AST =T−1DT k k B B B B whichisablockuppertriangularmatrix.
Asexplainedabove, multiplicationbythevarious diagonal unitary matrices does not aﬀect this form.
Therefore, for large k, A is approxi- k mately a block upper triangular matrix.
Howwouldthischangeiftheaboveassumptiononthesizeoftheeigenvalueswererelaxed but the matrix was still nondefective with appropriate matrices having an LU factorization as above?
It would mean the blocks on the diagonal would be larger.
This immediately makestheproblemmorecumbersometodealwith.
However,inthecasethattheeigenvalues of A are distinct, the above situation really is typical of what occurs and in any case can be quickly reduced to this case.
To see this, suppose condition (15.19) is violated and λ ,··· ,λ are complex eigen- j j+p values having nonzero imaginary parts such that each has the same absolute value but they are all distinct.
Then let µ > 0 and consider the matrix A+µI.
Thus the correspond- ing eigenvalues of A + µI are λ + µ,··· ,λ + µ.
A short computation shows shows j j+p |λ +µ|,··· ,|λ +µ|arealldistinctandsotheabovesituationof(15.19)isobtained.
Of j j+p course, if there are repeated eigenvalues, it may not be possible to reduce to the case above and you would end up with large blocks on the main diagonal which could be diﬃcult to deal with.
So how do you identify the eigenvalues?
You know A and behold that it is close to a k block upper triangular matrix T′ .
You know A is also similar to A.
Therefore, T′ has B k B eigenvalues which are close to the eigenvalues of A and hence those of A provided k is k suﬃciently large.
See Theorem 7.9.2 which depends on complex analysis or the exercise on Page 197 which gives another way to see this.
Thus you ﬁnd the eigenvalues of this block triangular matrix T′ and assert that these are good approximations of the eigenvalues of B A and hence to those of A.
How do you ﬁnd the eigenvalues of a block triangular matrix?
k This is easy from Lemma 7.4.5.
Say   B ··· ∗ 1 TB′ = ... ...  0 B m Then forming λI−T′ and taking the determinant, it follows from Lemma 7.4.5 this equals B ∏m det(λI −B ) j j j=1 and so all you have to do is take the union of the eigenvalues for each B .
In the case j emphasized here this is very easy because these blocks are just 2×2 matrices.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation398 NUMERICAL METHODS FOR FINDING EIGENVALUES How do you identify approximate eigenvectors from this?
First try to ﬁnd the approx- imate eigenvectors for A .
Pick an approximate eigenvalue λ, an exact eigenvalue for T′ .
k B Then ﬁnd v solving T′ v=λv.
It follows since T′ is close to A that B B k A v≈λv k and so Q(k)AQ(k)Tv=A v≈λv k Hence AQ(k)Tv≈λQ(k)Tv and so Q(k)Tv is an approximation to the eigenvector which goes with the eigenvalue of A which is close to λ.
Example 15.2.7 Here is a matrix.
  3 2 1  −2 0 −1  −2 −2 0 It happens that the eigenvalues of this matrix are 1,1+i,1−i.
Lets apply the QR algorithm as if the eigenvalues were not known.
Applying the QR algorithm to this matrix yields the following sequence of matrices.
  1.2353 1.9412 4.3657 A = −.39215 1.5425 5.3886×10−2  1 −.16169 −.18864 .22222 .
.
.
  9.1772×10−2 .63089 −2.0398 A = −2.8556 1.9082 −3.1043  12 1.0786×10−2 3.4614×10−4 1.0 At this point the bottom two terms on the left part of the bottom row are both very small so it appears the real eigenvalue is near 1.0.
The complex eigenvalues are obtained from solving ( ( ) ( )) 1 0 9.1772×10−2 .63089 det λ − =0 0 1 −2.8556 1.9082 This yields λ=1.0−.98828i, 1.0+.98828i Example 15.2.8 Theequation x4+x3+4x2+x−2=0 has exactlytwo realsolutions.
You can see this by graphing it.
However, the rational root theorem from algebra shows neither of these solutions are rational.
Also, graphing it does not yield any information about the complex solutions.
Lets use the QR algorithm to approximate all the solutions, real and complex.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.2.
THE QR ALGORITHM 399 A matrix whose characteristic polynomial is the given polynomial is   −1 −4 −1 2    1 0 0 0    0 1 0 0 0 0 1 0 Using the QR algorithm yields the following sequence of iterates for A k   .99999 −2.5927 −1.7588 −1.2978  2.1213 −1.7778 −1.6042 −.99415  A1 = 0 .34246 −.32749 −.91799  0 0 −.44659 .10526 .
.
.
  −.83412 −4.1682 −1.939 −.7783  1.05 .14514 .2171 2.5474×10−2  A9 = 0 4.0264×10−4 −.85029 −.61608  0 0 −1.8263×10−2 .53939 Now this is similar to A and the eigenvalues are close to the eigenvalues obtained from the two blocks on the diagonal.
Of course the lower left corner of the bottom block is vanishing but it is still fairly large so the eigenvalues are approximated by the solution to ( ( ) ( )) 1 0 −.85029 −.61608 det λ − =0 0 1 −1.8263×10−2 .53939 The solution to this is λ=−.85834, .54744 and for the complex eigenvalues, ( ( ) ( )) 1 0 −.83412 −4.1682 det λ − =0 0 1 1.05 .14514 The solution is λ=−.34449−2.0339i, −.34449+2.0339i How close are the complex eigenvalues just obtained to giving a solution to the original equation?
Try −.34449+2.0339i .
When this is plugged in it yields −.0012+2.0068×10−4i which is pretty close to 0.
The real eigenvalues are also very close to the corresponding real solutions to the original equation.
It seems like most of the attention to the QR algorithm has to do with ﬁnding ways to get it to “converge” faster.
Great and marvelous are the clever tricks which have been proposedtodothisbutmyintentistopresentthebasicideas,nottogointothenumerous reﬁnementsofthisalgorithm.
However,thereisonethingwhichisusuallydone.
Itinvolves reducing to the case of an upper Hessenberg matrix which is one which is zero below the main sub diagonal.
To see that every matrix is unitarily similar to an upper Hessenberg matrix , see Problem 1 on Page 273.
What follows is a construction which also proves this.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation400 NUMERICAL METHODS FOR FINDING EIGENVALUES Let A be an invertible n×n matrix.
Let Q′ be a unitary matrix 1  √∑      n |a |2 a Q′1 a2...1 = j=0..2 j1 ≡ 0...  a .
n1 0 0 The vector Q′ is multiplying is just the bottom n−1 entries of the ﬁrst column of A.
Then 1 let Q1 be ( ) 1 0 0 Q′ 1 It follows   a a ··· a ( )  11 12 1n ( ) 1 0  a  1 0 Q1AQ∗1 = 0 Q′1 AQ∗1 = ... A′  0 Q′1∗ 1 0   ∗ ∗ ··· ∗    a  = .
  .
 .
A 1 0 Now let Q′ be the n−2×n−2 matrix which does to the ﬁrst column of A the same sort 2 1 of thing that the n−1×n−1 matrix Q′ did to the ﬁrst column of A.
Let 1 ( ) I 0 Q ≡ 2 0 Q′ 2 where I is the 2×2 identity.
Then applying block multiplication,   ∗ ∗ ··· ∗ ∗  ∗ ∗ ··· ∗ ∗    Q Q AQ∗Q∗ = 0 ∗  2 1 1 2  .
.
  .
.
 .
.
A 2 0 0 where A is now an n−2×n−2 matrix.
Continuing this way you eventually get a unitary 2 matrix Q which is a product of those discussed above such that   ∗ ∗ ··· ∗ ∗  ∗ ∗ ··· ∗ ∗     .
 QAQT = 0 ∗ ∗ ..   ... ... ... ... ∗  0 0 ∗ ∗ This matrix equals zero below the subdiagonal.
It is called an upper Hessenberg matrix.
IthappensthatintheQR algorithm, ifA isupperHessenberg, soisA .
Toseethis, k k+1 note that the matrix is upper Hessenberg means that A =0 whenever i−j ≥2.
ij A =R Q k+1 k k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation15.3.
EXERCISES 401 where A =Q R .
Therefore as shown before, k k k A =R A R−1 k+1 k k k Let the ijth entry of A be ak.
Then if i−j ≥2 k ij ∑n ∑j ak+1 = r ak r−1 ij ip pq qj p=iq=1 It is given that ak =0 whenever p−q ≥2.
However, from the above sum, pq p−q ≥i−j ≥2 and so the sum equals 0.
Since upper Hessenberg matrices stay that way in the algorithm and it is closer to being upper triangular, it is reasonable to suppose the QR algorithm will yield good results more quickly for this upper Hessenberg matrix than for the original matrix.
This would be especially true if the matrix is good sized.
The other important thing to observe is that, starting with an upper Hessenberg matrix, the algorithm will restrict the size of the blocks which occur to being 2×2 blocks which are easy to deal with.
These blocks allow you to identify the complex roots.
15.3 Exercises In these exercises which call for a computation, don’t waste time on them unless you use a computer or calculator which can raise matrices to powers and take QR factorizations.
1.
In Example 15.1.10 an eigenvalue was found correct to several decimal places along with an eigenvector.
Find the other eigenvalues along with their eigenvectors.
  3 2 1   2.
Find the eigenvalues and eigenvectors of the matrix A = 2 1 3 numerically.
1 3 2 √ In this case the exact eigenvalues are ± 3,6.
Compare with the exact answers.
  3 2 1   3.
Find the eigenvalues and eigenvectors of the matrix A = 2 5 3 numerically.
1 3 2 √ √ The exact eigenvalues are 2,4+ 15,4− 15.
Compare your numerical results with the exact values.
Is it much fun to compute the exact eigenvectors?
  0 2 1   4.
Find the eigenvalues and eigenvectors of the matrix A = 2 5 3 numerically.
1 3 2 I don’t know the exact eigenvalues in this case.
Check your answers by multiplying your numerically computed eigenvectors by the matrix.
  0 2 1   5.
Find the eigenvalues and eigenvectors of the matrix A = 2 0 3 numerically.
1 3 2 I don’t know the exact eigenvalues in this case.
Check your answers by multiplying your numerically computed eigenvectors by the matrix.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation402 NUMERICAL METHODS FOR FINDING EIGENVALUES   3 2 3 6.
Consider the matrix A =  2 1 4  and the vector (1,1,1)T .
Find the shortest 3 4 0 distancebetweentheRayleighquotientdeterminedbythisvectorandsomeeigenvalue of A.
  1 2 1 7.
Consider the matrix A =  2 1 4  and the vector (1,1,1)T .
Find the shortest 1 4 5 distancebetweentheRayleighquotientdeterminedbythisvectorandsomeeigenvalue of A.
  3 2 3 8.
Consider the matrix A= 2 6 4  and the vector (1,1,1)T .
Find the shortest 3 4 −3 distancebetweentheRayleighquotientdeterminedbythisvectorandsomeeigenvalue of A.
9.
Using Gerschgorin’s theorem, ﬁnd upper and lower bounds for the eigenvalues of A= 3 2 3   2 6 4 .
3 4 −3 10.
Tellhowtoﬁndamatrixwhosecharacteristicpolynomialisagivenmonicpolynomial.
Thisiscalledacompanionmatrix.
Findtherootsofthepolynomialx3+7x2+3x+7.
11.
Find the roots to x4+3x3+4x2+x+1.
It has two complex roots.
12.
Suppose A is a real symmetric matrix and the technique of reducing to an upper Hessenbergmatrixisfollowed.
ShowtheresultingupperHessenbergmatrixisactually equal to 0 on the top as well as the bottom.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationPositive Matrices Earlier theorems about Markov matrices were presented.
These were matrices in which all the entries were nonnegative and either the columns or the rows added to 1.
It turns out that many of the theorems presented can be generalized to positive matrices.
When this is done,theresultingtheoryismainlyduetoPerronandFrobenius.
Iwillgiveanintroduction to this theory here following Karlin and Taylor [18].
Deﬁnition A.0.1 For A a matrix or vector, the notation, A >> 0 will mean every entry of A is positive.
By A > 0 is meant that every entry is nonnegative and at least one is positive.
By A ≥ 0 is meant that every entry is nonnegative.
Thus the matrix or vector consisting only of zeros is ≥ 0.
An expression like A >> B will mean A−B >> 0 with similar modiﬁcations for > and ≥.
For the sake of this section only, deﬁne the following for x=(x ,··· ,x )T , a vector.
1 n |x|≡(|x |,··· ,|x |)T .
1 n Thus |x| is the vector which results by replacing each entry of x with its absolute value1.
Also deﬁne for x∈Cn, ∑ ||x|| ≡ |x |.
1 k k Lemma A.0.2 Let A>>0 and let x>0.
Then Ax>>0.
∑ Proof: (Ax) = A x >0 because all the A >0 and at least one x >0.
i j ij j ij j Lemma A.0.3 Let A>>0.
Deﬁne S ≡{λ:Ax>λx for some x>>0}, and let K ≡{x≥0 such that ||x|| =1}.
1 Now deﬁne S ≡{λ:Ax≥λx for some x∈K}.
1 Then sup(S)=sup(S ).
1 1This notation is just about the most abominable thing imaginable.
However, it saves space in the presentation of this theory of positive matrices and avoids the use of new symbols.
Please forget about it whenyouleavethissection.
403 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation404 POSITIVE MATRICES Proof: Letλ∈S.Thenthereexistsx>>0suchthatAx>λx.Considery≡x/||x|| .
1 Then ||y|| = 1 and Ay > λy.
Therefore, λ ∈ S and so S ⊆ S .
Therefore, sup(S) ≤ 1 1 1 sup(S ).
1 Now let λ ∈ S .
Then there exists x ≥ 0 such that ||x|| = 1 so x>0 and Ax > λx.
1 1 Letting y ≡ Ax, it follows from Lemma A.0.2 that Ay >> λy and y>>0.
Thus λ ∈ S and so S ⊆S which shows that sup(S )≤sup(S).
(cid:4) 1 1 Thislemmaissigniﬁcantbecausetheset,{x≥0 such that ||x|| =1}≡K isacompact 1 set in Rn.
Deﬁne λ ≡sup(S)=sup(S ).
(1.1) 0 1 The following theorem is due to Perron.
Theorem A.0.4 Let A>>0 be an n×n matrix and let λ be given in (1.1).
Then 0 1. λ >0 and there exists x >>0 such that Ax =λ x so λ is an eigenvalue for A.
0 0 0 0 0 0 2.
If Ax=µx where x̸=0, and µ̸=λ .
Then |µ|<λ .
0 0 3.
The eigenspace for λ has dimension 1.
0 Proof: To see λ >0, consider the vector, e≡(1,··· ,1)T. Then 0 ∑ (Ae) = A >0 i ij j and so λ0 is at least as large as ∑ min A .
ij i j Let {λ } be an increasing sequence of numbers from S converging to λ .
Letting x be k 1 0 k the vector from K which occurs in the deﬁnition of S , these vectors are in a compact set.
1 Therefore, there exists a subsequence, still denoted by x such that x → x ∈ K and k k 0 λ →λ .
Then passing to the limit, k 0 Ax ≥λ x , x >0.
0 0 0 0 If Ax > λ x , then letting y ≡ Ax , it follows from Lemma A.0.2 that Ay >> λ y and 0 0 0 0 0 y>>0.
But this contradicts the deﬁnition of λ as the supremum of the elements of S 0 because since Ay >> λ y, it follows Ay >> (λ +ε)y for ε a small positive number.
0 0 Therefore, Ax = λ x .
It remains to verify that x >> 0.
But this follows immediately 0 0 0 0 from ∑ 0< A x =(Ax ) =λ x .
ij 0j 0 i 0 0i j This proves 1.
Next suppose Ax = µx and x̸=0 and µ ̸= λ .
Then |Ax| = |µ||x|.
But this implies 0 A|x|≥|µ||x|.
(See the above abominable deﬁnition of |x|.)
Case 1: |x|̸=x and |x|̸=−x.
In this case, A|x| > |Ax| = |µ||x| and letting y = A|x|, it follows y>>0 and Ay>>|µ|y which shows Ay >> (|µ|+ε)y for suﬃciently small positive ε and veriﬁes |µ|<λ .
0 Case 2: |x|=x or |x|=−x In this case, the entries of x are all real and have the same sign.
Therefore, A|x| = |Ax| = |µ||x|.
Now let y≡|x|/||x|| .
Then Ay = |µ|y and so |µ| ∈ S showing that 1 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation405 |µ| ≤ λ .
But also, the fact the entries of x all have the same sign shows µ = |µ| and so 0 µ∈S .
Since µ̸=λ , it must be that µ=|µ|<λ .
This proves 2.
1 0 0 Itremainstoverify3.
SupposethenthatAy=λ yandforallscalarsα,αx ̸=y.
Then 0 0 ARey=λ Rey,AImy=λ Imy.
0 0 If Rey = α x and Imy = α x for real numbers, α ,then y = (α +iα )x and it is 1 0 2 0 i 1 2 0 assumed this does not happen.
Therefore, either tRey̸=x for all t∈R 0 or tImy̸=x for all t∈R.
0 Assumetheﬁrstholds.
Thenvaryingt∈R,thereexistsavalueoftsuchthatx +tRey>0 0 butitisnotthecasethatx +tRey>>0.ThenA(x +tRey)>>0byLemmaA.0.2.
But 0 0 this implies λ (x +tRey)>>0 which is a contradiction.
Hence there exist real numbers, 0 0 α and α such that Rey = α x and Imy = α x showing that y=(α +iα )x .
This 1 2 1 0 2 0 1 2 0 proves 3.
It is possible to obtain a simple corollary to the above theorem.
Corollary A.0.5 If A > 0 and Am >> 0 for some m ∈ N, then all the conclusions of the above theorem hold.
Proof: Thereexistsµ >0suchthatAmy =µ y fory >>0byTheoremA.0.4and 0 0 0 0 0 µ =sup{µ:Amx≥µx for some x∈K}.
0 Let λm =µ .
Then 0 0 ( ) (A−λ I) Am−1+λ Am−2+···+λm−1I y =(Am−λmI)y =0 0 0 0 0 0 0 ( ) and so letting x ≡ Am−1+λ Am−2+···+λm−1I y , it follows x >> 0 and Ax = 0 0 0 0 0 0 λ x .
0 0 Suppose now that Ax = µx for x̸=0 and µ ̸= λ .
Suppose |µ| ≥ λ .
Multiplying both 0 0 sides by A, it follows Amx=µmx and |µm|=|µ|m ≥λm =µ and so from Theorem A.0.4, 0 0 since |µm|≥µ , and µm is an eigenvalue of Am, it follows that µm =µ .
But by Theorem 0 0 A.0.4 again, this implies x = cy for some scalar, c and hence Ay = µy .
Since y >> 0, 0 0 0 0 it follows µ≥0 and so µ=λ , a contradiction.
Therefore, |µ|<λ .
0 0 Finally, ifAx=λ x, thenAmx=λmxandsox=cy forsomescalar, c.Consequently, 0 0 0 ( ) ( ) Am−1+λ Am−2+···+λm−1I x = c Am−1+λ Am−2+···+λm−1I y 0 0 0 0 0 = cx .
0 Hence mλm−1x=cx 0 0 which shows the dimension of the eigenspace for λ is one.
(cid:4) 0 Thefollowingcorollaryisanextremelyinterestingconvergenceresultinvolvingthepow- ers of positive matrices.
Corollary A.0.6 Let A > 0 and Am >> 0 for som(cid:12)e(cid:12)(m ∈) N. Th(cid:12)(cid:12)en for λ0 given in (1.1), (cid:12)(cid:12) m (cid:12)(cid:12) there exists a rank one matrix P such that limm→∞(cid:12)(cid:12) λA0 −P(cid:12)(cid:12)=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation406 POSITIVE MATRICES Proof:ConsideringAT,andthefactthatAandAT havethesameeigenvalues,Corollary A.0.5 implies the existence of a vector, v>>0 such that ATv=λ v. 0 Alsoletx denotethevectorsuchthatAx =λ x withx >>0.
FirstnotethatxTv>0 0 0 0 0 0 0 because both these vectors have all entries positive.
Therefore, v may be scaled such that vTx =xTv=1.
(1.2) 0 0 Deﬁne P ≡x vT. 0 Thanks to (1.2), ( ) ( ) A A A P =x vT =P, P =x vT =x vT =P, (1.3) λ 0 λ 0 λ 0 0 0 0 and P2 =x vTx vT =vTx =P.
(1.4) 0 0 0 Therefore, ( ) ( ) ( ) A 2 A 2 A −P = −2 P +P2 λ λ λ 0 0 0 ( ) A 2 = −P.
λ 0 Continuing this way, using (1.3) repeatedly, it follows (( ) ) ( ) A m A m −P = −P.
(1.5) λ λ 0 0 ( ) The eigenvalues of A −P are of interest because it is powers of this matrix which λ0 ( ) m determine the convergence of A to P. Therefore, let µ be a nonzero eigenvalue of this λ0 matrix.
Thus (( ) ) A −P x=µx (1.6) λ 0 for x̸=0,andµ̸=0.
ApplyingP tobothsidesandusingthesecondformulaof(1.3)yields ( ( ) ) A 0=(P −P)x= P −P2 x=µPx.
λ 0 But since Px=0, it follows from (1.6) that Ax=λ µx 0 which implies λ µ is an eigenvalue of A.
Therefore, by Corollary A.0.5 it follows that either 0 λ µ=λ in which case µ=1, or λ |µ|<λ which implies |µ|<1.
But if µ=1, then x is 0 0 0 0 a multiple of x and (1.6) would yield 0 (( ) ) A −P x =x λ 0 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation407 whichsaysx −x vTx =x andsoby(1.2),x =0contrarytothepropertythatx >>0.
0 0 0 0 0 0 T(he)refore, |µ| < 1 and so this has shown that the absolute values of all eigenvalues of A −P are less than 1.
By Gelfand’s theorem, Theorem 14.3.3, it follows λ0 (cid:12)(cid:12)(( ) ) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A −P m(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1/m <r <1 λ 0 whenever m is large enough.
Now by (1.5) this yields (cid:12)(cid:12)( ) (cid:12)(cid:12) (cid:12)(cid:12)(( ) ) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A m−P(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A −P m(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤rm λ λ 0 0 whenever m is large enough.
It follows (cid:12)(cid:12)( ) (cid:12)(cid:12) lim (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A m−P(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=0 m→∞ λ0 as claimed.
What about the case when A>0 but maybe it is not the case that A>>0?
As before, K ≡{x≥0 such that ||x|| =1}.
1 Now deﬁne S ≡{λ:Ax≥λx for some x∈K} 1 and λ ≡sup(S ) (1.7) 0 1 Theorem A.0.7 Let A > 0 and let λ be deﬁned in (1.7).
Then there exists x > 0 such 0 0 that Ax =λ x .
0 0 0 Proof: LetE consistofthematrixwhichhasaoneineveryentry.
ThenfromTheorem A.0.4 it follows there exists x >>0 , ||x || =1, such that (A+δE)x =λ x where δ δ 1 δ 0δ δ λ ≡sup{λ:(A+δE)x≥λx for some x∈K}.
0δ Now if α<δ {λ:(A+αE)x≥(cid:21)x for some x∈K}⊆ {λ:(A+δE)x≥(cid:21)x for some x∈K} and so λ ≥λ because λ is the sup of the second set and λ is the sup of the ﬁrst.
It 0δ 0α 0δ 0α followsthelimit,λ1 ≡limδ→0+λ0δ exists.
Takingasubsequenceandusingthecompactness of K, there exists a subsequence, still denoted by δ such that as δ → 0, x → x ∈ K. δ Therefore, Ax=λ x 1 and so, in particular, Ax≥λ x and so λ ≤λ .
But also, if λ≤λ , 1 1 0 0 λx≤Ax<(A+δE)x showing that λ ≥λ for all such λ.
But then λ ≥λ also.
Hence λ ≥λ , showing these 0δ 0δ 0 1 0 two numbers are the same.
Hence Ax=λ x.
(cid:4) 0 If Am >> 0 for some m and A > 0, it follows that the dimension of the eigenspace for λ is one and that the absolute value of every other eigenvalue of A is less than λ .
If it is 0 0 only assumed that A > 0, not necessarily >> 0, this is no longer true.
However, there is something which is very interesting which can be said.
First here is an interesting lemma.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation408 POSITIVE MATRICES Lemma A.0.8 Let M be a matrix of the form ( ) A 0 M = B C or ( ) A B M = 0 C where A is an r × r matrix and C is an (n−r) × (n−r) matrix.
Then det(M) = det(A)det(B) and σ(M)=σ(A)∪σ(C).
Proof: To verify the claim about the determinants, note ( ) ( )( ) A 0 A 0 I 0 = B C 0 I B C Therefore, ( ) ( ) ( ) A 0 A 0 I 0 det =det det .
B C 0 I B C But it is clear from the method of Laplace expansion that ( ) A 0 det =detA 0 I and from the multilinear properties of the determinant and row operations that ( ) ( ) I 0 I 0 det =det =detC.
B C 0 C The case where M is upper block triangular is similar.
This immediately implies σ(M)=σ(A)∪σ(C).
Theorem A.0.9 Let A > 0 and let λ be given in (1.7).
If λ is an eigenvalue for A such 0 that |λ|=λ , then λ/λ is a root of unity.
Thus (λ/λ )m =1 for some m∈N.
0 0 0 Proof: Applying Theorem A.0.7 to AT, there exists v>0 such that ATv = λ v. In 0 theﬁrstpartoftheargumentitisassumedv>>0.NowsupposeAx=λx,x̸=0andthat |λ|=λ .
Then 0 A|x|≥|λ||x|=λ |x| 0 and it follows that if A|x|>|λ||x|, then since v>>0, ( ) λ (v,|x|)<(v,A|x|)= ATv,|x| =λ (v,|x|), 0 0 a contradiction.
Therefore, A|x|=λ |x|.
(1.8) 0 It follows that (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∑ (cid:12) ∑ (cid:12)(cid:12) Aijxj(cid:12)(cid:12)=λ0|xi|= Aij|xj| (cid:12) (cid:12) j j and so the complex numbers, A x , A x ij j ik k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation409 musthavethesameargumentforeveryk,j becauseequalityholdsinthetriangleinequality.
Therefore, there exists a complex number, µ such that i A x =µ A |x | (1.9) ij j i ij j and so, letting r ∈N, A x µr =µ A |x |µr.
ij j j i ij j j Summing on j yields ∑ ∑ A x µr =µ A |x |µr.
(1.10) ij j j i ij j j j j Also, summing (1.9) on j and using that λ is an eigenvalue for x, it follows from (1.8) that ∑ ∑ λx = A x =µ A |x |=µ λ |x |.
(1.11) i ij j i ij j i 0 i j j From (1.10) and (1.11), ∑ ∑ A x µr = µ A |x |µr ij j j i ij j j j j ∑ szee}(1|.1{1) = µ A µ |x | µr−1 i ij j j j j ( ) ∑ λ = µ A x µr−1 i ij λ j j 0 (j ) ∑ λ = µ A x µr−1 i λ ij j j 0 j Now from (1.10) with r replaced by r−1, this equals ( ) ( ) ∑ ∑ λ λ µ2 A |x |µr−1 = µ2 A µ |x |µr−2 i λ ij j j i λ ij j j j 0 0 j j ( ) λ 2∑ = µ2 A x µr−2.
i λ ij j j 0 j Continuing this way, ( ) ∑ λ k∑ A x µr =µk A x µr−k ij j j i λ ij j j 0 j j and eventually, this shows ( ) ∑ λ r∑ A x µr = µr A x ij j j i λ ij j 0 j ( ) j λ r = λ(x µr) λ i i 0 ( ) ( ) r+1 and this says λ is an eigenvalue for A with the eigenvector being λ0 λ0 (x µr,··· ,x µr)T .
1 1 n n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation410 POSITIVE MATRICES ( ) ( ) ( ) 2 3 4 Now recall that r ∈N was arbitrary and so this has shown that λ , λ , λ ,··· ( ) λ0 λ0 λ0 are each eigenvalues of A which has only ﬁnitely many and hence this sequence must ( ) λ0 repeat.
Therefore, λ is a root of unity as claimed.
This proves the theorem in the case λ0 that v>>0.
Now it is necessary to consider the case where v>0 but it is not the case that v>>0.
Then in this case, there exists a permutation matrix P such that   v 1  .
  .
  .
 ( )   Pv= v0r ≡ u0 ≡v1  .
  .
 .
0 Then λ v=ATv=ATPv .
0 1 Therefore, λ v =PATPv =Gv 0 1 1 1 Now P2 =I because it is a permutation matrix.
Therefore, the matrix G≡PATP and A are similar.
Consequently, they have the same eigenvalues and it suﬃces from now on to consider the matrix G rather than A.
Then ( ) ( )( ) u M M u λ = 1 2 0 0 M M 0 3 4 where M is r×r and M is (n−r)×(n−r).
It follows from block multiplication and the 1 4 assumption that A and hence G are >0 that ( ) A′ B G= .
0 C Now let λ be an eigenvalue of G such that |λ| = λ .
Then from Lemma A.0.8, either 0 λ ∈ σ(A′) or λ ∈ σ(C).
Suppose without loss of generality that λ ∈ σ(A′).
Since A′ > 0 it has a largest positive eigenvalue λ′ which is obtained from (1.7).
Thus λ′ ≤ λ but λ 0 0 0 being an eigenvalue of A′, has its absolute value bounded by λ′ and so λ =|λ|≤λ′ ≤λ 0 0 0 0 showing that λ ∈σ(A′).
Now if there exists v>>0 such that A′Tv=λ v, then the ﬁrst 0 0 part of this proof applies to the matrix A and so (λ/λ ) is a root of unity.
If such a vector, 0 v does not exist, then let A′ play the role of A in the above argument and reduce to the consideration of ( ) A′′ B′ G′ ≡ 0 C′ where G′ is similar to A′ and λ,λ ∈ σ(A′′).
Stop if A′′Tv = λ v for some v >> 0.
0 0 Otherwise,decomposeA′′ similartotheaboveandaddanotherprime.
Continuingthisway you must eventually obtain the situation where (A′···′)T v=λ v for some v>>0.
Indeed, 0 this happens no later than when A′···′ is a 1×1 matrix.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationFunctions Of Matrices The existence of the Jordan form also makes it possible to deﬁne various functions of ma- trices.
Suppose ∑∞ f(λ)= a λn (2.1) n n=0 ∑ for all |λ| < R. There is a formula for f(A) ≡ ∞ a An which makes sense whenever n=0 n ρ(A)<R.Thusyoucanspeakofsin(A)oreA forAann×nmatrix.
Tobeginwith,deﬁne ∑P f (λ)≡ a λn P n n=0 so for k <P ∑P f(k)(λ) = a n···(n−k+1)λn−k P n n=k ( ) ∑P n = a k!λn−k.
(2.2) n k n=k Thus ( ) f(k)(λ) ∑P n P = a λn−k (2.3) k!
n k n=k To begin with consider f(J (λ)) where J (λ) is an m×m Jordan block.
Thus J (λ)= m m m D+N where Nm =0 and N commutes with D. Therefore, letting P >m ( ) ∑P ∑P ∑n n a J (λ)n = a Dn−kNk n m n k n=0 n=0 k=0 ( ) ∑P ∑P n = a Dn−kNk n k k=0n=k ( ) m∑−1 ∑P n = Nk a Dn−k.
(2.4) n k k=0 n=k From (2.3) this equals ( ) m∑−1 f(k)(λ) f(k)(λ) Nkdiag P ,··· , P (2.5) k!
k!
k=0 411 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation412 FUNCTIONS OF MATRICES wherefork =0,··· ,m−1,deﬁnediagk(a1,··· ,am−k)them×mmatrixwhichequalszero everywhere except on the kth super diagonal where this diagonal is ﬁlled with the numbers, {a1,··· ,am−k} from the upper left to the lower right.
With no subscript, it is just the diagonal matrices having the indicated entries.
Thus in 4×4 matrices, diag (1,2) would 2 be the matrix   0 0 1 0    0 0 0 2   .
0 0 0 0 0 0 0 0 Then from (2.5) and (2.2), ( ) ∑P m∑−1 f(k)(λ) f(k)(λ) a J (λ)n = diag P ,··· , P .
n m k k!
k!
n=0 k=0 ∑ Therefore, P a J (λ)n = n=0 n m    fP (λ) fP′1(!λ) fP(22)!
(λ) ··· fP((mm−−11)()λ!)
  fP (λ) fP′1(!λ) ... ...   fP (λ) ...
fP(22)!
(λ)  (2.6)  ... fP′(λ)  1!
0 f (λ) P Now let A be an n×n matrix with ρ(A) < R where R is given above.
Then the Jordan form of A is of the form   J 0 1    J2  J = ...  (2.7) 0 J r where J =J (λ ) is an m ×m Jordan block and A=S−1JS.
Then, letting P >m k mk k k k k for all k, ∑P ∑P a An =S−1 a JnS, n n n=0 n=0 and because of block multiplication of matrices,  ∑  P a Jn 0  n=0 n 1  n∑P=0anJn = ... ... ∑  0 P a Jn n=0 n r ∑ and from (2.6) P a Jn converges as P →∞ to the m ×m matrix n=0 n k k k   f(λ ) f′(λk) f(2)(λk) ··· f(m−1)(λk)  0k f(1λ!k) f′(21λ!
!k) ... (mk...−1)!
  0... 0 f(.λ..k) ...... ff(2′()2λ(!λkk))  (2.8) 1!
0 0 ··· 0 f(λ ) k Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation413 There is no convergence problem because |λ| < R for all λ ∈ σ(A).
This has proved the following theorem.
Theorem B.0.10 Let f be given by (2.1) and suppose ρ(A)<R where R is the radius of convergence of the power series in (2.1).
Then the series, ∑∞ a An (2.9) n k=0 converges in the space L(Fn,Fn) with respect to any of the norms on this space and further- more,  ∑  ∞ a Jn 0  n=0 n 1  k∑∞=0anAn =S−1 ... ... ∑ S 0 ∞ a Jn n=0 n r ∑ where ∞ a Jn is an m ×m matrix of the form given in (2.8) where A=S−1JS and n=0 n k k k the Jordan form of A, J is given by (2.7).
Therefore, you can deﬁne f(A) by the series in (2.9).
Here is a simple example.
  4 1 −1 1  1 1 0 −1  Example B.0.11 Find sin(A) where A= 0 −1 1 −1 .
−1 2 1 4 In this case, the Jordan canonical form of the matrix is not too hard to ﬁnd.
    4 1 −1 1 2 0 −2 −1  10 −11 10 −−11 = 10 −04 −−22 −11 · −1 2 1 4 −1 4 4 2    4 0 0 0 1 1 0 1  0 2 1 0  21 −23 0 −21   0 0 2 1  08 18 −1 18 .
4 4 4 0 0 0 2 0 1 1 1 2 2 2 Then from the above theorem sin(J) is given by     4 0 0 0 sin4 0 0 0  0 2 1 0   0 sin2 cos2 −sin2  sin = 2 .
0 0 2 1 0 0 sin2 cos2 0 0 0 2 0 0 0 sin2 Therefore, sin(A)=     2 0 −2 −1 sin4 0 0 0 1 1 0 1  1 −4 −2 −1  0 sin2 cos2 −sin2  21 −23 0 −21   0 0 −2 1  0 0 sin2 co2s2  08 18 −1 18 =M 4 4 4 −1 4 4 2 0 0 0 sin2 0 1 1 1 2 2 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation414 FUNCTIONS OF MATRICES where the columns of M are as follows from left to right,       sin4 sin4−sin2−cos2 −cos2  1sin4− 1sin2   1sin4+ 3sin2−2cos2   sin2   2 0 2 , 2 −2cos2 , sin2−cos2  −1sin4+ 1sin2 −1sin4− 1sin2+3cos2 cos2−sin2  2 2 2 2 sin4−sin2−cos2  1sin4+ 1sin2−2cos2   2 −2cos2 .
−1sin4+ 1sin2+3cos2 2 2 Perhaps this isn’t the ﬁrst thing you would think of.
Of course the ability to get this nice closedformdescriptionofsin(A)wasdependentonbeingabletoﬁndtheJordanformalong with a similarity transformation which will yield the Jordan form.
The following corollary is known as the spectral mapping theorem.
Corollary B.0.12 Let A be an n×n matrix and let ρ(A)<R where for |λ|<R, ∑∞ f(λ)= a λn.
n n=0 Then f(A) is also an n×n matrix and furthermore, σ(f(A))=f(σ(A)).
Thus the eigen- values of f(A) are exactly the numbers f(λ) where λ is an eigenvalue of A.
Furthermore, the algebraic multiplicity of f(λ) coincides with the algebraic multiplicity of λ.
All of these things can be generalized to linear transformations deﬁned on inﬁnite di- mensional spaces and when this is done the main tool is the Dunford integral along with the methods of complex analysis.
It is good to see it done for ﬁnite dimensional situations ﬁrst because it gives an idea of what is possible.
Actually, some of the most interesting functions in applications do not come in the above form as a power series expanded about 0.
One example of this situation has already been encountered in the proof of the right polar decomposition with the square root of an Hermitian transformation which had all nonnegative eigenvalues.
Another example is that of taking the positive part of an Hermi- tian matrix.
This is important in some physical models where something may depend on the positive part of the strain which is a symmetric real matrix.
Obviously there is no way to consider this as a power series expanded about 0 because the function f(r)=r+ is not even diﬀerentiable at 0.
Therefore, a totally diﬀerent approach must be considered.
First the notion of a positive part is deﬁned.
Deﬁnition B.0.13 Let A be an Hermitian matrix.
Thus it suﬃces to consider A as an element of L(Fn,Fn) according to the usual notion of matrix multiplication.
Then there exists an orthonormal basis of eigenvectors, {u ,··· ,u } such that 1 n ∑n A= λ u ⊗u , j j j j=1 for λ the eigenvalues of A, all real.
Deﬁne j ∑n A+ ≡ λ+u ⊗u j j j j=1 where λ+ ≡ |λ|+λ.
2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation415 This gives us a nice deﬁnition of what is meant but it turns out to be very important in the applications to determine how this function depends on the choice of symmetric matrix A.
The following addresses this question.
Theorem B.0.14 If A,B be Hermitian matrices, then for |·| the Frobenius norm, (cid:12) (cid:12) (cid:12)A+−B+(cid:12)≤|A−B|.
∑ ∑ Proof: Let A = λ v ⊗v and let B = µ w ⊗w where {v } and {w } are i i i i j j j j i j orthonormal bases of eigenvectors.
  2 (cid:12) (cid:12) ∑ ∑ (cid:12)A+−B+(cid:12)2 =trace λ+v ⊗v − µ+w ⊗w  = i i i j j j i j  ∑( ) ∑( ) trace λ+ 2v ⊗v + µ+ 2w ⊗w i i i j j j i j  ∑ ∑ − λ+µ+(w ,v )v ⊗w − λ+µ+(v ,w )w ⊗v  i j j i i j i j i j j i i,j i,j Since the trace of v ⊗w is (v ,w ), a fact which follows from (v ,w ) being the only i j i j i j possibly nonzero eigenvalue, ∑( ) ∑( ) ∑ = λ+ 2+ µ+ 2−2 λ+µ+|(v ,w )|2.
(2.10) i j i j i j i j i,j Since these are orthonormal bases, ∑ ∑ |(v ,w )|2 =1= |(v ,w )|2 i j i j i j and so (2.10) equals ∑∑(( ) ( ) ) = λ+ 2+ µ+ 2−2λ+µ+ |(v ,w )|2.
i j i j i j i j Similarly, ∑∑( ( ) ) |A−B|2 = (λ )2+ µ 2−2λ µ |(v ,w )|2.
i j i j i j i j ( ) ( ) ( ) Now it is easy to check that (λ )2+ µ 2−2λ µ ≥ λ+ 2+ µ+ 2−2λ+µ+.
(cid:4) i j i j i j i j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation416 FUNCTIONS OF MATRICES Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationApplications To Diﬀerential Equations C.1 Theory Of Ordinary Diﬀerential Equations HereIwillpresentfundamentalexistenceanduniquenesstheoremsforinitialvalueproblems for the diﬀerential equation, ′ x =f(t,x).
Suppose that f :[a,b]×Rn →Rn satisﬁes the following two conditions.
|f(t,x)−f(t,x )|≤K|x−x |, (3.1) 1 1 f is continuous.
(3.2) The ﬁrst of these conditions is known as a Lipschitz condition.
Lemma C.1.1 Suppose x:[a,b]→Rn is a continuous function and c∈[a,b].
Then x is a solution to the initial value problem, ′ x =f(t,x), x(c)=x (3.3) 0 if and only if x is a solution to the integral equation, ∫ t x(t)=x + f(s,x(s))ds.
(3.4) 0 c Proof: If x solves (3.4), then since f is continuous, we may apply the fundamental theorem of calculus to diﬀerentiate both sides and obtain x′(t) = f(t,x(t)).
Also, letting t = c on both sides, gives x(c) = x .
Conversely, if x is a solution of the initial value 0 problem, we may integrate both sides from c to t to see that x solves (3.4).
(cid:4) Theorem C.1.2 Let f satisfy (3.1) and (3.2).
Then there exists a unique solution to the initial value problem, (3.3) on the interval [a,b].
{ } Proof: Let||x|| ≡sup eλt|x(t)|:t∈[a,b] .Thenthisnormisequivalenttotheusual λ normonBC([a,b],Fn)describedinExample14.6.2.
Thismeansthatfor||·||thenormgiven there, there exist constants δ and ∆ such that ||x|| δ ≤||x||≤∆||x|| λ 417 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation418 APPLICATIONS TO DIFFERENTIAL EQUATIONS for all x∈BC([a,b],Fn).
In fact, you can take δ ≡eλa and ∆≡eλb in case λ>0 with the two reversed in case λ < 0.
Thus BC([a,b],Fn) is a Banach space with this norm, ||·|| .
λ Then let F :BC([a,b],Fn)→BC([a,b],Fn) be deﬁned by ∫ t Fx(t)≡x + f(s,x(s))ds.
0 c Let λ<0.
It follows (cid:12) ∫ (cid:12) (cid:12) t (cid:12) eλt|Fx(t)−Fy(t)| ≤ (cid:12)(cid:12)eλt |f(s,x(s))−f(s,y(s))|ds(cid:12)(cid:12) (cid:12)∫ c (cid:12) (cid:12) t (cid:12) ≤ (cid:12)(cid:12) Keλ(t−s)|x(s)−y(s)|eλsds(cid:12)(cid:12) c ∫ t K ≤||x−y|| Keλ(t−s)ds≤||x−y|| λ λ |λ| a and therefore, K ||Fx−Fy|| ≤||x−y|| .
λ |λ| If |λ| is chosen larger than K, this implies F is a contraction mapping on BC([a,b],Fn).
Therefore, thereexistsauniqueﬁxedpoint.
WithLemmaC.1.1thisprovesthetheorem.
(cid:4) C.2 Linear Systems As an example of the above theorem, consider for t∈[a,b] the system ′ x =A(t)x(t)+g(t), x(c)=x (3.5) 0 where A(t) is an n × n matrix whose entries are continuous functions of t,(a (t)) and ij g(t) is a vector whose components are continuous functions of t satisﬁes the conditions of Theorem C.1.2 with f(t,x) = A(t)x + g(t).
To see this, let x=(x ,··· ,x )T and 1 n x =(x ,···x )T .
Then letting M =max{|a (t)|:t∈[a,b],i,j ≤n}, 1 11 1n ij |f(t,x)−f(t,x )|=|A(t)(x−x )| 1 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12)    (cid:12) (cid:12) (cid:12) (cid:12)2 1/2(cid:12) (cid:12) 2 1/2(cid:12) =(cid:12)(cid:12)(cid:12)∑n (cid:12)(cid:12)(cid:12)∑n aij(t)(xj −x1j)(cid:12)(cid:12)(cid:12)  (cid:12)(cid:12)(cid:12)≤M(cid:12)(cid:12)(cid:12)∑n ∑n |xj −x1j|  (cid:12)(cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i=1 j=1 (cid:12) (cid:12) i=1 j=1 (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12)   (cid:12)(cid:12) ∑n ∑n 1/2(cid:12)(cid:12) ∑n 1/2 ≤M(cid:12) n |x −x |2 (cid:12)=Mn |x −x |2 =Mn|x−x |.
(cid:12) j 1j (cid:12) j 1j 1 (cid:12) (cid:12) i=1 j=1 j=1 Therefore, let K =Mn.
This proves Theorem C.2.1 Let A(t) be a continuous n×n matrix and let g(t) be a continuous vector for t ∈ [a,b] and let c ∈ [a,b] and x ∈ Fn.
Then there exists a unique solution to (3.5) 0 valid for t∈[a,b].
This includes more examples of linear equations than are typically encountered in an entire diﬀerential equations course.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.3.
LOCAL SOLUTIONS 419 C.3 Local Solutions Lemma C.3.1 Let D(x ,r) ≡ {x∈Fn :|x−x |≤r} and suppose U is an open set con- 0 0 tainingD(x0,r)suchthatf :U →Fn isC1(U).
(Recallthismeansallpartialderiv(cid:12)ativeso(cid:12)f (cid:12) (cid:12) f exist and are continuous.)
Then for K =Mn, where M denotes the maximum of (cid:12) ∂f (z)(cid:12) ∂xi for z∈D(x ,r), it follows that for all x,y∈D(x ,r), 0 0 |f(x)−f(y)|≤K|x−y|.
Proof: Let x,y ∈ D(x ,r) and consider the line segment joining these two points, 0 x+t(y−x) for t∈[0,1].
Letting h(t)=f(x+t(y−x)) for t∈[0,1], then ∫ 1 f(y)−f(x)=h(1)−h(0)= h′(t)dt.
0 Also, by the chain rule, ∑n ∂f h′(t)= (x+t(y−x))(y −x ).
∂x i i i i=1 Therefore, |f(y)−f(x)|= (cid:12) (cid:12) (cid:12)(cid:12)∫ 1∑n ∂f (cid:12)(cid:12) (cid:12) (x+t(y−x))(y −x )dt(cid:12) (cid:12) ∂x i i (cid:12) ∫ 0 i=1(cid:12) i (cid:12) ≤ 1∑n (cid:12)(cid:12)(cid:12)∂∂xf (x+t(y−x))(cid:12)(cid:12)(cid:12)|yi−xi|dt 0 i=1 i ∑n ≤ M |y −x |≤Mn|x−y|.
(cid:4) i i i=1 Now consider the map, P which maps all of Rn to D(x ,r) given as follows.
For 0 x∈D(x ,r),Px=x.Forx∈/D(x ,r),PxwillbetheclosestpointinD(x ,r)tox.Such 0 0 0 aclosestpointexistsbecauseD(x ,r)isaclosedandboundedset.
Takingf(y)≡|y−x|, 0 it follows f is a continuous function deﬁned on D(x ,r) which must achieve its minimum 0 value by the extreme value theorem from calculus.
x (cid:18) Px (cid:27) 9 z D(x ,r) 0 x 0 Lemma C.3.2 For any pair of points, x,y∈Fn, |Px−Py|≤|x−y|.
Proof: The above picture suggests the geometry of what is going on.
Letting z ∈ D(x ,r), it follows that for all t∈[0,1], 0 |x−Px|2 ≤|x−(Px+t(z−Px))|2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation420 APPLICATIONS TO DIFFERENTIAL EQUATIONS =|x−Px|2+2tRe((x−Px)·(Px−z))+t2|z−Px|2 Hence 2tRe((x−Px)·(Px−z))+t2|z−Px|2 ≥0 and this can only happen if Re((x−Px)·(Px−z))≥0.
Therefore, Re((x−Px)·(Px−Py)) ≥ 0 Re((y−Py)·(Py−Px)) ≥ 0 and so Re(x−Px−(y−Py))·(Px−Py)≥0 which implies Re(x−y)·(Px−Py)≥|Px−Py|2 Then using the Cauchy Schwarz inequality it follows |x−y|≥|Px−Py|.
(cid:4) With this here is the local existence and uniqueness theorem.
Theorem C.3.3 Let [a,b] be a closed interval and let U be an open subset of Fn.
Let f :[a,b]×U →Fn be continuous and suppose that for each t∈[a,b], the map x→ ∂f (t,x) is continuous.
Also let x ∈ U and c ∈ [a,b].
Then there exists an interval, I ⊆ [a∂,xbi] such 0 that c∈I and there exists a unique solution to the initial value problem, ′ x =f(t,x), x(c)=x (3.6) 0 valid for t∈I.
Proof: Consider the following picture.
U x 0 D(x ,r) 0 The large dotted circle represents U and the little solid circle represents D(x ,r) as 0 indicated.
Here r is so small that D(x ,r) is contained in U as shown.
Now let P denote 0 the projection map deﬁned above.
Consider the initial value problem ′ x =f(t,Px), x(c)=x .
(3.7) 0 From Lemma C.3.1 and the continuity of x→ ∂f (t,x), there exists a constant, K such that if x,y ∈ D(x ,r), then |f(t,x)−f(t,y)| ≤∂xKi |x−y| for all t ∈ [a,b].
Therefore, by 0 Lemma C.3.2 |f(t,Px)−f(t,Py)|≤K|Px−Py|≤K|x−y|.
It follows from Theorem C.1.2 that (3.7) has a unique solution valid for t ∈ [a,b].
Since x is continuous, it follows that there exists an interval, I containing c such that for t ∈ I, Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.4.
FIRST ORDER LINEAR SYSTEMS 421 x(t)∈D(x ,r).Therefore,forthesevaluesoft,f(t,Px)=f(t,x)andsothereisaunique 0 solution to (3.6) on I.
(cid:4) Now suppose f has the property that for every R > 0 there exists a constant, K such R that for all x,x ∈B(0,R), 1 |f(t,x)−f(t,x )|≤K |x−x |.
(3.8) 1 R 1 Corollary C.3.4 Let f satisfy (3.8) and suppose also that (t,x) → f(t,x) is continuous.
Suppose now that x is given and there exists an estimate of the form |x(t)| < R for all 0 t∈[0,T) where T ≤∞ on the local solution to ′ x =f(t,x), x(0)=x .
(3.9) 0 Then there exists a unique solution to the initial value problem, (3.9) valid on [0,T).
Proof: Replace f(t,x) with f(t,Px) where P is the projection onto B(0,R).
Then by Theorem C.1.2 there exists a unique solution to the system ′ x =f(t,Px), x(0)=x 0 validon[0,T ]foreveryT <T.Therefore,theabovesystemhasauniquesolutionon[0,T) 1 1 and from the estimate, Px=x.
(cid:4) C.4 First Order Linear Systems Here is a discussion of linear systems of the form ′ x =Ax+f(t) where A is a constant n×n matrix and f is a vector valued function having all entries continuous.
Ofcoursetheexistencetheoryisaveryspecialcaseofthegeneralconsiderations above but I will give a self contained presentation based on elementary ﬁrst order scalar diﬀerential equations and linear algebra.
Deﬁnition C.4.1 Suppose t → M(t) is a matrix valued function of t. Thus M(t) = (mij(t)).
Then deﬁne ( ) M′(t)≡ m′ (t) .
ij In words, the derivative of M(t) is the matrix whose entries consist of the derivatives of the entries of M(t).
Integrals of matrices are deﬁned the same way.
Thus ( ) ∫ ∫ b b M(t)di≡ m (t)dt .
ij a a In words, the integral of M(t) is the matrix obtained by replacing each entry of M(t) by the integral of that entry.
With this deﬁnition, it is easy to prove the following theorem.
Theorem C.4.2 Suppose M(t) and N(t) are matrices for which M(t)N(t) makes sense.
Then if M′(t) and N′(t) both exist, it follows that ′ ′ ′ (M(t)N(t)) =M (t)N(t)+M(t)N (t).
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation422 APPLICATIONS TO DIFFERENTIAL EQUATIONS Proof: ( ) ( ) ( )′ ∑ ′ (M(t)N(t))′ ≡ (M(t)N(t)) = M(t) N(t) ij ij ik kj ∑ k ( ) ′ ′ = (M(t) ) N(t) +M(t) N(t) ik kj ik kj ∑k ( ) ( ) ≡ M(t)′ N(t) +M(t) N(t)′ ik kj ik kj k ≡ (M′(t)N(t)+M(t)N′(t)) (cid:4) ij In the study of diﬀerential equations, one of the most important theorems is Gronwall’s inequality which is next.
Theorem C.4.3 Suppose u(t)≥0 and for all t∈[0,T], ∫ t u(t)≤u + Ku(s)ds.
(3.10) 0 0 where K is some nonnegative constant.
Then u(t)≤u eKt.
(3.11) 0 ∫ t Proof: Let w(t) = u(s)ds.
Then using the fundamental theorem of calculus, (3.10) 0 w(t) satisﬁes the following.
u(t)−Kw(t)=w′(t)−Kw(t)≤u , w(0)=0.
(3.12) 0 Multiply both sides of this inequality by e−Kt and using the product rule and the chain rule, ( ) d e−Kt(w′(t)−Kw(t))= e−Ktw(t) ≤u e−Kt.
dt 0 Integrating this from 0 to t, ∫ ( ) t e−tK −1 e−Ktw(t)≤u e−Ksds=u − .
0 0 K 0 Now multiply through by eKt to obtain ( ) e−tK −1 u u w(t)≤u − eKt =− 0 + 0etK.
0 K K K Therefore, (3.12) implies ( ) u u u(t)≤u +K − 0 + 0etK =u eKt.
0 K K 0 (cid:4) With Gronwall’s inequality, here is a theorem on uniqueness of solutions to the initial value problem, ′ x =Ax+f(t), x(a)=x , (3.13) a in which A is an n×n matrix and f is a continuous function having values in Cn.
Theorem C.4.4 Suppose x and y satisfy (3.13).
Then x(t)=y(t) for all t. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.4.
FIRST ORDER LINEAR SYSTEMS 423 Proof: Let z(t)=x(t+a)−y(t+a).
Then for t≥0, ′ z =Az, z(0)=0.
(3.14) Note that for K =max{|a |}, where A=(a ), ij ij (cid:12) (cid:12) (cid:12) (cid:12) ( ) |(Az,z)|=(cid:12)(cid:12)(cid:12)(cid:12)∑aijzjzi(cid:12)(cid:12)(cid:12)(cid:12)≤K∑|zi||zj|≤K∑ |z2i|2 + |z2j|2 =nK|z|2.
ij ij ij (For x and y real numbers, xy ≤ x2 + y2 because this is equivalent to saying (x−y)2 ≥0.)
2 2 Similarly, |(z,Az)|≤nK|z|2.Thus, |(z,Az)|,|(Az,z)|≤nK|z|2.
(3.15) Now multiplying (3.14) by z and observing that ( ) d |z|2 =(z′,z)+(z,z′)=(Az,z)+(z,Az), dt it follows from (3.15) and the observation that z(0)=0, ∫ t |z(t)|2 ≤ 2nK|z(s)|2ds 0 and so by Gronwall’s inequality, |z(t)|2 =0 for all t≥0.
Thus, x(t)=y(t) for all t≥a.
Now let w(t) = x(a−t)−y(a−t) for t ≥ 0.
Then w′(t) = (−A)w(t) and you can repeat the argument which was just given to conclude that x(t)=y(t) for all t≤a.
(cid:4) Deﬁnition C.4.5 Let A be an n×n matrix.
We say Φ(t) is a fundamental matrix for A if ′ Φ (t)=AΦ(t), Φ(0)=I, (3.16) and Φ(t)−1 exists for all t∈R.
Whyshouldanyonecareaboutafundamentalmatrix?
Thereasonisthatsuchamatrix valued function makes possible a convenient description of the solution of the initial value problem, ′ x =Ax+f(t), x(0)=x , (3.17) 0 on the interval, [0,T].
First consider the special case where n = 1.
This is the ﬁrst order linear diﬀerential equation, ′ r =λr+g, r(0)=r , (3.18) 0 where g is a continuous scalar valued function.
First consider the case where g =0.
Lemma C.4.6 There exists a unique solution to the initial value problem, ′ r =λr, r(0)=1, (3.19) and the solution for λ=a+ib is given by r(t)=eat(cosbt+isinbt).
(3.20) This solution to the initial value problem is denoted as eλt.
(If λ is real, eλt as deﬁned here reducestotheusualexponentialfunctionsothereisnocontradictionbetweenthisandearlier notation seen in Calculus.)
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation424 APPLICATIONS TO DIFFERENTIAL EQUATIONS Proof: From the uniqueness theorem presented above, Theorem C.4.4, applied to the case where n = 1, there can be no more than one solution to the initial value problem, (3.19).
Therefore, it only remains to verify (3.20) is a solution to (3.19).
However, this is an easy calculus exercise.
(cid:4) Note the diﬀerential equation in (3.19) says ( ) d eλt =λeλt.
(3.21) dt With this lemma, it becomes possible to easily solve the case in which g ̸=0.
Theorem C.4.7 There exists a unique solution to (3.18) and this solution is given by the formula, ∫ t r(t)=eλtr +eλt e−λsg(s)ds.
(3.22) 0 0 Proof: By the uniqueness theorem, Theorem C.4.4, there is no more than one solution.
∫ It only remains to verify that (3.22) is a solution.
But r(0) = eλ0r + 0e−λsg(s)ds = r 0 0 0 and so the initial condition is satisﬁed.
Next diﬀerentiate this expression to verify the diﬀerential equation is also satisﬁed.
Using (3.21), the product rule and the fundamental theorem of calculus, ∫ t r′(t)=λeλtr +λeλt e−λsg(s)ds+eλte−λtg(t)=λr(t)+g(t).
(cid:4) 0 0 Now consider the question of ﬁnding a fundamental matrix for A.
When this is done, it will be easy to give a formula for the general solution to (3.17) known as the variation of constants formula, arguably the most important result in diﬀerential equations.
The next theorem gives a formula for the fundamental matrix (3.16).
It is known as Putzer’s method [1],[21].
Theorem C.4.8 Let A be an n×n matrix whose eigenvalues are {λ ,··· ,λ }.
Deﬁne 1 n ∏k P (A)≡ (A−λ I), P (A)≡I, k m 0 m=1 and let the scalar valued functions, r (t) be deﬁned as the solutions to the following initial k value problem         r′ (t) 0 r (0) 0 0 0  r1′ (t)   λ1r1(t)+r0(t)   r1(0)   1   r2′ (t) = λ2r2(t)+r1(t) ,  r2(0) = 0   .
  .
  .
  .
  .
  .
  .
  .
 .
.
.
.
rn′ (t) λnrn(t)+rn−1(t) rn(0) 0 Note the system amounts to a list of single ﬁrst order linear diﬀerential equations.
Now deﬁne n∑−1 Φ(t)≡ r (t)P (A).
k+1 k k=0 Then ′ Φ (t)=AΦ(t), Φ(0)=I.
(3.23) Furthermore, if Φ(t) is a solution to (3.23) for all t, then it follows Φ(t)−1 exists for all t and Φ(t) is the unique fundamental matrix for A. Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.4.
FIRST ORDER LINEAR SYSTEMS 425 Proof: The ﬁrst part of this follows from a computation.
First note that by the Cayley Hamilton theorem, P (A)=0.
Now for the computation: n n∑−1 n∑−1 ′ ′ Φ (t)= r (t)P (A)= (λ r (t)+r (t))P (A)= k+1 k k+1 k+1 k k k=0 k=0 n∑−1 n∑−1 n∑−1 λ r (t)P (A)+ r (t)P (A)= (λ I−A)r (t)P (A)+ k+1 k+1 k k k k+1 k+1 k k=0 k=0 k=0 n∑−1 n∑−1 r (t)P (A)+ Ar (t)P (A) k k k+1 k k=0 k=0 n∑−1 n∑−1 n∑−1 =− r (t)P (A)+ r (t)P (A)+A r (t)P (A).
(3.24) k+1 k+1 k k k+1 k k=0 k=0 k=0 Now using r (t)=0, the ﬁrst term equals 0 ∑n n∑−1 n∑−1 − r (t)P (A)=− r (t)P (A)=− r (t)P (A) k k k k k k k=1 k=1 k=0 and so (3.24) reduces to n∑−1 A r (t)P (A)=AΦ(t).
k+1 k k=0 This shows Φ′(t)=AΦ(t).
That Φ(0)=0 follows from n∑−1 Φ(0)= r (0)P (A)=r (0)P =I.
k+1 k 1 0 k=0 It remains to verify that if (3.23) holds, then Φ(t)−1 exists for all t. To do so, consider v̸=0 and suppose for some t , Φ(t )v=0.
Let x(t)≡Φ(t +t)v. Then 0 0 0 ′ x (t)=AΦ(t +t)v=Ax(t), x(0)=Φ(t )v=0.
0 0 But also z(t)≡0 also satisﬁes ′ z (t)=Az(t), z(0)=0, andsobythetheoremonuniqueness,itmustbethecasethatz(t)=x(t)forallt,showing that Φ(t+t )v=0 for all t, and in particular for t=−t .
Therefore, 0 0 Φ(−t +t )v=Iv=0 0 0 and so v=0, a contradiction.
It follows that Φ(t) must be one to one for all t and so, Φ(t)−1 exists for all t. It only remains to verify the solution to (3.23) is unique.
Suppose Ψ is another funda- mental matrix solving (3.23).
Then letting v be an arbitrary vector, z(t)≡Φ(t)v, y(t)≡Ψ(t)v both solve the initial value problem, ′ x =Ax, x(0)=v, Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation426 APPLICATIONS TO DIFFERENTIAL EQUATIONS and so by the uniqueness theorem, z(t) = y(t) for all t showing that Φ(t)v = Ψ(t)v for all t. Since v is arbitrary, this shows that Φ(t)=Ψ(t) for every t. (cid:4) It is useful to consider the diﬀerential equations for the r for k ≥ 1.
As noted above, k r0(t)=0 and r1(t)=eλ1t.
′ r =λ r +r , r (0)=0.
k+1 k+1 k+1 k k+1 Thus ∫ t rk+1(t)= eλk+1(t−s)rk(s)ds.
0 Therefore, ∫ r (t)= teλ2(t−s)eλ1sds= eλ1t−eλ2t 2 −λ +λ 0 2 1 assuming λ ̸=λ .
1 2 Sometimes people deﬁne a fundamental matrix to be a matrix Φ(t) such that Φ′(t) = AΦ(t) and det(Φ(t)) ̸= 0 for all t. Thus this avoids the initial condition, Φ(0) = I.
The next proposition has to do with this situation.
Proposition C.4.9 Suppose A is an n×n matrix and suppose Φ(t) is an n×n matrix for each t∈R with the property that ′ Φ (t)=AΦ(t).
(3.25) Then either Φ(t)−1 exists for all t∈R or Φ(t)−1 fails to exist for all t∈R.
Proof: SupposeΦ(0)−1existsand(3.25)holds.
LetΨ(t)≡Φ(t)Φ(0)−1.ThenΨ(0)= I and Ψ′(t)=Φ′(t)Φ(0)−1 =AΦ(t)Φ(0)−1 =AΨ(t) so by Theorem C.4.8, Ψ(t)−1 exists for all t. Therefore, Φ(t)−1 also exists for all t. Next suppose Φ(0)−1 does not exist.
I need to show Φ(t)−1 does not exist for any t. Suppose then that Φ(t )−1 does exist.
Then letΨ(t) ≡ Φ(t +t)Φ(t )−1.
Then Ψ(0) = 0 0 0 I and Ψ′ = AΨ so by Theorem C.4.8 it follows Ψ(t)−1 exists for all t and so for all t,Φ(t+t )−1 must also exist, even for t=−t which implies Φ(0)−1 exists after all.
(cid:4) 0 0 TheconclusionofthispropositionisusuallyreferredtoastheWronskianalternativeand anotherwaytosayitisthatif(3.25)holds, theneitherdet(Φ(t))=0foralltordet(Φ(t)) is never equal to 0.
The Wronskian is the usual name of the function, t→det(Φ(t)).
The following theorem gives the variation of constants formula,.
Theorem C.4.10 Let f be continuous on [0,T] and let A be an n×n matrix and x a 0 vector in Cn.
Then there exists a unique solution to (3.17), x, given by the variation of constants formula, ∫ t x(t)=Φ(t)x +Φ(t) Φ(s)−1f(s)ds (3.26) 0 0 for Φ(t) the fundamental matrix for A.
Also, Φ(t)−1 = Φ(−t) and Φ(t+s) = Φ(t)Φ(s) for all t,s and the above variation of constants formula can also be written as ∫ t x(t) = Φ(t)x + Φ(t−s)f(s)ds (3.27) 0 ∫0 t = Φ(t)x + Φ(s)f(t−s)ds (3.28) 0 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.4.
FIRST ORDER LINEAR SYSTEMS 427 Proof: From the uniqueness theorem there is at most one solution to (3.17).
Therefore, if (3.26) solves (3.17), the theorem is proved.
The veriﬁcation that the given formula works is identical with the veriﬁcation that the scalar formula given in Theorem C.4.7 solves the initialvalueproblemgiventhere.
Φ(s)−1iscontinuousbecauseoftheformulafortheinverse of a matrix in terms of the transpose of the cofactor matrix.
Therefore, the integrand in (3.26)iscontinuousandthefundamentaltheoremofcalculusapplies.
Toverifytheformula for the inverse, ﬁx s and consider x(t)=Φ(s+t)v, and y(t)=Φ(t)Φ(s)v. Then ′ x (t)=AΦ(t+s)v=Ax(t), x(0)=Φ(s)v ′ y (t)=AΦ(t)Φ(s)v=Ay(t), y(0)=Φ(s)v. By the uniqueness theorem, x(t) = y(t) for all t. Since s and v are arbitrary, this shows Φ(t+s) = Φ(t)Φ(s) for all t,s.
Letting s = −t and using Φ(0) = I veriﬁes Φ(t)−1 = Φ(−t).
Next, note that this also implies Φ(t−s)Φ(s)=Φ(t) and so Φ(t−s)=Φ(t)Φ(s)−1.
Therefore, this yields (3.27) and then (3.28)follows from changing the variable.
(cid:4) If Φ′ =AΦ and Φ(t)−1 exists for all t, you should verify that the solution to the initial value problem ′ x =Ax+f, x(t )=x 0 0 is given by ∫ t x(t)=Φ(t−t )x + Φ(t−s)f(s)ds.
0 0 t0 Theorem C.4.10 is general enough to include all constant coeﬃcient linear diﬀerential equations or any order.
Thus it includes as a special case the main topics of an entire elementary diﬀerential equations class.
This is illustrated in the following example.
One canreduceanarbitrarylineardiﬀerentialequationtoaﬁrstordersystemandthenapplythe above theory to solve the problem.
The next example is a diﬀerential equation of damped vibration.
Example C.4.11 The diﬀerential equation is y′′+2y′+2y =cost and initial conditions, y(0)=1 and y′(0)=0.
To solve this equation, let x =y and x =x′ =y′.
Then, writing this in terms of these 1 2 1 new variables, yields the following system.
x′ +2x +2x =cost 2 2 1 x′ =x 1 2 This system can be written in the above form as ( ) ( ) ( ) ( )( ) ( ) ′ x x 0 0 1 x 0 1 = 2 + = 1 + .
x −2x −2x cost −2 −2 x cost 2 2 1 2 and the initial condition is of the form ( ) ( ) x 1 1 (0)= x 0 2 Now P (A)≡I.
The eigenvalues are −1+i,−1−i and so 0 (( ) ( )) ( ) 0 1 1 0 1−i 1 P (A)= −(−1+i) = .
1 −2 −2 0 1 −2 −1−i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation428 APPLICATIONS TO DIFFERENTIAL EQUATIONS Recall r (t)≡0 and r (t)=e(−1+i)t. Then 0 1 r′ =(−1−i)r +e(−1+i)t, r (0)=0 2 2 2 and so e(−1+i)t−e(−1−i)t r (t)= =e−tsin(t) 2 2i Putzer’s method yields the fundamental matrix as ( ) ( ) 1 0 1−i 1 Φ(t) = e(−1+i)t +e−tsin(t) 0 1 −2 −1−i ( ) e−t(cos(t)+sin(t)) e−tsint = −2e−tsint e−t(cos(t)−sin(t)) From variation of constants formula the desired solution is ( ) ( )( ) x e−t(cos(t)+sin(t)) e−tsint 1 1 (t)= x −2e−tsint e−t(cos(t)−sin(t)) 0 2 ∫ ( )( ) t e−s(cos(s)+sin(s)) e−ssins 0 + −2e−ssins e−s(cos(s)−sin(s)) cos(t−s) 0 ( ) ∫ ( ) e−t(cos(t)+sin(t)) t e−ssin(s)cos(t−s) = + ds −2e−tsint e−s(coss−sins)cos(t−s) 0 ( ) ( ) e−t(cos(t)+sin(t)) −1(cost)e−t− 3e−tsint+ 1cost+ 2sint = + 5 5 5 5 −2e−tsint −2(cost)e−t+ 4e−tsint+ 2cost− 1sint 5 5 5 5 ( ) 4(cost)e−t+ 2e−tsint+ 1cost+ 2sint = 5 5 5 5 −6e−tsint− 2(cost)e−t+ 2cost− 1sint 5 5 5 5 Thus y(t)=x (t)= 4(cost)e−t+ 2e−tsint+ 1cost+ 2sint.
1 5 5 5 5 C.5 Geometric Theory Of Autonomous Systems Here a suﬃcient condition is given for stability of a ﬁrst order system.
First of all, here is a fundamental estimate for the entries of a fundamental matrix.
Lemma C.5.1 Let the functions, r be given in the statement of Theorem C.4.8 and sup- k pose that A is an n × n matrix whose eigenvalues are {λ ,··· ,λ }.
Suppose that these 1 n eigenvalues are ordered such that Re(λ )≤Re(λ )≤···≤Re(λ )<0.
1 2 n Then if 0 > −δ > Re(λ ) is given, there exists a constant, C such that for each k = n 0,1,··· ,n, |r (t)|≤Ce−δt (3.29) k for all t>0.
Proof: Thisis obviousfor r (t)because it is identicallyequal to 0.
Fromthe deﬁnition 0 of the rk,r1′ =λ1r1,r1(0)=1 and so r1(t)=eλ1t which implies |r (t)|≤eRe(λ1)t. 1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.5.
GEOMETRIC THEORY OF AUTONOMOUS SYSTEMS 429 Suppose for some m≥1 there exists a constant, C such that m |r (t)|≤C tmeRe(λm)t k m for all k ≤m for all t>0.
Then ′ r (t)=λ r (t)+r (t), r (0)=0 m+1 m+1 m+1 m m+1 and so ∫ t r (t)=eλm+1t e−λm+1sr (s)ds.
m+1 m 0 Then by the induction hypothesis, ∫ t(cid:12) (cid:12) |r (t)| ≤ eRe(λm+1)t (cid:12)e−λm+1s(cid:12)C smeRe(λm)sds m+1 m ∫0 t ≤ eRe(λm+1)t smC e−Re(λm+1)seRe(λm)sds m ∫0 t C ≤ eRe(λm+1)t smC ds= m tm+1eRe(λm+1)t m m+1 0 It follows by induction there exists a constant, C such that for all k ≤n, |r (t)|≤CtneRe(λn)t k and this obviously implies the conclusion of the lemma.
The proof of the above lemma yields the following corollary.
Corollary C.5.2 Let the functions, r be given in the statement of Theorem C.4.8 and k suppose that A is an n×n matrix whose eigenvalues are {λ ,··· ,λ }.
Suppose that these 1 n eigenvalues are ordered such that Re(λ )≤Re(λ )≤···≤Re(λ ).
1 2 n Then there exists a constant C such that for all k ≤m |r (t)|≤CtmeRe(λm)t. k With the lemma, the following sloppy estimate is available for a fundamental matrix.
Theorem C.5.3 Let A be an n×n matrix and let Φ(t) be the fundamental matrix for A.
That is, ′ Φ (t)=AΦ(t), Φ(0)=I.
Suppose also the eigenvalues of A are {λ ,··· ,λ } where these eigenvalues are ordered such 1 n that Re(λ )≤Re(λ )≤···≤Re(λ )<0.
1 2 n (cid:12) (cid:12) (cid:12) (cid:12) Then if 0 > −δ > Re(λ ), is given, there exists a constant, C such that (cid:12)Φ(t) (cid:12) ≤ Ce−δt n ij for all t>0.
Also |Φ(t)x|≤Cn3/2e−δt|x|.
(3.30) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation430 APPLICATIONS TO DIFFERENTIAL EQUATIONS Proof: Let {(cid:12) (cid:12) } (cid:12) (cid:12) M ≡max (cid:12)P (A) (cid:12) for all i,j,k .
k ij ThenfromPutzer’sformulaforΦ(t)andLemmaC.5.1,thereexistsaconstant,C suchthat (cid:12)(cid:12) (cid:12)(cid:12) n∑−1 (cid:12)Φ(t) (cid:12)≤ Ce−δtM.
ij k=0 Let the new C be given by nCM.
(cid:4) Next,     2 2 ∑n ∑n ∑n ∑n |Φ(t)x|2 ≡  Φ (t)x  ≤  |Φ (t)||x | ij j ij j i=1 j=1 i=1 j=1   2 ∑n ∑n ∑n ≤  Ce−δt|x| =C2e−2δt (n|x|)2 =C2e−2δtn3|x|2 i=1 j=1 i=1 This proves (3.30) and completes the proof.
Deﬁnition C.5.4 Let f : U → Rn where U is an open subset of Rn such that a ∈ U and f(a)=0.Apoint, awheref(a)=0iscalledanequilibriumpoint.
Thenaisasymptotically stable if for any ε > 0 there exists r > 0 such that whenever |x −a| < r and x(t) the 0 solution to the initial value problem, ′ x =f(x), x(0)=x , 0 it follows lim x(t)=a, |x(t)−a|<ε t→∞ A diﬀerential equation of the form x′ = f(x) is called autonomous as opposed to a nonau- tonomous equation of the form x′ = f(t,x).
The equilibrium point a is stable if for every ε>0 there exists δ >0 such that if |x −a|<δ, then if x is the solution of 0 ′ x =f(x), x(0)=x , (3.31) 0 then |x(t)−a|<ε for all t>0.
Obviously asymptotic stability implies stability.
An ordinary diﬀerential equation is called almost linear if it is of the form ′ x =Ax+g(x) where A is an n×n matrix and g(x) lim =0.
x→0 |x| Now the stability of an equilibrium point of an autonomous system, x′ = f(x) can always be reduced to the consideration of the stability of 0 for an almost linear system.
Here is why.
If you are considering the equilibrium point, a for x′ = f(x), you could deﬁne a new variable, y by a+y=x.
Then asymptotic stability would involve |y(t)| < ε and limt→∞y(t) = 0 while stability would only require |y(t)| < ε.
Then since a is an equilibrium point, y solves the following initial value problem.
y′ =f(a+y)−f(a), y(0)=y , 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.5.
GEOMETRIC THEORY OF AUTONOMOUS SYSTEMS 431 where y =x −a.
0 0 Let A=Df(a).
Then from the deﬁnition of the derivative of a function, ′ y =Ay+g(y), y(0)=y (3.32) 0 where g(y) lim =0.
y→0 |y| Thus there is never any loss of generality in considering only the equilibrium point 0 for an almost linear system.1 Therefore, from now on I will only consider the case of almost linear systems and the equilibrium point 0.
Theorem C.5.5 Consider the almost linear system of equations, ′ x =Ax+g(x) (3.33) where g(x) lim =0 x→0 |x| and g is a C1 function.
Suppose that for all λ an eigenvalue of A, Reλ < 0.
Then 0 is asymptotically stable.
Proof: By Theorem C.5.3 there exist constants δ > 0 and K such that for Φ(t) the fundamental matrix for A, |Φ(t)x|≤Ke−δt|x|.
Let ε>0 be given and let r be small enough that Kr <ε and for |x|<(K+1)r,|g(x)|< η|x| where η is so small that Kη < δ, and let |y | < r. Then by the variation of constants 0 formula, the solution to (3.33), at least for small t satisﬁes ∫ t y(t)=Φ(t)y + Φ(t−s)g(y(s))ds.
0 0 The following estimate holds.
∫ ∫ t t |y(t)|≤Ke−δt|y |+ Ke−δ(t−s)η|y(s)|ds<Ke−δtr+ Ke−δ(t−s)η|y(s)|ds.
0 0 0 Therefore, ∫ t eδt|y(t)|<Kr+ Kηeδs|y(s)|ds.
0 By Gronwall’s inequality, eδt|y(t)|<KreKηt and so |y(t)|<Kre(Kη−δ)t <εe(Kη−δ)t Therefore, |y(t)| < Kr < ε for all t and so from Corollary C.3.4, the solution to (3.33) exists for all t≥0 and since Kη−δ <0, lim |y(t)|=0.
(cid:4) t→∞ 1Thisisnolongertruewhenyoustudypartialdiﬀerentialequationsasordinarydiﬀerentialequationsin inﬁnitedimensionalspaces.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation432 APPLICATIONS TO DIFFERENTIAL EQUATIONS C.6 General Geometric Theory HereIwillconsiderthecasewherethematrixAhasbothpositiveandnegativeeigenvalues.
First here is a useful lemma.
Lemma C.6.1 Suppose A is an n×n matrix and there exists δ >0 such that 0<δ <Re(λ )≤···≤Re(λ ) 1 n where {λ ,··· ,λ } are the eigenvalues of A, with possibly some repeated.
Then there exists 1 n a constant, C such that for all t<0, |Φ(t)x|≤Ceδt|x| Proof: I want an estimate on the solutions to the system ′ Φ (t)=AΦ(t), Φ(0)=I.
for t<0.
Let s=−t and let Ψ(s)=Φ(t).
Then writing this in terms of Ψ, Ψ′(s)=−AΨ(s), Ψ(0)=I.
Now the eigenvalues of −A have real parts less than −δ because these eigenvalues are obtained from the eigenvalues of A by multiplying by −1.
Then by Theorem C.5.3 there exists a constant, C such that for any x, |Ψ(s)x|≤Ce−δs|x|.
Therefore, from the deﬁnition of Ψ, |Φ(t)x|≤Ceδt|x|.
(cid:4) Here is another essential lemma which is found in Coddington and Levinson [6] Lemma C.6.2 Let p (t) be polynomials with complex coeﬃcients and let j ∑m f(t)= p (t)eλjt j j=1 where m≥1,λ ̸=λ for j ̸=k, and none of the p (t) vanish identically.
Let j k j σ =max(Re(λ ),··· ,Re(λ )).
1 m Then there exists a positive number, r and arbitrarily large positive values of t such that e−σt|f(t)|>r.
In particular, |f(t)| is unbounded.
Proof: Suppose the largest exponent of any of the p is M and let λ =a +ib .
First j j j j assume each a = 0.
This is convenient because σ = 0 in this case and the largest of the j Re(λ ) occurs in every λ .
j j Then arranging the above sum as a sum of decreasing powers of t, f(t)=tMf (t)+···+tf (t)+f (t).
M 1 0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.6.
GENERAL GEOMETRIC THEORY 433 Then ( ) 1 t−Mf(t)=f (t)+O M t ( ) where the last term means that tO 1 is bounded.
Then t ∑m f (t)= c eibjt M j j=1 It can’t be the case that all the c are equal to 0 because then M would not be the highest j power exponent.
Suppose c ̸=0.
Then k ∫ ∫ 1 T ∑m 1 T Tl→im∞T 0 t−Mf(t)e−ibktdt= j=1cjT 0 ei(bj−bk)tdt=ck ̸=0.
(cid:12) (cid:12) Letting r = |ck/2|, it follows (cid:12)t−Mf(t)e−ibkt(cid:12) > r for arbitrarily large values of t. Thus it is also true that |f(t)|>r for arbitrarily large values of t. Next consider the general case in which σ is given above.
Thus ∑ e−σtf(t)= p (t)ebjt+g(t) j j:aj=σ ∑ wherelimt→∞g(t)=0,g(t)beingoftheform sps(t)e(as−σ+ibs)t whereas−σ <0.Then this reduces to the case above in which σ =0.
Therefore, there exists r >0 such that (cid:12) (cid:12) (cid:12)e−σtf(t)(cid:12)>r for arbitrarily large values of t. (cid:4) Next here is a Banach space which will be useful.
Lemma C.6.3 For γ >0,let { } E = x∈BC([0,∞),Fn):t→eγtx(t) is also in BC([0,∞),Fn) γ and let the norm be given by {(cid:12) (cid:12) } ||x|| ≡sup (cid:12)eγtx(t)(cid:12):t∈[0,∞) γ Then E is a Banach space.
γ Proof: Let {x } be a Cauchy sequence in E .
Then since BC([0,∞),Fn) is a Banach k γ space, there exists y∈BC([0,∞),Fn) such that eγtx (t) converges uniformly on [0,∞) to k y(t).
Therefore e−γteγtx (t) = x (t) converges uniformly to e−γty(t) on [0,∞).
Deﬁne k k x(t)≡e−γty(t).
Then y(t)=eγtx(t) and by deﬁnition, ||x −x|| →0.
k γ (cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation434 APPLICATIONS TO DIFFERENTIAL EQUATIONS C.7 The Stable Manifold Here assume ( ) A− 0 A= (3.34) 0 A + where A− and A+ are square matrices of size k×k and (n−k)×(n−k) respectively.
Also assume A− has eigenvalues whose real parts are all less than −α while A+ has eigenvalues whose real parts are all larger than α.
Assume also that each of A− and A+ is upper triangular.
Also, I will use the following convention.
For v∈Fn, ( ) v− v= v + where v− consists of the ﬁrst k entries of v. Then from Theorem C.5.3 and Lemma C.6.1 the following lemma is obtained.
Lemma C.7.1 Let A be of the form given in (3.34) as explained above and let Φ (t) and + Φ−(t) be the fundamental matrices corresponding to A+ and A− respectively.
Then there exist positive constants, α and γ such that |Φ (t)y|≤Ceαt for all t<0 (3.35) + |Φ−(t)y|≤Ce−(α+γ)t for all t>0.
(3.36) Also for any nonzero x∈Cn−k, |Φ (t)x| is unbounded.
(3.37) + Proof: The ﬁrst two claims have been established already.
It suﬃces to pick α and γ suchthat−(α+γ)islargerthanalleigenvaluesofA− andαissmallerthanalleigenvalues of A .
It remains to verify (3.37).
From the Putzer formula for Φ (t), + + n∑−1 Φ (t)x= r (t)P (A)x + k+1 k k=0 where P (A)≡I.
Now each r is a polynomial (possibly a constant) times an exponential.
0 k This follows easily from the deﬁnition of the r as solutions of the diﬀerential equations k ′ r =λ r +r .
k+1 k+1 k+1 k Now by assumption the eigenvalues have positive real parts so σ ≡max(Re(λ1),··· ,Re(λn−k))>0.
It can also be assumed Re(λ1)≥···≥Re(λn−k) By Lemma C.6.2 it follows |Φ (t)x| is unbounded.
This follows because + n∑−1 Φ (t)x=r (t)x+ r (t)y , r (t)=eλ1t.
+ 1 k+1 k 1 k=1 Since x̸=0, it has a nonzero entry, say x ̸= 0.
Consider the mth entry of the vector m Φ (t)x.
By this Lemma the mth entry is unbounded and this is all it takes for x(t) to be + unbounded.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.7.
THE STABLE MANIFOLD 435 Lemma C.7.2 Consider the initial value problem for the almost linear system ′ x =Ax+g(x), x(0)=x , 0 where g is C1and A is of the special form ( ) A− 0 A= 0 A + in which A− is a k×k matrix which has eigenvalues for which the real parts are all negative and A is a (n−k)×(n−k) matrix for which the real parts of all the eigenvalues are + positive.
Then 0 is not stable.
More precisely, there exists a set of points (a−, (a−)) for a− small such that for x0 on this set, lim x(t,x )=0 0 t→∞ and for x not on this set, there exists a δ >0 such that |x(t,x )| cannot remain less than 0 0 δ for all positive t. Proof: Consider the initial value problem for the almost linear equation, ( ) ′ a− x =Ax+g(x), x(0)=a= .
a + Then by the variation of constants formula, a local solution has the form ( )( ) Φ−(t) 0 a− x(t,a) = 0 Φ (t) a + + ∫ ( ) t Φ−(t−s) 0 + g(x(s,a))ds (3.38) 0 Φ (t−s) 0 + Write x(t) for x(t,a) for short.
Let ε > 0 be given and suppose δ is such that if |x| < δ, then |g±(x)| < ε|x|.
Assume from now on that |a| < δ.
Then suppose |x(t)| < δ for all t>0.
Writing (3.38) diﬀerently yields ( )( ) ( ∫ ) x(t,a) = Φ−(t) 0 a− + 0tΦ−(t−s)g−(x(s,a))ds 0 Φ (t) a 0 + + ( ) 0 ∫ + tΦ (t−s)g (x(s,a))ds 0 + + ( )( ) ( ∫ ) = Φ−(t) 0 a− + 0tΦ−(t−s)g−(x(s,a))ds 0 Φ (t) a 0 + + ( ) ∫ 0 ∫ + ∞Φ (t−s)g (x(s,a))ds− ∞Φ (t−s)g (x(s,a))ds .
0 + + t + + These improper integrals converge thanks to the assumption that x is bounded and the estimates (3.35) and (3.36).
Continuing the rewriting, ( ) ( ( ∫ ) ) x−(t) = Φ−(t)(a−+ ∫0tΦ−(t−s)g−(x(s,a))ds) x+(t) Φ (t) a + ∞Φ (−s)g (x(s,a))ds ( + + 0 + + ) ∫ 0 + − ∞Φ (t−s)g (x(s,a))ds .
t + + Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation436 APPLICATIONS TO DIFFERENTIAL EQUATIONS It follows from Lem∫ma C.7.1 that if |x(t,a)| is bounded by δ as asserted, then it must be the case that a + ∞Φ (−s)g (x(s,a))ds=0.
Consequently, it must be the case that + 0 + + ( ) ( ∫ ) x(t)=Φ(t) a0− + −∫0t∞Φ−Φ(t(−t−s)sg)−g(x(x(s(,sa,)a))d)sds (3.39) t + + Letting t→0, this requires that for a solution to the initial value problem to exist and also satisfy |x(t)|<δ for all t>0 it must be the case that ( ) ∫ a− x(0)= − ∞Φ (−s)g (x(s,a))ds 0 + + where x(t,a) is the solution of ( ) ′ ∫ a− x =Ax+g(x), x(0)= − ∞Φ (−s)g (x(s,a))ds 0 + + This is because in (3.39), if x is bounded by δ then the reverse steps show x is a solution of the above diﬀerential equation and initial condition.
It follows if I can show that for all a− suﬃciently small and a=(a−,0)T, there exists a solution to (3.39) x(s,a) on (0,∞) for which |x(s,a)|<δ, then I can deﬁne ∫ ∞ (a)≡− Φ (−s)g (x(s,a))ds + + 0 and conclude that |x(t,x0)| < δ for all t > 0 if and only if x0 = (a−, (a−))T for some suﬃciently small a−.
LetC,α,γ betheconstantsofLemmaC.7.1.
Letη beasmallpositivenumbersuchthat Cη 1 < α 6 Notethat ∂g (0)=0.Therefore,byLemmaC.3.1,thereexistsδ >0suchthatif|x|,|y|≤δ, ∂xi then |g(x)−g(y)|<η|x−y| and in particular, |g±(x)−g±(y)|<η|x−y| (3.40) because each ∂g (x) is very small.
In particular, this implies ∂xi |g−(x)|<η|x|,|g+(x)|<η|x|.
For x∈Eγ deﬁned in Lemma C.6.3 and |a−|< 2δC, ( ∫ ) Fx(t)≡ Φ−(t−)a∫−∞+Φ 0t(Φt−−(st)−g s)(xg−(s()x)d(ss))ds .
t + + IneedtoﬁndaﬁxedpointofF.Letting||x|| <δ,andusingtheestimatesofLemmaC.7.1, γ ∫ t eγt|Fx(t)| ≤ eγt|Φ−(t)a−|+eγt Ce−(α+γ)(t−s)η|x(s)|ds ∫ 0 ∞ +eγt Ceα(t−s)η|x(s)|ds t Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationC.7.
THE STABLE MANIFOLD 437 ∫ δ t ≤ eγtC e−(α+γ)t+eγt||x|| Cη e−(α+γ)(t−s)e−γsds 2C γ ∫ 0 ∞ +eγtCη eα(t−s)e−γsds||x|| γ ∫t ∫ δ t ∞ < +δCη e−α(t−s)ds+Cηδ e(α+γ)(t−s)ds 2 0 ( t ) δ 1 δCη 1 Cη 2δ < +δCη + ≤δ + < .
2 α α+γ 2 α 3 Thus F maps every x∈E having ||x|| <δ to Fx where ||Fx|| ≤ 2δ.
γ γ γ 3 Now let x,y∈E where ||x|| ,||y|| <δ.
Then γ γ γ ∫ t eγt|Fx(t)−Fy(t)| ≤ eγt |Φ−(t−s)|ηe−γseγs|x(s)−y(s)|ds 0∫ ∞ +eγt |Φ (t−s)|e−γseγsη|x(s)−y(s)|ds + t (∫ ) ∫ t ∞ ≤Cη||x−y|| e−α(t−s)ds + e(α+γ)(t−s)ds γ 0 t ( ) 1 1 2Cη 1 ≤Cη + ||x−y|| < ||x−y|| < ||x−y|| .
α α+γ γ α γ 3 γ ItfollowsfromLemma14.6.4,foreacha−suchthat|a−|< δ ,thereexistsauniquesolution 2C to (3.39) in E .
γ As pointed out earlier, if ∫ ∞ (a)≡− Φ (−s)g (x(s,a))ds + + 0 then for x(t,x ) the solution to the initial value problem 0 ′ x =Ax+g(x), x(0)=x 0 ( ) has the property that if x is not of the form a− , then |x(t,x )| cannot be less 0 (a−) 0 than δ for all t>0.
( ) On the other hand, if x0 = a(a−−) for |a−| < 2δC, then x(t,x0),the solution to (3.39) is the unique solution to the initial value problem ′ x =Ax+g(x), x(0)=x .
0 and it was shown that ||x(·,x )|| <δ and so in fact, 0 γ |x(t,x )|≤δe−γt 0 showing that lim x(t,x )=0.
0 t→∞ (cid:4) The following theorem is the main result.
It involves a use of linear algebra and the above lemma.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation438 APPLICATIONS TO DIFFERENTIAL EQUATIONS Theorem C.7.3 Consider the initial value problem for the almost linear system ′ x =Ax+g(x), x(0)=x 0 in which g is C1 and where at there are k < n eigenvalues of A which have negative real parts and n−k eigenvalues of A which have positive real parts.
Then 0 is not stable.
More precisely, there exists a set of points (a, (a)) for a small and in a k dimensional subspace such that for x on this set, 0 lim x(t,x )=0 0 t→∞ and for x not on this set, there exists a δ >0 such that |x(t,x )| cannot remain less than 0 0 δ for all positive t. Proof: This involves nothing more than a reduction to the situation of Lemma C.7.2.
FromTheorem10.5.2o(nPage10.5.
)2AissimilartoamatrixoftheformdescribedinLemma C.7.2.
Thus A=S−1 A− 0 S. Letting y=Sx, it follows 0 A + ( ) ( ) y′ = A− 0 y+g S−1y 0 A + (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) (cid:12) Now |x|=(cid:12)S−1Sx(cid:12)≤(cid:12)(cid:12)S−1(cid:12)(cid:12)|y| and |y|=(cid:12)SS−1y(cid:12)≤||S|||x|.
Therefore, (cid:12)(cid:12) (cid:12)(cid:12) 1 |y|≤|x|≤(cid:12)(cid:12)S−1(cid:12)(cid:12)|y|.
||S|| It follows all conclusions of Lemma C.7.2 are valid for this theorem.
(cid:4) The set of points (a, (a)) for a small is called the stable manifold.
Much more can be said about the stable manifold and you should look at a good diﬀerential equations book for this.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationCompactness And Completeness D.0.1 The Nested Interval Lemma First, here is the one dimensional nested interval lemma.
Lemma D.0.4 Let I =[a ,b ] be closed intervals, a ≤b , such that I ⊇I for all k. k k k k k k k+1 Thenthereexistsapointcwhichiscontainedinalltheseintervals.
Iflimk→∞(bk−ak)=0, then there is exactly one such point.
Proof: Note that the {a } are an increasing sequence and that {b } is a decreasing k k sequence.
Now note that if m<n, then a ≤a ≤b m n n while if m>n, b ≥b ≥a .
n m m It follows that a ≤b for any pair m,n.
Therefore, each b is an upper bound for all the m n n a and so if c≡sup{a }, then for each n, it follows that c≤b and so for all, a ≤c≤b m k n n n which shows that c is in all of these intervals.
If the condition on the lengths of the intervals holds, then if c,c′ are in all the intervals, theniftheyarenotequal,theneventually,forlargeenoughk,theycannotbothbecontained in [a ,b ] since eventually b −a < |c−c′|.
This would be a contradiction.
Hence c = c′.
k k k k (cid:4) Deﬁnition D.0.5 The diameter of a set S, is deﬁned as diam(S)≡sup{|x−y|:x,y∈S}.
Thus diam(S) is just a careful description of what you would think of as the diameter.
It measures how stretched out the set is.
Here is a multidimensional version of the nested interval lemma.
∏ [ ] { [ ]} Lemma D.0.6 Let I = p ak,bk ≡ x∈Rp :x ∈ ak,bk and suppose that for all k i=1 i i i i i k =1,2,··· , I ⊇I .
k k+1 Then there exists a point c∈Rp which is an element of every Ik.
If limk→∞diam(Ik)=0, then the point c is unique.
[ ] [ ] Proof: For eac[h i =]1,··· ,p, aki,bki ⊇ aki+1,bki+1 and so, by Lemma D.0.4, there exists a point ci ∈ aki,bki for all k. Then letting c≡(c1,··· ,cp) it follows c∈[Ik for]all k. Iftheconditiononthediametersholds,thenthelengthsoftheintervalslimk→∞ aki,bki =0 and so by the same lemma, each c is unique.
Hence c is unique.
(cid:4) i 439 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation440 COMPACTNESS AND COMPLETENESS D.0.2 Convergent Sequences, Sequential Compactness A mapping f : {k,k+1,k+2,···}→Rp is called a sequence.
We usually write it in the form {a } where it is understood that a ≡f(j).
j j Deﬁnition D.0.7 A sequence, {a } is said to converge to a if for every ε>0 there exists k nε such that if n > nε, then |a−an| < ε.
The usual notation for this is limn→∞an = a although it is often written as a →a.
A closed set K ⊆ Rn is one which has the property n that if {k }∞ is a sequence of points of K which converges to x, then x∈K.
j j=1 One can also deﬁne a subsequence.
Deﬁnition D.0.8 {a } is a subsequence of {a } if n <n <···.
nk n 1 2 The following theorem says the limit, if it exists, is unique.
Theorem D.0.9 If a sequence, {a } converges to a and to b then a=b.
n Proof: There exists n such that if n > n then |a −a| < ε and if n > n , then ε ε n 2 ε |a −b|< ε.
Then pick such an n. n 2 ε ε |a−b|<|a−a |+|a −b|< + =ε.
n n 2 2 Since ε is arbitrary, this proves the theorem.
(cid:4) The following is the deﬁnition of a Cauchy sequence in Rp.
Deﬁnition D.0.10 {a } is a Cauchy sequence if for all ε > 0, there exists n such that n ε whenever n,m≥n , if follows that |a −a |<ε.
ε n m A sequence is Cauchy, means the terms are “bunching up to each other” as m,n get large.
Theorem D.0.11 The set of terms in a Cauchy sequence in Rp is bounded in the sense that for all n, |a |<M for some M <∞.
n Proof: Let ε=1 in the deﬁnition of a Cauchy sequence and let n>n .
Then from the 1 deﬁnition, |a −a |<1.It follows that for all n>n ,|a |<1+|a |.Therefore, for all n, n n1 1 n n1 ∑n1 |a |≤1+|a |+ |a |.
(cid:4) n n1 k k=1 Theorem D.0.12 If a sequence {a } in Rp converges, then the sequence is a Cauchy se- n quence.
Also,ifsomesubsequenceofaCauchysequenceconverges,thentheoriginalsequence converges.
Proof: Letε>0begivenandsupposea →a.
Thenfromthedeﬁnitionofconvergence, n there exists n such that if n>n , it follows that |a −a|< ε .
Therefore, if m,n≥n +1, ε ε n 2 ε it follows that ε ε |a −a |≤|a −a|+|a−a |< + =ε n m n m 2 2 showing that, since ε > 0 is arbitrary, {a } is a Cauchy sequence.
It remains to that the n last claim.
Suppose then that {an} is a Cauchy sequence and a = limk→∞ank where {ank}∞k=1 is a subsequence.
Let ε > 0 be given.
Then there exists K such that if k,l ≥ K, then Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation441 |a −a |< ε.
Then if k >K, it follows n >K because n ,n ,n ,··· is strictly increasing k l 2 k 1 2 3 as the subscript increases.
Also, there exists K such that if k > K ,|a −a| < ε.
Then 1 1 nk 2 letting n>max(K,K ), pick k >max(K,K ).
Then 1 1 ε ε |a−a |≤|a−a |+|a −a |< + =ε.
n nk nk n 2 2 Therefore, the sequence converges.
(cid:4) Deﬁnition D.0.13 A set K in Rp is said to be sequentially compact if every sequence in K has a subsequence which converges to a point of K. ∏ Theorem D.0.14 If I = p [a ,b ] where a ≤b , then I is sequentially compact.
0 i=1 i i i i 0 ∏ Proof: Le[t {ak}∞k=]1 ⊆ I0 and con[sider all]sets of the form pi=1[ci,di] where [ci,di] equals either ai,ai+2bi or [ci,di] = ai+2bi,bi .
Thus there are 2p of these sets because there are two choices for the ith slot for i=1,··· ,p. Also, if x and y are two points in one ( ) ∑ 1/2 of these sets, |x −y |≤2−1|b −a | where diam(I )= p |b −a |2 , i i i i 0 i=1 i i ( ) ( ) ∑p 1/2 ∑p 1/2 |x−y|= |x −y |2 ≤2−1 |b −a |2 ≡2−1diam(I ).
i i i i 0 i=1 i=1 In particular, since d≡(d ,··· ,d ) and c≡(c ,··· ,c ) are two such points, 1 p 1 p ( ) ∑p 1/2 D ≡ |d −c |2 ≤2−1diam(I ) 1 i i 0 i=1 Denote by {J ,··· ,J } these sets determined above.
Since the union of these sets equals 1 2p all of I ≡I, it follows that for some J , the sequence, {a } is contained in J for inﬁnitely 0 k i k many k. Let that one be called I .
Next do for I what was done for I to get I ⊆ I 1 1 0 2 1 such that the diameter is half that of I and I contains {a } for inﬁnitely many values 1 2 k of k. Continue in this way obtaining a nested sequence {I } such that I ⊇ I , and if k k k+1 x,y ∈ I , then |x−y| ≤ 2−kdiam(I ), and I contains {a } for inﬁnitely many values of k 0 n k k for each n. Then by the nested interval lemma, there exists c such that c is contained in each I .
Pick a ∈ I .
Next pick n > n such that a ∈ I .
If a ,··· ,a have been k n1 1 2 1 n2 2 n1 nk chosen, let a ∈I and n >n .
This can be done because in the construction, I nk+1 k+1 k+1 k n contains {a } for inﬁnitely many k. Thus the distance between a and c is no larger than 2−kdiam(I0k), and so limk→∞ank =c∈I0.
(cid:4) nk Corollary D.0.15 Let K be a closed and bounded set of points in Rp.
Then K is sequen- tially compact.
∏ Proof: Since K is closed and bounded, there exists a closed rectangle, p [a ,b ] k=1 k k which contains K. Now let {xk} be a sequence of∏points in K. By Theorem D.0.14, there exists a subsequence {x } such that x → x ∈ p [a ,b ].
However, K is closed and nk nk k=1 k k each x is in K so x∈K.
(cid:4) nk Theorem D.0.16 Every Cauchy sequence in Rp converges.
∏ Proof:Let{a }beaCauchysequence.
ByTheoremD.0.11,thereissomebox p [a ,b ] k i=1 i i containing all∏the terms of {ak}.
Therefore, by Theorem D.0.14, a subsequence converges to a point of p [a ,b ].
By Theorem D.0.12, the original sequence converges.
(cid:4) i=1 i i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation442 COMPACTNESS AND COMPLETENESS Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationThe Fundamental Theorem Of Algebra The fundamental theorem of algebra states that every non constant polynomial having coeﬃcients in C has a zero in C. If C is replaced by R, this is not true because of the example, x2+1=0.Thistheoremisaveryremarkableresultandnotwithstandingitstitle, all the best proofs of it depend on either analysis or topology.
It was proved by Gauss in 1797 then proved with no loose ends by Argand in 1806 although others also worked on it.
The proof given here follows Rudin [22].
See also Hardy [12] for another proof, more discussion and references.
Recall De Moivre’s theorem on Page 17 which is listed below for convenience.
Theorem E.0.17 Let r >0 be given.
Then if n is a positive integer, [r(cost+isint)]n =rn(cosnt+isinnt).
Now from this theorem, the following corollary on Page 1.5.5 is obtained.
Corollary E.0.18 Letz beanonzerocomplexnumberandletk beapositiveinteger.
Then there are always exactly k kth roots of z in C. ∑ Lemma E.0.19 Let a ∈C for k =1,··· ,n and let p(z)≡ n a zk.
Then p is contin- k k=1 k uous.
Proof: (cid:12) (cid:12) |azn−awn|≤|a||z−w|(cid:12)zn−1+zn−2w+···+wn−1(cid:12).
Then for |z−w|<1, the triangle inequality implies |w|<1+|z| and so if |z−w|<1, |azn−awn|≤|a||z−w|n(1+|z|)n. If ε>0 is given, let ( ) ε δ <min 1, .
|a|n(1+|z|)n It follows from the above inequality that for |z−w| < δ, |azn−awn| < ε.
The function of thelemmaisjustthesumoffunctionsofthissortandsoitfollowsthatitisalsocontinuous.
Theorem E.0.20 (FundamentaltheoremofAlgebra)Letp(z)beanonconstantpolynomial.
Then there exists z ∈C such that p(z)=0.
443 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation444 THE FUNDAMENTAL THEOREM OF ALGEBRA Proof: Suppose not.
Then ∑n p(z)= a zk k k=0 where a ̸=0, n>0.
Then n n∑−1 |p(z)|≥|a ||z|n− |a ||z|k n k k=0 and so lim |p(z)|=∞.
(5.1) |z|→∞ Now let λ≡inf{|p(z)|:z ∈C}.
By(5.1),thereexistsanR>0suchthatif|z|>R,itfollowsthat|p(z)|>λ+1.Therefore, λ≡inf{|p(z)|:z ∈C}=inf{|p(z)|:|z|≤R}.
The set {z :|z|≤R} is a closed and bounded set and so this inﬁmum is achieved at some pointw with|w|≤R.Acontradictionisobtainedif|p(w)|=0soassume|p(w)|>0.Then consider p(z+w) q(z)≡ .
p(w) It follows q(z) is of the form q(z)=1+c zk+···+c zn k n where c ̸= 0, because q(0) = 1.
It is also true that |q(z)| ≥ 1 by the assumption that k |p(w)| is the smallest value of |p(z)|.
Now let θ ∈C be a complex number with |θ|=1 and θc wk =−|w|k|c |.
k k If (cid:12) (cid:12) −(cid:12)wk(cid:12)|c | w ̸=0,θ = k wkc k and if w =0, θ =1 will work.
Now let ηk =θ and let t be a small positive number.
q(tηw)≡1−tk|w|k|c |+···+c tn(ηw)n k n which is of the form 1−tk|w|k|c |+tk(g(t,w)) k where limt→0g(t,w)=0.
Letting t be small enough, |g(t,w)|<|w|k|c |/2 k and so for such t, |q(tηw)|<1−tk|w|k|c |+tk|w|k|c |/2<1, k k a contradiction to |q(z)|≥1.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationFields And Field Extensions F.1 The Symmetric Polynomial Theorem First here is a deﬁnition of polynomials in many variables which have coeﬃcients in a commutative ring.
A commutative ring would be a ﬁeld except you don’t know that every nonzero element has a multiplicative inverse.
If you like, let these coeﬃcients be in a ﬁeld it is still interesting.
A good example of a commutative ring is the integers.
In particular, every ﬁeld is a commutative ring.
Deﬁnition F.1.1 Let k≡(k ,k ,··· ,k ) where each k is a nonnegative integer.
Let 1 2 n i ∑ |k|≡ k i i Polynomials of degree p in the variables x ,x ,··· ,x are expressions of the form 1 2 n ∑ g(x ,x ,··· ,x )= a xk1···xkn 1 2 n k 1 n |k|≤p where each a is in a commutative ring.
If all a = 0, the polynomial has no degree.
Such k k a polynomial is said to be symmetric if whenever σ is a permutation of {1,2,··· ,n}, ( ) g x ,x ,··· ,x =g(x ,x ,··· ,x ) σ(1) σ(2) σ(n) 1 2 n An example of a symmetric polynomial is ∑n s (x ,x ,··· ,x )≡ x 1 1 2 n i i=1 Another one is s (x ,x ,··· ,x )≡x x ···x n 1 2 n 1 2 n Deﬁnition F.1.2 The elementary symmetric polynomial s (x ,x ,··· ,x ),k = 1,··· ,n k 1 2 n is the coeﬃcient of (−1)kxn−k in the following polynomial.
(x−x )(x−x )···(x−x ) 1 2 n =xn−s xn−1+s xn−2−···±s 1 2 n Thus s =x +x +···+x ∑ 1 ∑1 2 n s = x x , s = x x x ,..., s =x x ···x 2 i j 3 i j k n 1 2 n i<j i<j<k 445 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation446 FIELDS AND FIELD EXTENSIONS Thenthefollowingresultisthefundamentaltheoreminthesubject.
Itisthesymmetric polynomial theorem.
It says that these elementary symmetric polynomials are a lot like a basis for the symmetric polynomials.
Theorem F.1.3 Let g(x ,x ,··· ,x ) be a symmetric polynomial.
Then g(x ,x ,··· ,x ) 1 2 n 1 2 n equals a polynomial in the elementary symmetric functions.
∑ g(x ,x ,··· ,x )= a sk1···skn 1 2 n k 1 n k and the a are unique.
k Proof: If n = 1, it is obviously true because s = x .
Suppose the theorem is true for 1 1 n−1 and g(x ,x ,··· ,x ) has degree d. Let 1 2 n g′(x1,x2,··· ,xn−1)≡g(x1,x2,··· ,xn−1,0) By induction, there are unique a such that k ∑ g′(x1,x2,··· ,xn−1)= aks′1k1···s′nk−n−11 k where s′i is the corresponding symmetric polynomial which pertains to x1,x2,··· ,xn−1.
Note that sk(x1,x2,··· ,xn−1,0)=s′k(x1,x2,··· ,xn−1) Now consider ∑ g(x ,x ,··· ,x )− a sk1···skn−1 ≡q(x ,x ,··· ,x ) 1 2 n k 1 n−1 1 2 n k isasymmetricpolynomialanditequals0whenx equals0.
Sinceitissymmetric,itisalso n 0 whenever x =0.
Therefore, i q(x ,x ,··· ,x )=s h(x ,x ,··· ,x ) 1 2 n n 1 2 n and it follows that h(x ,x ,··· ,x ) is symmetric of degree no more than d − n and is 1 2 n uniquely determined.
Thus, if g(x ,x ,··· ,x ) is symmetric of degree d, 1 2 n ∑ g(x ,x ,··· ,x )= a sk1···skn−1 +s h(x ,x ,··· ,x ) 1 2 n k 1 n−1 n 1 2 n k wherehhasdegreenomorethand−n.
Nowapplythesameargumenttoh(x ,x ,··· ,x ) 1 2 n and continue, repeatedly obtaining a sequence of symmetric polynomials h , of strictly de- i creasing degree, obtaining expressions of the form ∑ g(x1,x2,··· ,xn)= bksk11···sknn−−11sknn +snhm(x1,x2,··· ,xn) k Eventually h must be a constant or zero.
By induction, each step in the argument yields m uniqueness and so, the ﬁnal sum of combinations of elementary symmetric functions is uniquely determined.
(cid:4) HereisaveryinterestingresultwhichIsawclaimedinapaperbySteinbergandRedheﬀer on Lindemannn’s theorem which follows from the above corollary.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.2.
THE FUNDAMENTAL THEOREM OF ALGEBRA 447 Theorem F.1.4 Let α ,··· ,α be roots of the polynomial equation 1 n anxn+an−1xn−1+···+a1x+a0 =0 whereeacha isaninteger.
Thenanysymmetricpolynomialinthequantitiesa α ,··· ,a α i n 1 n n having integer coeﬃcients is also an integer.
Also any symmetric polynomial in the quanti- ties α ,··· ,α having rational coeﬃcients is a rational number.
1 n Proof: Let f(x ,··· ,x ) be the symmetric polynomial.
Thus 1 n f(x ,··· ,x )∈Z[x ···x ] 1 n 1 n From Corollary F.1.3 it follows there are integers ak1···kn such that ∑ f(x1,··· ,xn)= ak1···knpk11···pknn k1+···+kn≤m where the p are the elementary symmetric polynomials deﬁned as the coeﬃcients of i ∏n (x−x ) j j=1 Thus f(a α ,··· ,a α ) ∑n 1 n n = ak1···knpk11(anα1,··· ,anαn)···pknn(anα1,··· ,anαn) k1+···+kn Now the given polynomial is of the form ∏n a (x−α ) n j j=1 and so the coeﬃcient of xn−k is pk(α1,··· ,αn)an =an−k.
Also p (a α ,··· ,a α )=akp (α ,··· ,α )=akan−k k n 1 n n n k 1 n n a n It follows ( ) ( ) ( ) f(anα1,··· ,anαn)= ∑ ak1···kn a1naan−1 k1 a2naan−2 k2··· annaa0 kn k1+···+kn n n n which is an integer.
To see the last claim follows from this, take the symmetric polynomial in α ,··· ,α and multiply by the product of the denominators of the rational coeﬃcients 1 n to get one which has integer coeﬃcients.
Then by the ﬁrst part, each homogeneous term is just an integer divided by a raised to some power.
(cid:4) n F.2 The Fundamental Theorem Of Algebra This is devoted to a mostly algebraic proof of the fundamental theorem of algebra.
It dependsontheinterestingresultsaboutsymmetricpolynomialswhicharepresentedabove.
I found it on the Wikipedia article about the fundamental theorem of algebra.
You google Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation448 FIELDS AND FIELD EXTENSIONS “fundamental theorem of algebra” and go to the Wikipedia article.
It gives several other proofs in addition to this one.
According to this article, the ﬁrst completely correct proof of this major theorem is due to Argand in 1806.
Gauss and others did it earlier but their arguments had gaps in them.
You can’t completely escape analysis when you prove this theorem.
The necessary anal- ysis is in the following lemma.
Lemma F.2.1 Suppose p(x) = xn +an−1xn−1 +···+a1x+a0 where n is odd and the coeﬃcients are real.
Then p(x) has a real root.
Proof: This follows from the intermediate value theorem from calculus.
Next is an algebraic consideration.
First recall some notation.
∏m a ≡a a ···a i 1 2 m i=1 Recall a polynomial in {z ,··· ,z } is symmetric only if it can be written as a sum of 1 n elementary symmetric polynomials raised to various powers multiplied by constants.
This followsfromPropositionF.1.3orTheoremF.1.3bothofwhicharethetheoremonsymmetric polynomials.
The following is the main part of the theorem.
In fact this is one version of the funda- mental theorem of algebra which people studied earlier in the 1700’s.
Lemma F.2.2 Let p(x)=xn+an−1xn−1+···+a1x+a0 be a polynomial with real coef- ﬁcients.
Then it has a complex root.
Proof: It is possible to write n=2km where m is odd.
If n is odd, k = 0.
If n is even, keep dividing by 2 until you are left with an odd number.
If k = 0 so that n is odd, it follows from Lemma F.2.1 that p(x) has a real, hence complex root.
The proof will be by induction on k, the case k = 0 being done.
Suppose then that it works for n = 2lm where m is odd and l ≤ k−1 and let n = 2km where m is odd.
Let {z ,··· ,z } be the roots of the polynomial in a splitting ﬁeld, the 1 n existence of this ﬁeld being given by the above proposition.
Then ∏n ∑n p(x)= (x−z )= (−1)kp xk (6.1) j k j=1 k=0 where p is the kth elementary symmetric polynomial.
Note this shows k an−k =pk(−1)k. (6.2) Thereisanotherpolynomialwhichhascoeﬃcientswhicharesumsofrealnumberstimes the p raised to various powers and it is k ∏ q (x)≡ (x−(z +z +tz z )), t∈R t i j i j 1≤i<j≤n I need to verify this is really the case for q (x).
When you switch any two of the z in q (x) t i t the polynomial does not change.
For example, let n=3 when q (x) is t (x−(z +z +tz z ))(x−(z +z +tz z ))(x−(z +z +tz z )) 1 2 1 2 1 3 1 3 2 3 2 3 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.2.
THE FUNDAMENTAL THEOREM OF ALGEBRA 449 and you can observe the assertion about the polynomial is true when you switch two dif- ferent z .
Thus the coeﬃcients of q (x) must be symmetric polynomials in the z with real i t i coeﬃcients.
Hence by Proposition F.1.3 these coeﬃcients are real polynomials in terms of the elementary symmetric polynomials p .
Thus by (6.2) the coeﬃcients of q (x) are real k t polynomials in terms of the a of the original polynomial.
Recall these were all real.
It k follows, and this is what was wanted(, that)qt(x) has all real coeﬃcients.
n Note that the degree of q (x) is because there are this number of ways to pick t 2 i<j out of {1,··· ,n}.
Now ( ) n n(n−1) ( ) = =2k−1m 2km−1 2 2 =2k−1(odd) and so by induction, for each t∈R,q (x) has a complex root.
t There must exist s̸=t such that for a single pair of indices i,j, with i<j, (z +z +tz z ),(z +z +sz z ) i j i j i j i j are both complex.
Here is why.
Let A(i,j) denote those t∈R such that (z +z +tz z ) is i j i j complex.
It was just shown that every t ∈ R must be in some A(i,j).
There are inﬁnitely many t∈R and so some A(i,j) contains two of them.
Now for that t,s, z +z +tz z = a i j i j z +z +sz z = b i j i j where t̸=s and so by Cramer’s rule, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) a t (cid:12) (cid:12) (cid:12) b s zi+zj = (cid:12)(cid:12) (cid:12)(cid:12) ∈C (cid:12) 1 t (cid:12) (cid:12) (cid:12) 1 s and also (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 a (cid:12) (cid:12) (cid:12) 1 b zizj = (cid:12)(cid:12) (cid:12)(cid:12) ∈C (cid:12) 1 t (cid:12) (cid:12) (cid:12) 1 s At this point, note that z ,z are both solutions to the equation i j x2−(z +z )x+z z =0, 1 2 1 2 whichfromtheabovehascomplexcoeﬃcients.
Bythequadraticformulathez ,z areboth i j complex.
Thus the original polynomial has a complex root.
(cid:4) Withthislemma,itiseasytoprovethefundamentaltheoremofalgebra.
Thediﬀerence betweenthelemmaandthistheoremisthatinthetheorem,thecoeﬃcientsareonlyassumed tobecomplex.
Whatthismeansisthatifyouhaveanypolynomialwithcomplexcoeﬃcients ithasacomplexrootandsoitisnotirreducible.
Hencetheﬁeldextensionisthesameﬁeld.
Another way to say this is that for every complex polynomial there exists a factorization into linear factors or in other words a splitting ﬁeld for a complex polynomial is the ﬁeld of complex numbers.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation450 FIELDS AND FIELD EXTENSIONS Theorem F.2.3 Let p(x)≡anxn+an−1xn−1+···+a1x+a0 be any complex polynomial, n ≥ 1,a ̸= 0.
Then it has a complex root.
Furthermore, there exist complex numbers n z ,··· ,z such that 1 n ∏n p(x)=a (x−z ) n k k=1 Proof: First suppose a =1.
Consider the polynomial n q(x)≡p(x)p(x) this is a polynomial and it has real coeﬃcients.
This is because it equals ( ) (xn+an−1xn−1+···+a1x+a0)· xn+an−1xn−1+···+a1x+a0 The xj+k term of the above product is of the form a xka xj +a xka xj =xk+j(a a +a a ) k j k j k j k j and a a +a a =a a +a a k j k j k j k j so it is of the form of a complex number added to its conjugate.
Hence q(x) has real coeﬃcientsasclaimed.
Therefore,bybyLemmaF.2.2ithasacomplexrootz.
Henceeither p(z)=0 or p(z)=0.
Thus p(x) has a complex root.
Next suppose a ̸= 0.
Then simply divide by it and get a polynomial in which a = 1. n n Denote this modiﬁed polynomial as q(x).
Then by what was just shown and the Euclidean algorithm, there exists z ∈C such that 1 q(x)=(x−z )q (x) 1 1 where q (x) has complex coeﬃcients.
Now do the same thing for q (x) to obtain 1 1 q(x)=(x−z )(x−z )q (x) 1 2 2 and continue this way.
Thus ∏n p(x) = (x−z ) (cid:4) a j n j=1 Obviously this is a harder proof than the other proof of the fundamental theorem of algebra presented earlier.
However, this is a better proof.
Consider the algebraic num- bers A consisting of the real numbers which are roots of some polynomial having rational coeﬃcients.
By Theorem 8.3.32 they are a ﬁeld.
Now consider the ﬁeld A+iA with the usual conventions for complex arithmetic.
You could repeat the above argument with small changes and conclude that every polynomial having coeﬃcients in A+iA has a root in A+iA.
RecallfromProblem41onPage223thatAiscountableandsothisisalsothecase for A+iA.
Thus this gives an algebraically complete ﬁeld which is countable and so very diﬀerent than C. Of course there are other situations in which the above harder proof will work and yield interesting results.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.3.
TRANSCENDENTAL NUMBERS 451 F.3 Transcendental Numbers Most numbers are like this.
Here the algebraic numbers are those which are roots of a polynomial equation having rational numbers as coeﬃcients.
By the fundamental theorem of calculus, all these numbers are in C. There are only countably many of these algebraic numbers, (Problem 41 on Page 223).
Therefore, most numbers are transcendental.
Never- theless, it is very hard to prove that this or that number is transcendental.
Probably the most famous theorem about this is the Lindemannn Weierstrass theorem.
Theorem F.3.1 Let the α be distinct nonzero algebraic numbers and let the a be nonzero i i algebraic numbers.
Then ∑n a eai ̸=0 i i=1 IamfollowingtheinterestingWikepediaarticleonthissubject.
Youcanalsolookatthe book by Baker [4], Transcendental Number Theory, Cambridge University Press.
There are also many other treatments which you can ﬁnd on the web including an interesting article by Steinberg and Redheﬀer which appeared in about 1950.
The proof makes use of the following identity.
For f(x) a polynomial, ∫ s d∑eg(f) d∑eg(f) I(s)≡ es−xf(x)dx=es f(j)(0)− f(j)(s).
(6.3) 0 j=0 j=0 where f(j) denotes the jth derivative.
In this formula, s ∈ C and the integral is deﬁned in the natural way as ∫ 1 sf(ts)es−tsdt (6.4) 0 The identity follows from integration by parts.
∫ ∫ 1 1 sf(ts)es−tsdt = ses f(ts)e−tsdt 0 [0 ∫ ] e−ts 1 e−ts = ses − f(ts)|1+ sf′(st)dt s 0 s [ 0 ∫ ] 1 e−s 1 = ses f(s)− f(0)+ e−tsf′(st)dt s s ∫ 0 1 = f(0)−esf(s)+ ses−tsf′(st)dt ∫0 s ≡ f(0)−f(s)es+ es−xf′(x)dx 0 Continuing this way establishes the identity.
Lemma F.3.2 If K and c are nonzero integers, and β ,··· ,β are the roots of a single 1 m polynomial with integer coeﬃcients, Q(x)=vxm+···+u where v,u̸=0, then ( ) K+c eβ1 +···+eβm ̸=0.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation452 FIELDS AND FIELD EXTENSIONS Letting v(m−1)pQp(x)xp−1 f(x)= (p−1)!
and I(s) be deﬁned in terms of f(x) as above, it follows, ∑m lim I(β )=0 p→∞ i i=1 and ∑n f(j)(0)=vp(m−1)up+m (p)p 1 j=0 ∑m ∑n f(j)(β )=m (p)p i 2 i=1j=0 where m (p) is some integer.
i Proof: Let p be a prime number.
Then consider the polynomial f(x) of degree n ≡ pm+p−1, v(m−1)pQp(x)xp−1 f(x)= (p−1)!
From (6.3)   ∑m ∑m ∑n ∑n c I(β )=c eβi f(j)(0)− f(j)(β ) i i i=1 i=1 j=0 j=0 ( ) ∑m ∑n ∑n ∑m ∑n = K+c eβi f(j)(0)−K f(j)(0)−c f(j)(β ) (6.5) i i=1 j=0 j=0 i=1j=0 ∑ Claim 1: limp→∞ c mi=1I(βi)=0.
( ) Proof: This follows right away from the deﬁnition of I β and the deﬁnition of f(x).
j ∫ (cid:12) ( )(cid:12) 1(cid:12) ( ) (cid:12) (cid:12)I β (cid:12)≤ (cid:12)β f tβ eβj−tβj(cid:12)dt j j j 0 ∫ 1(cid:12)(cid:12)(cid:12)|v|(m−1)p(cid:12)(cid:12)Q(tβ )(cid:12)(cid:12)ptp−1(cid:12)(cid:12)β (cid:12)(cid:12)p−1 (cid:12)(cid:12)(cid:12) ≤ (cid:12) j j dt(cid:12) (cid:12) (p−1)!
(cid:12) 0 which clearly converges to 0.
This proves the claim.
The next thing to consider is the term on the end in (6.5), ∑n ∑m ∑n K f(j)(0)+c f(j)(β ) (6.6) i j=0 i=1j=0 The idea is to show that∑for large enough p it is always an integer.
When this is done, it can’t happen that K+c m eβi =0 because if this were so, you would have a very small i=1 number equal to an integer.
Now   p z Q}(|x) {   v(m−1)pv(x−β )(x−β )···(x−β ) xp−1 1 2 m f(x) = (p−1)!
vmp((x−β )(x−β )···(x−β ))pxp−1 = 1 2 m (6.7) (p−1)!
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.3.
TRANSCENDENTAL NUMBERS 453 Itfollowsthatforj <p−1,f(j)(0)=0.Thisisbecauseofthattermxp−1.
Ifj ≥p,f(j)(0) is an integer multiple of p. Here is why.
The terms in this derivative which are nonzero involve taking p−1 derivatives of xp−1 and this introduces a (p−1)!
which cancels out the denominator.
Then there are some other derivatives of the product of the (x−β ) raised i to the power p. By the chain rule, these all involve a multiple of p. Thus this jth derivative is of the form pg(x,vβ ,··· ,vβ ), (6.8) 1 m where g(x,vβ ,··· ,vβ ) is a polynomial in x with coeﬃcients which are symmetric poly- 1 m nomials in {vβ ,··· ,vβ } having integer coeﬃcients.
Then derivatives of g with respect 1 m to x also yield polynomials in x which have coeﬃcients which are symmetric polynomials in {vβ ,··· ,vβ } having integer coeﬃcients.
Evaluating g at x = 0 must therefore yield 1 m a polynomial which is symmetric in the {vβ ,··· ,vβ } with integer coeﬃcients.
Since the 1 m {β ,··· ,β } are the roots of a polynomial having integer coeﬃcients with leading coeﬃ- 1 m cient v, it follows from Theorem F.1.4 that this last polynomial is an integer and so the jth derivative of f given by (6.8) when evaluated at x = 0 yields an integer times p. Now consider the case of the (p−1) derivative of f. The only nonzero term of f(j)(0) is the one which comes from taking p−1 derivatives of xp−1 and so it reduces to vmp(−1)mp(β β ···β )p 1 2 m Now Q(0)=v(−1)m(β β ···β )=u and so vp(−1)mp(β β ···β )p =up which yields 1 2 m 1 2 m f(p−1)(0)=vmpupv−p =vp(m−1)up Note this is not necessarily a multiple of p and in fact will not be so if p>u,v because p is a prime number.
It follows ∑n f(j)(0)=vp(m−1)up+m(p)p j=0 where m(p) is some integer.
Now consider the other sum in (6.6), ∑m ∑n c f(j)(β ) i i=1j=0 Using the formula in (6.7) it follows that for j < p,f(j)(β ) = 0.
This is because for such i derivatives, each term will have that product of the (x−β ) in it.
Next consider the case i where j ≥ p. In this case, the nonzero terms must involve at least p derivatives of the expression ((x−β )(x−β )···(x−β ))p 1 2 m since otherwise, when evaluated at any β the result would be 0.
Hence the (p−1)!
will k vanish from the denominator and so all coeﬃcients of the polynomials in the β and x will j be integers and in fact, there will be an extra factor of p left over.
Thus the jth derivatives for j ≥p involve taking the kth derivative, k ≥0 with respect to x of pvmpg(x,β ,··· ,β ) 1 m where g(x,β ,··· ,β ) is a polynomial in x having coeﬃcients which are integers times 1 m symmetric polynomials in the {β ,··· ,β }.
It follows that the kth derivative for k ≥ 0 1 m Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation454 FIELDS AND FIELD EXTENSIONS is also a polynomial in x having the same properties.
Therefore, taking the kth derivative where k corresponds to j ≥p and adding, yields ∑m ∑m pvmpg (β ,β ,··· ,β )= f(j)(β ) (6.9) ,k i 1 m i i=1 i=1 where g denotes the kth derivative of g taken with respect to x.
Now ,k ∑m g (β ,β ,··· ,β ) ,k i 1 m i=1 is a symmetric polynomial in the {β ,··· ,β } with no term having degree more than mp 1 m and1 so by Corollary F.1.3 this is of the form ∑m ∑ g,k(βi,β1,··· ,βm)= ak1···kmpk11···pkmm i=1 k1,···,km where the ak1···km are integers and the pk are the elementary symmetric polynomials in {β ,··· ,β }.
Recall these were roots of vxm +···+u and so from the deﬁnition of the 1 m elementary symmetric polynomials given in Deﬁnition F.1.2, these p are each an integer k divided by v, the integers being the coeﬃcients of Q(x).
Therefore, from (6.9) ∑m ∑m f(j)(β )=pvmp g (β ,β ,··· ,β ) i ,k i 1 m i=1 i=1 ∑ =pvmp ak1···kmpk11···pkmm k1,···,km which is pvmp times an expression which consists of integers times products of coeﬃcients of Q(x) divided by v raised to various powers, the sum of which is always no more than mp.
Therefore, it reduces to an integer multiple of p and so the same is true of ∑m ∑n c f(j)(β ) i i=1j=0 which just involves adding up these integer multiples of p. Therefore, (6.6) is of the form Kvp(m−1)up+M(p)p for some integer M(p).
Summarizing, it follows ( ) ∑m ∑m ∑n c I(β )= K+c eβi f(j)(0)+Kvp(m−1)up+M(p)p i i=1 i=1 j=0 wheretheleftsideisverysmallwheneverpislargeenough.
Letpbelargerthanmax(K,v,u).
Sincepisprime,itfollowsitcannotdivideKvp(m−1)up andsothelasttwotermsmustsum to a nonzero integer and so the equation cannot hold unless ∑m K+c eβi ̸=0 (cid:4) i=1 1Notetheclaimaboutthisbeingasymmetricpolynomialisaboutthesum,notanindividualterm.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.3.
TRANSCENDENTAL NUMBERS 455 Note this shows π is irrational.
If π = k/m where k,m are integers, then both iπ and −iπ are roots of the polynomial with integer coeﬃcients, m2x2+k2 which would require from what was just shown that 0̸=2+eiπ+e−iπ which is not the case since the sum on the right equals 0.
The following corollary follows from this.
Corollary F.3.3 Let K and c for i=1,··· ,n be nonzero integers.
For each k between 1 i and n let {β(k) }m(k) be the roots of a polynomial with integer coeﬃcients, i i=1 Qk(x)≡vkxmk +···+uk where v ,u ̸=0.
Then k k       ∑m1 ∑m2 ∑mn K+c1 eβ(1)j+c2 eβ(2)j+···+cn eβ(n)j̸=0.
j=1 j=1 j=1 Proof: Deﬁning f (x) and I (s) as in Lemma F.3.2, it follows from Lemma F.3.2 that k k for each k =1,··· ,n, ( ) ∑mk ∑mk de∑g(fk) ck Ik(β(k)i) = Kk+ck eβ(k)i fk(j)(0) i=1 i=1 j=0 de∑g(fk) ∑mk de∑g(fk) −K f(j)(0)−c f(j)(β(k) ) k k k k i j=0 i=1 j=0 This is exactly the same computation as in the beginning of that lemma except one adds ∑ ∑ and subtracts K deg(fk)f(j)(0) rather than K deg(fk)f(j)(0) where the K are chosen k j=0 k j=0 k k such that their sum equals K. By Lemma F.3.2, ( ) ∑mk ∑mk ( ) ck Ik(β(k)i)= Kk+ck eβ(k)i vk(mk−1)pupk+Nkp i=1 i=1 ( ) −K v(mk−1)pup +N p −c N′p k k k k k k and so ( ) ∑mk ∑mk ( ) ck Ik(β(k)i)= Kk+ck eβ(k)i vk(mk−1)pupk+Nkp i=1 i=1 −K v(mk−1)pup +M p k k k k for some integer M .
By multiplying each Q (x) by a suitable constant, it can be assumed k k without loss of generality that all the vmk−1u are equal to a constant integer U.
Then the k k above equals ( ) ∑mk ∑mk ck Ik(β(k)i)= Kk+ck eβ(k)i (Up+Nkp) i=1 i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation456 FIELDS AND FIELD EXTENSIONS −K Up+M p k k Adding these for all k gives ( ) ∑n ∑mk ∑n ∑mk ck Ik(β(k)i)=Up K+ ck eβ(k)i −KUp+Mp k=1 i=1 k=1 i=1 ( ) ∑n ∑mk + Nkp Kk+ck eβ(k)i (6.10) k=1 i=1 For large p it follows from Lemma F.3.2 that the left side is very small.
If ∑n ∑mk K+ ck eβ(k)i =0 k=1 i=1 ∑ ∑ then nk=1ck mi=k1eβ(k)i is an integer and so the last term in (6.10) is an integer times p. Thus for large p it reduces to small number=−KUp+Ip where I is an integer.
Picking prime p > max(U,K) it follows −KUp +Ip is a nonzero integerandthiscontradictstheleftsidebeingasmallnumberlessthan1inabsolutevalue.
(cid:4) Next is an even more interesting Lemma which follows from the above corollary.
Lemma F.3.4 If b ,b ,··· ,b are non zero integers, and γ ,··· ,γ are distinct algebraic 0 1 n 1 n numbers, then b0eγ0 +b1eγ1 +···+bneγn ̸=0 Proof: Assume b0eγ0 +b1eγ1 +···+bneγn =0 (6.11) Divide by eγ0 and letting K =b0, K+b eα(1)+···+b eα(n) =0 (6.12) 1 n where α(k)=γ −γ .
These are still distinct algebraic numbers none of which is 0 thanks k 0 to Theorem 8.3.32.
Therefore, α(k) is a root of a polynomial vkxmk +···+uk (6.13) having integer coeﬃcients, v ,u ̸= 0.
Recall algebraic numbers were deﬁned as roots of k k polynomialequationshavingrationalcoeﬃcients.
Justmultiplybythedenominatorstoget one with integer coeﬃcients.
Let the roots of this polynomial equation be { } α(k) ,··· ,α(k) 1 mk and suppose they are listed in such a way that α(k) = α(k).
Letting i be an integer in 1 k {1,··· ,m } it follows from the assumption (6.11) that k ∏ ( ) K+b1eα(1)i1 +b2eα(2)i2 +···+bneα(n)in =0 (6.14) (i1,···,in) ik∈{1,···,mk} Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.3.
TRANSCENDENTAL NUMBERS 457 Thisisbecauseoneofthefactorsistheoneoccurringin(6.12)wheni =1foreveryk.
The k product is taken over all distinct ordered lists (i ,··· ,i ) where i is as indicated.
Expand 1 n k this possibly huge product.
This will yield something like the following.
( ) ( ) K′+c1 eβ(1)1 +···+eβ(1)(cid:22)(1) +c2 eβ(2)1 +···+eβ(2)(cid:22)(2) +···+ ( ) cN eβ(N)1 +···+eβ(N)(cid:22)(N) =0 (6.15) Theseintegersc comefromproductsoftheb andK.Theβ(i) arethedistinctexponents j i j which result.
Note that a typical term in this product (6.14) would be something like z int}e|ger { z (cid:12)}(j|)r { Kp+1bk1···bkn−peα(k1)i1 +α(k2)i2···+α(kn−p)in−p the k possibly not distinct and each i ∈{1,··· ,m }.
Other terms of this sort are j k ik Kp+1bk1···bkn−peα(k1)i′1+α(k2)i′2···+α(kn−p)i′n−p, Kp+1bk1···bkn−peα(k1)1+α(k2)1···+α(kn−p)1 where each i′k is another index in{{1,··· ,mik} a}nd so forth.
A given j in the sum of (6.15) corresponds to such a choice of b ,··· ,b which leads to Kp+1b ···b times a k1 kn−p k1 kn−p sum of exponentials like those just described.
Since the product in (6.14) is taken over all choices i ∈{1,··· ,m }, it follows that if you switch α(r) and α(r) , two of the roots of k k i j the polynomial v xmr +···+u r r mentioned above, the result in (6.15) would be the same except for permuting the β(s) ,β(s) ,··· ,β(s) .
1 2 µ(s) Thus a symmetric polynomial in β(s) ,β(s) ,··· ,β(s) 1 2 µ(s) is also a symmetric polynomial in the α(k) ,α(k) ,··· ,α(k) for each k. Thus for a given r,β(r) ,··· ,β(r) are roots of the p1olynom2ial mk 1 µ(r) ( ) (x−β(r) )(x−β(r) )··· x−β(r) 1 2 µ(r) whosecoeﬃcientsaresymmetricpolynomialsintheβ(r) whichisasymmetricpolynomial j in the α(k) ,j = 1,··· ,m for each k. Letting g be one of these symmetric polynomials j k and writing it in terms of the α(k) you would have i ∑ Al1···lnα(n)l11α(n)l22···α(n)lmnn l1,···,ln where Al1···ln is a symmetric polynomial in α(k)j,j =1,··· ,mk for each k ≤n−1.
These coeﬃcientsareintheﬁeld(Proposition8.3.31)Q[A(1),··· ,A(n−1)]whereA(k)denotes { } α(k) ,··· ,α(k) 1 mk and so from Proposition F.1.3, the above symmetric polynomial is of the form ∑ ( ) ( ) Bk1···kmnpk11 α(n)1,··· ,α(n)mn ···pkmmnn α(n)1,··· ,α(n)mn (k1···kmn) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation458 FIELDS AND FIELD EXTENSIONS whereBk1···kmn isasymmetricpolynomialinα(k)j,j =1,··· ,mk foreachk ≤n−1.
Now do for each Bk1···kmn what was just done for g featuring this time { } α(n−1) ,··· ,α(n−1) 1 mn−1 and continuing this way, it must be the case that eventually you have a sum of integer multiples of products of elementary symmetric polynomials in α(k) ,j = 1,··· ,m for j k each k ≤n.
By Theorem F.1.4, these are each rational numbers.
Therefore, each such g is a rational number and so the β(r) are algebraic.
Now (6.15) contradicts Corollary F.3.3.
j (cid:4) Note this lemma is suﬃcient to prove Lindemann’s theorem that π is transcendental.
Here is why.
If π is algebraic, then so is iπ and so from this lemma, e0+eiπ ̸=0 but this is not the case because eiπ =−1.
The next theorem is the main result, the Lindemannn Weierstrass theorem.
Theorem F.3.5 Suppose a(1),··· ,a(n) are nonzero algebraic numbers and suppose α(1),··· ,α(n) are distinct algebraic numbers.
Then a(1)eα(1)+a(2)eα(2)+···+a(n)eα(n) ̸=0 Proof: Suppose a(j)≡a(j) is a root of the polynomial 1 v xmj +···+u j j where v ,u ̸= 0.
Let the roots of this polynomial be a(j) ,··· ,a(j) .
Suppose to the j j 1 mj contrary that a(1) eα(1)+a(2) eα(2)+···+a(n) eα(n) =0 1 1 1 Then consider the big product ∏ ( ) a(1) eα(1)+a(2) eα(2)+···+a(n) eα(n) (6.16) i1 i2 in (i1,···,in) ik∈{1,···,mk} the product taken over all ordered lists (i ,··· ,i ).
This product equals 1 n 0=b eβ(1)+b eβ(2)+···+b eβ(N) (6.17) 1 2 N where the β(j) are the distinct exponents which result.
The β(i) are clearly algebraic because they are the sum of the α(i).
Since the product in (6.16) is taken for all ordered listsasdescribedabove,itfollowsthatforagivenk,ifα(k) isswitchedwithα(k) ,thatis, i j twooftherootsofvkxmk+···+uk areswitched,thentheproductisunchangedandso(6.17) is also unchanged.
Thus each b is a symmetric polynomial in the a(k) ,j =1,··· ,m for k j k each k. It follows ∑ bk = Aj1,···,jmna(n)j11···a(n)jmmnn ({j1,···,jmn) } aﬁnelddt(hPisroipsossyitmiomne8t.r3ic.3i1n)tQhe[A(a1()n,)·1··,·,·A· (,na(−n)1m)]nwhtehreecAoe(kﬃ)cdieenntostAesj1,···,jmn being in the a(k) ,··· ,a(k) 1 mk Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.4.
MORE ON ALGEBRAIC FIELD EXTENSIONS 459 and so from Proposition F.1.3, ∑ ( ) ( ) bk = Bj1,···,jmnpj11 a(n)1···a(n)mn ···pjmmnn a(n)1···a(n)mn (j1,···,jmn) { } where the Bj1,···,jmn are symmetric in a(k)j mj=k1 for each k ≤ n − 1.
Now doing to Binjt1e,g··e·,rjsmtnimwehsatelwemasenjutasrtydponoelytnoombkiaalnsdincothnteinvuairniogutsh{isaw(kay),}itmfkollfoowrskbk≤isn.a BﬁnyitTehseuomremof j j=1 F.1.4 this is a rational number.
Thus b is a rational number.
Multiplying by the product k of all the denominators, it follows there exist integers c such that i 0=c eβ(1)+c eβ(2)+···+c eβ(N) 1 2 N which contradicts Lemma F.3.4.
(cid:4) This theorem is suﬃcient to show e is transcendental.
If it were algebraic, then ee−1+(−1)e0 ̸=0 but this is not the case.
If a̸=1 is algebraic, then ln(a) is transcendental.
To see this, note that 1eln(a)+(−1)ae0 =0 whichcannothappenaccordingtotheabovetheorem.
Ifaisalgebraicandsin(a)̸=0,then sin(a) is transcendental because 1 1 eia− e−ia+(−1)sin(a)e0 =0 2i 2i which cannot occur if sin(a) is algebraic.
There are doubtless other examples of numbers which are transcendental by this amazing theorem.
F.4 More On Algebraic Field Extensions The next few sections have to do with ﬁelds and ﬁeld extensions.
There are many linear algebratechniqueswhichareusedinthisdiscussionanditseemstometobeveryinteresting.
However,thisisdeﬁnitelyfarremovedfrommyownexpertisesotheremaybesomepartsof this which are not too good.
I am following various algebra books in putting this together.
Considerthenotionofsplittingﬁelds.
Itisdesiredtoshowthatanytwoareisomorphic, meaning that there exists a one to one and onto mapping from one to the other which preserves all the algebraic structure.
To begin with, here is a theorem about extending homomorphisms.
[17] Deﬁnition F.4.1 Suppose F,F¯ are two ﬁelds and that f :F→F¯ is a homomorphism.
This means that f(xy)=f(x)f(y), f(x+y)=f(x)+f(y) An isomorphism is a homomorphism which is one to one and onto.
A monomorphism is a homomorphism which is one to one.
An automorphism is an isomorphism of a single ﬁeld.
Sometimes people use the symbol ≃ to indicate something is an isomorphism.
Then if p(x)∈F[x], say ∑n p(x)= a xk, k k=0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation460 FIELDS AND FIELD EXTENSIONS p¯(x) will be the polynomial in F¯[x] deﬁned as ∑n p¯(x)≡ f(a )xk.
k k=0 Also consider f as a homomorphism of F[x] and F¯[x] in the obvious way.
f(p(x))=p¯(x) The following is a nice theorem which will be useful.
Theorem F.4.2 Let F be a ﬁeld and let r be algebraic over F. Let p(x) be the minimal polynomial of r. Thus p(r) = 0 and p(x) is monic and no nonzero polynomial having coeﬃcients in F of smaller degree has r as a root.
In particular, p(x) is irreducible over F. Then deﬁne f :F[x]→F[r], the polynomials in r by ( ) ∑m ∑m f a xi ≡ a ri i i i=0 i=0 Then f is a homomorphism.
Also, deﬁning g :F[x]/(p(x)) by g([q(x)])≡f(q(x))≡q(r) it follows that g is an isomorphism from the ﬁeld F[x]/(p(x)) to F[r] .
Proof: First of all, consider why f is a homomorphism.
The preservation of sums is obvious.
Consider products.
    ∑ ∑ ∑ ∑ f a xi b xj = f a b xi+j= a b ri+j i j i j i j i j i,j ij ( )   ∑ ∑ ∑ ∑ = a ri b rj =f a xi f b xj i j i j i j i j Thus it is clear that f is a homomorphism.
First consider why g is even well deﬁned.
If [q(x)]=[q (x)], this means that 1 q (x)−q(x)=p(x)l(x) 1 for some l(x)∈F[x].
Therefore, f(q (x)) = f(q(x))+f(p(x)l(x)) 1 = f(q(x))+f(p(x))f(l(x)) ≡ q(r)+p(r)l(r)=q(r)=f(q(x)) Now from this, it is obvious that g is a homomorphism.
g([q(x)][q (x)]) = g([q(x)q (x)])=f(q(x)q (x))=q(r)q (r) 1 1 1 1 g([q(x)])g([q (x)]) ≡ q(r)q (r) 1 1 Similarly,gpreservessums.
Nowwhyisgonetoone?Itsuﬃcestoshowthatifg([q(x)])=0, then [q(x)]=0.
Suppose then that g([q(x)])≡q(r)=0 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.4.
MORE ON ALGEBRAIC FIELD EXTENSIONS 461 Then q(x)=p(x)l(x)+ρ(x) where the degree of ρ(x) is less than the degree of p(x) or else ρ(x)=0.
If ρ(x)̸=0, then it follows that ρ(r)=0 andρ(x)hassmallerdegreethanthatofp(x)whichcontradictsthedeﬁnitionofp(x)asthe minimal polynomial of r. Since p(x) is irreducible, F[x]/(p(x)) is a ﬁeld.
It is clear that g is onto.
Therefore, F[r] is a ﬁeld also.
(This was shown earlier by diﬀerent reasoning.)
(cid:4) Here is a diagram of what the following theorem says.
Extending f to g F →f F¯ ≃ p(x)∑∈F[x] →f ∑ p¯(x)∈F¯[x] p(x)= n a xk → n f(a )xk =p¯(x) k=0 k k=0 k p(r)=0 p¯(r¯)=0 F[r] →g F¯[r¯] ≃ r →g r¯ One such g for each r¯ Theorem F.4.3 Let f : F→F¯ be an isomorphism of the two ﬁelds.
Let r be algebraic over F with minimal polynomial p(x) and suppose there exists r¯algebraic over F¯ such that p¯(r¯) = 0.
Then there exists an isomorphism g : F[r] → F¯[r¯] which agrees with f on F. If g :F[r]→F¯[r¯] is an isomorphism which agrees with f on F and if α([k(x)])≡k(r) is the homomorphism mapping F[x]/(p(x)) to F[r], then there must exist r¯ such that p¯(r¯) = 0 and g =βα−1 where β β :F[x]/(p(x))→F¯[r¯] is given by β([k(x)])=k¯(r¯).
In particular, g(r)=r¯.
Proof: From Theorem F.4.2, there exists α, an isomorphism in the following picture, α([k(x)])=k(r).
F[r]←α F[x]/(p(x))→β F¯[r¯] where β([k(x)])≡k¯(r¯).
(k¯(x) comes from f as described in the above deﬁnition.)
This β is a well deﬁned monomorphism because of the assumption that p¯(r¯)=0.
This needs to be veriﬁed.
Assume then that it is so.
Then just let g =βα−1.
Whyisβ welldeﬁned?
Suppose[k(x)]=[k′(x)]sothatk(x)−k′(x)=l(x)p(x).Then since f is a homomorphism, k¯(x)−k¯′(x)=¯l(x)p¯(x), k¯(r¯)−g¯k¯′(r¯)=¯l(r¯)p¯(r¯)=0 soβisindeedwelldeﬁned.
Itisclearfromthedeﬁnitionthatβisahomomorphism.
Suppose β([k(x)])=0.
Does it follow that [k(x)]=0?
By assumption, g¯(r¯)=0 and also, k¯(x)=p¯(x)¯l(x)+ρ¯(x) where the degree of ρ¯(x) is less than the degree of p¯(x) or else it equals 0.
But then, since f is an isomorphism, k(x)=p(x)l(x)+ρ(x) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation462 FIELDS AND FIELD EXTENSIONS where the degree of ρ(x) is less than the degree of p(x).
However, the above shows that ρ(r)=0 contrary to p(x) being the minimal polynomial.
Hence ρ(x)=0 and this implies that [k(x)] = 0.
Thus β is one to one and a homomorphism.
Hence g = βα−1 works if it is also onto.
However, it is clear that α−1 is onto and that β is onto.
Hence the desired extension exists.
Now suppose such an isomorphism g exists.
Then r¯must equal g(r) and 0=g(p(r))=p¯(g(r))=p¯(r¯) Hence, β can be deﬁned as above as β([k(x)])≡k¯(r¯) relative to this r¯≡g(r) and βα−1(k(r))≡β([k(x)])≡k¯(g(r))=g(k(r)) (cid:4) What is the meaning of the above in simple terms?
It says that the monomorphisms from F[r] to a ﬁeld K¯ containing F¯ correspond to the roots of p¯(x) in K¯.
That is, for each root of p¯(x), there is a monomorphism and for each monomorphism, there is a root.
Also, for each root r¯of p¯(x) in K¯, there is an isomorphism from F[r] to F¯[r¯].
Note that if p(x) is a monic irreducible polynomial, then it is the minimal polynomial for each of its roots.
This is the situation which is about to be considered.
It involves the splittingﬁeldsK,K¯ ofp(x),p¯(x)whereη isanisomorphismofFandF¯ asdescribedabove.
See [17].
Here is a little diagram which describes what this theorem says.
Deﬁnition F.4.4 Thesymbol[K:F]whereKisaﬁeldextensionofFmeansthedimension of the vector space K with ﬁeld of scalars F. F →η F¯ ≃ p(x) “ηp(x)=p¯(x)” p¯(x) F[r ,··· ,r ] →ζi F¯[r ,··· ,r ] 1 n { ≃ 1 n m≤[K:F] i=1,··· ,m, m=[K:F],r¯ ̸=r¯ i j Theorem F.4.5 Let η be an isomorphism from F to F¯ and let K = F[r ,··· ,r ],K¯ = 1 n F¯[r¯ ,··· ,r¯ ] be splitting ﬁelds of p(x) and p¯(x) respectively.
Then there exist at most 1 n [K:F] isomorphisms ζ : K→K¯ which extend η.
If {r¯ ,··· ,r¯ } are distinct, then there i 1 n exist exactly [K:F] isomorphisms of the above sort.
In either case, the two splitting ﬁelds are isomorphic with any of these ζ serving as an isomorphism.
i Proof: Suppose [K:F]=1.
Say a basis for K is {r}.
Then {1,r} is dependent and so there exist a,b∈F, not both zero such that a+br =0.
Then it follows that r ∈F and so in this case F=K.
Then the isomorphism which extends η is just η itself and there is exactly 1 isomorphism.
Next suppose [K:F] > 1.
Then p(x) has an irreducible factor over F of degree larger than 1, q(x).
If not, you would have p(x)=xn+an−1xn−1+···+an and it would factor as =(x−r )···(x−r ) 1 n with each r ∈F, so F=K contrary to [K:F]>1.Without loss of generality, let the roots j of q(x) in K be {r ,··· ,r }.
Thus 1 m ∏m ∏n q(x)= (x−r ), p(x)= (x−r ) i i i=1 i=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.4.
MORE ON ALGEBRAIC FIELD EXTENSIONS 463 Now q¯(x) deﬁned analogously to p(x), also has degree at least 2.
Furthermore, it divides p¯(x) all of whose roots are in K¯.
Denote the roots of q¯(x) in K as {r¯ ,··· ,r¯ } where they 1 m are counted according to multiplicity.
Then from Theorem F.4.3, there exist k ≤ m one to one homomorphisms ζ mapping i F[r ] to K¯, one for each distinct root of q¯(x) in K¯.
If the roots of p¯(x) are distinct, then 1 this is suﬃcient to imply that the roots of q¯(x) are also distinct, and k = m. Otherwise, maybe k <m.
(It is conceivable that q¯(x) might have repeated roots in K¯.)
Then [K:F]=[K:F[r ]][F[r ]:F] 1 1 andsincethedegreeofq(x)>1andq(x)isirreducible,thisshowsthat[F[r ]:F]=m>1 1 and so [K:F[r ]]<[K:F] 1 Therefore, by induction, each of these k ≤ m = [F[r ]:F] one to one homomorphisms 1 extends to an isomorphism from K to K¯ and for each of these ζ , there are no more than i [K:F[r ]] of these isomorphisms extending F. If the roots of p¯(x) are distinct, then there 1 are exactly m of these ζ and for each, there are [K:F[r ]] extensions.
Therefore, if the i 1 roots of p¯(x) are distinct, this has identiﬁed [K:F[r ]]m=[K:F[r ]][F[r ]:F]=[K:F] 1 1 1 isomorphisms of K to K¯ which agree with η on F. If the roots of p¯(x) are not distinct, then maybe there are fewer than [K:F] extensions of η.
Is this all of them?
Suppose ζ is such an isomorphism of K and K¯.
Then consider its restriction to F[r ].
By Theorem F.4.3, this restriction must coincide with one of the ζ 1 i chosen earlier.
Then by induction, ζ is one of the extensions of the ζ just mentioned.
(cid:4) i Deﬁnition F.4.6 Let K be a ﬁnite dimensional extension of a ﬁeld F such that every el- ement of K is algebraic over F, that is, each element of K is a root of some polynomial in F[x].
Then K is called a normal extension if for every k ∈ K all roots of the minimal polynomial of k are contained in K. So what are some ways to tell a ﬁeld is a normal extension?
It turns out that if K is a splitting ﬁeld of f(x)∈F[x], then K is a normal extension.
I found this in [17].
This is an amazing result.
Proposition F.4.7 LetKbeasplittingﬁeldoff(x)∈F[x].
ThenKisanormalextension.
In fact, if L is an intermediate ﬁeld between F and K, then L is also a normal extension of F. Proof: Let r ∈ K be a root of g(x), an irreducible monic polynomial in F[x].
It is required to show that every other root of g(x) is in K. Let the roots of g(x) in a splitting ﬁeldbe{r =r,r ,··· ,r }.
Nowg(x)istheminimalpolynomialofr overFbecauseg(x) 1 2 m j is irreducible.
Recall why this was.
If p(x) is the minimal polynomial of r , j g(x)=p(x)l(x)+r(x) wherer(x)eitheris0orithasdegreelessthanthedegreeofp(x).However, r(r )=0and j this is impossible if p(x) is the minimal polynomial.
Hence r(x) = 0 and now it follows that g(x) was not irreducible unless l(x)=1.
By Theorem F.4.3, there exists an isomorphism η of F[r ] and F[r ] which ﬁxes F and 1 j mapsr tor .
NowK[r ]andK[r ]aresplittingﬁeldsoff(x)overF[r ]andF[r ]respec- 1 j 1 j 1 j tively.
By Theorem F.4.5, the two ﬁelds K[r ] and K[r ] are isomorphic, the isomorphism, 1 j ζ extending η.
Hence [K[r ]:K]=[K[r ]:K] 1 j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation464 FIELDS AND FIELD EXTENSIONS But r ∈ K and so K[r ] = K. Therefore, K = K[r ] and so r is also in K. Thus all the 1 1 j j roots of g(x) are actually in K. Consider the last assertion.
Suppose r = r ∈ L where the minimal polynomial for r is denoted by q(x).
Then 1 letting the roots of q(x) in K be {r ,··· ,r }.
By Theorem F.4.3 applied to the identity 1 m map on L, there exists an isomorphism θ :L[r ] →L[r ] which ﬁxes L and takes r to r .
1 j 1 j But this implies that 1=[L[r ]:L]=[L[r ]:L] 1 j Hence r ∈L also.
Since r was an arbitrary element of L, this shows that L is normal.
(cid:4) j Deﬁnition F.4.8 WhenyouhaveF[a ,··· ,a ]witheacha algebraicsothatF[a ,··· ,a ] 1 m i 1 m is a ﬁeld, you could consider ∏m f(x)≡ f (x) i i=1 where f (x) is the minimal polynomial of a .
Then if K is a splitting ﬁeld for f(x), this K i i is called the normal closure.
It is at least as large as F[a ,··· ,a ] and it has the advantage 1 m of being a normal extension.
F.5 The Galois Group In the case where F=F¯, the above suggests the following deﬁnition.
Deﬁnition F.5.1 When K is a splitting ﬁeld for a polynomial p(x) having coeﬃcients in F, we say that K is a splitting ﬁeld of p(x) over the ﬁeld F. Let K be a splitting ﬁeld of p(x) over the ﬁeld F. Then G(K,F) denotes the group of automorphisms of K which leave F ﬁxed.
For a ﬁnite set S, denote by |S| as the number of elements of S. More generally, when K is a ﬁnite extension of L, denote by G(K,L) the group of automorphisms of K which leave L ﬁxed.
It is shown later that G(K,F) really is a group according to the strict deﬁnition of a group.
Forrightnow,justregarditasasetofautomorphismswhichkeepsFﬁxed.
Theorem F.4.5 implies the following important result.
Theorem F.5.2 Let K be a splitting ﬁeld of p(x) over the ﬁeld F. Then |G(K,F)|≤[K:F] When the roots of p(x) are distinct, equality holds in the above.
So how large is |G(K,F)| in case p(x) is a polynomial of degree n which has n distinct roots?
Letp(x)beamonicpolynomialwithrootsinK,{r ,··· ,r }andsupposethatnone 1 n of the r is in F. Thus i p(x) = xn+a xn−1+a xn−2+···+a 1 2 n ∏n = (x−r ), a ∈F k i k=1 Thus K consists of all rational functions in the r ,··· ,r .
Let σ be a mapping from 1 n {r ,··· ,r } to {r ,··· ,r }, say r → r .
In other words σ produces a permutation of 1 n 1 n j ij these roots.
Consider the following way of obtaining something in G(K,F) from σ.
If you haveatypicalthinginK,youcanobtainanotherthinginKbyreplacingeachr withr in j ij Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.5.
THE GALOIS GROUP 465 arationalfunction,aquotientoftwopolynomialswhichhavecoeﬃcientsinF.
Furthermore, ifyoudothis,youcanseerightawaythattheresultingmapformKtoKisobviouslyanau- tomorphism,preservingtheoperationsofmultiplicationandaddition.
DoesitkeepFﬁxed?
Of course.
You don’t change the coeﬃcients of the polynomials in the rational function which are always in F. Thus every permutation of the roots determines an automorphism of K. Now suppose σ is an automorphism of K. Does it determine a permutation of the roots?
0=σ(p(r ))=σ(p(σ(r ))) i i and so σ(r ) is also a root, say r .
Thus it is clear that each σ ∈ G(K,F) determines i ij a permutation of the roots.
Since the roots are distinct, it follows that |G(K,F)| equals the number of permutations of {r ,··· ,r } which is n!
and that there is a one to one 1 n correspondence between the permutations of the roots and G(K,F).
More will be done on this later after discussing permutation groups.
Thisisagoodtimetomakeaveryimportantobservationaboutirreduciblepolynomials.
Lemma F.5.3 Suppose q(x)̸=p(x) are both irreducible polynomials over a ﬁeld F. Then for K a ﬁeld which contains all the roots of both polynomials, there is no root common to both p(x) and q(x).
Proof: If l(x) is a monic polynomial which divides them both, then l(x) must equal 1.
Otherwise, it would equal p(x) and q(x) which would require these two to be equal.
Thus p(x) and q(x) are relatively prime and there exist polynomials a(x),b(x) having coeﬃcients in F such that a(x)p(x)+b(x)q(x)=1 Now if p(x) and q(x) share a root r, then (x−r) divides both sides of the above in K[x], but this is impossible.
(cid:4) Now here is an important deﬁnition of a class of polynomials which yield equality in the inequality of Theorem F.5.2.
Deﬁnition F.5.4 Let p(x) be a polynomial having coeﬃcients in a ﬁeld F. Also let K be a splitting ﬁeld.
Then p(x) is separable if it is of the form ∏m p(x)= q (x)ki i i=1 where each q (x) is irreducible over F and each q (x) has distinct roots in K. From the i i above lemma, no two q (x) share a root.
Thus i ∏m p (x)≡ q (x) 1 i i=1 has distinct roots in K. For example, consider the case where F=Q and the polynomial is of the form ( ) ( ) x2+1 2 x2−2 2 =x8−2x6−3x4+4x2+4 [ √ ] Then let K be the splitting ﬁeld over Q, Q i, 2 .The polynomials x2+1 and x2−2 are irreducible over Q and each has distinct roots in K. This is also a convenient time to show that G(K,F) for K a ﬁnite extension of F really is a group.
First, here is the deﬁnition.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation466 FIELDS AND FIELD EXTENSIONS Deﬁnition F.5.5 A group G is a nonempty set with an operation, denoted here as · such that the following axioms hold.
1.
For α,β,γ ∈G,(α·β)·γ =α·(β·γ).
We usually don’t bother to write the ·.
2.
There exists ι∈G such that αι=ια=α 3.
For every α∈G, there exists α−1 ∈G such that αα−1 =α−1α=ι.
Then why is G ≡ G(K,F), where K is a ﬁnite extension of F, a group?
If you simply look at the automorphisms of K then it is obvious that this is a group with the operation being composition.
Also, from Theorem F.4.5 |G(K,F)| is ﬁnite.
Clearly ι ∈ G. It is just the automorphism which takes everything to itself.
The operation in this case is just composition.
Thus the associative law is obvious.
What about the existence of the inverse?
Clearly,youcandeﬁnetheinverseofα,butdoesitﬁxF?
Ifα=ι,thentheinverseisclearly ι. Otherwise,considerα,α2,··· .Since|G(K,F)|is(ﬁnit)e,eventuallythereisarepeat.
Thus αm = αn, n > m. Simply multiply on the left by α−1 m to get ι = ααn−m.
Hence α−1 is a suitable power of α and so α−1 obviously leaves F ﬁxed.
Thus G(K,F) which has been called a group all along, really is a group.
Then the following corollary is the reason why separable polynomials are so important.
Also,onecanshowthatifFcontainsaﬁeldwhichisisomorphictoQtheneverypolynomial with coeﬃcients in F is separable.
This will be done later after presenting the big results.
This is equivalent to saying that the ﬁeld has characteristic zero.
In addition, the property of being separable holds in other situations which are described later.
Corollary F.5.6 Let K be a splitting ﬁeld of p(x) over the ﬁeld F. Assume p(x) is separable.
Then |G(K,F)|=[K:F] Proof: Just note that K is also the splitting ﬁeld of p (x), the product of the distinct 1 irreduciblefactorsandthatfromLemmaF.5.3,p (x)hasdistinctroots.
Thustheconclusion 1 follows from Theorem F.4.5.
(cid:4) What if L is an intermediate ﬁeld between F and K?
Then p (x) still has coeﬃcients in 1 L and distinct roots in K and so it also follows that |G(K,L)|=[K:L] Deﬁnition F.5.7 Let G be a group of automorphisms of a ﬁeld K. Then denote by K the G ﬁxed ﬁeld of G. Thus K ≡{x∈K:σ(x)=x for all σ ∈G} G Thus there are two new things, the ﬁxed ﬁeld of a group of automorphisms H denoted by K and the Gallois group G(K,L).
Howare these related?
First here is a simple lemma H which comes from the deﬁnitions.
Lemma F.5.8 Let K be an algebraic extension of L (each element of L is a root of some polynomial in L) for L,K ﬁelds.
Then ( ) G(K,L)=G K,KG(K,L) Proof: It is clear that L ⊆ KG(K,L) because if r ∈ L then by deﬁnition, everything in G(K,L) ﬁxes r and so r is in KG(K,L).
Therefore, ( ) G(K,L)⊇G K,KG(K,L) .
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.5.
THE GALOIS GROUP 467 Now let σ ∈ G(K,L) then it is one of the automorph(isms of K w)hich ﬁxes everything in the ﬁxed ﬁeld of G(K,L).
Thus, by deﬁnition, σ ∈ G K,KG(K,L) and so the two are the same.
(cid:4) Now the following says that you can start with L, go to the group G(K,L) and then to the ﬁxed ﬁeld of this group and end up back where you started.
More precisely, Proposition F.5.9 If K is a splitting ﬁeld of p(x) over the ﬁeld F for separable p(x), and if L is a ﬁeld between K and F, then K is also a splitting ﬁeld of p(x) over L and also L=KG(K,L) Proof: By the above lemma, and Corollary F.5.6, [ ][ ] |G(K,L)| = (cid:12)[K(:L]= K:K)(cid:12)G[(K,L) KG(K],L) :L [ ] = (cid:12)G K,KG(K,L) (cid:12) KG(K,L) :L =|G(K,L)| KG(K,L) :L [ ] which shows that KG(K,L) :L =1 and so, since L⊆KG(K,L), it follows that L=KG(K,L).
(cid:4) This has shown the following diagram in the context of K being a splitting ﬁeld of a separable polynomial over F and L being an intermediate ﬁeld.
L→G(K,L)→KG(K,L) =L In particular, every intermediate ﬁeld is a ﬁxed ﬁeld of a subgroup of G(K,F).
Is every subgroupofG(K,F)obtainedintheformG(K,L)forsomeintermediateﬁeld?
Thisinvolves another estimate which is apparently due to Artin.
I also found this in [17].
There is more there about some of these things than what I am including.
Theorem F.5.10 Let K bea ﬁeld and let G bea ﬁnite groupof automorphisms of K. Then [K:K ]≤|G| G Proof: Let G = {σ ,··· ,σ },σ = ι the identity map and suppose {u ,··· ,u } is a 1 n 1 1 m linearly independent set in K with respect to the ﬁeld K .
Suppose m>n.
Then consider G the system of equations σ (u )x +σ (u )x +···+σ (u )x =0 1 1 1 1 2 2 1 m m σ (u )x +σ (u )x +···+σ (u )x =0 2 1 1 2 2 2 2 m m .
(6.18) .
.
σ (u )x +σ (u )x +···+σ (u )x =0 n 1 1 n 2 2 n m m which is of the form Mx=0 for x ∈ Km.
Since M has more columns than rows, there exists a nonzero solution x ∈ Km to the above system.
Note that this could not happen if x∈Km becauseofindependenceof{u ,··· ,u }andthefactthatσ =ι.
Letthesolution G 1 m 1 xbeonewhichhastheleastpossiblenumberofnonzeroentries.
Withoutlossofgenerality, some x = 1 for some k. If σ (x ) = x for all x and for each r, then the x are each k r k k k k in K and so the ﬁrst equation above would be impossible as just noted.
Therefore, there G exists l ̸= k and σ such that σ (x ) ̸= x .
For purposes of illustration, say l > k. Now r r l l do σ to both sides of all the above equations.
This yields, after re ordering the resulting r equations a list of equations of the form σ (u )σ (x )+···+σ (u )1+···+σ (u )σ (x )+···+σ (u )σ (x )=0 1 1 r 1 1 k 1 l r l 1 m r m σ (u )σ (x )+···+σ (u )1+···+σ (u )σ (x )+···+σ (u )σ (x )=0 2 1 r 1 2 k 2 l r l 2 m r m .
.
.
σ (u )σ (x )+···+σ (u )1+···+σ (u )σ (x )+···+σ (u )σ (x )=0 n 1 r 1 n k n l r l n m r m Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation468 FIELDS AND FIELD EXTENSIONS This is because σ(1) = 1 if σ is an automorphism.
The original system in (6.18) is of the form σ (u )x +···+σ (u )1+···+σ (u )x +···+σ (u )x =0 1 1 1 1 k 1 l l 1 m m σ (u )x +···+σ (u )1+···+σ (u )x +···+σ (u )x =0 2 1 1 2 k 1 l l 2 m m .
.
.
σ (u )x +···+σ (u )1+···+σ (u )x +···+σ (u )x =0 n 1 1 n k 1 l l n m m Now replace the kth equation with the diﬀerence of the kth equations in the original system and the one in which σ was done to both sides of the equations.
Since σ (x ) ̸= x the r r l l result will be a linear system of the form My=0 where y̸=0 has fewer nonzero entries than x, contradicting the choice of x.
(cid:4) Withtheaboveestimate,hereisanotherrelationbetweentheﬁxedﬁeldsandsubgroups of automorphisms.
It doesn’t seem to depend on anything being a splitting ﬁeld of a separable polynomial.
Proposition F.5.11 Let H be a ﬁnite group of automorphisms deﬁned on a ﬁeld K. Then for K the ﬁxed ﬁeld, H G(K,K )=H H Proof: If σ ∈ H, then by deﬁnition, σ ∈ G(K,K ).
It is clear that H ⊆ G(K,K ).
H H Then by Proposition F.5.10 and Theorem F.5.2, |H|≥[K:K ]≥|G(K,K )|≥|H| H H and so H = G(K,K ).
(cid:4) H This leads to the following interesting correspondence in the case where K is a splitting ﬁeld of a separable polynomial over a ﬁeld F. Fixed ﬁelds L→β G(K,L) Subgroups of G(K,F) (6.19) K ←α H H Then αβL=L and βαH =H.
Thus there exists a one to one correspondence between the ﬁxed ﬁelds and the subgroups of G(K,F).
The following theorem summarizes the above result.
Theorem F.5.12 Let K be a splitting ﬁeld of a separable polynomial over a ﬁeld F. Then there exists a one to one correspondence between the ﬁxed ﬁelds K for H a subgroup of H G(K,F) and the intermediate ﬁelds as described in the above.
H ⊆ H if and only if 1 2 K ⊇K .
Also H1 H2 |H|=[K:K ] H Proof: The one to one correspondence is established above.
The claim about the ﬁxed ﬁeldsisobviousbecauseifthegroupislarger,thentheﬁxedﬁeldmustgetharderbecauseit ismorediﬃculttoﬁxeverythingusingmoreautomorphismsthanwithfewerautomorphisms.
Consider the estimate.
From Theorem F.5.10, |H| ≥ [K:K ].
But also, H = G(K,K ) H H from Proposition F.5.11 G(K,K )=H and from Theorem F.5.2, H |H|=|G(K,K )|≤[K:K ].
H H (cid:4) Note that from the above discussion, when K is a splitting ﬁeld of p(x) ∈ F[x], this impliesthatifLisanintermediateﬁeld,thenitisalsoaﬁxedﬁeldofasubgroupofG(K,F).
In fact, from the above, L=KG(K,L) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.6.
NORMAL SUBGROUPS 469 If H is a subgroup, then it is also the Galois group H =G(K,K ).
H By Proposition F.4.7, each of these intermediate ﬁelds L is also a normal extension of F. Nowthereisalsosomethingcalledanormalsubgroupwhichwillendupcorrespondingwith these normal ﬁeld extensions consisting of the intermediate ﬁelds between F and K. F.6 Normal Subgroups When you look at groups, one of the ﬁrst things to consider is the notion of a normal subgroup.
Deﬁnition F.6.1 Let G be a group.
Then a subgroup N is said to be a normal subgroup if whenever α∈G, α−1Nα⊆N The important thing about normal subgroups is that you can deﬁne the quotient group G/N.
Deﬁnition F.6.2 Let N be a subgroup of G. Deﬁne an equivalence relation ∼ as follows.
α∼β means α−1β ∈N Why is this an equivalence relation?
It is clear that α∼α because α−1α=ι∈N since N is a subgroup.
If α∼β, then α−1β ∈N and so, since N is a subgroup, ( ) α−1β −1 =β−1α∈N which shows that β ∼ α.
Now suppose αα ∼ β, then α−1β ∈ N and so, since N is a subgroup, ( ) α−1β −1 =β−1α∈N which shows that β ∼ α.
Now suppose α ∼ β and β ∼ γ.
Then α−1β ∈ N and β−1γ ∈ N. Then since N is a subgroup α−1ββ−1γ =α−1γ ∈N and so α ∼ γ which shows that it is an equivalence relation as claimed.
Denote by [α] the equivalence class determined by α.
Now in the case of N a normal subgroup, you can consider the quotient group.
Deﬁnition F.6.3 Let N be a normal subgroup of a group G and deﬁne G/N as the set of all equivalence classes with respect to the above equivalence relation.
Also deﬁne [α][β]≡[αβ] Proposition F.6.4 The above deﬁnition is well deﬁned and it also makes G/N into a group.
Proof: First consider the claim that the deﬁnition is well deﬁned.
Suppose then that α∼α′ and β ∼β′.
It is required to show that [ ] ′ ′ [αβ]= αβ Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation470 FIELDS AND FIELD EXTENSIONS But z∈}N| { (αβ)−1α′β′ = β−1α−1α′β′ =β−1α−1α′β′ z ∈}N| {z∈}N| { ( ) = β−1 α−1α′ ββ−1β′ =n n ∈N 1 2 Thus the operation[is w]ell deﬁned.
Clearly the identity is [ι] where ι is the identity in G and the inverse is α−1 where α−1 is the inverse for α in G. The associative law is also obvious.
(cid:4) Notethatitwasimportanttohavethesubgroupbenormalinordertohavetheoperation deﬁned on the quotient group.
F.7 Normal Extensions And Normal Subgroups WhenKisasplittingﬁeldofaseparablepolynomialhavingcoeﬃcientsinF,theintermediate ﬁelds are each normal extensions from the above.
If L is one of these, what about G(L,F)?
isthisanormalsubgroupofG(K,F)?
Moregenerally,considerthefollowingdiagramwhich has now been established in the case that K is a splitting ﬁeld of a separable polynomial in F[x].
F≡L ⊆L ⊆L ··· ⊆L ⊆L ≡K 0 1 2 k−1 k (6.20) G(F,F)={ι} ⊆G(L1,F) ⊆G(L2,F) ··· ⊆G(Lk−1,F) ⊆G(K,F) TheintermediateﬁeldsL areeachnormalextensionsofFeachelementofL beingalgebraic.
i i As implied in the diagram, there is a one to one correspondence between the intermediate ﬁelds and the Galois groups displayed.
Is G(Lj−1,F) a normal subgroup of G(Lj,F)?
Let σ ∈ G(Lj,F) and let η ∈ G(Lj−1,F).
Then is σ−1ησ ∈ G(Lj−1,F)?
Let r = r1 be something in Lj−1 and let {r1,··· ,rm} be the roots of the minimal polynomial of r denoted by f(x), a polynomial having coeﬃcients in F. Then 0 = σf(r) = f(σ(r)) and so σ(r)= rj for some j.
Since Lj−1 is normal, σ(r) ∈Lj−1.
Therefore, it is ﬁxed by η.
It follows that σ−1ησ(r)=σ−1σ(r)=r and so σ−1ησ ∈G(Lj−1,F).
Thus G(Lj−1,F) is a normal subgroup of G(Lj,F) as hoped.
This leads to the following fundamental theorem of Galois theory.
Theorem F.7.1 LetKbeasplittingﬁeldofaseparablepolynomialp(x)havingcoeﬃcients in a ﬁeld F. Let {L }k be the increasing sequence of intermediate ﬁelds between F and K i i=0 as shown above in (6.20).
Then each of these is a normal extension of F and the Galois group G(Lj−1,F) is a normal subgroup of G(Lj,F).
In addition to this, G(L ,F)≃G(K,F)/G(K,L ) j j where the symbol ≃ indicates the two spaces are isomorphic.
Proof: All that remains is to check that the above isomorphism is valid.
Let θ :G(K,F)/G(K,Lj)→G(Lj,F), θ[σ]≡σ|Lj In other words, this is just the restriction of σ to L .
Is θ well deﬁned?
If [σ ]=[σ ], then j 1 2 by deﬁnition, σ σ−1 ∈ G(K,L ) and so σ σ−1 ﬁxes everything in L .
It follows that the 1 2 j 1 2 j restrictions of σ and σ to L are equal.
Therefore, θ is well deﬁned.
It is obvious that 1 2 j Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.8.
CONDITIONS FOR SEPARABILITY 471 θ is a homomorphism.
Why is θ onto?
This follows right away from Theorem F.4.5.
Note that K is the splitting ﬁeld of p(x) over L since L ⊇ F. Also if σ ∈ G(L ,F) so it is an j j j automorphismofL ,then, sinceitﬁxesF, p(x)=p¯(x)inthattheorem.
Thusσ extendsto j ζ, an automorphism of K. Thus θζ = σ.
Why is θ one to one?
If θ[σ] = θ[α], this means σ = α on L .
Thus σα−1 is the identity on L .
Hence σα−1 ∈ G(K,L ) which is what it j j j means for [σ]=[α].
(cid:4) There is an immediate application to a description of the normal closure of an algebraic extension F[a ,a ,··· ,a ].
To begin with, recall the following deﬁnition.
1 2 m Deﬁnition F.7.2 WhenyouhaveF[a ,··· ,a ]witheacha algebraicsothatF[a ,··· ,a ] 1 m i 1 m is a ﬁeld, you could consider ∏m f(x)≡ f (x) i i=1 where f (x) is the minimal polynomial of a .
Then if K is a splitting ﬁeld for f(x), this K i i is called the normal closure.
It is at least as large as F[a ,··· ,a ] and it has the advantage 1 m of being a normal extension.
Let G(K,F)={η ,η ,··· ,η }.
The conjugate ﬁelds are the ﬁelds 1 2 m η (F[a ,··· ,a ]) j 1 m Thus each of these ﬁelds is isomorphic to any other and they are all contained in K. Let K′ denote the smallest ﬁeld contained in K which contains all of these conjugate ﬁelds.
Note that if k ∈ F[a ,··· ,a ] so that η (k) is in one of these conjugate ﬁelds, then η η (k) is 1 m i j i also in a conjugate ﬁeld because η η is one of the automorphisms of G(K,F).
Let j i { } S = k ∈K′ :η (k)∈K′ each j .
j Then from what was just shown, each conjugate ﬁeld is in S. Suppose k ∈ S. What about k−1?
( ) ( ) η (k)η k−1 =η kk−1 =η (1)=1 j j j j ( ) ( ) ( ) and so η (k) −1 = η k−1 .
Now η (k) −1 ∈ K′ because K′ is a ﬁeld.
Therefore, ( ) j j j η k−1 ∈ K′.
Thus S is closed with respect to taking inverses.
It is also closed with j respect to products.
Thus it is clear that S is a ﬁeld which contains each conjugate ﬁeld.
However,K′ wasdeﬁnedasthesmallestﬁeldwhichcontainstheconjugateﬁelds.
Therefore, S = K′ and so this shows that each η maps K′ to itself while ﬁxing F. Thus G(K,F) ⊆ j G(K′,F).
However, since K′ ⊆ K, it follows that also G(K′,F) ⊆ G(K,F).
Therefore, G(K′,F)=G(K,F), and by the one to one correspondence between the intermediate ﬁelds and the Galois groups, it follows that K′ =K.
This proves the following lemma.
Lemma F.7.3 Let K denote the normal extension of F[a ,··· ,a ] with each a algebraic 1 m i so that F[a ,··· ,a ] is a ﬁeld.
Thus K is the splitting ﬁeld of the product of the minimal 1 m polynomials of the a .
Then K is also the smallest ﬁeld containing the conjugate ﬁelds i η (F[a ,··· ,a ]) for {η ,η ,··· ,η }=G(K,F).
j 1 m 1 2 m F.8 Conditions For Separability So when is it that a polynomial having coeﬃcients in a ﬁeld F is separable?
It turns out thatthisisalwaysthecaseforﬁeldswhichareenoughliketherationalnumbers.
Itinvolves consideringthederivativeofapolynomial.
Indoingthis,therewillbenoanalysisused,just Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation472 FIELDS AND FIELD EXTENSIONS the rule for diﬀerentiation which we all learned in calculus.
Thus the derivative is deﬁned as follows.
( ) anxn+an−1xn−1+···+a1x+a0 ′ ≡ nanxn−1+an−1(n−1)xn−2+···+a1 This kind of formal manipulation is what most students do anyway, never thinking about where it comes from.
Here na means to add a to itself n times.
With this deﬁnition, it n n is clear that the usual rules such as the product rule hold.
This discussion follows [17].
Deﬁnition F.8.1 A ﬁeld has characteristic 0 if na̸=0 for all n∈N and a̸=0.
Otherwise a ﬁeld F has characteristic p if p·1 = 0 for p·1 deﬁned as 1 added to itself p times and p is the smallest positive integer for which this takes place.
Note that with this deﬁnition, some of the terms of the derivative of a polynomial could vanishinthecasethattheﬁeldhascharacteristicp.
Iwillgoaheadandwritethemanyway.
For example, if the ﬁeld has characteristic p, then (xp−a)′ =0 because formally it equals p·1xp−1 =0xp−1, the 1 being the 1 in the ﬁeld.
NotethattheﬁeldZ doesnothavecharacteristic0becausep·1=0.
Thusnotallﬁelds p have characteristic 0.
How can you tell if a polynomial has no repeated roots?
This is the content of the next theorem.
Theorem F.8.2 Let p(x) be a monic polynomial having coeﬃcients in a ﬁeld F, and let K be a ﬁeld in which p(x) factors ∏n p(x)= (x−r ), r ∈K.
i i i=1 Then the r are distinct if and only if p(x) and p′(x) are relatively prime over F. i Proof: Supposeﬁrstthatp′(x)andp(x)arerelativelyprimeoverF.
Sincetheyarenot both zero, there exists polynomials a(x),b(x) having coeﬃcients in F such that ′ a(x)p(x)+b(x)p (x)=1 Now suppose p(x) has a repeated root r. Then in K[x], p(x)=(x−r)2g(x) and so p′(x)=2(x−r)g(x)+(x−r)2g′(x).
Then in K[x], ( ) a(x)(x−r)2g(x)+b(x) 2(x−r)g(x)+(x−r)2g′(x) =1 Then letting x=r, it follows that 0=1.
Hence p(x) has no repeated roots.
Next suppose there are no repeated roots of p(x).
Then ∑n ∏ p′(x)= (x−r ) j i=1j̸=i Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.8.
CONDITIONS FOR SEPARABILITY 473 p′(x) cannot be zero in this case because n∏−1 p′(r )= (r −r )̸=0 n n j j=1 because it is the product of nonzero elements of K. Similarly no term in the sum for p′(x) can equal zero because ∏ (r −r )̸=0.
i j j̸=i Then if q(x) is a monic polynomial of degree larger than 1 which divides p(x), then the roots of q(x) in K are a subset of {r ,··· ,r }.
Without loss of generality, suppose these 1 n rootsofq(x)are{r ,··· ,r }, k ≤n−1,sinceq(x)dividesp′(x)whichhasdegreeatmost 1∏ k n−1.
Then q(x)= k (x−r ) but this fails to divide p′(x) as polynomials in K[x] and i=1 i so q(x) fails to divide p′(x) as polynomials in F[x] either.
Therefore, q(x) = 1 and so the two are relatively prime.
(cid:4) The following lemma says that the usual calculus result holds in case you are looking at polynomials with coeﬃcients in a ﬁeld of characteristic 0.
Lemma F.8.3 Suppose that F has characteristic 0.
Then if f′(x)=0, it follows that f(x) is a constant.
Proof: Suppose f(x)=anxn+an−1xn−1+···+a1x+a0 Then take the derivative n−1 times to ﬁnd that a multiplied by a positive integer ma n n equals 0.
Therefore, a = 0 because, by assumption ma ̸= 0 if a ̸= 0.
Now repeat the n n n argument with f1(x)=an−1xn−1+···+a1x+a0 and continue this way to ﬁnd that f(x)=a ∈F.
(cid:4) 0 Now here is a major result which applies to ﬁelds of characteristic 0.
Theorem F.8.4 If F is a ﬁeld of characteristic 0, then every polynomial p(x), having coeﬃcients in F is separable.
Proof: It is required to show that the irreducible factors of p(x) have distinct roots in K a splitting ﬁeld for p(x).
So let q(x) be an irreducible monic polynomial.
If l(x) is a monic polynomial of positive degree which divides both q(x) and q′(x), then since q(x) is irreducible,itmustbethecasethatl(x)=q(x)whichforcesq(x)todivideq′(x).However, the degree of q′(x) is less than the degree of q(x) so this is impossible.
Hence l(x)=1 and so q′(x) and q(x) are relatively prime which implies that q(x) has distinct roots.
(cid:4) It follows that the above theory all holds for any ﬁeld of characteristic 0.
For example, if the ﬁeld is Q then everything holds.
Proposition F.8.5 If a ﬁeld F has characteristic p, then p is a prime.
Proof: First note that if n·1 = 0, if and only if for all a ̸= 0,n·a = 0 also.
This just follows from the distributive law and the deﬁnition of what is meant by n·1, meaning that you add 1 to itself n times.
Suppose then that there are positive integers, each larger than 1 n,m such that nm·1 = 0.
Then grouping the terms in the sum associated with nm·1, it follows that n(m·1) = 0.
If the characteristic of the ﬁeld is nm, this is a contradiction because then m·1̸=0 but n times it is, implying that n<nm but n·a=0 for a nonzero a.
Hence n·1=0 showing that mn is not the characteristic of the ﬁeld after all.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation474 FIELDS AND FIELD EXTENSIONS Deﬁnition F.8.6 A ﬁeld F is called perfect if every polynomial p(x) having coeﬃcients in F is separable.
The above shows that ﬁelds of characteristic 0 are perfect.
The above theory about Galoisgroupsandﬁxedﬁeldsallworksforperfectﬁelds.
Whataboutﬁeldsofcharacteristic p where p is a prime?
The following interesting lemma has to do with a nonzero a ∈ F having a pth root in F. Lemma F.8.7 Let F be a ﬁeld of characteristic p. Let a ̸= 0 where a ∈ F. Then either xp−a is irreducible or there exists b∈F such that xp−a=(x−b)p. Proof: Suppose that xp −a is not irreducible.
Then xp −a = g(x)f(x) where the degreeofg(x),k islessthanpandatleastaslargeas1.
Thenletbbearootofg(x).
Then bp−a=0.
Therefore, xp−a=xp−bp =(x−b)p. That is right.
xp −bp = (x−b)p just like many beginning calculus students believe.
It happens because of the binomial theorem and the fact that the other terms have a factor of p. Hence xp−a=(x−b)p =g(x)f(x) and so g(x) divides (x−b)p which requires that g(x) = (x−b)k since g(x) has degree k. It follows, since g(x) is given to have coeﬃcients in F, that bk ∈F.
Also bp ∈F.
Since k,p are relatively prime, due to the fact that k < p with p prime, there are integers m,n such that 1=mk+np Thenfromwhatyoumeanbyraisingbtoanintegerpowerandtheusualrulesofexponents for integer powers, ( ) b= bk m(bp)n ∈F.
(cid:4) So when is a ﬁeld of characteristic p perfect?
As observed above, for a ﬁeld of charac- teristic p, (a+b)p =ap+bp.
Also, (ab)p =apbp It follows that a → ap is a homomorphism.
This is also one to one because, as mentioned above (a−b)p =ap−bp Therefore, if ap = bp, it follows that a = b.
Therefore, this homomorphism is also one to one.
Let Fp be the collection of ap where a ∈ F. Then clearly Fp is a subﬁeld of F because it is the image of a one to one homomorphism.
What follows is the condition for a ﬁeld of characteristic p to be perfect.
Theorem F.8.8 Let F be a ﬁeld of characteristic p. Then F is perfect if and only if F=Fp.
Proof: SupposeF=Fpﬁrst.
Letf(x)beanirreduciblepolynomialoverF.
ByTheorem F.8.2,iff′(x)andf(x)arerelativelyprimeoverFthenf(x)hasnorepeatedroots.
Suppose thenthatthetwopolynomialsarenotrelativelyprime.
Ifd(x)dividesbothf(x)andf′(x) with degree of d(x) ≥ 1.
Then, since f(x) is irreducible, it follows that d(x) is a multiple Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.9.
PERMUTATIONS 475 of f(x) and so f(x) divides f′(x) which is impossible unless f′(x) = 0.
But if f′(x) = 0, then f(x) must be of the form a +a xp+a x2p+···+a xnp 0 1 2 n since if it had some other nonzero term with exponent not a multiple of p then f′(x) could notequalzerosinceyouwouldhavesomethingsurvivingintheexpressionforthederivative after taking out multiples of p which is like kaxk−1 where a̸=0 and k <p.
Thus ka̸=0.
Hence the form of f(x) is as indicated above.
If a =bp for some b ∈F, then the expression for f(x) is k k k bp+bpxp+bpx2p+···+bpxnp (0 1 2 n ) = b +b x+b x2+···+b xn p 0 1 x n because of the fact noted earlier that a → ap is a homomorphism.
However, this says that f(x) is not irreducible after all.
It follows that there exists a such that a ∈/ Fp contrary k k totheassumptionthatF=Fp.
Hencethegreatestcommondivisoroff′(x)andf(x)must be 1.
Next consider the other direction.
Suppose F ̸= Fp.
Then there exists a ∈ F\Fp.
Consider the polynomial xp−a.
As noted above, its derivative equals 0.
Therefore, xp−a and its derivative cannot be relatively prime.
In fact, xp−a would divide both.
(cid:4) Now suppose F is a ﬁnite ﬁeld.
If n·1 is never equal to 0 then, since the ﬁeld is ﬁnite, k·1= m·1, for some k <m.
m>k, and (m−k)·1=0 which is a contradiction.
Hence F is a ﬁeld of characteristic p for some prime p, by Proposition F.8.5.
The mapping a→ap was shown to be a homomorphism which is also one to one.
Therefore, Fp is a subﬁeld of F. It follows that it has characteristic q for some q a prime.
However, this requires q = p and so Fp =F.
Then the following corollary is obtained from the above theorem.
Corollary F.8.9 If F is a ﬁnite ﬁeld, then F is perfect.
Withthisinformation,hereisaconvenientversionofthefundamentaltheoremofGalois theory.
Theorem F.8.10 Let K be a splitting ﬁeld of any polynomial p(x) ∈ F[x] where F is either of characteristic 0 or of characteristic p with Fp = F. Let {L }k be the increasing i i=0 sequence of intermediate ﬁelds between F and K. Then each of these is a normal extension of F and the Galois group G(Lj−1,F) is a normal subgroup of G(Lj,F).
In addition to this, G(L ,F)≃G(K,F)/G(K,L ) j j where the symbol ≃ indicates the two spaces are isomorphic.
F.9 Permutations Let {a ,··· ,a } be a set of distinct elements.
Then a permutation of these elements is 1 n usually thought of as a list in a particular order.
Thus there are exactly n!
permutations of a set having n distinct elements.
With this deﬁnition, here is a simple lemma.
Lemma F.9.1 Every permutation can be obtained from every other permutation by a ﬁnite number of switches.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation476 FIELDS AND FIELD EXTENSIONS Proof:Thisisobviousifn=1or2.
Supposethenthatitistrueforsetsofn−1elements.
Take two permutations of {a ,··· ,a },P ,P .
To get from P to P using switches, ﬁrst 1 n 1 2 1 2 make a switch to obtain the last element in the list coinciding with the last element of P .
2 By induction, there are switches which will arrange the ﬁrst n−1 to the right order.
(cid:4) It is customary to consider permutations in terms of the set I ≡{1,··· ,n} to be more n speciﬁc.
Then one can think of a given permutation as a mapping σ from this set I to n itself which is one to one and onto.
In fact, σ(i) ≡ j where j is in the ith position.
Often people write such a σ in the following form ( ) 1 2 ··· n (6.21) i i ··· i 1 2 n An easy way to understand the above permutation is through the use of matrix multiplica- tion by permutation matrices.
The above vector (i ,··· ,i )T is obtained by 1 n   1   ( ) 2  ei1 ei2 ··· ein  ...  (6.22) n This can be seen right away from looking at a simple example or by using the deﬁnition of matrix multiplication directly.
Deﬁnition F.9.2 The sign of the permutation (6.21) is deﬁned as the determinant of the above matrix in (6.22).
In other words, the sign of the permutation ( ) 1 2 ··· n i i ··· i 1 2 n equals sgn(i ,··· ,i ) deﬁned earlier in Lemma 3.3.1.
1 n Notethatfromthefactthatthedeterminantiswelldeﬁnedanditsproperties,thesignof apermutationis1ifandonlyifthepermutationisproducedbyanevennumberofswitches and that the number of switches used to produce a given permutation must be either even or odd.
Of course a switch is a permutation itself and this is called a transposition.
Note also that all these matrices are orthogonal matrices so to take the inverse, it suﬃces to take a transpose, the inverse also being a permutation matrix.
TheresultinggroupconsistingofthepermutationsofI iscalledS .
Animportantidea n n is the notion of a cycle.
Let σ be a permutation, a one to one and onto function deﬁned on I .
A cycle is of the form n ( ) k,σ(k),σ2(k),σ3(k),··· ,σm−1(k) , σm(k)=k.
ThelastconditionmustholdforsomembecauseI isﬁnite.
Thenacyclecanbeconsidered n as a permutation as follows.
Let (i ,i ,··· ,i ) be a cycle.
Then deﬁne σ by σ(i ) = 1 2 m 1 i ,σ(i )=i ,··· ,σ(i )=i , and if k ∈/ {i ,i ,··· ,i }, then σ(k)=k.
2 2 3 m 1 1 2 m Note that if you have two cycles, (i ,i ,··· ,i ),(j ,j ,··· ,j ) which are disjoint in 1 2 m 1 2 m the sense that {i ,i ,··· ,i }∩{j ,j ,··· ,j }=∅, 1 2 m 1 2 m then they commute.
It is then clear that every permutation can be represented in a unique waybydisjointcycles.
Startwith1andformthecycledeterminedby1.
Thenstartwiththe Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.9.
PERMUTATIONS 477 smallest k ∈I which was not included and begin a cycle starting with this.
Continue this n way.
Use the convention that (k) is just the identity.
This representation is unique up to orderofthecycleswhichdoesnotmatterbecausetheycommute.
Notethatatransposition can be written as (a,b).
A cycle can be written as a product of non disjoint transpositions.
(i1,i2,··· ,im)=(im−1,im)···(i2,im)(i1,im) Thus if m is odd, the permutation has sign 1 and if m is even, the permutation has sign −1.
Also, it is clear that the inverse of the above permutation is (i ,i ,··· ,i )−1 = 1 2 m (i ,··· ,i ,i ).
m 2 1 Deﬁnition F.9.3 A is the subgroup of S such that for σ ∈ A , σ is the product of an n n n even number of transpositions.
It is called the alternating group.
The following important result is useful in describing A .
n Proposition F.9.4 Let n ≥ 3.
Then every permutation in A is the product of 3 cycles n and the identity.
Proof: In case n=3, you can list all of the permutations in A n ( ) ( ) ( ) 1 2 3 1 2 3 1 2 3 , , 1 2 3 2 3 1 3 1 2 In terms of cycles, these are (1,2,3),(1,3,2) You can easily check that they are inverses of each other.
Now suppose n ≥ 4.
The permutations in A are deﬁned as the product of an even number of transpositions.
There n are two cases.
The ﬁrst case is where you have two transpositions which share a number, (a,c)(c,b)=(a,c,b) Thus when they share a number, the product is just a 3 cycle.
Next suppose you have the productoftwotranspositionswhicharedisjoint.
Thiscanhappenbecausen≥4.Firstnote that (a,b)=(c,b)(b,a,c)=(c,b,a)(c,a) Therefore, (a,b)(c,d) = (c,b,a)(c,a)(a,d)(d,c,a) = (c,b,a)(c,a,d)(d,c,a) and so every product of disjoint transpositions is the product of 3 cycles.
(cid:4) Lemma F.9.5 If n ≥ 5, then if B is a normal subgroup of A , and B is not the identity, n then B must contain a 3 cycle.
Proof: Let α be the permutation in B which is “closest” to the identity without being the identity.
That is, out of all permutations which are not the identity, this is one which has the most ﬁxed points or equivalently moves the fewest numbers.
Then α is the product of disjoint cycles.
Suppose that the longest cycle is the ﬁrst one and it has at least four numbers.
Thus α=(i ,i ,i ,i ,··· ,m)γ ···γ 1 2 3 4 1 p Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation478 FIELDS AND FIELD EXTENSIONS Since B is normal, α ≡(i ,i ,i )(i ,i ,i ,i ,··· ,m)(i ,i ,i )γ ···γ ∈A 1 3 2 1 1 2 3 4 1 2 3 1 p m Then consider α α−1 = 1 (i ,i ,i )(i ,i ,i ,i ,··· ,m)(i ,i ,i )(m,···i ,i ,i ,i ) 3 2 1 1 2 3 4 1 2 3 4 3 2 1 Then for this permutation, i → i ,i → i ,i → i ,i → i .
The other numbers not in 1 3 2 2 3 4 4 1 {i ,i ,i ,i } are ﬁxed, and in addition i is ﬁxed which did not happen with α.
Therefore, 1 2 3 4 2 this new permutation moves only 3 numbers.
Since it is assumed that m ≥ 4, this is a contradiction to α ﬁxing the most points.
It follows that α=(i ,i ,i )γ ···γ (6.23) 1 2 3 1 p or else α=(i ,i )γ ···γ (6.24) 1 2 1 p In the ﬁrst case, say γ =(i ,i ,···).
Multiply as follows α = 1 4 5 1 (i ,i ,i )(i ,i ,i )(i ,i ,···)γ ···γ (i ,i ,i )∈B 4 2 1 1 2 3 4 5 2 p 1 2 4 Then form α α−1 ∈B given by 1 (i ,i ,i )(i ,i ,i )(i ,i ,···)γ ···γ (i ,i ,i )γ−1···γ−1(i ,i ,i ) 4 2 1 1 2 3 4 5 2 p 1 2 4 p 1 3 2 1 =(i ,i ,i )(i ,i ,i )(i ,i ,···)(i ,i ,i )(··· ,i ,i )(i ,i ,i ) 4 2 1 1 2 3 4 5 1 2 4 5 4 3 2 1 Then i → i ,i → i ,i → i ,i → i ,i → i and other numbers are ﬁxed.
Thus α α−1 1 4 2 3 3 5 4 2 5 1 1 moves 5 points.
However, α moves more than 5 if γ is not the identity for any i ≥ 2.
It i follows that α=(i ,i ,i )γ 1 2 3 1 and γ can only be a transposition.
However, this cannot happen because then the above 1 α would not even be in A .
Therefore, γ =ι and so n 1 α=(i ,i ,i ) 1 2 3 Thus in this case, B contains a 3 cycle.
Now consider case (6.24).
None of the γ can be a cycle of length more than 4 since i the above argument would eliminate this possibility.
If any has length 3 then the above argument implies that α equals this 3 cycle.
It follows that each γ must be a 2 cycle.
Say i α=(i ,i )(i ,i )γ ···γ 1 2 3 4 2 p Thus it moves at least four numbers, greater than four if any of γ for i ≥ 2 is not the i identity.
As before, α ≡ 1 (i ,i ,i )(i ,i )(i ,i )γ ···γ (i ,i ,i ) 4 2 1 1 2 3 4 2 p 1 2 4 = (i ,i ,i )(i ,i )(i ,i )(i ,i ,i )γ ···γ ∈B 4 2 1 1 2 3 4 1 2 4 2 p Then α α−1 = 1 (i ,i ,i )(i ,i )(i ,i )(i ,i ,i )γ ···γ γ−1···γ−1γ−1(i ,i )(i ,i ) 4 2 1 1 2 3 4 1 2 4 2 p p 2 1 3 4 1 2 = (i ,i ,i )(i ,i )(i ,i )(i ,i ,i )(i ,i )(i ,i )∈B 4 2 1 1 2 3 4 1 2 4 3 4 1 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.10.
SOLVABLE GROUPS 479 Then i →i ,i →i ,i →i ,i →i so this moves exactly four numbers.
Therefore, none 1 3 2 4 3 1 4 3 of the γ is diﬀerent than the identity for i≥2.
It follows that i α=(i ,i )(i ,i ) (6.25) 1 2 3 4 and α moves exactly four numbers.
Then since B is normal, α ≡ 1 (i ,i ,i )(i ,i )(i ,i )(i ,i ,i )∈B 5 4 3 1 2 3 4 3 4 5 Then α α−1 = 1 (i ,i ,i )(i ,i )(i ,i )(i ,i ,i )(i ,i )(i ,i )∈B 5 4 3 1 2 3 4 3 4 5 3 4 1 2 Then i → i ,i → i ,i → i ,i → i ,i → i .
Thus this permutation moves only three 1 1 2 2 3 4 4 5 5 3 numbersandsoα cannotbeoftheformgivenin(6.25).
Itfollowsthatcase(6.24)doesnot occur.
(cid:4) Deﬁnition F.9.6 A group G is said to be simple if its only normal subgroups are itself and the identity.
The following major result is due to Galois [17].
Proposition F.9.7 Let n≥5.
Then A is simple.
n Proof: From Lemma F.9.5, if B is a normal subgroup of A , B ̸={ι}, then it contains n a 3 cycle α=(i1,i2,i3), ( ) i i i 1 2 3 i i i 2 3 1 Now let (j ,j ,j ) be another 3 cycle.
1 2 3 ( ) j j j 1 2 3 j j j 2 3 1 Let σ be a permutation which satisﬁes σ(i )=j k k Then σασ−1(j ) = σα(i )=σ(i )=j 1 1 2 2 σασ−1(j ) = σα(i )=σ(i )=j 2 2 3 3 σασ−1(j ) = σα(i )=σ(i )=j 3 3 1 1 whileσασ−1 leavesallothernumbersﬁxed.
Thusσασ−1 isthegiven3cycle.
Itfollowsthat B contains every 3 cycle.
By Proposition F.9.4, this implies B = A .
The only problem is n that it is not know whether σ is in A .
This is where n ≥ 5 is used.
You can modify σ on n two numbers not equal to any of the {i ,i ,i } by multiplying by a transposition so that 1 2 3 the possibly modiﬁed σ is expressed as an even number of transpositions.
(cid:4) F.10 Solvable Groups Recall the fundamental theorem of Galois theory which established a correspondence be- tween the normal subgroups of G(K,F) and normal ﬁeld extensions.
Also recall that if H is one of these normal subgroups, then there was an isomorphism between G(K ,F) and H the quotient group G(K,F)/H.
The general idea of a solvable group is given next.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation480 FIELDS AND FIELD EXTENSIONS Deﬁnition F.10.1 A group G is solvable if there exists a decreasing sequence of subgroups {H }m such that Hi is a normal subgroup of H(i−1), i i=0 G=H ⊇H ⊇···⊇H ={ι}, 0 1 m and each quotient group Hi−1/Hi is Abelian.
That is, for [a],[b]∈Hi−1/Hi, [ab]=[a][b]=[b][a]=[ba] Note that if G is an Abelian group, then it is automatically solvable.
In fact you can just consider H =G,H ={ι}.
In this case H /H is just the group G which is Abelian.
0 1 0 1 There is another idea which helps in understanding whether a group is solvable.
It involves the commutator subgroup.
This is a very good idea because this subgroup is deﬁned in terms of the group G. Deﬁnition F.10.2 Let a,b∈G a group.
Then the commutator is aba−1b−1 The commutator subgroup, denoted by G′, is the smallest subgroup which contains all the commutators.
The nice thing about the commutator subgroup is that it is a normal subgroup.
There are also many other amazing properties.
Theorem F.10.3 Let G be a group and let G′ be the commutator subgroup.
Then G′ is a normal subgroup.
Also the quotient group G/G′ is Abelian.
If H is any normal subgroup of G such that G/H is Abelian, then H ⊇G′.
If G′ ={ι}, then G must be Abelian.
Proof: The elements of G′ are just ﬁnite products of things like aba−1b−1.
Note that the inverse of something like this is also one of these.
( ) aba−1b−1 −1 =bab−1a−1.
Thus the collection of ﬁnite products is indeed a subgroup.
Now consider h∈G.
Then haba−1b−1h−1 =hah−1hbh−1ha−1h−1hb−1h−1 ( ) ( ) =hah−1hbh−1 hah−1 −1 hbh−1 −1 which is another one of those commutators.
Thus for c a commutator and h∈G, hch−1 =c 1 another commutator.
If you have a product of commutators c c ···c , then 1 2 m ∏m ∏m hc c ···c h−1 = hc h−1 = d ∈G′ 1 2 m i i i=1 i=1 where the d are each commutators.
Hence G′ is a normal subgroup.
i Consider now the quotient group.
Is [g][h] = [h][g]?
In other words, is [gh] = [hg]?
In other words, is gh(hg)−1 = ghg−1h−1 ∈ G′?
Of course.
This is a commutator and G´′ consists of products of these things.
Thus the quotient group is Abelian.
Now let H be a normal subgroup of G such that G/H is Abelian.
Then if g,h∈G, [gh]=[hg], gh(hg)−1 =ghg−1h−1 ∈H Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.10.
SOLVABLE GROUPS 481 Thus every commutator is in H and so H ⊇G.
The last assertion is obvious because G/{ι} is isomorphic to G. Also, to say that G′ ={ι} is to say that aba−1b−1 =ι which implies that ab=ba.
(cid:4) Let G be a group and let G′ be its commutator subgroup.
Then the commutator sub- group of G′ is G′′ and so forth.
To save on notation, denote by G(k) the kth commutator subgroup.
Thus you have the sequence G(0) ⊇G(1) ⊇G(2) ⊇G(3)··· eachG(i) beinganormalsubgroupofG(i−1) althoughitispossiblethatG(i) isnotanormal subgroup of G. Then there is a useful criterion for a group to be solvable.
Theorem F.10.4 Let G be a group.
It is solvable if and only if G(k) ={ι} for some k. Proof: IfG(k) ={ι}thenGisclearlysolvablebecauseofTheoremF.10.3.
Thesequence of commutator subgroups provides the necessary sequence of subgroups.
Next suppose that you have G=H ⊇H ⊇···⊇H ={ι} 0 1 m where each is normal in the preceding and the quotient groups are Abelian.
Then from Theorem F.10.3, G(1) ⊆H .
Thus H′ ⊇G(2).
But also, from Theorem F.10.3, since H /H 1 1 1 2 is Abelian, H ⊇H′ ⊇G(2).
2 1 Continuing this way G(k) ={ι} for some k ≤m.
(cid:4) Theorem F.10.5 If G is a solvable group and if H is a homomorphic image of G, then H is also solvable.
Proof: By the above theorem, it suﬃces to show that H(k) = {ι} for some k. Let f be the homomorphism.
Then(H′ = f()G′).
To see this, consider(a com)mutator of H, f(a)f(b)f(a)−1f(b)−1 = f aba−1b−1 .
It follows that H(1) = f G(1) .
Now con- tinue this way, letting G(1) play the role of G and H(1) the role of H. Thus, since G is solvable, some G(k) ={ι} and so H(k) ={ι} also.
(cid:4) Now as an important example, of a group which is not solvable, here is a theorem.
Theorem F.10.6 For n≥5,S is not solvable.
n Proof: ItisclearthatA isanormalsubgroupofS becauseifσisapermutation,then n n it has the same sign as σ−1.
Thus σασ−1 ∈A if α∈A .
If H is a normal subgroup of S , n n n forwhichS /H isAbelian,thenH containsthecommutatorG′.However,ασα−1σ−1 ∈A n n obviously so A ⊇ S′.
By Proposition F.9.7, this forces S′ = A .
So what is S′′?
If it is n n n n n S , then S(k) ̸= {ι} for any k and it follows that S is not solvable.
If S′′ = {ι}, the only n n n n other possibility, then A /{ι} is Abelian and so A is Abelian, but this is obviously false n n because the cycles (1,2,3),(2,1,4) are both in A .
However, (1,2,3)(2,1,4) is n ( ) 1 2 3 4 4 2 1 3 while (2,1,4)(1,2,3) is ( ) 1 2 3 4 1 3 4 2 (cid:4) Note that the above shows that A is not Abelian for n=4 also.
n Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation482 FIELDS AND FIELD EXTENSIONS F.11 Solvability By Radicals First of all, there exists a ﬁeld which has all the nth roots of 1.
You could simply deﬁne it to be the smallest sub ﬁeld of C such that it contains these roots.
You could also enlarge it by including some other numbers.
For example, you could include Q.
Observe that if ξ ≡ ei2π/n, then ξn = 1 but ξk ̸= 1 if k < n and that if k < l < n, ξk ̸= ξl.
Such a ﬁeld hascharacteristic0becauseform aninteger, m·1̸=0.
ThefollowingisfromHerstein[13].
This is the kind of ﬁeld considered here.
Lemma F.11.1 Suppose a ﬁeld F has all the nth roots of 1 for a particular n and suppose there exists ξ such that the nth roots of 1 are of the form ξk for k = 1,··· ,n, the ξk being distinct.
Let a ∈ F be nonzero.
Let K denote the splitting ﬁeld of xn−a over F, thus K is a normal extension of F. Then K = F[u] where u is any root of xn−a.
The Galois group G(K,F) is Abelian.
Proof: Let u be a root of xn−a and let K equal F[u].
Then let ξ be the nth root of unity mentioned.
Then ( ) n ξku =(ξn)kun =a { } andsoeachξkuisarootofxn−aandthesearedistinct.
Itfollowsthat u,ξu,··· ,ξn−1u are the roots of xn−a and all are in F[u].
Thus F[u] = K. Let σ ∈ G(K,F) and observe that since σ ﬁxes F, (( ) ) ( ( )) n n 0=σ ξku −a = σ ξku −a Itfollowsthat σ mapsrootsof xn−a torootsof xn−a.
Therefore, if σ,α aretwoelements of G(K,F), there exist i,j each no larger than n−1 such that σ(u)=ξiu, α(u)=ξju A typical thing in F[u] is p(u) where p(x)∈F[x].
Then ( ) ( ) σα(p(u)) = p ξjξiu =p ξi+ju ( ) ( ) ασ(p(u)) = p ξiξju =p ξi+ju Therefore, G(K,F) is Abelian.
(cid:4) Deﬁnition F.11.2 For F a ﬁeld, a polynomial p(x) ∈ F[x] is solvable by radicals over F≡F0 if there is a sequence of ﬁelds F1 =F[a1],F2 =F1[a2],··· ,Fk =Fk−1[ak] such that for each i≥1,akii ∈Fi−1 and Fk contains a splitting ﬁeld K for p(x) over F. Lemma F.11.3 In the above deﬁnition, you can assume that F is a normal extension of k F. Proof: First note that F = F[a ,a ,··· ,a ].
Let G be the normal extension of F .
k 1 2 k k By Lemma F.7.3, G is the smallest ﬁeld which contains the conjugate ﬁelds [ ] η (F[a ,a ,··· ,a ])=F η a ,η a ,··· ,η a j 1 2 k j 1 j 2 j k ( ) ( ) for {η1,η2,··· ,ηm}=G(Fk,F).
Also, ηjai ki =ηj akii ∈ηjFi−1,ηjF=F.
Then G=F[η (a ),η (a ),··· ,η (a ),η (a ),η (a ),··· ,η (a )···] 1 1 1 2 1 k 2 1 2 2 2 k and this is a splitting ﬁeld so is a normal extension.
Thus G could be the new F with k respect to a longer sequence but would now be a splitting ﬁeld.
(cid:4) Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationF.11.
SOLVABILITY BY RADICALS 483 At this point, it is a good idea to recall the big fundamental theorem mentioned above whichgivesthecorrespondencebetweennormalsubgroupsandnormalﬁeldextensionssince it is about to be used again.
F≡F ⊆F ⊆F ··· ⊆F ⊆F ≡K 0 1 2 k−1 k (6.26) G(F,F)={ι} ⊆G(F1,F) ⊆G(F2,F) ··· ⊆G(Fk−1,F) ⊆G(Fk,F) Theorem F.11.4 LetKbeasplittingﬁeldofanypolynomialp(x)∈F[x]whereFiseither of characteristic 0 or of characteristic p with Fp =F.
Let {F }k be the increasing sequence i i=0 of intermediate ﬁelds between F and K. Then each of these is a normal extension of F and the Galois group G(Fj−1,F) is a normal subgroup of G(Fj,F).
In addition to this, G(F ,F)≃G(K,F)/G(K,F ) j j where the symbol ≃ indicates the two spaces are isomorphic.
Theorem F.11.5 Let f(x) be a polynomial in F[x] where F is a ﬁeld of characteristic 0 which contains all nth roots of unity for each n∈N.
Let K be a splitting ﬁeld of f(x).
Then if f(x) is solvable by radicals over F, then the Galois group G(K,F) is a solvable group.
Proof: Using the deﬁnition given above for f(x) to be solvable by radicals, there is a sequence of ﬁelds F =F⊆F ⊆···⊆F , K⊆F , 0 1 k k where Fi =Fi−1[ai], akii ∈Fi−1, and each ﬁeld extension is a normal extension of the pre- cedingone.
YoucanassumethatF isthesplittingﬁeldofapolynomialhavingcoeﬃcients k in Fj−1.
This follows from the Lemma F.11.3 above.
Then starting the hypotheses of the theorem at Fj−1 rather than at F, it follows from Theorem F.11.4 that G(Fj,Fj−1)≃G(Fk,Fj−1)/G(Fk,Fj) By Lemma F.11.1, the Galois group G(Fj,Fj−1) is Abelian and so this requires that G(F ,F) is a solvable group.
k Of course K is a normal ﬁeld extension of F because it is a splitting ﬁeld.
By Theo- rem F.10.5, G(F ,K) is a normal subgroup of G(F ,F).
Also G(K,F) is isomorphic to k k G(F ,F)/G(F ,K) and so G(K,F) is a homomorphic image of G(F ,F) which is solv- k k k able.
Here is why this last assertion is so.
Deﬁne θ : G(F ,F)/G(F ,K) → G(K,F) by k k θ[σ] ≡ σ|K.
Then this is clearly a homomorphism if it is well deﬁned.
If [σ] = [α] this means σα−1 ∈G(F ,K) and so σα−1 ﬁxes everything in K so that θ is indeed well deﬁned.
k Therefore, by Theorem F.10.5, G(K,F) must also be solvable.
(cid:4) Now this result implies that you can’t solve the general polynomial equation of degree 5 or more by radicals.
Let {a ,a ,··· ,a } ⊆ G where G is some ﬁeld which contains a ﬁeld 1 2 n F .
Let 0 F≡F (a ,a ,··· ,a ) 0 1 2 n the ﬁeld of all rational functions in the numbers a ,a ,··· ,a .
I am using this notation 1 2 n because I don’t want to assume the a are algebraic over F. Now consider the equation i p(t)=tn−a tn−1+a tn−2+···±a .
1 2 n and suppose that p(t) has distinct roots, none of them in F. Let K be a splitting ﬁeld for p(t) over F so that ∏n p(t)= (t−r ) i k=1 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation484 FIELDS AND FIELD EXTENSIONS Then it follows that a =s (r ,··· ,r ) i i 1 n where the s are the elementary symmetric functions deﬁned in Deﬁnition F.1.2.
For σ ∈ i G(K,F) you can deﬁne σ¯ ∈S by the rule n σ¯(k)≡j where σ(r )=r .
k j RecallthattheautomorphismsofG(K,F)takerootsofp(t)torootsofp(t).
Thismapping σ → σ¯ is onto, a homomorphism, and one to one because the symmetric functions s i are unchanged when the roots are permuted.
Thus a rational function in s ,s ,··· ,s is 1 2 n unaﬀected when the roots r are permuted.
It follows that G(K,F) cannot be solvable if k n≥5 because S is not solvable.
n Forexample,consider3x5−25x3+45x+1orequivalentlyx5−25x3+15x+1.Itclearly 3 3 has no rational roots and a graph will show it has 5 real roots.
Let F be the smallest ﬁeld containedinCwhichcontainsthecoeﬃcientsofthepolynomialandallrootsofunity.
Then probably none of these roots are in F and they are all distinct.
In fact, it appears that the real numbers which are in F are rational.
Therefore, from the above, none of the roots are solvable by radicals involving numbers from F. Thus none are solvable by radicals using numbers from the smallest ﬁeld containing the coeﬃcients either.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationBibliography [1] Apostol T., Calculus Volume II Second edition, Wiley 1969.
[2] Artin M., Algebra, Pearson 2011.
[3] Baker, Roger, Linear Algebra, Rinton Press 2001.
[4] Baker, A. Transcendental Number Theory, Cambridge University Press 1975.
[5] Chahal J.S., Historical Perspective of Mathematics 2000 B.C.
- 2000 A.D. Kendrick Press, Inc. (2007) [6] Coddington and Levinson,Theory of Ordinary Diﬀerential Equations McGrawHill 1955.
[7] Davis H. and Snider A., Vector Analysis Wm.
C. Brown 1995.
[8] Edwards C.H., Advanced Calculus of several Variables, Dover 1994.
[9] Friedberg S. Insel A. and Spence L., Linear Algebra, Prentice Hall, 2003.
[10] Golub,G.andVanLoan,C.,MatrixComputations,JohnsHopkinsUniversityPress, 1996.
[11] Gurtin M., An introduction to continuum mechanics, Academic press 1981.
[12] HardyG.,ACourseOfPureMathematics,Tenthedition,CambridgeUniversityPress 1992.
[13] Herstein I. N., Topics In Algebra, Xerox, 1964.
[14] Hofman K. and Kunze R., Linear Algebra, Prentice Hall, 1971.
[15] Householder A.
The theory of matrices in numberical analysis , Dover, 1975.
[16] Horn R. and Johnson C., matrix Analysis, Cambridge University Press, 1985.
[17] Jacobsen N. Basic Algebra Freeman 1974.
[18] Karlin S. and Taylor H., A First Course in Stochastic Processes, Academic Press, 1975.
[19] Marcus M., and Minc H., A Survey Of Matrix Theory and Matrix Inequalities, Allyn and Bacon, INc. Boston, 1964 [20] Nobel B. and Daniel J., Applied Linear Algebra, Prentice Hall, 1977.
[21] E. J. Putzer, American Mathematical Monthly, Vol.
73 (1966), pp.
2-7.
485 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation486 BIBLIOGRAPHY [22] Rudin W., Principles of Mathematical Analysis, McGraw Hill, 1976.
[23] Rudin W., Functional Analysis, McGraw Hill, 1991.
[24] Salas S. and Hille E., Calculus One and Several Variables, Wiley 1990.
[25] StrangGilbert,LinearAlgebraanditsApplications,HarcourtBraceJovanovich1980.
[26] Wilkinson, J.H., The Algebraic Eigenvalue Problem, Clarendon Press Oxford 1965.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationAnswers To Selected Exercises G.1 Exercises 7 x=2−2t,y =−t,z =t.
1.6 9 x=t,y =s+2,z =−s,w =s 1 (5+i9)−1 = 5 − 9 i 106 106 G.3 Exercises √ √ 3 −(1−i) 2,(1+i) 2.
1.14 4 This makes no sense at all.
You can’t add diﬀerent size vectors.
G.4 Exercises 1.17 ( ) ∑ ∑ 1/2 3 | n β a b |≤ n β |a |2 · 4 k=1 k k k k=1 k k ( ) ∑ 1/2 n β |b |2 k=1 k k z 5 If z ̸=0, let ω = |z| 4 The inequality still holds.
See the proof of the in- equality.
7 sin(5x)=5cos4xsinx−10cos2xsin3x+sin5x cos(5x)=cos5x−10cos3xsin2x+5cosxsin4x G.5 Exercises ( ( √ ))( ( √ )) 9 (x+2) x− i 3+1 x− 1−i 3 2.2 ( ( √ ))( ( √ )) 11 x− (1−i) 2 x− −(1+i) 2 · ( ( √ ))( ( √ )) 2 A= A+AT + A−AT x− −(1−i) 2 x− (1+i) 2 2 2 √ 3 You know that Aij = −Aji.
Let j = i to conclude 15 There is no single −1.
that A =−A and so A =0.
ii ii ii 5 0′ =0+0′ =0.
G.2 Exercises 6 0A = (0+0)A = 0A+0A.
Now add the additive 1.11 inverse of 0A to both sides.
1 x=2−4t,y =−8t,z =t.
7 0=0A=(1+(−1))A=A+(−1)A.Hence,(−1)A is the unique additive inverse of A.
Thus −A = 3 These are invalid row operations.
(−1)A. TheadditiveinverseisuniquebecauseifA 1 isanadditiveinverse,thenA =A +(A+(−A))= 1 1 5 x=2,y =0,z =1.
(A +A)+(−A)=−A.
1 487 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation488 ANSWERS TO SELECTED EXERCISES ∑ ∑ ∑ G.6 Exercises 10 (Ax,y)= (Ax) y = A x y ( ) ∑i i∑i( )i k ik∑k ∑i x,ATy = x AT y = x A y , k k i ki i k i k ik i 2.7 the same as above.
Hence the two are equal.
( ) 1 ShowthemapT :Rn →Rm deﬁnedbyT (x)=Ax 11 (AB)T x,y ≡ whereAisanm×nmatrixandxisanm×1column (x,(AB)y)= vector is a linear transformation.
( ) ( ) ATx,By = BTATx,y (.
Since this holds for ev)- This follows from matrix multiplication rules.
eryx,y,youhaveforally, (AB)T x−BTATx,y .
3 Findthematrixforthelineartransformationwhich rotates every vector in R2 through an angle of π/4.
Let y=(AB)T x−BTATx.
Then since x is arbi- ( ) ( √ √ ) trary, the result follows.
cos(π/4) −sin(π/4) = 12√2 −12√ 2 sin(π/4) cos(π/4) 1 2 1 2 13 Giveanexampleofmatrices,A,B,C suchthatB ̸= 2 2 C, A̸=0, and yet AB =AC.
5 Findthematrixforthelineartransformationwhich ( )( ) ( ) rotateseveryvectorinR2 throughanangleof2π/3.
1 1 1 −1 0 0 ( ) ( √ ) 1 1 −1 1 = 0 0 2cos(π/3) −2sin(π/3) √1 − 3 ( )( ) ( ) = 2sin(π/3) 2cos(π/3) 3 1 1 1 −1 1 0 0 = 1 1 1 −1 0 0 7 Findthematrixforthelineartransformationwhich rotateseveryvectorinR2 throughanangleof2π/3 15 It appears that there are 8 ways to do this.
and then reﬂects across the x axis.
( )( ) 17 ABB−1A−1 =AIA−1 =I 1 0 cos(2π/3) −sin(2π/3) B−1A−1AB =B−1IB =I (0 −1 sin√(2π)/3) cos(2π/3) Thenbythedeﬁnitionoftheinverseanditsunique- = −√12 −12 3 ness, it follows that (AB)−1 exists and (AB)−1 = −1 3 1 2 2 B−1A−1.
9 Findthematrixforthelineartransformationwhich 19 Multiply both sides on the left by A−1.
rotates every vector in R2 through an angle of π/4 ( )( ) ( ) a(nd then r)eﬂ(ects across the x axis. )
1 1 1 −1 0 0 1 0 cos(π/4) −sin(π/4) 21 = 1 1 −1 1 0 0 0 −1 sin(π/4) cos(π/4) ( √ √ ) 23 (Almost a)ny(thing wo)rks.
( ) = 12 √2 −12√2 1 2 1 2 5 2 −1 2 −1 2 = 2 2 3 4 2 0 11 6 ( )( ) ( ) 11 Findthematrixforthelineartransformationwhich 1 2 1 2 7 10 reﬂectseveryvectorinR2acrossthexaxisandthen = 2 0 3 4 2 4 rotates every vector through an angle of π/4.
( )( ) ( ) −z −w cos(π/4) −sin(π/4) 1 0 25 ,z,w arbitrary.
sin(π/4) cos(π/4) 0 −1 z w ( √ √ )     1√2 1 √2 1 2 3 −1 −2 4 −5 = 21 2 −21 2 27  2 1 4  = 0 1 −2  2 2 1 0 2 1 −2 3 13 Findthematrixforthelineartransformationwhich   reﬂectseveryvectorinR2acrossthexaxisandthen 1 0 5 rotates every vector through an angle of π/6.
29 Row echelon form:  0 1 323 .
A has no in- ( cos(π/6) −sin(π/6) )( 1 0 ) 0 0 0 sin(π/6) cos(π/6) 0 −1 verse.
( √ ) 1 3 1√ = 2 2 1 −1 3 2 2 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationG.7.
EXERCISES 489 15 Findthematrixforthelineartransformationwhich 41 Obviously not.
Because of the Coriolis force expe- rotateseveryvectorinR2throughanangleof5π/12.
rienced by the ﬁred bullet which is not experienced Hint: Note that 5π/12=2π/3−π/4.
by the dropped bullet, it will not be as simple as ( ) cos(2π/3) −sin(2π/3) in the physics books.
For example, if the bullet is ﬁred East, then y′sinϕ > 0 and will contribute to sin(2π/3) cos(2π/3) ( ) a force acting on the bullet which has been ﬁred cos(−π/4) −sin(−π/4) · whichwillcauseittohitthegroundfasterthanthe sin(−π/4) cos(−π/4) one dropped.
Of course at the North pole or the ( √ √ √ √ √ √ ) 1√2√3− 1√2 −1√ 2√ 3− 1√ 2 South pole, things should be closer to what is ex- = 4 4 4 4 1 2 3+ 1 2 1 2 3− 1 2 pectedinthephysicsbooksbecausetheresinϕ=0.
4 4 4 4 Also, if you ﬁre it North or South, there seems to 17 Find the matrix for proj (v) where u=(1,5,3)T .
be no extra force because y′ =0.
u   1  1 5 3  G.7 Exercises 5 25 15 35 3 15 9 3.2 ( ) ( ) 19 Giveanexampleofa2×2matrixAwhichhasallits 2 1=det AA−1 =det(A)det A−1 .
entriesnonzeroandsatisﬁesA2 =A.Suchamatrix ( ) is called idempotent.
3 det(A)=det AT =det(−A)=det(−I)det(A)= You know it can’t be invertible.
So try this.
(−1)ndet(A)=−det(A).
( ) ( ) a a 2 a2+ba a2+ba 6 Each time you take out an a from a row, you mul- = tiply by a the determinant of the matrix which re- b b b2+ab b2+ab mains.
Since there are n rows, you do this n times, hence you get an.
Leta2+ab=a,b2+ab=b.Asolutionwhichyields ( ) ( ) a nonzero matrix is 9 detA=det P−1BP =det P−1 det(B)det(P) ( ) ( ) =det(B)det P−1P =det(B).
2 2 −1 −1 11 Ifthatdeterminantequals0thenthematrixλI−A has no inverse.
It is not one to one and so there 21 x2 =−12t1−12t2−t3,x1 =−2t1−t2+t3 where the exists x̸=0 such that (λI−A)x=0.
Also recall ti are arbitrary.
the process for ﬁnding the inverse.
      −2t1−t2+t3 4 e−t 0 0 23  −12t1−t112t2−t3 + 70/2 ,ti ∈F 13  00 −ee−−tt((ccoosstt+−ssinint)t) −(c(osisnt)t)ee−−tt      t2 0 ( ) t 0 15 You have to have det(Q)det QT = det(Q)2 = 1 3 and so det(Q)=±1.
That second vector is a particular solution.
25 Show that the function T deﬁned by T (v)≡v− u u G.8 Exercises proj (v) is also a linear transformation.
u This is the sum of two linear transformations so it 3.6 is obviously linear.
  1 2 3 2 33 Let a basis for W be {w1,··· ,wr} Then if there  −6 3 2 3  exists v ∈ V \W, you could add in v to the basis 5 det 5 2 2 3 =5 and obtain a linearly independent set of vectors of 3 4 6 4 V which implies that the dimension of V is at least r+1 contrary to assumption.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation490 ANSWERS TO SELECTED EXERCISES   1e−t 0 1e−t 5 Here they are.
6  1cos2t+ 1sint −sint 1sint2− 1cost        2 2 2 2 1sint− 1cost cost −1cost− 1sint 1 0 0 0 0 1 0 1 0 2 2 2 2       ( ) 0 1 0 , 1 0 0 , 0 0 1 8 det(λI −A)=det λI−S−1BS 0 0 1 0 1 0 1 0 0 ( )       =det λS−1S−S−1BS 0 1 0 1 0 0 0 0 1 ( )       =det S−1(λI−B)S 1 0 0 , 0 0 1 , 0 1 0 ( ) 0 0 1 0 1 0 1 0 0 =det S−1 det(λI −B)det(S) ( ) =det S−1S det(λI−B)=det(λI−B) Sowhatisthedimensionofthespanofthese?
One way to systematically accomplish this is to unravel 9 FromtheCayleyHamiltontheorem,An+an−1An−1+ them and then use the row reduced echelon form.
···+a A+a I =0.Alsothecharacteristicpolyno- Unraveling these yields the column vectors 1 0        mial is det(tI−A) and the constant term is 1 0 0 0 1 0 (−1)ndet(A).
Thus a0 ̸=0 if and only if det(A)̸=  0  0  1  1  0  0  0 if and only if A−1 has an inverse.
Thus if A−1  0  1  0  0  0  1         exists, it follows that  0  1  0  1  0  0  ( )        a0I =( − An+an−1An−1+···+a1A)  10  00  01  00  01  10  =A −A(n−1−an−1An−2−···−a1I an)d also  0  0  1  0  0  1         a0I = −An−1−an−1An−2−···−a1I A There- 0 1 0 0 1 0 fore, the inverse is 1 0 0 1 0 0 ( ) a10 −An−1−an−1An−2−···−a1I Tyihelednsathrreafnoglilnowginthgeaselonasg wthiethciotlsurmonwsroedfuacemdaetcrhix- 11 Saythecharacteristicpolynomialisq(t)whichisof elon form.
  degree 3.
Then if n≥3,tn =q(t)l(t)+r(t) where 1 0 0 0 1 0 the degree of r(t) is either less than 3 or it equals    0 0 1 1 0 0  zero.
Thus An = q(A)l(A)+r(A) = r(A) and    0 1 0 0 0 1  so all the terms An for n≥3 can be replaced with    0 1 0 1 0 0  somer(A)wherethedegreeofr(t)isnomorethan    1 0 0 0 0 1 , row echelon form: 2.
Thus, assuming there are no conve∑rgence issues,  0 0 1 0 1 0  the inﬁnite sum must be of the form 2 b Ak.
  k=0 k  0 0 1 0 0 1    0 1 0 0 1 0 G.9 Exercises 1 0 0 1 0 0   1 0 0 0 0 1   4.6  0 1 0 0 0 1     0 0 1 0 0 1  1 ∑A typical thing in {Ax:x∈P (u1,··· ,un)} is  0 0 0 1 0 −1  nk=1tkAuk :tk ∈[0,1] and so it is just  0 0 0 0 1 −1  P (Au ,··· ,Au ).
 0 0 0 0 0 0  1 n   ( )  0 0 0 0 0 0    1 1 0 0 0 0 0 0 2 E = 0 1 0 0 0 0 0 0 The dimension is 5.
10 Itisbecauseyoucannothavemorethanmin(m,n) nonzero rows in the row reduced echelon form.
Re- call that the number of pivot columns is the same asthenumberofnonzerorowsfromthedescription P(e ,e ) E(P(e ,e )) 1 2 1 2 of this row reduced echelon form.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationG.10.
EXERCISES 491     11 It follows from the fact that e ,··· ,e occur as 1 0 0 0 a d c b 1 m     columns in row reduced echelon form that the di-  0 0 0 1   n g h t   =  mensionofthecolumnspaceofAisnandso, since 0 0 1 0 j m l k thiscolumnspaceisA(Rn),itfollowsthatitequals 0 1 0 0 e h z f Fm.
More formally, the iith entry of PijAPij is ∑ 12 Since m > n the dimension of the column space of PijA Pij =PijA Pij =A is sp pi ij jj ji ij AisnomorethannandsothecolumnsofAcannot s,p span Fm.
∑ 31 IfAhasaninverse,thenitisonetoone.
Hencethe 15 ∑If icizi = 0, apply A to both sides to obtain columns are independent.
Therefore, they are each c w =0.
By assumption, each c =0.
pivot columns.
Therefore, the row reduced echelon i i i i form of A is I.
This is what was needed for the 19 There are more columns than rows and at most m procedure to work.
can be pivot columns so it follows at least one col- umn is a linear combination of the others hence A G.10 Exercises is not one too one.
21 |b−Ay|2 =|b−Ax+Ax−Ay|2 5.8   =|b−Ax|2+|Ax−Ay|2+2(b−Ax,A(x−y)) 1 2 0 ( )   =|b−Ax|2+|Ax−Ay|2+2 ATb−ATAx,(x−y) 1 2 1 3 .= 1 2 3 =|b−Ax|2+|Ax−Ay|2and so, Ax is closest to b    1 0 0 1 2 0 out of all vectors Ay.
 2 1 0  0 −3 3    1 0 2 0 1 0 1 0 0 3        0 1 1 7  27 No.
  1 2 1 1 0 0 0 0 0 1 3  1 2 2 .= 0 0 1 · 0 0 0 0 2 1 1 0 1 0    29 Let A be an m×n matrix.
Then ker(A) is a sub- 1 0 0 1 2 1 space of Fn.
Is it true that every subspace of Fn is  2 1 0  0 −3 −1  the kernel or null space of some matrix?
Prove or 1 0 1 0 0 1 disprove.
    1 2 1 1 0 0 0 LetM beasubspaceofFn.Ifitequals{0},consider     thematrixI.
Otherwise,ithasabasis{m ,··· ,m }.
5  1 2 2 .= 0 0 0 1 · 1 k 2 4 1 0 0 1 0 Consider the matrix 3 2 1 0 1 0 0 ( )    m ··· m 0 1 0 0 0 1 2 1 1 k  3 1 0 0  0 −4 −2  where 0 is either not there in case k = n or has  2 0 1 0  0 0 −1  n−k columns.
1 0 −1 1 0 0 0   30 This is easy to see when you consider that Pij is 1 2 1 0 itsowninverseandthatPij multipliedontheright 9  3 0 1 1  switches the ith and jth columns.
Thus you switch 1 0 2 1  √ √ √  thecolumnsandthenyouswitchtherows.
Thishas 1 √11 1 √10 √11 √0 √ the eﬀect of switching Aii and Ajj.For example, = 131√11 −113 √10√11 − 1√ 2√ 5 · 11 110 10 1 0 0 0 a b c d 1 11 − 1 10 11 3 2 5     √ 11 √110 √ 10 √   000 001 010 100  nje fkt hzl mhg ·  011 1211√21101√111 2121√6√110√1√111 −52514√√1 1√01√1 11  0 0 1 2 5 1 2 5 2 5 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation492 ANSWERS TO SELECTED EXERCISES   G.11 Exercises −1 −1 7 7  −1 0 4 , eigenvectors: 6.6 −1 −1 5      3   2  1 Themaximumis7anditoccurswhenx =7,x = 1 2 1 ↔ 1, 1 ↔ 2.
This is a defective ma- 0,x =0,x =3,x =5,x =0.
    3 4 5 6 1 1 2 Maximize and minimize the following if possible.
trix.
  All variables are nonnegative.
−7 −12 30 9  −3 −7 15 , eigenvectors: (a) Theminimumis−7andithappenswhenx = 1 −3 −6 14 0,x2 =7/2,x3 =0.
       −2 5   2  (b) The maximum is 7 and it occurs when x = 1  1 , 0  ↔−1,  1  ↔2 7,x =0,x =0.
    2 3 0 1 1 (c) Themaximumis14andithappenswhenx = 1 This matrix is not defective because, even though 7,x =x =0.
2 3 λ = 1 is a repeated eigenvalue, it has a 2 dimen- (d) The minimum is 0 when x1 =x2 =0,x3 =1.
sional eigenspace.
  4 Findasolutiontothefollowinginequalitiesforx,y ≥ 3 −2 −1   0ifitispossibletodoso.
Ifitisnotpossible,prove 11 0 5 1 , eigenvectors: it is not possible.
0 2 4        1 0   −1  (a) There is no solution to these inequalities with x ,x ≥0.
 0 , −12 ↔3, 1 ↔6 1 2 0 1 1 (b) A solution is x =8/5,x =x =0.
1 2 3 This matrix is not defective.
(c) There will be no solution to these inequalities   5 2 −5 for which all the variables are nonnegative.
13  12 3 −10 , eigenvectors: (d) There is a solution when x2 = 2,x3 = 0,x1 = 12 4 −11 0.
     −1 5  (e) There is no solution to this system of inequal-  13 , 06  ↔−1 ities because the minimum value of x is not   7 0 1 0.
This matrix is defective.
In this case, there is only one eigenvalue, −1 of multiplicity 3 but the dimen- G.12 Exercises sion of the eigenspace is only 2.
  1 26 −17 7.3 15  4 −4 4 , eigenvectors: 1 Because the vectors which result are not parallel to −9 −18 9     the vector you begin with.
 −1   −2  3 λ→λ−1 and λ→λm.
 233 ↔0, 1 ↔−12, 1 0   5 Letxbetheeigenvector.
ThenAmx=λmx,Amx=  −1  Ax=λx and so  0  ↔18 λm =λ   1 Hence if λ̸=0, then     −2 1 2  3  λm−1 =1 17  −11 −2 9 , eigenvectors:  414 ↔1 −8 0 7 1 and so |λ|=1.
This is defective.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationG.13.
EXERCISES 493   4 −2 −2 and so λ = −λ¯.
Thus a+ib = −(a−ib) and so 19  0 2 −2 , eigenvectors: a=0.
2 0 2     31 This follows from the observation that if Ax=λx,  1   −i  then Ax=λx  −1  ↔4,  −i  ↔2−2i,           1  1  33  −11 ,1, −112 ,1, 11 ,1  i  2 2 3 1 1 0  i ↔2+2i   1 −1   35  1 (acos(t)+bsin(t)), 4 −2 −2 1 21  0 2 −2 , eigenvectors:   2 0 2 0 ( (√ ) (√ ))      −1  csin 2t +dcos 2t ,  1   −i  1  −1  ↔4,  −i  ↔2−2i,       2 1 1    1 (ecos(2t)+fsin(2t))wherea,b,c,d,e,f are  i  1  i ↔2+2i scalars.
1   1 1 −6 G.13 Exercises 23  7 −5 −6 , eigenvectors: −1 7 2 7.10      1   −i  1 To get it, you must be able to get the eigenvalues  −1  ↔−6,  −i  ↔2−6i,     and this is typically not possible.
1 1 ( ) ( )( )   0 −1 0 −1 2 0  i  4 =  i  ↔2+6i 2 (0 )(1 0 ) 0 1   1 2 0 0 −1 A = 1 0 1 1 0 This is not defective.
( ) 0 −2 25 First consider the eigenvalue λ=1.
Then you have = 1 0 ax = 0,bx = 0.
If neither a nor b = 0 then ( ) ( )( ) 2 3 λ = 1 would be a defective eigenvalue and the ma- 0 −2 0 −1 1 0 = trix would be defective.
If a = 0, then the dimen- 1 0 1 0 0 2 ( )( ) ( ) sion of the eigenspace is clearly 2 and so the ma- 1 0 0 −1 0 −1 trix would be nondefective.
If b = 0 but a ̸= 0, A2 = 0 2 1 0 = 2 0 .
Now thenyouwouldhaveadefectivematrixbecausethe it is back to where you started.
Thus the algo- eigenspace would have dimension less than 2.
If r(ithm mer)ely bou(nces betw)een the two matrices c ̸= 0, then the matrix is defective.
If c = 0 and 0 −1 0 −2 and and so it can’t possi- a=0,thenitisnondefective.
Basically,ifa,c̸=0, 2 0 1 0 then the matrix is defective.
bly converge.
27 A(x+iy) = (a+ib)(x+iy).
Now just take com- 15 B(1+2i,6),B(i,3),B(7,11) plex conjugates of both sides.
19 Gerschgorin’stheoremshowsthattherearenozero 29 LetAbeskewsymmetric.
Thenifxisaneigenvec- eigenvalues and so the matrix is invertible.
tor for λ, 21 6x′2+12y′2+18z′2.
√ √ √ λxTx¯=xTATx¯=−xTAx¯=−xTx¯λ¯ 23 (x′)2+ 1 3x′−2(y′)2− 1 2y′−2(z′)2− 1 6z′ 3 2 6 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation494 ANSWERS TO SELECTED EXERCISES ∑ ∑ ∑ 25 (0,−1,0)(4,−1,0) saddle point.
(2,−1,−12) local 43 ∑Suppo∑se ni=1aigi =0.Then0=∑ iai jAijfj = minimum.
f A a .
It follows that A a = 0 for j j i ij i i ij i each j.
Therefore, since AT is invertible, it follows 27 (1,1),(−1,1),(1,−1),(−1,−1) saddle points.
( √ √ ) ( √ √ ) thateacha =0.
Hencethefunctionsg arelinearly i i −1 5 6,0 , 1 5 6,0 Local minimums.
independent.
6 6 29 Critical points: (0,1,0), Saddle point.
G.15 Exercises 31 ±1 9.5 G.14 Exercises 1 This is because ABC is one to one.
8.4 7 In the following examples, a linear transformation, T isgivenbyspecifyingitsactiononabasisβ.
Find 1 The ﬁrst three vectors form a basis and the dimen- its matrix with respect to this basis.
sion is 3.
( ) 2 0 3 No.
Not a subspace.
Consider (0,0,1,0) and mul- (a) tiply by −1.
( 1 1 ) 2 1 5 NO.
Multiply something by −1.
(b) 1 0 ( ) 7 No.
Take something nonzero in M where say u1 = 1 1 (c) 1.
Now multiply by 100.
2 −1   9 Suppose {x ,··· ,x } is a set of vectors from Fn.
1 k 0 1 0 0 Show that 0 is in span(x ,··· ,x ).
  ∑ 1 k  0 0 2 0  11 A=  0= 0x 0 0 0 3 i i 0 0 0 0 11 It is asubspace.
It is spanned by   3 2 1 1 0 2 0 0  11 , 11 , 00 .
These are also indepen- 13  00 10 01 60 102  0 0 1  0 0 0 1 0  dent so they constitute a basis.
0 0 0 0 1 13 Pick n points {x1,··· ,xn}.
Then let ei(x)=0 un- 15 You can see these are not similar by noticing that lessx=xiwhenitequals1.
Then{ei}ni=1islinearly the second has an eigenspace of dimension equal to independent, this for any n. 1 so it is not similar to any diagonal matrix which { } is what the ﬁrst one is.
15 1,x,x2,x3,x4 ∑ ∑ 19 Thisisbecausethegeneralsolutionisy +y where 17 L( n c v )≡ n c w p i=1 i i i=1 i i Ay = b and Ay=0.
Now A0=0 and so the so- p 19 No.
There is a spanning set having 5 vectors and lution is unique precisely when this is the only so- this would need to be as long as the linearly inde- lution y to Ay=0.
pendent set.
G.16 Exercises 23 No.
It can’t.
It does not contain 0.
25 No.
This would lead to 0 = 1.The last one must 10.6 notbeapivotcolumnandtheonestotheleftmust ( ) ( ) 1 1 1 0 each be pivot columns.
2 Consider , .
These are both in 0 1 0 1 Jordan form.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationG.19.
EXERCISES 495   8 λ3−λ2+λ−1 0 −1 −1 0  −1 0 −1 0  10 λ2 8  1 1 2 0  3 3 3 1 11 λ3−3λ2+14     1 1 0 0 1 0 0  0 1 0 0  9  0 0 1  16  0 0 2 1  0 1 0 ( ) ( ) 0 0 0 2 1/2 1/3 −1 −1 12 Try , 2 1/2 2/3 1 5 G.17 Exercises 3 G.19 Exercises 10.9 4 λ3−3λ2+14 12.7   ( ) 0 0 −14 17 1 15   1 5 1 0 0 45 0 1 3  √   √ √   √   0 0 0 −3  2  161√√66 , −131√0√5√5√66 , 250√5   1 0 0 −1  61 6 −61 5 6 −1 5 6  0 1 0 −11  3 15 5 0 0 1 8 3 |(Ax,y)|≤(Ax,x)1/2(Ay,y)   { √ √ ( ) } 2 0 0 1, 3√(2x(−1),6 5 x2−x+)1 9 6 7  0 0 −7  ,20 7 x3− 3x2+ 3x− 1 2 5 20 0 1 −2 11 2x3− 9x2+ 2x− 1     7 7 70 0 −1 0 1 0 0  √  8  1 0 0 ,Q, 0 i 0 ,Q+iQ  −194√6 146  0 0 1 0 0 −i 14  723 √146  7 146 146 0 G.18 Exercises 16 |x+y|2+|x−y|2 =(x+y,x+y)+(x−y,x−y) 11.4   =|x|2+|y|2+2(x,y)+|x|2+|y|2−2(x,y).
.6 1  .9  21 Give an example of two vectors in R4 x,y and a 1 subspace V such that x·y = 0 but Px·Py ̸= 0 where P denotes the projection map which sends x 6 The columns are    to its closest point on V. 1 −(−1)n+1 1 −1  22n−3(−1)n+1   22n −1  Try this.
V is the span of e1 and e2 and x=e3+  21n −2(−1)n+1 , 21n −1 , e1,y=e4+e1.
2n 2n 1 −2(−1)n+1 1 −1 Px=(e +e ,e )e +(e +e ,e )e =e  2n   2n  3 1 1 1 3 1 2 2 1 0 (−1)n− 2 +1 Py=(e +e ,e )e +(e +e ,e )e =e  01 , 32((−−11))nn−−22n43n ++11  Px·Py=4 1 1 1 1 4 1 2 2 1 2n 2n 0 2(−1)n− 2 +1 22 y = 13x− 2 2n 5 5 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation496 ANSWERS TO SELECTED EXERCISES G.20 Exercises G.23 Exercises 12.9 15.3   √ 1 2 3 1 volume is 218   1 2 2 1.0 , eigenvectors: 3 0.
3 1 4    0.53491  G.21 Exercises  0.39022  ↔6.662,   0.7494   13.12  0.13016   0.83832  ↔1.6790, 13 Thisiseasybecauseyoushowitpreservesdistances.
 −0.52942    15 (Ax,x)=(UDU∗x,x)=(DU∗x,U∗x)≥δ2|U∗x|2 =  0.83483  δ2|x|2  −0.38073  ↔−1.341   −0.39763 16 0>((A+A∗)x,x)=(Ax,x)+(A∗x,x)   = (Ax,x)+(Ax,x) Now let Ax = λx.
Then you 3 2 1.0   get 0>λ|x|2+λ¯|x|2 =Re(λ)|x|2 2 2 1 3 , eigenvectors: 1 3 2   19 If Ax = λx, then you can take the norm of both  0.57735  sides and conclude that |λ| =1.
It follows that the  0.57735  ↔6.0, eigenvaluesofAareeiθ,e−iθ andanotheronewhich   0.57735 has magnitude 1 and is real.
This can only be 1 or   −1.Sincethedeterminantisgiventobe1,itfollows  0.78868  that it is 1.
Therefore, there exists an eigenvector  −0.21132 ↔1.7321, for the eigenvalue 1.
−0.57735    0.21132  G.22 Exercises  −0.78868 ↔−1.7321 0.57735   14.7 3 2 1.0     3 2 5 3 , eigenvectors: 0.09   1 3 2 1 0.21   0.43  0.41601     0.77918  ↔7.8730, 4.2373×10−2   0.46885 3  7.6271×10−2    0.71186  0.90453   −0.30151  ↔2.0,   28 You have H =U∗DU where U is unitary and D is −0.30151   a real diagonal matrix.
Then you have  9.3568×10−2  eiH =U∗∑∞ (iDn!
)nU =U∗ eiλ1 ... U  −00.8.534092522 ↔0.12702 n=0 eiλn 0 2 1.0   4 2 5 3 , eigenvectors: and this is clearly unitary because each matrix in 1 3 2   the product is.
 0.28433   0.81959  ↔7.5146,   0.49743 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationG.23.
EXERCISES 497    0.20984   0.45306  ↔0.18911,   −0.86643    0.93548   −0.35073  ↔−0.70370   4.3168×10−2   0 2 1.0   5 2 0 3 , eigenvectors: 1 3 2    0.3792   0.58481  ↔4.9754,   0.71708    0.81441   0.15694  ↔−0.30056,   −0.55866    0.43925   −0.79585  ↔−2.6749   0.41676 6 |7.3333−λ |≤0.47141 q 7 |7−λ |=2.4495 q 8 |λ −8|≤3.2660 q 9 −10≤λ≤12 10 x3+7x2+3x+7.0=0, Solution is:    [x=−0.14583+1.011i],  [x=−0.14583−1.011i],   [x=−6.7083] 11 −1.4755+1.1827i, −1.4755−1.1827i,−0.02444+0.52823i, −0.02444−0.52823i 12 Let QTAQ = H where H is upper Hessenberg.
Then take the transpose of both sides.
This will show that H = HT and so H is zero on the top as well.
Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationIndex ∩, 11 centripetal acceleration, 66 ∪, 11 characteristic and minimal polynomial, 242 characteristic equation, 157 A close to B characteristic polynomial, 97, 240 eigenvalues, 188 characteristic value, 157 A invariant, 249 codomain, 12 Abel’s formula, 103, 264, 265 cofactor, 77, 91 absolute convergence column rank, 94, 110 convergence, 351 commutative ring, 445 adjugate, 80, 93 commutator, 480 algebraic number commutator subgroup, 480 minimal polynomial, 216 companion matrix, 267, 383 algebraic numbers, 215 complete, 360 ﬁeld, 217 completeness axiom, 20 algebraically complete ﬁeld complex conjugate, 16 countable one, 450 complex numbers almost linear, 430 absolute value, 16 almost linear system, 431 ﬁeld, 16 alternating group, 477 complex numbers, 15 3 cycles, 477 complex roots, 17 analytic function of matrix, 413 composition of linear transformations, 234 Archimedean property, 22 comutator, 198 assymptotically stable, 430 condition number, 347 augmented matrix, 28 conformable, 42 automorphism, 459 conjugate ﬁelds, 471 autonomous, 430 conjugate linear, 293 converge, 440 Banach space, 337 convex combination, 244 basic feasible solution, 136 convex hull, 243 basic variables, 136 compactness, 244 basis, 59, 200 coordinate axis, 32 Binet Cauchy coordinates, 32 volumes, 306 Coriolis acceleration, 66 Binet Cauchy formula, 89 Coriolis acceleration block matrix, 99 earth, 68 multiplication, 100 Coriolis force, 66 block multiplication, 98 counting zeros, 187 bounded linear transformations, 340 Courant Fischer theorem, 315 Cramer’s rule, 81, 94 Cauchy Schwarz inequality, 34, 288, 337 cyclic set, 251 Cauchy sequence, 301, 337, 440 Cayley Hamilton theorem, 97, 263, 274 damped vibration, 427 centrifugal acceleration, 66 498 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationINDEX 499 defective, 162 elementary matrices, 105 DeMoivre identity, 17 elementary symmetric polynomials, 445 dense, 22 empty set, 11 density of rationals, 22 equality of mixed partial derivatives, 183 determinant equilibrium point, 430 block upper triangular matrix, 174 equivalence class, 210, 230 deﬁnition, 86 equivalence of norms, 340 estimate for Hermitian matrix, 286 equivalence relation, 210, 229 expansion along a column, 77 Euclidean algorithm, 23 expansion along a row, 77 exchange theorem, 57 expansion along row, column, 91 existence of a ﬁxed point, 362 Hadamard inequality, 286 inverse of matrix, 80 ﬁeld matrix inverse, 92 ordered, 14 partial derivative, cofactor, 103 ﬁeld axioms, 13 permutation of rows, 86 ﬁeld extension, 211 product, 89 dimension, 212 product of eigenvalues, 180 ﬁnite, 212 product of eigenvalules, 191 ﬁeld extensions, 213 row, column operations, 79, 88 ﬁelds summary of properties, 96 characteristic, 473 symmetric deﬁnition, 87 perfect, 474 transpose, 87 ﬁelds diagonalizable, 232, 307 perfect, 474 minimal polynomial condition, 266 ﬁnite dimensional inner product space basis of eigenvectors, 171 closest point, 291 diameter, 439 ﬁnite dimensional normed linear space diﬀerentiable matrix, 62 completeness, 339 diﬀerential equations equivalence of norms, 339 ﬁrst order systems, 194 ﬁxed ﬁeld, 466 digraph, 44 ﬁxed ﬁelds and subgroups, 468 dimension of vector space, 203 Foucalt pendulum, 68 direct sum, 74, 246 Fourier series, 301 directed graph, 44 Fredholm alternative, 117, 298 discrete Fourier transform, 335 free variable, 30 distinct roots Frobenius polynomial and its derivative, 472 inner product, 197 division of real numbers, 23 Frobenius norm, 329 Dolittle’s method, 124 singular value decomposition, 329 domain, 12 Frobinius norm, 334 dot product, 33 functions, 12 dyadics, 226 fundamental matrix, 366, 423 dynamical system, 171 fundamental theorem of algebra, 443, 450 fundamental theorem of algebra eigenspace, 159, 248 plausibility argument, 19 eigenvalue, 76, 157 fundamental theorem of arithmetic, 26 eigenvalues, 97, 187, 240 fundamental theorem of Galois theory, 470 AB and BA, 101 eigenvector, 76, 157 Galois group, 464 eigenvectors size, 464 distinct eigenvalues independence, 162 gambler’s ruin, 282 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation500 INDEX Gauss Jordan method for inverses, 48 parallelogram identity, 289 Gauss Seidel method, 356 triangle inequality, 289 Gelfand, 349 integers mod a prime, 223 generalized eigenspace, 75 integral generalized eigenspaces, 248, 258 operator valued function, 367 generalized eigenvectors, 259 vector valued function, 367 Gerschgorin’s theorem, 186 intersection, 11 Gram Schmidt procedure, 134, 173, 290 intervals Gram Schmidt process, 289, 290 notation, 11 Gramm Schmidt process, 173 invariant, 310 greatest common divisor, 23, 207 subspace, 249 characterization, 23 invariant subspaces greatest lower bound, 20 direct sum, block diagonal matrix, 250 Gronwall’s inequality, 368, 422 inverses and determinants, 92 group invertible, 47 deﬁnition, 466 invertible matrix group product of elementary matrices, 115 solvable, 480 irreducible, 207 relatively prime, 208 Hermitian, 177 isomorphism, 459 orthonormal basis eigenvectors, 313 extensions, 461 positive deﬁnite, 318 iterative methods real eigenvalues, 179 alternate proof of convergence, 365 Hermitian matrix convergence criterion, 360 factorization, 286 diagonally dominant, 365 positive part, 414 proof of convergence, 363 positive part, Lipschitz continuous, 414 Hermitian operator, 293 Jocobi method, 354 largest, smallest, eigenvalues, 314 Jordan block, 256, 258 spectral representation, 312 Jordan canonical form Hessian matrix, 184 existence and uniqueness, 259 Hilbert space, 313 powers of a matrix, 260 Holder’s inequality, 343 ker, 115 homomorphism, 459 kernel, 55 Householder kernel of a product reﬂection, 131 direct sum decomposition, 246 Householder matrix, 130 Krylov sequence, 251 idempotent, 72, 489 Lagrange form of remainder, 183 impossibility of solution by radicals, 483 Laplace expansion, 91 inconsistent, 29 least squares, 121, 297, 491 initial value problem least upper bound, 20 existence, 366, 417 Lindemann Weierstrass theorem, 219, 458 global solutions, 421 linear combination, 39, 56, 88 linear system, 418 linear transformation, 53, 225 local solutions, existence, uniqueness, 420 deﬁned on a basis, 226 uniqueness, 368, 417 dimension of vector space, 226 injective, 12 existence of eigenvector, 241 inner product, 33, 287 kernel, 245 inner product space, 287 matrix, 54 adjoint operator, 292 minimal polynomial, 241 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationINDEX 501 rotation, 235 not commutative, 41 linear transformations properties, 46 a vector space, 225 vectors, 39 composition, matrices, 234 matrix of linear transformation sum, 225, 295 orthonormal bases, 231 linearly dependent, 56 migration matrix, 279 linearly independent, 56, 200 minimal polynomial, 75, 240, 248 linearly independent set eigenvalues, eigenvectors, 241 extend to basis, 204 ﬁnding it, 263 Lipschitz condition, 417 generalized eigenspaces, 248 LU factorization minor, 77, 91 justiﬁcation for multiplier method, 127 mixed partial derivatives, 182 multiplier method, 123 monic, 207 solutions of linear systems, 125 monomorphism, 459 Moore Penrose inverse, 331 main diagonal, 78 least squares, 332 Markov chain, 279, 280 moving coordinate system, 63 Markov matrix, 275 acceleration , 66 limit, 278 regular, 278 negative deﬁnite, 318 steady state, 275, 278 principle minors, 319 mathematical induction, 21 Neuman matrices series, 370 commuting, 309 nilpotent notation, 38 block diagonal matrix, 256 transpose, 46 Jordan form, uniqueness, 257 matrix, 37 Jordan normal form, 256 diﬀerentiation operator, 228 non defective, 266 injective, 61 non solvable group, 481 inverse, 47 nonnegative self adjoint left inverse, 93 square root, 319 lower triangular, 78, 94 norm, 287 Markov, 275 strictly convex, 364 non defective, 177 uniformly convex, 364 normal, 177 normal, 324 rank and existence of solutions, 116 diagonalizable, 178 rank and nullity, 115 non defective, 177 right and left inverse, 61 normal closure, 464, 471 right inverse, 93 normal extension, 463 right, left inverse, 93 normal subgroup, 469, 480 row, column, determinant rank, 94 normed linear space, 287, 337 self adjoint, 170 normed vector space, 287 stochastic, 275 norms surjective, 61 equivalent, 338 symmetric, 169 null and rank, 302 unitary, 173 null space, 55 upper triangular, 78, 94 nullity, 115 matrix exponential, 366 matrix multiplication one to one, 12 deﬁnition, 40 onto, 12 entries of the product, 42 operator norm, 340 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation502 INDEX orthogonal matrix, 76, 83, 130, 175 principle minors, 318 orthogonal projection, 291 product rule orthonormal basis, 289 matrices, 62 orthonormal polynomials, 299 projection map convex set, 302 p norms, 343 Putzer’s method, 424 axioms of a norm, 343 parallelepiped QR algorithm, 190, 387 volume, 303 convergence, 390 partitioned matrix, 98 convergence theorem, 390 Penrose conditions, 332 non convergence, 394 permutation, 85 nonconvergence, 191 even, 107 QR factorization, 131 odd, 107 existence, 133 permutation matrices, 105, 476 Gram Schmidt procedure, 134 permutations quadratic form, 181 cycle, 476 quotient group, 469 perp, 117 quotient space, 223 Perron’s theorem, 404 quotient vector space, 223 pivot column, 113 random variables, 279 PLU factorization, 126 range, 12 existence, 130 rank, 111 polar decomposition number of pivot columns, 115 left, 324 rank of a matrix, 94, 110 right, 322 rank one transformation, 295 polar form complex number, 16 rational canonical form, 267 polynomial, 206 uniqueness, 270 degree, 206 Rayleigh quotient, 383 divides, 207 how close?, 384 division, 206 real numbers, 12 equal, 206 real Schur form, 175 Euclidean algorithm, 206 regression line, 297 greatest common divisor, 207 regular Sturm Liouville problem, 300 greatest common divisor description, 207 relatively prime, 23 greatest common divisor, uniqueness, 207 Riesz representation theorem, 292 irreducible, 207 right Cauchy Green strain tensor, 322 irreducible factorization, 208 right polar decomposition, 322 relatively prime, 207 row equivalelance root, 206 determination, 114 polynomials row equivalent, 114 canceling, 208 row operations, 28, 105 factorization, 209 inverse, 28 positive deﬁnite linear relations between columns, 111 postitive eigenvalues, 318 row rank, 94, 110 principle minors, 318 row reduced echelon form postitive deﬁnite, 318 deﬁnition, 112 power method, 373 examples, 112 prime number, 23 existence, 112 prime numbers uniqueness, 114 inﬁnity of primes, 222 principle directions, 165 scalar product, 33 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor FoundationINDEX 503 scalars, 18, 32, 37 subsequence, 440 Schur’s theorem, 174, 310 subspace, 56, 200 inner product space, 310 basis, 60, 205 second derivative test, 185 dimension, 60 self adjoint, 177, 293 invariant, 249 self adjoint nonnegative subspaces roots, 320 direct sum, 246 separable direct sum, basis, 246 polynomial, 465 surjective, 12 sequential compactness, 441 Sylvester, 74 sequentially compact, 441 law of inertia, 196 set notation, 11 dimention of kernel of product, 245 sgn, 84 Sylvester’s equation, 306 uniqueness, 85 symmetric, 47, 169 shifted inverse power method, 376 symmetric polynomial theorem, 446 complex eigenvalues, 381 symmetric polynomials, 445 sign of a permutation, 85 system of linear equations, 30 similar matrix and its transpose, 266 tensor product, 295 similar matrices, 82, 103, 229 trace, 180 similarity transformation, 229 AB and BA, 180 simple ﬁeld extension, 218 sum of eigenvalues, 191 simple groups, 479 transpose, 46 simplex tableau, 138 properties, 46 simultaneous corrections, 354 transposition, 476 simultaneously diagonalizable, 308 triangle inequality, 35 commuting family, 310 trivial, 56 singular value decomposition, 327 union, 11 singular values, 327 Unitary matrix skew symmetric, 47, 169 representation, 370 slack variables, 136, 138 upper Hessenberg matrix, 273, 399 solvable by radicals, 482 solvable group, 480 Vandermonde determinant, 104 space of linear transformations variation of constants formula, 195, 426 vector space, 295 variational inequality, 302 span, 56, 88 vector spanning set angular velocity, 64 restricting to a basis, 204 vector space spectral mapping theorem, 414 axioms, 38, 199 spectral norm, 341 basis, 59 spectral radius, 348, 349 dimension, 60 spectrum, 157 examples, 199 splitting ﬁeld, 214 vector space axioms, 33 splitting ﬁelds vectors, 39 isomorphic, 462 volume normal extension, 463 parallelepiped, 303 stable, 430 stable manifold, 437 well ordered, 21 stationary transition probabilities, 280 Wronskian, 103, 195, 264, 265, 426 Stochastic matrix, 280 Wronskian alternative, 195, 426 stochastic matrix, 275 Saylor URL: http://www.saylor.org/courses/ma212/ The Saylor Foundation
