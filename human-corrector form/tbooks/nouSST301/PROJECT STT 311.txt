 NATIONAL OPEN UNIVERSITY OF NIGERIA SCHOOL OF SCIENCES COURSE CODE: STT 311 COURSE TITLE: PROBABILITY DISTRIBUTION 2 0 STT 311 PROBABILITY DISTRIBUTION 2 Course Developer: MR A. ADELEKE.
Osurl State College of Technology ESA- OKE (LAGOS CENTRE) 1 TABLE OF CONTENTS UNIT 1: PROBABILITY SPACES, MEASURE AND DISTRIBUTION 1.0 Introduction 1.1 Objective 1.2 Probability space 1.3 Sample space and Event 1.4 Probability Measure 1.5 Theorem on probability space 1.6 Probability distribution 1.7 Conclusion 1.8 Summary 1.9 Tutor Marked Assignment 1.91 References.
UNIT 2 : DISTRIBUTION OF RANDOM VARIABLE SPACES 2.0 Introduction 2.1 Objective 2.2 Random Variables 2.3 Discrete Probability Distributions 2.4 Distribution Functions For Random Variables 2.5 Distribution Function For Discrete Random Variables.
2 2.6 Continuous Random Variables 2.7 Graphical Representation of Random Variables.
2.8 Joint Distributions 2.9 Independence of Random Variables 2.10 Conditional Distributions 2.11 Conclusion 2.12 Summary 2.13 Tutor Marked Assignment (TMA) 2.14 Reference / Further Reading / Other References.
UNIT 3 : EXPECTATION OF RANDOM VARIABLES.
3.0 Introduction 3.1 Objectives 3.2 What is Expectation of Random Variables?
3.3 Theorems on Expectation 3.4 The Variance and Standard Deviation.
3.5 Theorems on Variance 3.6 Moments 3.7 Moments Generating Functions 3.8 Theorems on Moment Generating Functions.
3.9 Characteristics Function.
3 3.10 Conclusion 3.11 Summary 3.12 Tutor Marked Assignment (TMA) 3.13 References / Further Reaching / Other Resources.
UNIT 4 : LIMIT THEOREM.
4.0 Introduction 4.1 Objectives 4.2 Chebyshev’s Inequality 4.3 Convergence of Random Variables 4.4 Demovre’s Theorem 4.5 Central Limit Theorem 4.6 Klinchine’s Theorem 4.7 Conclusion 4.8 Summary 4.9 Tutor Marked Assignment ( TMA) 5.0 References / Further Reading / Other Resources.
4 UNIT 1: PROBABILITY SPACES, MEASURE AND DISTRIBUTION 1.0 INTRODUCTION This Unit Focuses an Probability spaces, probability measures and probability distribution for continuous random variables.
It gives some basic definition and relevant working examples will be given to make the concept more meaningful for the learners.
1.1 OBJECTIVES.
At the end of this unit, student should be able to: • Understand the meaning of probability space and its notation.
• Define Sample space and event, and Event Space.
• Discuss Probability Measure and State its Theorems.
• Discuss Probability Distribution for Continuous Random Variables.
1.2 PROBABILITY SPACE A Probability Space is a triplet Finite measure space (Ω, (cid:217) , P[ . ]
) Where W is a sample space and each w˛ W is called a sample point, and R [ . ]
is a function that has D as its domain that is a single term that gives us an expedient way to assume the existence of all three components in its notation.
5  1.3 SAMPLE SPACE AND EVENT Definition of Sample Space : The sample space denoted by W , is the collection or totality of all possible outcomes of a conceptual experiment.
In addition to W , S, Z R, E m and A are the other sysbols to denotes sample space.
Event: An event is a subset of the sample space.
Event Space: The class of all events associated with a given experiment is defined to be the event spaces.
1.4 PROBABILITY MEASURE A probability measure is a normed non-negative, countable additive set function defined on the field of all events.
Definition:- A probability measure P on a s -field of subset A of set W is a real -valued function having domain A satisfying the following properties (i).
R ( W ) = 1 (ii).
R ( A ) ‡ 0 for all Ae D 6 (iii) If A n, n = 1, 2 ,...................are mutually disjoint sets in L , then R (UAn) = S R (An).
A probability space, denoted by (W ,A ,R ).
R ( A) is a conditional probability measure on W B ( ) We say that (W ,(cid:217) R ) is theProbabilty space obtained by conditioning W (cid:217) P by the event B.
If an event B depends on occurrence of event A or A then 1 2 R (B ) = R (B (cid:217) A ) + R (B (cid:217) A ) 1 2 ( ) ( ) = R (A ) R (B / A ) + P A P B/A 1 2 2 2 In general, if an event B depends on occurrence of events A , A ,........................A (cid:217) , then 1 2 (cid:217) R (B ) = ∑ R (B A i) n i (cid:217) = ∑ R (Ai)P(B/Ai) i 1.5 Theorem: let (W , (cid:217) ,R ) be a probability space then (W , (cid:217) , R ) is also a probability space.
B Proof: p(A B) P (A) = n ‡ O; B P(B) ( ) P W nB PB (W ) = ( ) = 1 P B 7 Let A , A ……… be disjoint event in I, then 1 2 [ [ ¥ [ ] U Ai nB R (UAi B) = 1 R (B) R U(AinB) = R ( B) Since (AnB), I = 1, 2 …………….. are disjoint events, we have l ¥ ¥ p (U A ) = ∑P (Ai) and so P is countably additive.
B i B B 1 1 Hence (W , (cid:217) , P ) is probability space B Lemma [ ] Let An be a sequence of independent measurable sets (i) If ∑ P (An) — ¥ then P (A .)
= O i ¥ (ii) If ∑ P (A n ) = ¥ then P (A.)
= 1 1 where A1 = Lim sup An Proof : (i) A = ˙ U An ˝ U Am m=n ¥ Since = Udecreaseb as n in creases,we have m=n P(A) £ P U¥ Am £ ∑¥ P (Am)   m=n m =n 8 Since ∑P — ¥ , then n ¥ lim ∑P(Am)= 0 ﬁn ¥ m =n ( )  ¥ ¥   ¥  (iii) P A • = U IAc £ ∑PIAc  m  m n =1m =n  n=m  ¥ ¥ = ∑(cid:213) (1- P(Am)= 0 n=mn=m Hence P(A) =1 1.6 PROBABILITY DISTRIBUTION Definition:- Let x be a random variable whose image set (s) is a continuous numbers such as an interval.
Then the set a £ x £ b is an event in S .
R ( a < x — b ) defined as The probability R ( a < x <b ) = ∫bf (x)d x a Is called continuous random variable, the function f is called the distribution or continuous probability function or density faction of X and it satisfies the following i. f ( X )> 0 ii ∫ f (x ) dx = 1 where R = (ab) 1 R 9 iii.
f (x ) is a non - decreasing function iv.
∫bf (x ) dx = f (b) - f (a) = p (a < x <b) a for example, if x is a random variable defined to take any value in interval (0,1) if a point is taken in this interval say 0.45, the probability that a point picked is 1 P (x = 0.45) = Uncountale points 1 = No of points btwtheinterval 1 = = 0 (cid:181) But it is easier to find the probability of a sub-interval within an interval, it is possible to calculate per (x — 0.45)etc.
The pr (x — 0.45) = pr(lenght O to 0.45) Ratio of the given Lenght = Total lenght Similarly x - x Pr (x — x — x ) = 2 1 1 2 Total Lenght 10 Example 1.6.1 :- Given the figure below A E C D D Find the probability of the shaded portion.
Solution: Area of D CDE Pr (Shaded portion) = Area of D ABC Example 1.6.2 : - The length of life measure in hours of a certain rare type of insect is a random, reliable x with portability density function ( ) 0 3 2x - x2 0< x<2 f (x ) = O 4 O elesewthere If the amount of food measured in milligrams consumed in a life time ( ) by such an insect defined by the function g x = x2, , where x is the lenght of life mesasued in hours, find the expected amount of food that will be consumed by an insect of this type.
11 Solution [ ] Expected amount of food = E g (x) =- ∫2 g (x) f (x) dn 00 =∫2 x2 ( 3 ( 2 x - x2 ) dx 4 0 = 1.2 mg Example 1.5.3:- Given a continuous random variable with the probability distribution function f (x) = ∫ kx 2 O £ x £ 10 O O elsewhere Find k such that f (x) is then Pdf Solution If F (x) is a Pdf, ∫1f (x) dx =1 0 ∫ f (x)dx = ∫ 10K x 2 d x 0 Kx3) 10 (10)3 (0)3 = 0 = k - k = 1 3 3 3 k = (103- 00) = 1 3 1000K = 1 3 12 3 k = 1000 1.7 CONCLUSION In this unit, you have learnt probability space, the notation of its components, the definition of sample space, event and event space.
You also learned probability measure and its main probabilities related theorems.
You also learned probability distribution of a continuous random variables with relevant working examples.
1.8 SUMMARY What you have learned in this unit are the following probability distribution concepts.
i.
The meaning of probability space and its notation ii.
Important definition of sample space, Event and event space .
iii.
Probability measure and its properties iv.
Part of probability distribution of a continuous random variables 13 EXERCISE 1.80.1 (SAE) The surface area measured in squaremeter of a flat metal disk manufactured by a certain process is a random variable x with probability density function.
f (x) = ∫6 (x - x2) O— x — 1 0 dscuhere Find the expected radius measured in meter of a flat metal disk manufactured by this process.
1.90.
TUTOR MARKED ASSIGNMENT (TMA) Exercise 1.8.1: The probability function of a random variable x is given by 2p x = 1  p x = 2 f(x) =  4p x = 3  0 otherwise Where P is a constant find (a) P (0 £ x < 3), (b) P(x > 1).
14 1.9 REFERENCES / FURTHER READING DR. R.A Kasumu (2003) probability theory first edition published by Fatol ventures Lagos.
Alexander M. Mood et al 1974 introduction to the theory of statistics third edition published by McGraw –Hill.
15 UNIT 2: DISTRIBUTION OF RANDOM VARIABLES SPACES 2.0 INTRODUCTION This unit concerns with the meaning and classification of random variables into discrete and continuous random variables are high lighted, distributive functions for discrete and continuous random variables and related examples are also given.
The unit further high light Graphical representation, Joint distribution for discrete and continuous random variable , Independence and Conditional probability of random variables and working example on each are given.
2.1 OBJECTIVE At the end of this unit student should be able to, - Understand the meaning of random variables.
- Classify random variables into discrete and continuous random variable with example.
- Define and state the properties of distribution function.
- State the distribution function for discrete and continuous random variables and solve example on each.
- Show the graphical representation of random variables.
16 - State the joint distributions for two random variables which are either both discrete or both continuous.
- State the independent of random variables for independent and dependent events.
- State the conditional probability function for discrete and continuous random variables.
- Solve related problems on the distribution of random variables spaces.
2.2 RANDOM VARIABLES A random variable is a function whose domain of definition is the simple space S of a random experiment and whose range is a set of real numbers.
Definition 2 A real valued measurable function ( ) X : W ﬁ R with respect to W , (cid:217) R , Is called a random variables.
Note: { } Suppose Xh is a sequence of random.
Variable If lim X n = X, then X is Random variable ﬁn ¥ 17 Example : suppose that a coin is tossed twice so that the sample space ( S = HH HTTH, TT).let X repsesent the number of heads that can come up.
For example X (HH) = 2, X (HT) = X (TH) = 1, X (TT) = 0 Since the domain of X is S and the range consists of real numbers, them x is a random variable.
A random variable that takes on a finite or countably infinite number of values is called a discrete random variable.
While the one which takes a non countable infinite number of values is called a nondiscrete / continuous random variable.
2.3 DISCRETE PROBABILITY DISTRIBUTIONS Let X be a discrete random variable, and suppose that the possible values that it can assume are given by x , x , x ,.............................., 1 2 3 arranged in some order.
Suppose also that these values are assumed with probabilities given by p (X = x ) = f (x ) K = 1, 2, ................(1) k k It is convenient to introduce the probability function, also suffered to as probability distribution, give by P (X = x ) = f (x) ...........................................................(2) 18 For X = xk , this reduces to equation given above while for other values of X f (x) = 0.
1 In general, f (x) is a probablity function if f (x) ‡ 0 1.
2 ∑ f (x) = 1 x Where the sum in equation (2) is taken over all possible values of x 2.4 DISTRIBUTION FUNCTION FOR RANDOM VARIABLES The cumulative distribution function, or briefly distribution function for a random variables X is defined by f (x) = P (X £ x) Where x is any real number, that is - ¥ — x — ¥ The distribution function F (x) hasthe following properties: [ ) 1. f (x) is non decreasing ie, f (x) £ f(y) if x £ y 2. l im F (x) = 0; lim f (x) =1 ﬁx ¥ hﬁ ¥   3. f (X) is continous fromthe right ie lim f (x+h) for al x .
  Vh ¥  19 2.5 DISTRIBUTION FUNCTION FOR DESECRATE RANDOM VARIABLES The distribution function for a discrete random variable x can be obtained from its probability function by noting that, for all x in (- ,¥ ,¥ ) p (x) = P (X £ x ) = S f (x) v£ h Where the sum is taken over all values m taken on by X for which m £ x if X takes on only a finite numbers of values x , x2 .......................xn,then the distribution function is given by 1 1 0 - ¥ < x < x 1  f(x ) x £ x £ x F(x) [ 1 )1 2 f (x ) + f (x x £ x — x  1 2 2 h 2 [F (x ) + - + F x ) x £ x — ¥ 1 n n n Example 2.5.1:- find the probability function corresponding to the random variable x when a coin is tossed twice; assuming that the coin is fair.
Solution p (HH) = 1 , P (HT) = 1 , P (TH) = 1 4 4 4 and P(TT) = 1 4 20 Then, P (X = 0) = P(TT) = 1 4 P (X =1= P (HT) + P(TH) = 1 + 1 = 1 4 4 2 P (X = 2) = P (HH) = 1 4 The probability function is given in the table below x 0 1 2 f (x) 1 1 1 4 2 4 Example 2.5.2 = (a) find the distribution for the random variable X from the working example above (b) obtain its graph Solution: The distribution function is 0 - ¥ — x — 0 sincethetable    1 £ — = 1  0 x 1x sincef (x)   4 1 4  = f (x)   3 £ — + =3 1 x 2 since f (x) f(x )  4 1 2 4   £ ¥— 1 + 1 + 1 1 2 x since  =   2 4 4 1  21 0 - ¥ — x — 0 since x = 0 from the above table  1   1 = 1  4 0 £ x — 1 since f (x ) 4   1   f (h ) 3 1 £ x — 2 sience f(x ) + f (x ) = 3  4 1 2 4    ie1 + 1 = 3  4 2 4   1 2 £ x —¥ since 1 + 1 + 1 =1   2 4 4  b. f (x) 1 3 4 ½ 1 2 1 31 4 4 0 1 2 x The following things about the above distribution function should be noted.
1.
The magnitudes of the jumps at 0, 1, 2 are ¼ , ½ , ¼ which are precisely the probabilities ie this fact enables one to obtain the probability function from the distribution function.
22 2.
Because of the appearance of the graph it is often called a staircase function or step function, and the value at the function at an integer is obtained from the higher step; thus the value at I is ¾ and not ¼ .
This is expressed mathematically by stating that the distribution function is continuous from the right at 0,1,2.
3.
As we proceed from left to right in the distribution function is monotonically increasing function.
2.6 CONTINUOUS RANDOM VARIABLES A non discrete random variable x is said to be absolutely continuous, or simply continuous, if its distribution function may be represented as x f(x = P (X £ x ) = ∫ f(u)du (- ¥ — x — ¥ ) 1 - ¥ Where the function f (x) has thefollowing proaperties 1. f (x) ‡ 0 ¥ 2 ∫ f (x)dx =1 ¥ It follows from the above that if X is a continuous random variable, then, the probability that X takes on only one particular value is zero, whereas the interval probability that X lies between two different values say a and b is given by 23 b p (a — x — b ) = ∫ f (x)dx a Example 2.6.1 (a) find the constant C such that the function ( ) cx2 0 — x — 3 f x =  0 otherwise Is a density function, and b.
Computer P (1— x — 2 ) Solution Since f(x) stratifies property (2) if C‡ 0, it must satisfy property 2 in order to be a density function ¥ 3 Now, ∫ f (x) dx = ∫cx2dx ¥ 0 Cx 3 = 3 3 0 C(3) (0) 3 = - C 3 3 27C 0 = - =9C 3 3 And since the integral equal to 1, we have 9C = 1 C = 1/ 9 b. P (1— x — 2) = ∫2 1 x2 dx 9 1 24 ( ) = x3 ∫2 = 2 3 - (1)3 27 1 27 27 8 1 7 = - = 27 27 27 2.7 GRAPHICAL REPRESENTATIONS OF RANDOM VARIABLE If f(x) is the density function for random variable x , then we can represent y = f (x) graphically by a curve as shown in the figure below.
Since f(x) ‡ O, the curve cannot fall below the x-anis the entire area bonded by the curve and the x-anis must be I because of the second ¥ property i.e ∫ f (x)dx =1).
¥ Geometrically the probability that x is between a and b, i.e p (a < x <b), is the represented by the area shown shaded from the first figure below 25  The destitution function F (X) = p (X £ x) is a monotonically increasing function which increases from 0 to a 1 and is represented by a curve as in the second figure.
26 2.8 JOINT DISTRIBUTIONS Joint distributions can easily be generalized to two more random variables.
We shall consider the typical case of two random variables that are either both discrete or both continuous.
Discrete case: - If x and Y are two discrete random variables, we define the joint probability function of x and y by ( ) P X = x, Y = y = f(x, y) Where (1) f (x, g) ‡ O (2) ∑∑ f (x,y) =1 x y i.e the sum over all values of x and y is 1 Suppose that x can assume any one of m values x x ,- - x and y can 1, 2 m assume any none of n values y , y , - - y .
1 2 n Then the probability of the event that x = xj and y = Y is given by j P(x = xj, Y = y ) = f (x j, y ) k k A joint probability fraction for x and y can be represented by a joint probability table as shown below 27 X Y Y --------- Yx Total Y 1 2 X F(x , y ) F(x y ) --------- F(x y )x ) F (x y )x ) 1 1 1 1 2 1 n 1 1 1 n 1 X F(x y ) F(x , y ) --------- F(x , y ) F (x ) 2 2 1 2 1 2 n 1 2 Xm F(x y ) F(x y ) F(x y ) F (x ) m 1 m 2 m n 1 m Totalsﬁ f (y ) f (y ) ------- f (y ) 1 2 1 2 2 2 n The probability that X = xj is obtained by adding all entries in the row corresponding to x and is given by i 2 P (x =h ) = f (x ) = ∑ f (xj y k) j 1 j k- 1 For J = 1, 2 -------------- m, these are indicated by the entry totals in the entrance right hand column or margin from the table above similarly the probability that Y= yk is obtained by adding all entries in the column corresponding to yk and is given by m p (Y = yk ) = f (y ) = ∑ f (xj, yk) 2 k j=1 For k =1,2,................................,n,these are indicated by the entry totals in the bottom row or margin of the probability table from the two equations given above f1(xj) and f (yk) or simply f (x) and f (y) 2 1 2 28 which are obtained from the margin of the table are refer to as the marginal functions of X and y, respectively it should be noted that m (cid:217) ∑ f (xi) =1 ∑ f (y ) =1 1 2 k j =1 k =1 Which can be written as follow m h ∑ ∑ f (xj, y ) =1 k j =1 K =1 This is simply the statement that the total probability of all entries is 1. the grand total of 1 is indicated in the lower right – hand of the probability table.
The joint distribution function of x and y is defined by f (x1y) = P (x £ x, y £ y) = ∑ ∑ f (x n ) 1 m £ h n £ y In the probability table f (x,y) is the sum of all eateries for which x j £ x and y £ y. k CONTINUOUS CASE: The case where both variables are continuous is obtained easily by analogy with the discrete case on replacing sums by integrals thus the joint probability function or joint density function for random variables x and y is defined by (1) f (x, y) ‡ O 29 ¥ ¥ (2) ∫ ∫ P(x y) dx dy =1 1 - ¥ - ¥ Graphically Z = f (x,y) represents a surface called the probability surface as indicated in the figure below.
The total volume bounded by this surface and the xy plane is equal to 1 in accordance with property 2 above.
The probability that x lies between a and b while y lies between c and d is given graphically by the shaded values of the figure below and mathematically by b d P(a — x — b,c— y — d) = ∫ ∫ f (x,y) dxdy X =a y =c 30 More generally, if A represents any event, there will be a region R of A the plan that corresponds to it.
In such case we can find the xy probability of A by performing the integration over R i.e A P (A) = ∫ ∫ f( x y) dxdy) 1 RA The joint distribution function of x and y in this case is defined by h y F (x,y) = p (X £ x, Y £ y) = ∫ ∫ f (u, v) du dv =m - ¥ =v - ¥ It follows in analogy with equation d F(x) = f (x), dx ¶ 2 F = f (x,y) ¶ x dy That is density function is obtained by differentiation the distribution function with respect to x and y from the joint distribution equation given above to obtain x h P (x £ x) = F (x) = ∫ ∫ f (u,v) dudv 1 =v ¥- =v - ¥ ¥ y P (y £ y) = f (y) = ∫ ∫ f (u,v) dudv 2 =m - ¥ =v - ¥ The two equation above are called the marginal distribution functions or simply the distribution function of x and y respectively.
31 The derivative of the equations with aspect to x and y are then called the marginal density functions or simply the density functions, of x and y which are given below ¥ ¥ F (x) = ∫ f (x,v)dv f (y)= ∫ f (u,y) du 1 2 =v- ¥ =u- ¥ 2.9 INDEPENDENCE OF RANDOM VARIABLES Suppose that x and y are discrete random variables.
If the events X = x and Y = y are independent events for all x and y, then we say that x and y are independent random variables.
In such case, P (X = x,Y=g) = p(X = x) p(Y=y) Or equivalently f(x,y) = f (x) f (y) 1 2 Conversely, if for all x and y the joint probability function f (x y) can be 1 expressed as the product of a function of x alone and a function of y alone (which are then the marginal probability function of X and Y) X and Y are independent.
If however, f (x,y) cannot be so expressed, then X and Y are dependent.
If X and Y are continuous random variables, we say that they are independent random variables if the events X £ x and Y £ y are independent events for all x and y.
In such case we can write 32 P (X £ x,Y £ y ) = p (X £ x ) p(Y £ y)or equivalently F (x y) = F (x) F (y) 1 1 2 Where f (x)h and f (y) are the (marginal) distribution functions of X and 1 2 Y, respectively conversely, X and Y are independent random variables if for all x and y, their joint distribution function F (x,y) can be expressed as a product of a function of x alone and a function of y alone (which are the marginal distribution of X and Y respectively) If however, f (x,y) cannot be expressed, then x and y are dependent.
For continuous independent random variables, it is also true that the joint density function f (x,y) is the product of a function of x alone, f (x) and a function 1 of y alone, f (y) , and these are the (marginal) density functions of x and y, 2 respectively.
2.10 CONDITIONAL DISTRIBUTIONS We already know that if p (A) > O, AnB) P(B/A) = P ( P(A) If x and y are discrete random variables and we have the events (A: x =x) (B: Y =y), the above equation becomes 33 ( ) f (x,y) PY = y X = x = f (x) 1 Where f (X,y) = P(x =X,y=y) is the joint probability fraction and f (x) is 1 the marginal probability function for x. f (x y) F(y x) = 1 f (x) 1 And call it the conditional probability function of Y given X.
Similarly, the conditional probability function of X given Y is f (x y) F (x y) = 1 f (y) 2 We can also denote f (x y) and f(y x) by f (xly)and f (ylx)respectively these ideas can easily l l i 2 be extended to the case where X, Y are continuous random variables.
For example the conditional density f unction of Y given X is f (x y) f (ylh ) = 1 f (x) 1 Where f (x,y) is the joint density function of x and y, and f (x) is the i marginal density function of x .
Using the equation above we can find the probability of y being between c and d given that x — X — x+dx is ( ) P c <Y < d x < X < x+dx = =∫d f (y x ) dy c 34 Example 10.1 2: 101 A random variable x has the density function C f (x) = , where - ¥ <x < ¥ .
(x2 +1) a.
Find the value of the constant C b.
Find the probability that X2 lies between 1/ and 1 3 Solution ¥ c. We must have ∫ f(x)dx =1 ie - ¥ ¥ c ∫ dx = C tan - 1x ¥ - ¥ x2 +1 - ¥ (cid:213)  (cid:213)  =C  - -  = 1  2  2  =C (cid:213) = 1 C = 1 (cid:213) 1 3 b.
If £ x2 £ 1 ,then either £ x £ or 3 3 3 - 1£ x £ - 3 Thus the required probability is 35 1 ∫- 533 dx + 1 ∫1 dx = 2 ∫1 dx (cid:213) - 1 x2 +1 (cid:213) 53 x2 +1 (cid:213) 53 x2 +1 3 3 2  J3  = tan - 1 (1) - tan - 1 ( ) (cid:213)  3  2 (cid:213) (cid:213)  1 =  -  = (cid:213)  4 6  6 Example 2:10:2 2: 102 find the distribution function corresponding to the density function of the example 2:10:1 given above.
Solution : u 1 m du f (x) = ∫ f (u) du = ∫ ¥ (cid:213) ¥ u2 +1 = 1 tan - 1 u ∫x ¥- (cid:213)  ¥  1 [ ] = tan - 1 x - tan - 1 (¥- ) (cid:213) 1  (cid:213)  1 1 = tan - 1 x + = + tan - 1 x   (cid:213)  2  2 (cid:213) Example 2:10:3 2: 10:3 The distribution function for a random variable x is  f (x) = 1 - e - 2m x ‡ 0 0 h 0 36 Find (a) the density function (b) the probability that x > 2 and (c) the probability that - 3 — x £ 4 Solution : f (x) = 1- e- 2x when x ‡ 0 a .
d (f (x) = 0 - (- 2 ) e- 2x = 2e - 2x dx d 2e- 2x x ‡ 0 2 > f (x) = f (x) =  dx 0 x < 0 ¥ (b) p (x > 2 ) = ∫ 2e - 2h dx = - e - 2m ¥ 2 2 .
= - e - 2 (¥ ) = - eo + e - 4 = e- 4 odx+ ∫42e - 2m dx c. p(- 3< x £ 4) = ∫4 f (x)dx= ∫o o - 3 - 3 = - e- 2x 4 = 1- e- 8 o OR p (- 3< x £ 4 ) = P (x £ 4) - P x £ - 3) P (4) - P (- 3) = (1- e- 8) - (o) = 1- e - 8 Example 2:10:4 The joint probability function of two discrete random variable X and Y - 1 is given by f (x,y) = C (2x + y), where x and y can assume all integers such that o £ x £ 2, o £ y £ 3, and f (x,y) = o othewise a. find the value of the constant C 37 b.
Find p (x = 2, y = 1).
(c) find p (x >1, y < 2 Solution: The sample points x, y) for which probabilities are different from zero are indicated below y 0 1 2 3 Totals x 0 0 C 2c 3c 6c 1 2c 3c 4c 5c 14c 2 4c 5c 6c 7c 22c Totals 6c 9c 12c 15c 42c The probabilities associated with these points, given by C (2x+ y), are shown in the table above Since the grand 24C must equal to 1 i.e 42 C = 1 C = 1 42 F(x y) b. P (x = 2 , Y = 1) = C (2x2)+ 1) + 1 grand total 5c 5c + 42c 5 = 5c + 42 38 c. p (x ‡ 1 , y £ 2 ) = ∑ f ∑ f(x,y) x=‡ 1 £y 2 = (2c + 3c + 4c ) + (4c + 5c + 6c) 24 4 = 24c = = 42 7 Example 2:10:5 Find (a) f (y2), (b) p (y =1 x = 2 for the distribution Find the example 2:10:4 above Solution Using the results from the above example f (x,y) (2x + y ) /42 f (y x) = = f (x) f (x) 1 1 So that with x = 2 (4+ y) /42 4+ y x 21 f (y2) = = 11 11 x 42 21 4+ y = 22 4 +1 p (y =1 x = 2) = f(1 2) = (b) 22 = 5 22 39 2.11 CONCLUSION In this unit, you studied random variables and classification, distributive functions for discrete and continuous random variables.
You also learned Graphical representation and joint distribution for discrete and continuous random variables.
Independence and conditional probability of random variables and related working examples are also learned from this its 2:12 SUMMARY In this unit distribution of random variables spaces that you studied included (1) Meaning of random variables and its classification.
(2) Distribution functions for discrete and continuous random variables.
(3) Graphical representation of random variables (4) Joint distribution for discrete and continuous random variables.
(5) Independence and conditional probability of random variables (6) Worked examples on each concept of random variables.
Exercise 2.12 .1 (S A E) 40 Suppose that a pair of fair dice are to be tossed, and let the random variable x denote the sum of the points.
Obtain the probability distribution for x.
2.13 TUTOR MARKED ASSIGNMENT (TMA) Exercises 2.131.
The joint density function of two continuous random variables x and y is { ( )  C x y o — x — 4,1 — y — 5 f x,y =  0 otherwise Find the value of the constant c (a) Find P ( 1 — X — 2, 2 — Y — 3) (b) Find p (X ‡ 3, Y £ 2).
2.14 REFERENCES/ FURTHER READING / OTHER RESOURCES Marry R Spiegel etal (2009) probability and statistics third edition published by mc craw hill 41 UNIT 3 EXPECTATION OF RANDOM VARIABLES.
3.0 INTRODUCTION.
This is a very important concept in probability and statistics.
The unit will forcusses on mathematical expectation of random variables.
Expected value for discrete and continuous random variables are stated.
Variance and Standard Deviation for discrete and continuous random variables are highlighted; also some important theorems on the expectation of random variables are discussed.
Moment and Moment generating functions for random variables are also learned from this unit.
Characteristics function of random variables are also learned and relevant working examples on each concept are given to make the unit more meaningful.
3.1 OBJECTIVE At the end of this unit, student should be able to 1.
Define Expectation of random variable 2.
Express mathematically the Expected Value of Mean for discrete and continuous random variables.
3.
State and prove Theorems on Expectation.
42 4.
State Variance and Standard Deviation for Discrete and Continuous Random Variables.
5.
Find the Mathematical Expectation of Moments and Moments Generation Function for Discrete and Continuous random variables.
6.
Find the characteristic function of a given random variable.
7.
Solve related examples on the mathematical expectation of random variables.
3.2 What Is Expectation of Random Variables?
Let X be a discrete random variable with probability function f (x) , Then the expected value of x E (x) is defined to be , h E (X) = ∑ X j f (xj) = ∑X f (x) J =1 = ∑x f (x) (1) If f (x) is a accurate characterization of the population frequency distribution, then E (x) = m ( the population mean) For a continuous random variable x having density function f (x) the expectation of x is defined as ¥ E (x) = ∫ x f (x) d x- - - (2) - ¥ 43 Provided that the integral converges absolutely.
Where f (.x) is the value of its probability density at x e.g If x is the member of point roll with a balance die.
F (x) = 1 for x = 1, 2, 3, 4, 5, 6 and its mathematical 6 1 1 1 1 1 1 expectation is E (x) = 1x + 2x + 3x + 4x + 5 x + 6x 6 6 6 6 6 6 1 2 3 4 5 6 = + + + + + 6 6 6 6 6 6 21 = =3 1 (3.5) 2 6 Also if X has the unform density function ( ) f x = 1 for 2 — x — 4 and f (x) = o elsewhere.
Then 2 E (x) = ∫4 x.
1 dx = 1 x2 1 =3 2 2 4 2 In many scientifical problem, we are interested not only the expected value of a random variable X but also in the expected value of random variable related to x.
Thus, we might be interested in random variable y whose values are related to those of X by y = g (x) [ ] The E g (x) = ∑ g (x) f (x)- - - (3) [ ] ¥ Where x is discrete and E (g(x)) = ∫ g(x) f (x) d x - ¥ For continuous case using the above example find the expectation of g (x) = x2 for the number of points rolls with a balance die 44 Solution [ ] E g(x) = ∑ x2 f (x) f (x) = 12,3, 4,5,6 ( f (x) = 1 6 [ ] 1 1 1 1 1 \ E g (x) = 1x + 4 x + 9 x + 16 x + 25 x +36 x1 6 6 6 6 6 6 =15 1 6 Similarly for the random variable with the uniform density function f (x) = 1 for 2 — x — 4 and f (x) = O elsewhere, we get 2 E[g (x)] = E(x2) = ∫4 x2 f (x) dx 2 = ∫4 x2(1) dx 2 2 = ∫41 x2 dx = 1x3 4 2 2 6 2 = 91 3 3.3 THEOREMS ON EXPECTATION Theorem 3.31: If c is any constant, then E(C X) = C E (X) [ ] [ ] Also, E C.g (x) = C E g(x) Theorem 3.3.2 : If x and y are any random variables, then E (X + Y) = E (x) + E (Y) 45 Theorem 3.3.3: If x and y are independent random variables, then E (X Y) = E (X) E (Y) THEOREM 3.3.4 [ ] h E (ax + b) h = ∑ (n) ah -i bi E (xh - i) i i=o For instance, if n = 1, [ ] n E (an+b)n = ∑ (1) a1- ibi E(x1- i) i i =o () () ( ) ( ) = 1 a E x + 1 bE 1 o 1 = a E (x) +b If I= 2 ( ) [ ] 2 [ ] E (ax+ b )2 = ∑ 2 a2- i bi E x2 - i i i =o ( ) ( ) ( ) ( ) ( ) ( ) a2 E X2 + 2 ab E X + 2 b2 E 1 20 1 2 = a2 E (x2) + 2ab E (x) + b2 Theorem 3.3.4 can easily be proved by mathematical induction If z is a random variable whose values are related to those of z and random variable x and y by means of equation z = g ( x,y) The mathematical Expectation is written as [ ] E g(x,y) = ∑ ∑g(x,y) f (x,y)- - (4) h y [ ] ¥ ¥ Or E g(x,y) = ∫ ∫ g (x,y) f (x,y) dx dy - - (5) - ¥ - ¥ 46 In equation (4) f (x,y) is the value of joint probability function of x and y at x, y while in equation (5) f (x,y)Corresponds to the value of the joint probability density 3.4 THE VARIANCE AND STANDARD DEVIATION We have already noted that the expectation of a random variable X is often called the means and is denoted by m another quality of great importance in probability and statistics is called variance and is defined by Var (x) = E (x-m )2 ...................................................(6) The variance is a non-negative number.
The positive squaeroot of the variance is called the Standard Deviatiation and is given by ([ ]) s = Var(x) = E X - x )2 - ................................. (7) x The standard derivation is often denoted by s instead of s , and the x variance in such case is s 2 If x is a discrete random variable taking the values X ,X ……….Xn 1 2 and having probability function f (x),, then the variance is given by n a 2 = E (X - m )2 = ∑(x; - m )2 f (x ) = ∑(x - m )2 f (x) - ...........................(8) x j j =1 In the special case of (8) where the probabilities are all equal, we [ ] have s 2 = (x - m )2 +(x - m )2 +.............. (x _m )2 ) - ...................(9) 1 2 n n 47 Which is the variance for a set of n numbers x...................... x 1 n If X takes on an infinite number of values ( ) ¥ ( ) ( ) X , x ..................,then s 2 = E X - m 2 = ∫ X - m 2 f x dx................................(10) 1 2 x - ¥ Provided that the integral converges.
3.5.
THEOREMS ON VARIANCE Theorems 3.5.1 s 2 = E (x- m )2x) = E (x )- m 2 = 2 = E (x2) - (E(x)2 where m = E (x).
Theorem 3.5.2.
If C is any constant, Var(c x) = C2 Var (x) - ............................................................................(1) Theorem 3.5.3 The quantity E (( x - a)2) isa minimum when a = m =E (x) Theorem 3.5.4 If X and Y are independent random variables Var ( X +Y) =Var (X) + Var (Y) or s 2 = s 2 +s 2 ...............................................................(12) x1y x y Var(X - Y) =Var (X) - Var (Y) or s 2 - y =s 2 - s 2 ..........................................................................(13) x x y 48 3.6 MOMENTS The rth moment of a random variable X about the mean m , also called the rth central moment, is defined as m = E (x- m )r ..........................................................................(14) r Where r = o, 1, 2, ------ it follows that m =1, m , = o and m = s 2 i.e second 0 2 moment about the mean is the variance.
We have, assuming absolute convergence.
m = ∑ (x - m )r f (x) (discrete) .................................................................(15) r variabl;e ¥ m = ∫ (x- m )r f (x) dx (continuous) ..............................................(16) r - ¥ variable The rth moment of x about the origin, also called the rth raw moment, is defined as m 1 = E (x r) ......................................................................................(17) r The zero moment and the first moment about the mean are respectively 1 [ ] and 0 Since m = E (x - m )o = E(1) =1 o [ ] And m 1 = E (x - m )1 = E(x) = E(m ) = m - m = o The second movement called the variance and is dented by s 2.
[ ] ( ) m = E x - m 2 =s 2 2 This indicate the strength on dispersion of the distribution generated.
49 Generally moment about the mean describe the shape of the distribution of a random variable.
3.7 MOMENT GENERATING FUNCTIONS Although the moment of some distribution can be determined directly by evaluating the necessary integral or sum.
There exist on alternative technique which often provide considerable signification.
This technique is based on the moment generating function which is given by M (t) = E (e tx) = ∑ e t x f (x) ..........................................(18) x (discretevariable) ¥ M (t) = E(etx) = ∫ e t x f (x) d x ..................................(19) x - ¥ (continuousvariable) tx t2 x2 t3x3 tr xr but etxh =1+ + + -+ - - - - - !
2!
3!
r!
 t2 x2 tr xr  This for the discrete case M x (t) = ∑ 1+tx + -+ - - + - -  f (x)  21 r1  t2 t r ⇒ M x (t) = ∑ f (x + t ∑x f (x) + ∑x2 f (x)-+ - - ∑ xr f (x)-+ - - 2!
r!
t2 tr M (t) = 1+m 1t + m 1 -+ - - m 1 + .............................................................(20) x 2 2!
r !
Thus if we expand M x (t) as a power series in t, the coefficient of tr ism 1 r which is the rth moment about the origin of distribution X.
50 You observed that the maclaurins series of a function M x(t) with coefficient tr is the rth derivation of the function with respect to t at t = o. r!
Hence another way of determine the movement of a distribution is given by variation drM x(t) m 1 =   r  dtr  t=o 3.8 THEOREMS ON MOMENT GENERATING FUNCTION Theorems 3.8.1 If M x(t) is the moment generating function of the random ( ) variable x and a and b „ 0 are constants, then the moment generating function of x+a is M (t) = e atb Mx t   b  x+a b  b  Theorem 3.8.2 :- If X and Y are independent random variables having ( ) ( ) moment generating fractions M t and M t , respectively, then x r M (t) = M (t)M (t)..............................................................(21) x+y x y i.e The moment generating function of a sum of independent random variables is equal to the product of their moment generating functions.
51 3.9 CHARACTERISTIC FUNCTIONS Characteristic function of a random variable X(w) defined on (W , (cid:217) , p) provides a powerful and application tool in the theory of probability.
Characteristic function has one important advantages over moment generating function because it can be need to prove both the weak law of large numbers and the Central limit theorem which will be treated in the next unit.
Definition: Let X be a random variable with probability distribution function.
The characteristic function of x is defined for real t by y (t) = ∫ eitx d F(x) = ∫ eitx p(dx) R ( ) ( ) = E eitx = E eitx Where eitx = cost + i sint x ( ) ( ) E eitx = E cost x + i E sin t x Properties of characteristic function (a) (i) y (t) is informally continuous on the real line.
(ii) Y (o) =1 ( ) (iii) f t £ 1 for all t .
Since eitx = 1 52 Proof: { (i) f (t + h) - f (t) = ∫¥ ei(t + h) xeith }p(d x) - ¥ £ ∫¥ ei(t- h x - eitx p(dx) - ¥ ¥ ¥ =∫ eitx eihx- p(dx) £ ∫ eihx p(dx) - ¥ - ¥ By the dominated convergence theorem, we have ¥ Lim ¥ ∫ eitx e ihx - 1 p(dx) =∫ lim e ihx - p(dx) =0 hﬁ 0¥ - ¥ hﬁ o Note: (i) eih x - 1 £ 2 (ii) The limit tends to zero independently of t. Thus, f (t + h)- f (t) ﬁ o independent of t. Hence f (t) is uniformly continuous on the real line (iii) e itx = cost x + i sin t x = cos2 t x + sin2 t x = 1 f (t) = ∫eitx p(dx) £ ∫ eitx p(dx) =∫ p(dx) =1 (b) The characteristic function of the sum of independent random variables is the product of their characteristic functions 53 Proof : Let Sn = X +X +- - + + +Xn whereX , -X - - - X n are independent 1 2 1 2 random variables.
Then [ ] [ ( ) ] y (t) = E eitsn = E eit x +...............x 1 n sn = E (e itx ) E (e itx ) --- E (E (e itx ) 1 2 n = y (t) y (- t) - - y (t) for all real t. x x2 xn ( ) If X are independent and identically distributed then i y (t) = y (t) n sn x (c) Unlike movement generating functions, y (t) x Is finite for all variable x and all real number t. The reason being that eit is bounded while it is unbounded for - ¥ — — ¥ d. The distribution function of X and hence the Pdf, if it exists can be obtained from the characteristic function using an “Inversion formula:, ( ) 1 h If X is integer valued random variable then f n = ∫ e itn y (t) dt x 2(cid:213) (cid:213) x 1 ¥ If x is a continuous random variable, then f (x) = ∫ eit y (t) dt x 2(cid:213) ¥ x ¥ assuming ∫ y (t) — ¥ ¥ x 54 e. Properties of characteristic function enable us to prove both the weak law of large numbers and the central limit theorem.
Properties (c) (d) and (e) are important advantages of functions over moment generating function.
(f) If two random variables have the same characteristic function they have the same distribution function.
( ) (g) If x has finite nth moment, then m (n) t exists and is continuous in t. x m (n)(t) = dn E(eitx) E{(ix)n eitx} x dtn ( ) ( ) m (n) 0 Thus E xn = x in Example 3.9.1: let x have an exponential distribution with parameter b find the characteristic function of x Solution y (t)=E (eitx) = ∫¥ e itx be - bx dx x ¥ b = b ∫¥ e- (b- it) x dx= ¥ b - it Example 3:9:2 Let x be informally distributed an (-1,1).
Find the characteristics function of X.
55 Solution y (t) )= E = ∫1e itx 1dx x - 1 2 1 e = itx 1 t „ 0 2 it - 1 1 eit - e - it sint =   = 2  it  t e it = cost + i sint, Note: e - it = cost - i sint, e it - e - it 20 sint.
Example 3.9.3 : Find the characteristic function of the random variable X having density function give by  1 1x1 £ a f(x =  2a 0 otherwise Solution E (eitx) = ∫¥ e itx f (x) dx = 1 ∫ae itx dx ¥ 2a a 1 e itx eitx - e - itx = a = 2a it - a 2iat sin at sniJ = = at J Using Euler’s formula with J =at Example 3.9.4 : Find the expectation of a discrete random variable x whose probability function is given by 1x [ ] f (x) =  x = 1, 2, 3, ..................................... - 2 56 Solution ¥ 1x 1 1 1 We have E (x) = ∑x   = + 2   + 3  + 2 2 4 8 x =1 To find the sum, 1 1 1  1  Let S = + 2   + 3   + 4  + ……………………………….. 2 4 8 16 1 1 1  1  Then S = + 2   + 3   + ……………………………….
2 4 8 16 1 1 1 1 1 By subtracting S = + + + +...................... = 1 2 2 4 8 16 Therefore, S = 2 Example 3.9.5: A Continuous Random Variable X Has Probability 2e - 2x x > 0 Density given by f (n) =  0 x £ 0 Find (¥ ) E (x) (b) E (x2) ¥ ¥ (¥ ) E (x) = ∫ x f (x) dx = ∫ x (2e- 2x) dx ¥ ¥ ¥ = 2 ∫ xe - 2x dx ¥ ( ) e  e- 3x 1 = 2∫(x)  - 2x - (1) ∫∫¥ = 2  4 h 2 ¥ ¥ E(x2) = ∫ f (x) dx = 2 ∫ x2 e - 2xdx - ¥ o (b)  e- 2x ( )e- 2x  e- 2m  1 = 2(x2)  - 2  - 2x  4  + 4 8  ∫∫0e = 2 57  Example 3.9.6 Find (a) the variance, (b) The standard deviation of the sum obtained in tossing a pair of fair dice.
Solution 1 1 1 7 E (x) = E (y) = 1  + 2  +................... 6   = 6 6 6 2 1 1 1 Hence E (x2) = E (y2) = 12   + 2 2   + ..............62  6 6 6 1 1 1 = 1  + 4  + ............36  6 6 6 = 91 6 91 7 91 49 Var (X) = Vor (Y) = -   2 = - Then 6 2 6 4 = 35 12.
And since X and Y are independent var (x + y) = var (x) + var (y) 35 35 70 = + = 12 12 12 = 35 6 (c) Standard Deviation = varnance ( ) 35 ie s = var x+ y = x1+ y 6 Example 3.9.7: The random variable x can assume the values 1 and -1 with probability ½ each.
Find (a) the moment generating function (b) the first four moment about the origin.
58 Solution: ( ) (a.)
E eitx = et (1) 1 + et(- 1) 1 2 2 = 1 et+ e- 1  - ....................................................(1) 2 t2 t3 t4 (b) we have et = 1 + t + - + + ...................................(2) 2!
3!
4!
1 ( ) t2 t4 then from (1) et + e t = 1 + + + ......................................... 2 2!
4!
t2 t3 t4 But from (2) Mx (t) = 1 + m 12 +m 1 2!+m 13 3!
+ m 1 4 4!
+...................................... Then compering (1) and (2) we have m = 0, m 1 = 1, m 1 = 0, m 1 = 1 2 3 4 The odd moment are all zero, and the even moments are all one Example 3.9.8: A random variable x has density function given by ( ) 2e - 2x x ‡ 0 f x) =  0 x < 0 Find (a) the moment generating function, (b) the first four moments about the origin Solution: ( ) ¥ tx (a) M (t) = E etx = ∫ e f (x) dx x - ¥ ¥ = ∫ e tx (2e - 2x) dx ¥ ¥ = 2 ∫ e (t- 2) x dx ¥ (t- 2)x = 2e ¥ t - 2 o 59 2 = assuming t < 2 2- t (b) If t < 2 we have 2 1 t t2 t3 t4 = = 1 + + + + 2 - t 1- t 2 4 8 16 2 t t3 t4 But Mx (t) = 1+ m + m 1 + m 1 + m 1 + .................................. t 2 2!
3 3!
4 4!
Therefore, on comparing terms m = 1 2 1 3 3 m 1 = , m 1 = , m 1 = 2 2 3 4 4 2 3.10 CONCLUSION In this unit you have learnt mathematical expectation of random variables for discrete and continuous random variables.
You also learned Variance and Standard Deviation for discrete and continuous random variables.
Some important theorems on Expectation, Variance and Standard Deviation are stated.
Moment, Moment Generating functions and Characteristic function are fully treated and related working example on each concept are easily shown to make the learning of the unit more meaningful.
60 3.11 SUMMARY In this unit expectation of random variables that you have studied included the following: - Meaning of Expectation for Discreet and Continuous random variables - Mathematical Expectation for discreet and continuous random variables.
- Expected Value for Variance and Standard Deviation .
- Theorems on the Expectation of Random Variables - Moment, Moment Generating Function for Discrete and Continuous Random Variables - Characteristic Functions of Random Variables - Working examples on the Mathematical Expectation of Random Variables Exercise: 3.11.1 (ASE) The density function of a random variables X is given by 1  x 0 < x < 2 f (x) = 2  0 otherwise find (a) E (x) (b) E (x2) 61 3.1.2 TUTOR MARKED ASSIGNMENT (TMA) Exercises 3.12.1: if X is random variable of Exercise 3.11.1 above find E (3 x2 - 2 x) Exercises 3.12.2 A random variable X has E(x) = 2, E (x2) = 8 find (a) Var (x) (b) s x 3.13 Reference / Further Reading / other resources Murray R silage et al (2009) Probability and statistics.
Third addition published by Mc Graw Hill Dr R.A Kasumu (2003) probability theory first edition published by FATOL VENTURES LAGOS, DR S.A Okunuga (1998) Probability Distribution 2 lecturer Materials.
62 LIMIT THEOREM 4.0 INTRODUCTION The purpose of this unit is to acquaint the students with the liquid theorems on cheby shev’s inequality, convergence, weak laws of Lange numbers, strong law of large number.
Some of the theorems are proved and related working examples are shown 4.1 OBJECTIVES At the end of this unit student should be able to • State and prove chebyshev’s inequality • Define Convergence of random variables • State and prove some theorems on convergence in measure • State and prove weak law of large numbers • State the strong law large numbers 4.2 CHEBYSHEV’S INEQUALITY This is a important theorems in probability and statistics that reveals a general property of discrete or continuous random variable having finite mean and variance in known under the name of chebyshev’s inequality Themes 4.21 suppose that X is a random variable (discrete or continuous) 63 Having mean m and variance s 2, which are finite.
Then if epsilon (e) is a positive number, s 2 P (1x- m ‡ E) ‡ e 2 or with e = ks 1 P ( x - m 1 ‡ k s ) £ k2 Proof : We shall proof for continuous random variables.
A proof for discrete variables is similar it integrals are replaced by sums.
If f (x) is the density function of X, then s 2 = E (X - m )2 = ∫¥ (x- m )2 f (x)dx  ¥  Since the integrand is nonnegative, the value of the integral can only decrease when the range of integration is diminished .
Therefore, s 2 ‡ ∫ (x- m )2 f (x) dx ‡ ∫ e 2 f (x) dx 1x - m ) ‡ E 1- m 1 2E = e 2 ∫ f (x) dx 1-x m)‡ E But the last integral is equal to P(1x - x1‡ E Hence, 64 s 2 P (1x - m 1 ‡ e) £ e 2 4.3 CONVERGENCE OF RANDOM VARIABLE Definition 1 A sequence of random variables X is said to converge in n distribution or in law to X (we write Xn (cid:190) ﬁ(cid:190) L X) if the corresponding sequence of distribution functions Fn, Fn ﬁ F as n ﬁ ¥ .
In this case F is a distribution function of F. n Example: Consider the random variable X which is a Binomial n Bi (n,p) , then random variable Bi (1,p), Bi (2,p), Bi (3,p)..........................ﬁ X Bi (n, p) tends to normal Furthermore if the corresponding F distribution gives: ( ) Fn = Pr (X £ x) = ∑ n px qn- x ﬁ F n If Xn (cid:190)ﬁ(cid:190) l X ie Fn (X) ﬁ F distribution function of X Then (1) Cn(t) ﬁ C(t) Where Cn(t)stands for corresponding characteristic function of random variable X n (2) for any bounded continuous function ∫gd Fn (x) ﬁ ∫gd F { } Definition 2: The sequence Xn of random variables is said to converge to a random variable X if 65 { } P Xn - X > e ﬁ 0 as n ﬁ ¥ for ane > 0. we indicate this by Xn (cid:190) ﬁ(cid:190) P X { } Definition 3: The sequence Xn of random variables is said to converge in mean square to a random variable X it { } e Xn- X 2 - 0 as nﬁ ¥ .
we indicate this by Xn (cid:190) ﬁm(cid:190) .s X { } Def intion 4: The sequence Xn of random variab{le is said }to converge with probablity one(or almost surely , a.s.) to a constant C if P lim X = c = 1 ﬁn ¥ n We indicate this by Xn ﬁ c or X (cid:190) X(cid:190)(cid:190) n wﬁ(cid:190).p.
C n   or equivalently,lim PSup Xn - C >e = 0, for every e > 0.
  n‡ N Note: { } 1.
In general f ﬁ f does not imply that Xn converges to a random n ( ) variable for example, Supose X is N 0,1 and for all n, let Xn = - X ( ) ( ) Then Xn isN(0,1)That is, Fn x = F x 1 Fn (x) = (f(x) for all n. But { } { }  e  P Xn - x ‡ e = P - 2 x ‡ e = P  x‡  ﬁ 0 as n ﬁ ¥  2 Hence Xn (cid:190) ﬁ(cid:190) p X (ii)If X (cid:190) ﬁ(cid:190) P X then Fn ﬁ F. letting e ﬁ 0, n We see that F (x) ﬁ F(x) n 66 4.4 DEMOVRE’S THEOREM Let X be Bi (n,p) then, n Xn- np Yn = ﬁ N (o ,1) as n ﬁ ¥ npq ( ) Proof Pr (X = r) = n Pr qn- x n r Cn (t) = E (eitx) xn ¥ = ∑eitxPr(Xn =x) x=0 ¥ itx ( ) = ∑ e n P x qn- x r m =0 ¥ ( )( ) = ∑ n e itx P x qn- x r m =0 ( ) = Pe it + q n Therefore characteristic function of Yn = E (eityn)   itX itnp = Ee n -   jnpq npq    np  itxn  = e- it q E  e    npq   i.e Cy (t) = e- it nqp p evniptq+ qn n     67  it  np   ﬁ log C yn (t)= - it q + n log  Pe evnpq +1 - p    np   it  = - it +n log 1+ p e 1 q   jnpq -  z2 But ez = + z + + ................................................................ 2!
z2 z3 And log (1+ z) = z - + ................................................................ 2!
3!
\ log y (t) =it np  + n log 1 + p  it + (it)2 + (it)3 + }} 0⇒ order  n q    npq 2npq npq)3!
np   it t2  I  p2 - t2  1  = - it q + n Log 1+ P  npq - 2npq + 0 n32 ....- 2 npq + 0 n2    np np nt2p np2t2  1  = - it + it - + + O q q 2npq 2npq n1   2 t2  1  = - + 0  2  n  - t2 Cy (t) ﬁ e 2 as n ﬁ ¥ - t2 ( ) but e 2 is cheracteristic function of a stadard normal distrubtion N 0.1 ( ) \ - Yn (cid:190) ﬁ(cid:190) L N 0,1 Theorem If Xn is binomial Bi (n, p) asn ﬁ ¥ ,P ﬁ 0, such that 68 ( ) np = l , Xn (cid:190) ﬁ(cid:190) L Poison l ( ) Recall: Cx (t) = Peit + q n n l  l n  l ( )n  eit + + 1-  = 1+ eit - 1   n  n  n    zn lim 1+  = ez  n ( )  l ( )n lim Cxn t = lim 1+ e it - 1   ﬁnPﬁ ¥0 ﬁn ¥  n  npﬁ l = el eit - l ( ) Which is the characteristic function of poison l ,as n ﬁ ¥ , p ﬁ 0, np = l. 4.5 CENTRAL LIMIT THEOREM Let X , X , .............................., X be independently and identically 1 2 n distributed random variables each with mean m and variance ¶ 2 .
X + X + .............................+ x ) - m Let Y = ( 1 2 n n n ¶ x - m n = ¶ Then yn (cid:190) ﬁ(cid:190)L N (0,1) X1+ X2 + .............................. + Xn X = n 69 nX = X + X + ..................................+ X 1 2 n nX - m nX = ns Poof: ( ) Let C(t) be the characteristics function of X so that C(t) = E eitxi 1 Since the random variable are identical ( ) ( ) ⇒ E (eitx1) = E eitx2 =.....................................=E eitxn =C (t) ( ) Ctn (t) = E eity n  X + ............... X - m  = E eXi + 2 n .
 s n   eitx itX2 itX itu  = E  1 + .......... + n - .
 ns n s ns ns  itu  itx1   itx2   itxn  = e- E e ns .
E e - -  - - - E e  ns    ns   n s  itu   t n C (t) = e - C   Y- n ns   n s  (it)2 But C (t) =1+ a it +a ................................................... 1 2 2!
Where d = m 1 = m 1 1 s = m = m 2 +s 2 2 2 itu  tu  it 2 m 2  \ C (t) = e- 1+ i +   1 + 0(t3) Yn ns  ns  ns  2!
   70 Taking log to get e log C (t) = - itm + n log 1+ itm - t2m12 - + 0(t3)n e Yn ns  ns 2s n2  X2 X3 But log (1+X) = X – + - ................ 2 3 \ logC (t=) - it n m + n ∫ cntms - ( t2m1 + - (itm )3 + ) ........ Yn s 2 ns 2s 2n t2 = - (m 2 - m 2) as n ﬁ ¥ 2s 2 2 = - t2 t2 ) \ C (t) ﬁ e- (remove the log Yn 2 Hence Yn (cid:190) ﬁ(cid:190) L N (0,1) Definition 1: A sequence of random variable Xn is said to “ converge in probability”(weakly) to the constant C if the limit P(Xn - C > k ) = 0 where C > 0 This is written as Xn (cid:190) ﬁ(cid:190) P C Definition 2: A sequence of random variable Xn is said to converge in probability to X if Xn ﬁ X (cid:190) ﬁ(cid:190) P 0 71 4.6 KHINCHINE’S THEOREM (Weak law of large Numbers) let X1, X , ……………….. Xn be a 2 sequence of independently and identically distributed random variable each with mean m x + X + X +....................................+ Xn Let Xn = 1 2 3 n Then X n (cid:190) ﬁ(cid:190) P m Poof: C (t) = E (e itx ) 1 =1+ i tm + 0(t2) ( ) Let C (t) = E e itx n X ( ) = E e it(x1 + X +....................+X 2 n ( ) = E e itx E ( e1tx ).
E (eitx ) 1 2 2   t n ( ) ( ) ( ) =C n = E eit x1 .E eitx2 .E eitxn  itm ( )n = 1+ + 0 t2    n  =y Cxn (t) ﬁ eitm as n ﬁ ¥ hence, Xn (cid:190) ﬁ(cid:190) P m [ ] meaning l im P x - m > k = 0 ﬁn ¥ eitm is the characteristics function of a random variables taking the value of m with probability 1 72 THEOREM 4 { } The strong law of large Numbers let Xi be a sequence of independent random variables such that  ¥ 612  E Xn) = ), Var (X o) = 01- 2 and ∑ < ¥ ,    12  1 1 n Then the sequence ∑xi converge to 0 almost easily ie) n e=1 Proof: Xn Let Y = n = 1, 2,............................. n n E (Yn) = 0 and ∑Var (Yn) = ∑s 2n < ¥ , n2 ¥ ¥ Xn ∑Yn = ∑ n n=1 n=1 Converge almost easily and hence 1 lim ∑Xi = 0 a.e ﬁn ¥ n Corollary 1 If (Xi) is a sequence of independent identically distributed random variables such that E(Xi) = 0 and variance such that s 2 < ¥ ,then n ∑ Xi Sn = 1 ﬁ o almost surely n n 73 Corollary P Let everything be as from the above theorem except that E(Xn) = m and Var (Xn) for all n. Sn Then ﬁ m almost surely n Example 4.61  1 0 with Prob  n Suppose X  h =   1 1 with prob1-  n X = 1 with probability 1.
Solution.
Consider Xn - X .The only possible values of xh - x1are o and 1.So  1 0, with prob -   n Xn - X =  1  1 with prob   n 0 X < 0    { }  1  P Xn- X £ X =1- 0£ X < 1   n  1 X ‡ 1  74  Xn- X £ X  0 x < 0 Lim P =    1 x ‡ = 0 X (cid:190) ﬁ(cid:190) P X n 0, x < 0    1 0, X < 1 F (X) =  , 0£ x < 1, F (x) =   n n.
1, X ‡ 1 1, X ‡ 1  Fn (x) ﬁ F(x) forall X Note: X (cid:190) ﬁ(cid:190) d X ﬁ X n n Example 4.6.2 Let x , x , ----- X be a sequence of independent identically distributed 1 2 n poison random variable with parameter l.. Then  - } - l E  X - l 2 =Var (X ) = ﬁ o as n ﬁ ¥  n n n - - Therefore, X (cid:190) (cid:190)Mea(cid:190)n(cid:190)squﬁ(cid:190)are l or X (cid:190) (cid:190)quad(cid:190)ratic(cid:190) mﬁe(cid:190)an l n n Example 4.
Let X , X -------, Xh be a sequence of i i d 1 2 Poison random variables.
Than by chebychev’s inequality  - } Var (X ) pX- l >l ‡ n l  l = ﬁ 0 as n ﬁ ¥ nl2 75 Therefore, X (cid:190) ﬁ(cid:190) P l n EXAMPLE 4.6.3 Convergence in mean square implies convergence in probability.
By chebychev’s inequality  } (X - X 2) P  X - X >l £ E n ﬁ 0 as n ﬁ ¥ n l2  ¥ Thus X (cid:190) ﬁ(cid:190) P X n In general, convergence in rth mean implies convergence in probability.
4.7 CONCLUSION In this unit you have learned chebyshev’s inequality, the proving and the application of the theorem.
You have also learned convergence of random variables with different definitions, Demovre’s theorem, and Central limit theorem by using characteristics function for finding convergence.
Moreover, the weak and strong law of large numbers are discussed the and related working examples on the theorems are treated.
76 4.80 SUMMARY In this unit the following concept have been learned.
2.
Chebyshev’s Inequality 3.
Convergence of Random Variables 4.
Demovre’s Theorem 5.
Central limit Theorem 6.
Weak law of Large Numbers 7.
Strong law of Large Numbers 8.
Relevant examples on the theorems are treated.
EXERCISE: 4.80.1 (S A E) A random variable X has mean 3 and variance 2.
Use Chebyshev’s Inequality to obtain an upper bound for (a) P( x- 3 ‡ 2), (b) P ( x -3 ‡ ).
EXERCISE 4.80.2 Show that the (weak) law of large numbers can be stated as.
S Lim P( n - m <l) = n 77 4.90 TUTOR MARKED ASSIGNMENT (TMA) (a) Show that the sequence X of random variable q is said to n converge (i) In mean square to Random Variable X.
(ii) With probability one almost surely to a constant c. (b) State and Prove Central limit Theorem.
5.f References / Further Reading Dr RA kasumu (2003) Probability Theory first edition published by Fatol Ventures Lagos Dr S.A Okunuga (1998) Probability Distribution 2 lecture Materials Murray R. Spiegel et al Schaum’s Outhies (2009) Probability and Statistics Published by Mc Graw Hill.
78
