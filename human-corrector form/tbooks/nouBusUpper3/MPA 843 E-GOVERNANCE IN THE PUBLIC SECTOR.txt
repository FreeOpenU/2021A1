 MPA 843 E-GOVERNANCE IN PUBLIC SECTOR Course Writer Engr.
(Dr.) Idigo V.E Department of Electronics and Computer Engineering Nnamdi Azikiwe University Awka, Anambra State Nigeria Course Coordinator: Mrs. Ibeneme Nwamaka P School of Management Sciences National Open University of Nigeria UNIT 1 THE CONCEPT OF GOVERNMENT AND APPARATUS OF GOVERNMENT TABLE CONTENTS 1 1.0 Introduction 2.0 Objectives 3.0 Main content 3.1 History of government 3.2 Functions of government 3.3 Separation of power in government 3.4 Classification and types of government 3.5 Apparatus of government.
3.6 Governance and government services through ICT 4.0 Conclusion 5.0 Summary 6.0.
References.
1.0 INTRODUCTION Government is a political organization comprising the individuals and institutions authorized to formulate public policies and conduct affairs of state.
Governments are empowered to establish and regulate the interrelationships of the people within their territorial confines, the relations of the people with the community as a whole, and the dealings of the community with other political entities.
Government applies in this sense both to the governments of national states, such as the federal government of the U.S., and to the governments of subdivisions of national states, such as the state, county, and municipal governments of the U.S. and the governments of the provinces of Canada.
Such organizations as universities, labor unions, and churches are also broadly governmental in many of their functions.
The word government may refer to the people 2 who form the supreme administrative body of a country, as in the expression “the government of Prime Minister Churchill 2.0 OBJECTIVES After studying this unit, you should be able  Understand and explain the History of government  Understand and explain the the general Functions of government.
 Understand and Explain the Separation of power in government.
 Understand and explain various Classification and types of government  Understand and explain various apparatus of government  Understand and explain the various applications of ICT in provision Governance and government services.
3.1 HISTORY OF GOVERNMENT The despotic empires of Egypt, Sumer, Assyria, Persia, and Macedonia were followed by the rise of city-states, the first self-governing communities, in which the rule of law predominated and state officials were responsible to the citizens who chose them.
The city-states of Greece, such as Athens, Corinth, and Sparta, and of that part of Asia Minor dominated or influenced by the Greeks, provided the material for the speculative political theories of Plato and Aristotle.
Aristotle's system of classifying states, which influenced subsequent political thought for centuries, was based on a simple criterion: good governments are those that best serve the general welfare; bad governments are those that subordinate the general good to the good of the individuals in power.
Aristotle distinguished three categories of government: monarchy, government by a single individual; aristocracy, government by a select few; and democracy, government by many.
The later Greek philosophers, influenced by Aristotle, distinguished three degenerate forms of the classes of government defined by him.
These were, respectively, tyranny, rule by an individual in his or her own interest; oligarchy, rule by a few people in their own interest; and ochlocracy, mob rule.
Still other categories of lasting historical significance are theocracy, rule by religious leaders; and bureaucracy, the excessive domination of government by administrative officials.
Ancient Rome, which evolved from a city-republic to the seat of a world empire, also greatly influenced the development of government in the Western world.
This influence was derived in part from the great Roman achievement in formulating clearly for the first time the principle that constitutional law, establishing the sovereignty of the state, is superior to ordinary law, such as that created by legislative enactments.
After the fall of Rome, the Roman concept of a universal dominion was kept alive during the Middle Ages through the formation of the Holy Roman Empire; and also, in part, by the establishment, through canon law and ecclesiastical courts with jurisdiction over secular affairs, of the ruling body of the Roman Catholic church.
The effect of these influences was to retard the development of national territories and governments after tendencies in that direction had manifested themselves among the feudal principalities of Europe.
On the other hand, the struggle of the feudal barons to limit the absolute power of their monarchs eventually produced many contributions to the theory and institutions of representative government.
During the Middle Ages commercial city-states arose in Europe.
These city-states eventually formed the Hanseatic League and the powerful Italian city-republics, or communes 3 The final emergence of national governments is attributed to two principal causes.
One comprises a number of underlying economic causes, including a great expansion in trade and the development of manufacturing.
These conditions began to undermine the feudal system, which was based on isolated and self-sufficient economic units, and to make necessary the creation of large political units.
The other cause was the Reformation, which succeeded in eliminating the restraining influence of the Catholic church on political development in a number of European countries.
The modern nation-state became a definite form of government in the 16th century.
It was almost entirely dynastic and autocratic.
The will of the reigning monarch, in theory and often in practice, was unlimited; the famous aphorism of King Louis XIV of France, “L'état, c'est moi” (“I am the state”), was not an idle boast, but an expression of existing reality.
In time, however, the demand of the bourgeoisie for constitutional and representative government made itself felt, and the unlimited powers of monarchs began to be challenged.
In England, the Glorious Revolution in 1688 restricted such powers and established the preeminence of Parliament.
This tendency culminated in two events of historic importance, the American Revolution, beginning in 1775, and the French Revolution, beginning in 1789.
Historians generally date the rise of modern democratic government from these events.
The history of government in the 19th century and in part of the 20th is notable for the broadening of the political base of government through extension of suffrage and other reforms.
A tendency that became especially marked in the 20th century was the development and implementation of the concept that government, in addition to maintaining order and administering justice, must be an instrument for administering public and social services including, among many others, conservation of natural resources, scientific research, education, and social security.
Between 1945 and 1951, the Labour Party government of Britain extended the responsibilities of government to include nationalization of a number of basic industries in a need for stringent economic planning.
Other outstanding developments of the 20th century were the appearance of the corporative state and of totalitarian governments in a number of countries, and the first so-called proletarian dictatorship in history, the Union of Soviet Socialist Republics.
From the late 1940s until the end of the 1980s, most eastern European countries adjacent to or near the USSR had governments similar in many respects to that of the USSR 3.2 FUNCTIONS OF GOVERNMENT In the theory of political science, the function of government is to secure the common welfare of the members of the social aggregate over which it exercises control.
In different historical epochs, governments have endeavored to achieve the common welfare by various means.
Among primitive peoples, systems of social control were rudimentary; they arose directly from ideas of right and wrong common to the members of a social group and were enforced on individuals primarily through group pressure.
Among more civilized peoples, governments assumed institutional forms; they rested on defined legal bases, imposing penalties on violators of the law and using force to establish themselves and discharge their functions 3.3 SEPARATION OF POWERS IN GOVERNMENT The doctrine and practice of dividing the powers of a government among different branches to guard against abuse of authority.
A government of separated powers assigns different political and legal powers to the legislative, executive, and judicial branches.
The legislative branch has the power to make laws—for example, the declaration of what acts are to be regarded as criminal.
The executive branch has the authority to administer the law—primarily by bringing 4 lawbreakers to trial—and to appoint officials and oversee the administration of government responsibilities.
The judicial branch has the power to try cases brought to court and to interpret the meaning of laws under which the trials are conducted.
A government of separated powers is less likely to be tyrannical and more likely to follow the rule of law: the principle that government action must be constrained by laws.
A separation of powers can also make a political system more democratic by making it more difficult for a single ruler, such as a monarch or a president, to become dictatorial.
The division of powers also prevents one branch of government from dominating the others or dictating the laws to the public.
Most democratic systems have some degree of separation of powers, but the United States stands as the preeminent example of the practice The doctrine of separation of powers developed over many centuries.
The practice of this doctrine can be traced to the British Parliament’s gradual assertion of power and resistance to royal decrees during the 14th century.
English scholar James Harrington was one of the first modern philosophers to analyze the doctrine.
In his essay Commonwealth of Oceana (1656), Harrington—building on the work of earlier philosophers Aristotle, Plato, and Niccolò Machiavelli—described a utopian political system that included a separation of powers.
English political theorist John Locke gave the concept of separation of powers more refined treatment in his Second Treatise of Government (1690).
Locke argued that legislative and executive powers were conceptually different, but that it was not always necessary to separate them in government institutions.
Judicial power played no role in Locke’s thinking.
The modern idea of the separation of powers was explored in more depth in The Spirit of the Laws (1748), a study by French political writer Baron Montesquieu.
Montesquieu outlined a three-way division of powers in England among the Parliament, the king, and the courts, although such a division (he did not use the term “separation”) did not in fact exist at the time.
Montesquieu followed earlier thinkers in arguing that there was a necessary relationship between social divisions and these different powers.
In particular, Montesquieu contended that executive power could be exercised only by a monarch and not by an elected administrator—a view wholly discarded in the Constitution of the United States.
Harrington, Locke, Montesquieu, and other writers saw the concept of the separation of powers as a way to reduce or eliminate the arbitrary power of unchecked rulers.
Separation of powers thus became associated with the closely related concept of checks and balances—the notion that government power should be controlled by overlapping authority within the government and by giving citizens the rights to criticize state action and remove officials from office.
3.4CLASSIFICATION/TYPE OF GOVERNMENT Governments are classified in a great many ways and from a wide variety of standpoints; many of the categories inevitably overlap.
A familiar classification is that which distinguishes monarchic from republican governments.
Scholars in modern times, especially in the 20th century, have stressed the characteristics that distinguish democratic governments from dictatorships.
In one classification of governments, federal states are distinguished from unitary states.
Federal states, such as the U.S. and Switzerland, comprise unions of states in which the authority of the central or national government is constitutionally limited by the legally established powers of the constituent subdivisions.
In unitary states, such as the United Kingdom and Belgium, the constituent subdivisions of the state are subordinate to the authority of the national government.
The degree of subordination varies from country to country.
It may also 5 vary within a country from time to time and according to circumstance; for example, the central authority of the national government in Italy was greatly increased from 1922 to 1945, during the period of the Fascist dictatorship.
In one classification of democratic nations, parliamentary or cabinet governments are distinguished from presidential ones.
In parliamentary governments, of which the United Kingdom, India, and Canada are examples, the executive branch is subordinate to the legislature.
In presidential governments, such as in the U.S., the executive is independent of the legislature, although many of the executive's actions are subject to legislative review.
Still other classifications hinge on varying governmental forms and powers among the nations of the world.
Some types of government are explained below: Absolutism, political system in which there is no legal, customary, or moral limit on the government’s power.
The term is generally applied to political systems ruled by a single dictator, but it can also be applied to seemingly democratic systems that grant sweeping powers to the legislature or executive.
Absolutism was one of the most common forms of government for much of the 20th century, and it is still common today.
Absolutism has taken wide-ranging forms such as military dictatorships in Latin America, authoritarian communism in the Union of Soviet Socialist Republics (USSR) and Eastern Europe, and dictatorships in Africa.
Despite the wide range of forms of absolutism, most 20th-century absolutist regimes have shared a few basic traits.
These include centralization of power, close control of social groups, absence of competing political parties, and the outward appearance of popular representation Anarchism, political theory that is opposed to all forms of government.
Anarchists believe that the highest attainment of humanity is the freedom of individuals to express themselves, unhindered by any form of repression or control from without.
They hold that the perfection of humanity will not be attained until all government is abolished and each individual is left absolutely free.
One limitation on such freedom, however, is the ban against injuring other human beings.
From this limitation arises another: If any human being attempts to injure others, all well-meaning individuals have the right to organize against him or her, and the orderly class may repress the criminal class, although only by voluntary cooperation and not under any governmental organization.
The 19th-century French writer Pierre Joseph Proudhon is regarded as one of the founders of the system of so-called philosophic anarchism.
According to Proudhon and his followers, anarchism would exclude authority from society, setting up extreme individualism.
Philosophic anarchists, however, repudiate violent methods and hope for a gradual evolution of society toward anarchic organization.
Those anarchists who reject Proudhon’s theories maintain that the entire trend of human development is toward achievement by cooperation and that social cooperation can never be wholly voluntary.
Another school of anarchism, relying on organized action and even deeds of terror to achieve its purposes, grew out of the socialist movement and appeared toward the end of the 19th century.
At the congress of the International Workingmen's Association, or the First International, held at Basel, Switzerland, in 1869, the anarchists, led by the Russian revolutionist Mikhail Bakunin, were outvoted by the socialists; in 1872 the anarchists were expelled from the International.
Since that time socialism and anarchism have diverged sharply, although both are basically anticapitalist (Capitalism).
Philosophic anarchists continue to differ from socialists in their 6 emphasis on freedom from control, especially from state control.
Many anarchists participated in the trade union movement, usually as members of unions with a syndicalist program.
Others accepted the terrorist policy and practiced assassination.
Although most anarchists have not advocated terrorism, the popular view has unjustly identified all anarchism with violence.
Assassinations such as those of Humbert I, king of Italy; William McKinley, president of the United States; George I, king of Greece; and the president of France, Marie François Sadi Carnot—all committed by anarchists—have given support to the popular opinion of anarchy.
Some crimes that were blamed on anarchists, including a bomb thrown during the Haymarket Square Riot in Chicago (1886) and the payroll murder at South Braintree, Massachusetts, were almost certainly not committed by anarchists.
Persons who profess or advocate anarchism are denied admission to the United States by a federal law enacted in 1902.
Modern anarchists include the Russian follower of Bakunin, Prince Pyotr Alekseyevich Kropotkin, who called himself an anarchistic Communist, and the Americans Alexander Berkman and Emma Goldman.
Anarchism declined steadily as a political philosophy and as an organized movement during the second quarter of the 20th century Aristocracy (Greek aristos, “best”; kratos, “power”), form of government in which the sovereign power is vested in a small number of citizens who are theoretically the best qualified to rule, as opposed to monarchy, in which the supreme authority is vested in one person, and to democracy, in which the ultimate authority is exercised by the entire body of citizens or their representatives.
In an aristocracy, although the power of government is wielded by a few, theoretically the administration of government is carried on for the welfare of the many.
Whenever the interests of the people as a whole are made subservient to the selfish interests of the rulers, aristocracy becomes a form of government known as oligarchy.
Athens, before the period of the Persian wars of the 5th century BC, and Sparta, during practically its entire history, were aristocracies.
The same was true of Rome during the period of the Republic, lasting from the 6th to the 1st century BC.
During the Middle Ages no true aristocracy existed, for although political power reposed in the hands of a few, each feudal lord was sole master in his own domain.
In England, the government from the accession of the House of Hanover in 1714 through the 19th century, although parliamentary in form, was in fact an aristocracy, since king and Parliament alike were under the control of a few great Whig families.
At present the term aristocracy is used loosely and in a great variety of combinations to denote a select few with superiority in various categories, for example, an aristocracy of birth, of wealth, or of intellect.
Autocracy, political system under which one ruler wields unlimited power, restricted by no constitutional provisions or effective political opposition.
Totalitarianism is a modern form of an autocracy.
Usually, autocracy prevails in nations wherein the political, social, economic, or other conditions have made it impossible to develop institutions to protect the individual against the whims of one ruler.
Modern examples of autocratic government include the tsarist regimes in Russia, the National Socialist, or Nazi, party in Germany and the Communist rule of Mao Zedong in China.
An autocracy is considered the opposite of a democratic or constitutional government.
Republic (government) (Latin res publica, literally “the public thing”), form of state based on the concept that sovereignty resides in the people, who delegate the power to rule in their behalf to elected representatives and officials.
In practice, however, this concept has been variously 7 stretched, distorted, and corrupted, making any precise definition of the term republic difficult.
It is important, to begin with, to distinguish between a republic and a democracy.
In the theoretical republican state, where the government expresses the will of the people who have chosen it, republic and democracy may be identical (there are also democratic monarchies).
Historical republics, however, have never conformed to a theoretical model, and in the 20th century the term republic is freely used by dictatorships, one-party states, and democracies alike.
Republic has, in fact, come to signify any form of state headed by a president or some similarly titled figure, and not a monarch.
Representation, term applied to the system under which legislative, executive, and judicial officials may be chosen by vote of the people.
In most cases direct representation is used for legislative purposes alone.
The United States is an exception in that the same principle is adopted in the case of many executive and judicial offices: The president is the direct representative of the people, and most state constitutions provide for the popular election of the governor, judges, and other officials.
In England the political movements of the 17th century placed the supreme power in the hands of Parliament, which nominally represented the people.
The Reform Bills of the 18th and 19th centuries made this representation real.
The first American colony to obtain a representative legislature was Virginia, the general assembly of which first met in 1619.
The other colonies later obtained similar privileges.
At that time, however, the executive was in most cases entirely independent of the people and responsible to the Crown or proprietary alone, and this situation was one of the main causes of the friction that led to the American Revolution.
In the U.S. Congress, the number of members in the House of Representatives is apportioned to each state according to population, after the completion of a national census conducted every ten years.
Representatives are chosen either by the state electors at large or, generally, by the electors of the specific districts they are to represent.
Each state, regardless of population, is also represented by two senators in the Senate, or upper house of Congress.
Multiparty Coalition, temporary combination of groups or individuals formed to pursue specific objectives through joint action.
The term coalition is most often used in connection with political parties.
Coalition governments, which are frequently found in multiparty countries, such as France or Italy, may be formed when no single party is strong enough to obtain an electoral majority.
The resulting government usually distributes political posts to representatives of all coalition members.
In the United States, with its two-party system, coalitions occasionally cut across party lines to deal with special issues in Congress.
In international politics, nations sometimes band together to achieve a common purpose.
Such a coalition is likely to occur in a time of crisis, for example, when nations become allies during a war.
Monarchy, form of government in which one person has the hereditary right to rule as head of state during his or her lifetime; the term is also applied to the state so governed.
The power of the monarch varies from absolute to very limited; the latter is exemplified in modern-day constitutional monarchies.
Monarchs include such rulers as kings and queens, emperors and empresses, tsars, and kaisers.
Throughout history many monarchs have wielded absolute power, sometimes based on their presumed divinity.
In ancient Egypt, for example, the pharaoh was deified, as were certain Oriental rulers.
By the Middle Ages the monarchical system of government had spread over Europe, often based on the need for a strong ruler who could raise and command military forces 8 to defend the country.
European monarchies were dynastic, with the throne usually being passed on to the eldest son or nearest male descendant.
Many medieval rulers obtained soldiers and weapons from the feudal lords and thus were dependent on the allegiance of the nobility to maintain their power.
With the decline of feudalism and the rise of nation-states, power became centralized in the hands of the sovereigns.
At first these rulers were supported by the growing middle class, or bourgeoisie, who benefited from a strong central government that maintained order and provided a stable atmosphere in which trade could flourish.
By the 15th and 16th centuries absolute monarchs, such as King Henry VIII of England and King Louis XIV of France, ruled the countries of Europe.
Abuses of power, as well as growing dissatisfaction among the bourgeoisie, helped bring about the end of many absolute monarchies; revolutions in England in the 17th century and in France in the 18th century were major landmarks in the limitation of absolute power.
The modern concept of a limited, constitutional monarchy arose slowly throughout much of Europe.
During the 19th century, parliamentary authority grew while royal power diminished.
Many Western monarchies ceased to exist after World War I, including those in Russia, Germany, and Austria.
In later years others were replaced by Communist governments.
Some constitutional monarchies still survive, primarily as symbols of national unity.
Among the most enduring are those of the United Kingdom, the Netherlands, Norway, Sweden, Denmark, and Belgium.
Imperialism, practice by which powerful nations or peoples seek to extend and maintain control or influence over weaker nations or peoples.
Scholars frequently use the term more restrictively: Some associate imperialism solely with the economic expansion of capitalist states; others reserve it for European expansion after 1870.
Although imperialism is similar in meaning to colonialism, and the two terms are sometimes used interchangeably, they should be distinguished.
Colonialism usually implies formal political control, involving territorial annexation and loss of sovereignty.
Imperialism refers, more broadly, to control or influence that is exercised either formally or informally, directly or indirectly, politically or economically Federalism, also referred to as federal government, a national or international political system in which two levels of government control the same territory and citizens.
The word federal comes from the Latin term fidere, meaning “to trust.” Countries with federal political systems have both a central government and governments based in smaller political units, usually called states, provinces, or territories.
These smaller political units surrender some of their political power to the central government, relying on it to act for the common good.
In a federal system, laws are made both by state, provincial, or territorial governments and by a central government.
In the United States, for example, people who live in the state of Ohio must obey the laws made by the Ohio legislature and the Congress of the United States.
In Canada, residents of the province of Québec follow the laws made by Québec’s legislature and those made by the Canadian parliament.
In addition to the United States and Canada, countries that are considered federalist include Australia, Brazil, Germany, India, Malaysia, Mexico, Nigeria, and Switzerland.
Federal political systems divide power and resources between central and regional governments.
The balance of power between the two levels of government varies from country to country, but most federal systems grant substantial autonomy to state or provincial governments.
Central 9 governments decide issues that concern the whole country, such as organizing an army, building major roads, and making treaties with other countries.
Federalism varies in practice, however, and in some countries with federal systems the central government plays a large role in community planning, schools, and other local issues.
Federal political systems are relatively uncommon around the world.
Instead, most countries are unitary systems, with laws giving virtually all authority to the central government.
The central government may delegate duties to cities or other administrative units, but it retains final authority and can retract any tasks it has delegated.
The central government in a unitary system is much more powerful than the central government in a federal system.
Cameroon, France, Italy, Japan, Kenya, Morocco, South Korea, Sweden, and Uruguay are examples of unitary systems.
A confederation is similar to a federal system but gives less power to the central government.
The loose alliances of countries or other political entities that make up a confederation seek to cooperate with one another while retaining ultimate control of their own internal policies.
Unlike federal systems, confederations usually give each member nation absolute control over its citizens and territory.
The central government decides only issues that affect all members of the confederation.
In the 18th century the United States was founded as such a system under the Articles of Confederation.
More recently, the Soviet Union dissolved in 1991, and many of the former republics formed a confederation called the Commonwealth of Independent States (CIS) to coordinate domestic and foreign policy.
Confederations tend to be weak and unstable because member nations often resist relinquishing final authority on any matters and insist on their right to withdraw from the confederation at any time.
Confederations are uncommon; most are international bodies with limited and specific responsibilities, such as the European Community (EC) and the British Commonwealth.
Democracy (Greek demos,”the people”; kratein, “to rule”), political system in which the people of a country rule through any form of government they choose to establish.
In modern democracies, supreme authority is exercised for the most part by representatives elected by popular suffrage.
The representatives may be supplanted by the electorate according to the legal procedures of recall and referendum, and they are, at least in principle, responsible to the electorate.
In many democracies, such as the United States, both the executive head of government and the legislature are elected.
In typical constitutional monarchies such as the United Kingdom and Norway, only the legislators are elected, and from their ranks a cabinet and a prime minister are chosen.
Although often used interchangeably, the terms democracy and republic are not synonymous.
Both systems delegate the power to govern to their elected representatives.
In a republic, however, these officials are expected to act on their own best judgment of the needs and interests of the country.
The officials in a democracy more generally and directly reflect the known or ascertained views of their constituents, sometimes subordinating their own judgment.
Communism, a theory and system of social and political organization that was a major force in world politics for much of the 20th century.
As a political movement, communism sought to overthrow capitalism through a workers’ revolution and establish a system in which property is owned by the community as a whole rather than by individuals.
In theory, communism would create a classless society of abundance and freedom, in which all people enjoy equal social and economic status.
In practice, communist regimes have taken the form of coercive, authoritarian 10 governments that cared little for the plight of the working class and sought above all else to preserve their own hold on power.
The idea of a society based on common ownership of property and wealth stretches far back in Western thought.
In its modern form, communism grew out of the socialist movement of 19th- century Europe.
At that time, Europe was undergoing rapid industrialization and social change.
As the Industrial Revolution advanced, socialist critics blamed capitalism for creating a new class of poor, urban factory workers who labored under harsh conditions, and for widening the gulf between rich and poor.
Foremost among these critics were the German philosopher Karl Marx and his associate Friedrich Engels.
Like other socialists, they sought an end to capitalism and the exploitation of workers.
But whereas some reformers favored peaceful, longer-term social transformation, Marx and Engels believed that violent revolution was all but inevitable; in fact, they thought it was predicted by the scientific laws of history.
They called their theory “scientific socialism,” or communism.
In the last half of the 19th century the terms socialism and communism were often used interchangeably.
However, Marx and Engels came to see socialism as merely an intermediate stage of society in which most industry and property were owned in common but some class differences remained.
They reserved the term communism for a final stage of society in which class differences had disappeared, people lived in harmony, and government was no longer needed.
The meaning of the word communism shifted after 1917, when Vladimir Lenin and his Bolshevik Party seized power in Russia.
The Bolsheviks changed their name to the Communist Party and installed a repressive, single-party regime devoted to the implementation of socialist policies.
The Communists formed the Union of Soviet Socialist Republics (USSR, or Soviet Union) from the former Russian Empire and tried to spark a worldwide revolution to overthrow capitalism.
Lenin’s successor, Joseph Stalin, turned the Soviet Union into a dictatorship based on total state control of the economy and the suppression of any form of opposition.
As a result of Lenin’s and Stalin’s policies, many people came to associate the term communism with undemocratic or totalitarian governments that claimed allegiance to Marxist-Leninist ideals.
The term Marxism- Leninism refers to Marx’s theories as amended and put into practice by Lenin.
After World War II (1939-1945), regimes calling themselves communist took power in China, Eastern Europe, and other regions.
The spread of communism marked the beginning of the Cold War, in which the Soviet Union and the United States, and their respective allies, competed for political and military supremacy.
By the early 1980s, almost one-third of the world’s population lived under communist regimes.
These regimes shared certain basic features: an embrace of Marxism-Leninism, a rejection of private property and capitalism, state domination of economic activity, and absolute control of the government by one party, the communist party.
The party’s influence in society was pervasive and often repressive.
It controlled and censored the mass media, restricted religious worship, and silenced political dissent.
Communist societies encountered dramatic change in the late 1980s and early 1990s, as political and economic upheavals in the USSR, Eastern Europe, and elsewhere led to the disintegration of numerous communist regimes and severely weakened the power and influence of communist parties throughout the world.
The collapse of the USSR effectively ended the Cold War.
Today, single-party communist states are rare, existing only in China, Cuba, Laos, North Korea, and Vietnam.
Elsewhere, communist parties accept the principles of democracy and operate as part of multiparty systems.
11 This article provides a broad survey of communism.
It explores the philosophical roots of communism and explains how communism was practiced in the Soviet Union, China, Eastern Europe, and other regions.
It also examines the influence of nonruling communist parties.
Finally, the article describes the common features of communist states and assesses the future of communism.
Commonwealth, body of people in a politically organized community that is independent or semi-independent, and in which the government functions by the common consent of the people.
The United States and its separate, semiautonomous states are thus commonwealths, although only four states—Kentucky, Massachusetts, Pennsylvania, and Virginia—have officially designated themselves as such.
The term commonwealth is also sometimes used as a synonym for commonweal, or “general welfare.” In addition it is applied in a general sense to an association of persons having a common interest, as of art, literature, or learning, and to the uniting interest itself, as, for instance, the commonwealth of learning.
In English history, the Commonwealth was the government established by Parliament and headed by the English soldier Oliver Cromwell after the execution of King Charles I in 1649 and continuing until the Restoration in 1660.
The term is also applied specifically, and officially, to Australia and the Bahamas and to the association of countries known as the Commonwealth of Nations.
3.5 APPARATUS OF GOVERNMENT Louis althusser complicates Marx's understanding of the relation between base and superstructure by adding his concept of "ideological state apparatuses."
Marx distinguished among various "levels" in a society: the infrastructure or economic base and the superstructure, which includes political and legal institutions (law, the police, the government) as well as ideology (religious, moral, legal, political, etc.).
The superstructure has a relative autonomy with relation to the base; it relies on the economic base but can sometimes persist for a long period after major changes in the economic base.
Althusser does not reject the Marxist model; however, he does want to explore the ways in which ideology is more pervasive and more "material" than previously acknowledged.
As a result, he proposes to distinguish "ideological state apparatuses" (ISAs for short) from the repressive state apparatus (SA for short).
The state apparatus includes "the Government, the Administration, the Army, the Police, the Courts, the Prisons, etc.".
These are the agencies that function "by violence," by at some point imposing punishment or privation in order to enforce power.
To distinguish ISAs from the SA, Althusser offers a number of examples:  the religious ISA (the system of the different public and private 'Schools'),  the family ISA,  the legal ISA,  the political ISA (the political system, including the different Parties),  the trade union ISA,  the communications ISA (press, radio and television, etc.
),  the cultural ISA (Literature, the Arts, sports, etc.)
These ISAs, by contrast to the SA, are less centralized and more heterogeneous; they are also believed to access the private rather than the public realm of existence, although Althusser's goal here is to question the bourgeois distinction between private and public: "The distinction 12 between the public and the private is a distinction internal to bourgeois law, and valid in the (subordinate) domains in which bourgeois law exercises its 'authority'" .
The main thing that distinguishes the ISAs from the SAs is ideology: "the Repressive State Apparatus functions 'by violence,' whereas the Ideological State Apparatuses function 'by ideology'".
To be more precise, Althusser explains that the SA functions predominantly by violence or repression and only secondarily by ideology.
Similarly the ISAs function predominantly by ideology but can include punishment or repression secondarily: "Schools and Churches use suitable methods of punishment, expulsion, selection, etc., to 'discipline' not only their shepherds, but also their flocks.
The same is true of the Family...
The same is true of the cultural IS Apparatus (censorship, among other things), etc.".
Although the ISAs appear to be quite disparate, they are unified by subscribing to a common ideology in the service of the ruling class; indeed, the ruling class must maintain a degree of control over the ISAs in order to ensure the stability of the repressive state apparatus (the SA): "To my knowledge, no class can hold State power over a long period without at the same time exercising its hegemony over and in the State Ideological Apparatuses" .
It is much harder for the ruling class to maintain control over the multiple, heterogeneous, and relatively autonomous ISAs (alternative perspectives can be voiced in each ISA), which is why there is a continual struggle for hegemony in this realm.
It is also worth mentioning that, according to Althusser, "what the bourgeoisie has installed as its number-one, i.e.
as its dominant ideological State apparatus, is the educational apparatus, which has in fact replaced in its functions the previously dominant ideological State apparatus, the Church" .
Through education, each mass of individuals that leaves the educational system at various junctures (the laborers who leave the system early, the petty bourgeoisie who leave after their B.A.s, and the leaders who complete further specialist training) enters the work force with the ideology necessary for the reproduction of the current system: "Each mass ejected en route is practically provided with the ideology which suits the role it has to fulfill in class society".
Other ISAs contribute to the replication of the dominant ideology but "no other ideological State apparatus has the obligatory (and not least, free) audience of the totality of the children in the capitalist social formation, eight hours a day for five or six days out of seven".
The very importance of this function is why schools are invested in hiding their true purpose through an obfuscating ideology: "an ideology which represents the School as a neutral environment purged of ideology (because it is...lay), where teachers respectful of the 'conscience' and 'freedom' of the children who are entrusted to them (in complete confidence) by their 'parents' (who are free, too, i.e.
the owners of their children) open up for them the path to the freedom, morality and responsibility of adults by their own example, by knowledge, literature and their 'liberating' virtues" .
So pervasive is this ideology, according to Althusser, that "those teachers who, in dreadful conditions, attempt to turn the few weapons they can find in the history and learning they 'teach' against the ideology, the system and the practices in which they are trapped... are a kind of hero".
3.6 GOVERNANCE AND GOVERNMENT SERVICES THROUGH ICT Governance and the delivery of public services can be performed more efficiently through the use of ICT, which may include mobile/fixed telephony, internet, broadband and local/wide area networks.
A number of government agencies at federal, state and local government levels are deploying ICT facilities to improve service delivery, share of information, and reduce delay.
13 The place of telecommunications in the development of rural communities is also generally appreciated.
At all levels, concerted efforts are being made at improving access to telecommunications services in the rural areas, hence the various Rural Telecommunications and Universal Service initiatives.
The availability of telecommunications services to rural communities is essential to Nigeria with about 80% of our population located in rural areas.
Improvement in communications links to our rural areas is set to bring the following benefits of rural telecommunications to the populace: (i) Improvement of the living conditions of the people in the rural areas by allowing them to communicate easily amongst themselves and with relatives, friends and business associates living elsewhere.
(ii) Easier and faster access to up-to-date market and price information thereby assisting farmers and rural-based traders in their businesses.
(iii) More rural businesses and better employment opportunities that can greatly reduce the problem of rural-to-urban migration.
(iv) Better access to agricultural extension services such as prompt information on improved seeds, availability of fertilizers, weather forecasting and pest control.
(v) Improved health services including remote diagnosis and treatment advice.
(vi) More efficient handling of civil emergencies and natural disasters.
(vii) Wider access to education resources, especially through distance learning.
(viii) Easier access to government and wider awareness of government programmes and activities.
(ix) Enhanced security of lives and properties.
(x) Improved patrolling and monitoring of border villages and towns.
Information and Communication Technologies (ICT) provides increasingly powerful process tools that can be deployed to address traditional development problems in innovative ways.
ICT does not consist solely of electronic- and micro-chip-based e-governance solutions, but these predominate in most discussions of ICT in the context of public administration reform.
Such information technologies as Wide Area Networks and the Internet, can transform government interactions with citizens, businesses, and other parts of government.
However, the focus needs to be less on the technology per se and more on the transformation and reorganization of functions and interactions that it permits.
By locating service centres closer to clients, customers and partners, e-governance can facilitate better, user-friendly service delivery, improved and economical links with business, and more efficient and rigorous management of government business.
Innovative examples, even in otherwise technologically under-developed areas, have proven successful.
For example, community-based as well as computerized kiosks in India provide the basic information, documentation and forms needed for citizens in rural villages, saving people time and money and extending the range of services available locally.
In addition to enhancing relations with the public, e-government can also improve the internal working of an administration.
Introducing Management Information Systems within government departments, for example, can result in improved personnel management, cost reductions and improvements in service delivery and government procurement, better management of technical assistance funds and projects, and increased revenue collection.
However, e-government in so far as it is conceived of largely in terms of IT systems, tools and infrastructure does not automatically result in the streamlining and re-organization of government functions and 14 transformation of roles and responsibilities which need to be part of a larger transformation process.
CONCLUSION In this Unit we discussed Understand the History of government, the the general Functions of government to the governed, we also considered the meaning of Separation of power in government and the various Classification and types of government.
We also discussed the various apparatus of government as well as various applications of ICT in provision Governance and government services SUMMARY Government is a political organization comprising the individuals and institutions authorized to formulate public policies and conduct affairs of state.
Government could be described as good or bad based on the judgment of the governed.
Government is meant to address the various needs of the governed.
Separation of powers is one of the basic attributes of government which in turn determines the type of government in practice in a given location.
Every government discharges her responsibilities to her citizens through government apparatus.
The introduction of ICT in governance has done a lot in bridging the gap between the citizens the government in power REFERENCES 1.
"Anarchism."
Microsoft® Encarta® 2009 .
Redmond, WA: Microsoft Corporation, 2008.
2.
"Absolutism."
Microsoft® Encarta® 2009 [DVD].
Redmond, WA: Microsoft Corporation, 2008.
3. http://www.undp.org/policy/practicenotes.htm.
4.
Felluga, Dino.
"Modules on Althusser: On Ideological State Apparatuses."
Introductory Guide to Critical Theory 5.http://www.purdue.edu/guidetotheory/marxism/modules/althusserISAs.html UNIT 2 CONCEPT AND APPLICATION OF ICT TABLE CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main content 15 3.1 What is ICT 3.2 What is ICT system 3.3 Importance of ICT 3.4Difference between IT and ICT 3.5 Applications of ICT 3.6 Problems of ICT implementation 4.0 Conclusion 5.0 Summary 6.0.
References.
INTRODUCTION In the past 100 years we have experienced ‘the greatest social and scientific introduction advances since the industrial revolution started in the 18th century.
We have learned to communicate by means never thought of or imagined before.
The integration of technologies and systems have allowed for the creation of inventions that have carried us to the moon, brought the world’s images into our homes by the flick of a switch and allowed the deaf to hear: all due to the vision and the knowledge of people using technology to control the ‘human made world’ and ‘improve their surroundings’ ICT is an initiative aimed at bridging the digital divide (the disparity between technological ‘have’ and ‘have not’, geographic locations or demographic groups) and aiding economic development by ensuring equitable access to up-to-date communication technologies.
What does ICT mean to those who are disadvantaged by the ‘digital divide’ - an issue so eloquently explored by Jacques Chirac, President of France in his opening address to the recent UNESCO General Conference?
These inequalities are both between and within nations, and between the North and the South, giving rise to new inequalities in access to learning and work opportunities.
As we are all aware, Internet use is much more common among younger rather than older people, men rather than women, urban rather than rural dwellers, and people with higher levels of education and income.
To reinforce this point of Digital divide those without access to ICTs and without ICT skills become less and less capable of participating in the knowledge-based society, which makes increasing use of technology and information.
The resulting so-called digital divide represents a major challenge for policy-makers at all levels’ The term “technologically disconnected” is one that crops up often used.
Certainly the one third of the worlds population which does not have access to the electricity, phone lines and other infrastructure necessary to enable them to adapt information communication technologies in production and consumption are “technologically disconnected”.
A very telling statistic from the IL0 is that while ‘at least 70 per cent of the EU labour force is engaged in technology intensive work, more than half the world’s population has yet to place a telephone call’ (IL0 2001 Chapter 2).
There is however some optimism with commitments by governments, and groups such as the Heads of State and Government for initiatives which will enable their communities to have access to ICT which is slowly changing this scenario - but it is slow.
Initiatives such as these will have a follow on effect into the education sector.
Working in favour of these initiatives are trends towards: 16 Declining costs of telecommunications; Potential of wireless technologies to extend access across wide areas and remote rural communities, especially important in developing countries; .
Leverage effect that telecommunications can have on other social and economic processes (IL0 Chapter 2).
2.0 OBJECTIVES After studying this unit, you should be able  Understand and explain what ICT means.
 Understand and explain the the general structure of ICT systems.
 Understand and Explain the difference between ICT and IT.
 Understand and explain various application of ICT in different fields of life.
 Understand and explain the problems militating against the imolementation of ICT.
3.0 ICT 3.1 What is ICT?
ICT is an acronym that stands for Information Communications Tecnology However, apart from explaining an acronym, there is not a universally accepted defininition of ICT?
Why?
Because the concepts, methods and applications involved in ICT are constantly evolving on an almost daily basis.
Its difficult to keep up with the changes - they happen so fast.
A good way to think about ICT is to consider all the uses of digital technology that already exist to help individuals, businesses and organisations use information.
ICT covers any product that will store, retrieve, manipulate, transmit or receive information electronically in a digital form.
For example, personal computers, digital television, email, robots.
So ICT is concerned with the storage, retrieval, manipulation, transmission or receipt of digital data.
Importantly, it is also concerned with the way these different uses can work with each other.
ICT also includes any communications device – encompassing radio, television, mobile phones, computer and network hardware and software, satellite systems and so on, as well as the various services and applications associated with them, such as videoconferencing and distance learning 3.2 What is an ICT System?
An ICT system is a set-up consisting of hardware, software, data and the people who use them.
It commonly includes communications technology, such as the Internet.
ICT and computers are not the same thing.
Computers are the hardware that is often part of an ICT system.
It is the power of computers and communications that has allowed ICT systems to become so important.
Like any piece of equipment, the important thing about it is what it lets us do.
is shorthand for Information Communication Technology and refers to all the digital devices (computers, cell phones, pdas, smartphones, iPods, etc.)
that play a role in the creation of new media modes of interaction between people.
Information is basically data, which with the addition of learning becomes knowledge.
In other words learning which is based on the capacity to find, access, apply and transform information into new knowledge.
Important competencies which learners require to make this transformation are often called information literacy competencies and include awareness of the need for 17 information, the ability to critically analyse information and evaluate its usefulness and ultimately to be able to apply the information, turning it into knowledge.
Communication is that simple act of dialogue between peoples and cultures that takes on a new dimension when combined with ‘information’ and ‘technology’.
Technology is a broad concept that deals with a species' usage and knowledge of tools and crafts, and how it affects a species' ability to control and adapt to its environment.
However, a strict definition is elusive; "technology" can refer to material objects of use to humanity, such as machines, hardware or utensils, but can also encompass broader themes, including systems, methods of organization, and techniques such as CD ROM, video, television etc.
3.3.
The importance of ICT systems By using ICT systems we are:  more productive - we can complete a greater number of tasks in the same time at reduced cost by using computers than we could prior to their invention  able to deal with vast amounts of information and process it quickly  able to transmit and receive information rapidly 3.4 DIFFERENCE BETWEEN IT AND ICT Historically ICT has been emerging from the concepts of IT, meaning basically computers and communication technology, and digital data networks as the latest phase of development, but also TV, satellites, phone, etc.
Due to a trend of merging different technologies (all technologies seem to merge together in one way or another), there was a reason to start speaking of ICT as opposed to IT.
ICT captures all the latest technologies used for communication, data processing and data storage.
It encompasses computer systems, telecommunication, networks, and multimedia applications 3.5 APPLICATION OF ICT ICT is not a solution by itself - it can primarily act as an enabler to improve the state of affairs in developing regions.
Applying ICT to address problems in these areas is not an easy task and involves addressing several challenges including research, development, deployment and social challenges.
The research and development challenges cover several areas of computer science including networking, systems databases, security, user interfaces ICT Systems are used in a number of environments, such as:  offices  shops  factories  aircraft  ships ICT has being employed in various fields of human endeavours among which includes: - Healthcare - Education - Micro-finance and Rural Banking - E- Commerce - Agriculture - Traffic Control 18 - Industries - communications - E-Governance -Human Resource Development - Urban and Rural Development - Fiscal Measures - Arts, Culture & Tourism -Security and Law Enforcement In this unit we will look in detail some of these areas of Application while the student should carry out a research on the remaining areas 3.5.1 E- LEARNING Is ICT the key to better student outcomes, better teaching and learning?
If so, how can we take advantage of the opportunities that information and communications technology may provide educational systems to enhance access to education and hence move towards the reduction of poverty and generation of personal economic growth?
E-learning Is an attempts to use technology to expand the range and impact of the teacher.
This could be called remote classroom approach to teaching.
The idea is to set up a network of classrooms and to use technology, usually video by satellite or landline, to take teacher’s lesson live to students at the remote sites.
The system is interactive, meaning that students can ask questions.
E-learning encompasses learning at all levels, both formal and non-formal, that uses an information network—the Internet, an intranet (LAN) or extranet (WAN)—whether wholly or in part, for course delivery, interaction, evaluation and/or facilitation.
Others prefer the term online learning.
Web-based learning is a subset of e-learning and refers to learning using an Internet mainly using a browser (such as Chrome or Firefox or Internet Explorer).
What is blended learning?
Another term that is gaining currency is blended learning.This refers to learning models that combine traditional classroom practice with e-learning solutions.For example, students in a traditional class can be assigned both print-based and online materials, have online mentoring sessions with their teacher through chat, and are subscribed to a class email list.
Or a Web-based training course can be enhanced by periodic face-to-face instruction.“Blending”was prompted by the recognition that not all learning is best achieved in an electronically-mediated environment, particularly one that dispenses with a live instructor altogether.
Instead, consideration must be given to the subject matter, the learning objectives and outcomes, the characteristics of the learners, and the learning context in order to arrive at the optimum mix of instructional and delivery methods.
With the increased use of ICT as a means of instruction, ‘the decreased importance of physical distance means that the best (and the worst educational and corporate institutions) of any country can decide to open a branch anywhere in the world or to reach out across borders using the internet or satellite communication links, effectively competing with any national university on its own territory’ In brief, the application of ICT can take many forms and has the ability to ‘revolutionise the way teaching and learning occurs.
The concurrent use of multimedia and computers permits the development of new pedagogical approaches involving active and interactive learning’ e,g.
using 19 computer based learning methods, problem based learning, project based learning, online, video conferencing, satellite links.
ICTs are also transformational tools which, when used appropriately, can promote the shift to a learner-centered environment.
The following are some of ways in which ICT have enhance quality education: 1.
Motivating to learn: ICTs such as videos, television and multimedia computer software that combine text, sound, and colorful, moving images can be used to provide challenging and authentic content that will engage the student in the learning process and telecollaboration.Interactive radio likewise makes use of sound effects, songs, dramatizations, comic skits, and other performance conventions to compel the students to listen and become involved in the lessons being delivered.
More so than any other type of ICT, networked computers with Internet connectivity can increase learner motivation as it combines the media richness and interactivity of other ICTs with the opportunity to connect with real people and to participate in real world events.
2.
Facilitating the acquisition of basic skills: The transmission of basic skills and concepts that are the foundation of higher order thinking skills and creativity can be facilitated by ICTs through drill and practice.
Educational television programs such as “Who Want to be a Millionaire”; Nigeria’s biggest thought provoking program, enlightens people because of the questions that are required to be answered before the cash price is awarded.
Questions are drawn from all works of life ranging from religious, cultural, educational to contemporary issues, thereby facilitating the acquisition of basic skills amongst populace.
3.
Enhancing teacher training: ICTs have also been used to improve access to and the quality of teacher training.
For example, institutions like the Cyber Teacher Training Center (CTTC) in South Korea are taking advantage of the Internet to provide better teacher professional development opportunities to in-service teachers.
The government funded CTTC, established in 1997, offers self-directed, self-paced Web-based courses for primary and secondary school teachers.
Courses include “Computers in the Information Society,”“Education Reform,” and “Future Society and Education.” Online tutorials are also offered, with some courses requiring occasional face-to-face meetings.
In Nigeria, The National Open University of Nigeria, satellite-based video and audio conferencing was founded in 2000 by the then Nigerian President, Olusegun Obasanjo, supplemented by print-materials and recorded video, to train teachers who have not obtained the requisite degree for their current job placement from any geographical distance.
The teachers interacted with remote lecturers by telephone and fax.
3.5.2 E-COMMERCE Companies, individuals, and institutions use the Internet in many ways.
Companies use the Internet for electronic commerce, also called e-commerce, including advertising, selling, buying, distributing products, and providing customer service.
In addition, companies use the Internet for business-to-business transactions, such as exchanging financial information and accessing complex databases.
Businesses and institutions use the Internet for voice and video conferencing and other forms of communication that enable people to telecommute (work away from the office using a computer).
The use of e-mail speeds communication between companies, among coworkers, and among other individuals.
Media and entertainment companies run online news and weather services over the Internet, distribute music and movies, and actually broadcast audio 20 and video, including live radio and television programs.
File sharing services let individuals swap music, movies, photos, and applications, provided they do not violate copyright protections.
Online chat allows people to carry on discussions using written text.
Instant messaging enables people to exchange text messages; share digital photo, video, and audio files; and play games in real time.
Scientists and scholars use the Internet to communicate with colleagues, perform research, distribute lecture notes and course materials to students, and publish papers and articles.
Individuals use the Internet for communication, entertainment, finding information, and buying and selling goods and services.
3.5.3.HEALTH Health information technology (HIT) provides the umbrella framework to describe the comprehensive management of health information across computerized systems and its secure exchange between consumers, providers, government and quality entities, and insurers.
Health information technology (HIT) is “the application of information processing involving both computer hardware and software that deals with the storage, retrieval, sharing, and use of health care information, data, and knowledge for communication and decision making”.Broad and consistent utilization of HIT will:  Improve health care quality;  Prevent medical errors;  Reduce health care costs;  Increase administrative efficiencies  Decrease paperwork; and  Expand access to affordable care.
Types of technology In a recent study about the adoption of technology in health management, several classification of technology were mad which includes: applications for prescribing to include electronic medical records (EMR), clinical decision support (CDS), and computerized physician order entry (CPOE).
Applications for dispensing to include bar- coding at medication dispensing (BarD), robot for medication dispensing (ROBOT), and automated dispensing machines (ADM).
And, applications for administration to include electronic medication administration records (EMAR) and bar-coding at medication administration (BarA).
3.5.4 AGRICULTURE with the Industrial Revolution, farm machinery was introduced into agriculture (mechanized agriculture), exchange of skills and knowledge among farmers becomes an easy task.
Improved method of food preservation via electricity, as well as irrigation system that ensures food availability all year round.
Improvements in transportation affected agriculture.
Roads, canals, and rail lines enabled farmers to obtain needed supplies from remote suppliers and market their produce over a wider area.
Food could be protected during transport more economically than before as the result of rail, ship, and refrigeration developments in the late 19th and early 20th centuries.
Efficient use of these developments led to increasing specialization and eventual changes in the location of agricultural suppliers Scientific methods are now applied to pest control, limiting overuse of insecticides and fungicides and employing more varied and targeted application techniques.
New understanding 21 of significant biological control measures and the emphasis on integrated pest management make possible more effective control of certain kinds of insects Chemicals for weed control are important for a number of crops, such as cotton and corn New applications of technologies in the 1990s are increasing crop production.
Precision farming, also known as prescription farming, site specific farming, or variable rate farming, utilizes global positioning systems (GPS) and geographic information systems (GIS) in the satellite collection and transmission of data as farmers plant, fertilize, and harvest their crops.
Combines and other harvesting machines equipped with electronic scales, which are linked to a GPS, measure yield as a crop is being harvested.
A computerized yield map, which locates to within one meter (one yard) those spots in a field where the yield is highest and lowest, is produced.
The next time that field is planted and fertilized, the farmer adjusts seeding and fertilizer application rates according to information on the yield map.
This increases crop production rates according to information on the yield map.
This increases crop production while reducing the use of both fertilizers and fuel New applications of technologies at the beginning of the 21st century are further improving crop production.
Precision farming, also known as prescription farming, site-specific farming, or variable rate farming, utilizes global positioning systems (GPS) and geographic information systems (GIS) in the satellite collection and transmission of data to create yield maps of fields during harvest.
Farmers use the yield maps as they plant and fertilize their crops the following season.
This increases crop production while reducing the use of both fertilizers and fuel.
GPS also helps farmers comply with environmental regulations when applying fertilizers and pesticides.
Biotechnology is also increasing agricultural productivity.
In recent years farmers have begun producing a new, genetically engineered oil seed crop that grows from canola, an oil seed producing plant, to yield lauric oil, which comes naturally from coconuts and palm kernels.
New hybrid corn seed recently developed to resist the corn borer, an insect poisonous to that crop, and improved varieties of barley with disease resistant genes are now under cultivation, as well.
Biotechnology developments have also become increasingly controversial, however, because it is difficult to determine the environmental consequences of genetically engineered organisms.
Some people, including some scientists, object to any procedure that changes the genetic composition of an organism.
Critics are concerned that some of the genetically altered forms will eliminate existing species, thereby upsetting the natural balance of organisms Genetic Engineering, alteration of an organism's genetic, or hereditary, material to eliminate undesirable characteristics or to produce desirable new ones.
Genetic engineering is used to increase plant and animal food production; to help dispose of industrial wastes; and to diagnose disease, improve medical treatment, and produce vaccines and other useful drugs.
Included in genetic engineering techniques are the selective breeding of plants and animals, hybridization (reproduction between different strains or species), and recombinant.
Federal government police paper on implementation ICT in Agriculture was aimed at Developing Geographical Information Systems (GIS) to monitor the environment and plan sustainable environmental usage: ICT is used in land and water management, offshore resource exploitation, yield assessment and livestock management.
Government will revitalise agricultural extension services by empowering and equipping farm extension workers with ICT skills to support farmers through the use of ICT in areas such as: a.
Digital Mapping 22 b.
Land Use c. Soil types d. Meteorology e. Ecology f. Oceanography particularly off-shore fisheries exploitation g. Hydrology h. Agricultural records.
As well as Establishing an agricultural information system to provide support for planning, production, storage and distribution of horticultural crops, livestock, and fisheries products 3.5.5 BANKING In recent times, Information Communication Technology (ICT), which basically involves the use of electronic gadgets especially computers for storing, analyzing and distributing data, is having a dramatic influence on almost all aspects of individual lives and that of the national economy- the banking sector inclusive.
The increasing use of ICT has allowed for integration of different economic units in a spectacular way.
This phenomenon is not only applicable to Nigeria but other economies of the world, though the level of their usage may differ.
In Nigeria, ICT usage especially in the banking sector, has considerably improved, even though it may not been as high as those observed for advanced countries.
The revolution in ICT has made the banking sector changed from the traditional mode of operations to presumably better ways with technological innovation that improves efficiency.
ICT can enhance efficiency via its use and in recent times banks have been encouraged by the rapid decline in the price of ICT gadgets.
This has perhaps increased the bank level of ICT usage.The increase might have also be attributable to business environment that became relatively flexible to accommodate new forms of technological change as a result of reforms in the country.
Banking is becoming highly ICT based and because of its inter-sectoral link, it appears to be reaping most of the benefits of revolution in technology, as can be seen by its application to almost all areas of its activities.
It has broadened the scope of banking practices and changed the nature of banking as well as the competitive environment in which they operate.
A broad opening has been experienced around the world for banks and they are currently taking due advantage of these innovations to provide improved customer services in the face of competition and faster services that enhance productivity.
Technological advancement facilitates payments and creates convenient alternatives to cash and cheque for making transactions.
Such new practices have led to the development of a truly global, seamless and Internet enabled 24-hour business of banking.
Technological advance in payments are important due to the fact that it will be feasible to outsource quite a number of the banks’ role in the payments system.
Also banks’ regulation can be more technologically dependent and better focused rather than focusing on conceptual guidelines.
ICT revolution both in terms of innovation rate, speedy operation, and cost per unit (portraying reduction in average total and marginal costs) has made a good number of banks embrace the use of ICT infrastructure in their operations.
The technological innovation that is being witnessed currently in the Nigerian banking sector is possible of impacting on the banks’ mode of transactions especially in their payment systems.
23 The payment systems are made feasible by ICT gadgets such as Automated Teller Machine (ATM), Electronic Fund Transfer (EFT), Clearing House Automated Payments (CHAPs), Electronic Purse (E-PURSE), Automated Cheque Sorter (ACS) and Electronic and Transfer at Point of Sale (EFTPOS), which have made transactions easy and convenient.
This phenomenon is capable of bringing about speedy operations and enhanced productivity.
Though there may be little interruptions at times due to network failures, which may make customers unable to carry out transactions at that point in time.
This little shortcoming is not in any way comparable to the days when banking halls were characterized by long queues mainly as a result of delays in the traditional banking operations.
Now banks can provide comprehensive services to their customers by making them access their accounts via online services.
These instruments have an edge over the traditional payment instruments because it is safer, more efficient, convenient and cost effective.
Before the introduction of these ICT services in the banking industry, manual processing of documents were in use.
The bankers were made to cope with this onerous task, and the process made business transactions minimal.
Besides several hectic procedures, people had to contend with, banks’ customers were inevitably made to spend several hours in the congested banking halls in carrying out their transactions.
The ICT culture in Nigerian economy can be said to be on the increase.
Nigeria is the largest Internet subscriber in Africa with about 100,000 Internet users as at 2000, which was estimated to have grossly increased (Balancing Act, 2007).
It has also been observed that Nigeria’s teledensity had remarkably increased by more than 2,550% from 0.35% in 1992 to 9.3% in 2004, thereby greatly exceeding the International Telephone Union’s (ITU) benchmark of 1% .
This phenomenon has helped banks keep substantial information on-line which reduces the cost of marketing their products.
Being a competitive tool, it enhances the creation of customized services, reduces the cost of operation, and improves productivity as well as profitability.
More interestingly, almost all the banks in Nigeria have internet and on-line real time banking facilities which has improved the scope of Nigerian banking1.
It has aided transfer of funds from one location to another without any involvement of facial transactions thereby reducing the incidence of loss of funds to stealing and the likes.
Another recent one is the telephone banking technology that allows customers to have transactions on their accounts by calling a particular telephone number, through voice activation, and using a tone pad.
All of these improve the comfort of banking transactions.
ICT CONTRIBUTE TO ECONOMIC GROWTH ICT contribute to economic growth through: (1) increasing productivity across all sectors; (2) facilitating market expansion beyond borders to harvest economies of scale; (3) lowering costs of and facilitating access to services, notably in administration, education, health and banking; (4) providing access to research; (5) development of ICT products and services; (6) contributing to better govern-ance, a prerequisite to growth, through increased participation, accountability and transparency.
The use of ICT provides positive externalities, enhancing creativity, learning and problem-solving skills.
Its impact on employment, new types of exports, and FDI requires the interplay of a number of fac-tors: “It is the interaction among connectivity, access, network security, capability/skills, market struc-tures and firm governance, as well as the regulatory and facilitation environment, which determine whether firms from developing countries can participate effectively and efficiently in the information economy and compete in global e- marketplaces.” 24 ICT AND POVERTY REDUCTION ICT growth is only weakly correlated with poverty reduction due to the par-ticular situation of the poor.
They have low levels of skills.
They lack physical assets.
Their access to financial services is difficult.
In many cases, exclusion on grounds of rural isolation, ethnicity, lan-guage, religion or gender adds to the hurdles of overcoming the income, infrastructural and market barriers they face.
Economic research suggests that the contribution of ICT to pro-poor growth is de-pendent not on ICT infrastructure per se but on the role of ICT in supporting pro-poor initiatives.
Effective poverty reduction requires targeted pro-poor policies to provide infrastructure (including ICT), to strengthen physical access to markets and to invest in education and health.
As soon as ICT become affordable to low-income users, new employment, micro- entrepreneurial and social develop-ment opportunities emerge.
Poverty is a multi-dimensional phenomenon, encompassing a lack of opportunity, empower- ment and security.
ICT access can have powerful impact in addressing these constraints, giving the poor a stronger voice, facilitating their participation in decision-making processes and in demanding accountable government.
Local radio is a medium particularly suited to promoting economic devel-opment and empowerment, as it is affordable and accessible to listeners and demands few specific skills: a recent study found that radio plays a prominent role in tackling gender issues and promoting women’s visibility.
South Africa, where the average person listens to radio for more than four hours every day, and Mali are examples of countries with a vibrant radio landscape, including but not limited to community radio.
African governments have increasingly adopted legislative and administrative reforms allowing a greater variety of stations to operate and reducing public subsidies from state-owned national broadcasters.
3.6 PROBLEMS OF ICT IMPLEMENTATION Efforts to promote ICT development and expand effective access have met with limited success to date.
Progress in addressing key bottlenecks – infrastructure, access and the enabling environment will be determinant in ensuring the digital divide is bridged, and that ICT can play a supportive role in Africa’s economic, social and political development.
Practical factors.
In summay practical issues include: finance, infrastructure, Inadequate capacity and affordability, and Deficiencies in the regulatory and wider policy environment, illiteracy among others.
 The financial implications include: a) Capital required to establish an ICT infrastructure including quite high initial costs of hardware and software etc.
This quote from the experiences on one TAFE College is typical of the financial issues facing many establishments implementing ICT in carrying out their activities The increased use of multimedia means that more high-end equipment, software and peripherals are required and that the aging infrastructure needed a major upgrade.
The practice of purchasing one new computer and passing the older ones down the line to other staff or students is no longer acceptable.
The economics of this new approach is significant and will impact on the overall financial position of the Institute.
Leasing options may be a major consideration.
b) Funding required to train staff and to release them from normal duties for training.
c) Funding required to be able to pay computer teachers and support staff the equivalent to what they would receive in the market place.
25 d) What is the best and affordable means to provide users access to technology - should the institution/Government lend or perhaps even subsidize the purchase of equipment?
How are the funding implications of this practice met?
What is the overall infrastructure investment required and how can this be funded?.
Some institutions is are faced with costs of per year to replace obsolete equipment to meet current industry standards.
e) Issues of intellectual property and copyright, particularly the cost of adapting materials into, for example, online formats and providing access to resources online.
 Infrastructure problems include: a).
Inadequate and unevenly distributed infrastructure The primary obstacle in making use of ICT for economic growth or poverty reduction for many is the absence or limited scope of existing ICT infrastructure, particularly in rural areas.
Basic ICT infrastructure is concentrated in a few countries and in urban settings.
For most of Africa’s rural population, ICT are physically out of reach.
Where ICT infrastructure is in place, its use is often con-strained due to inadequate supportive infrastructure, in particular electricity and, to a lesser extent, transport systems.
Africa’s available ICT infrastructure is not fully utilised due to its low physical and techno- logical capacity (as is the case for Internet bandwidth) and due to gaps in interconnectivity, both at regional and international levels.
This means in practice that much of Africa’s Internet and telephone communication is routed via networks and technologies located in North America or Europe, resulting in substantially higher costs for the end user than in other regions of the world.
While there are numerous initiatives to address the lack of infrastructure, many of them are moving forward slowly and/or lagging behind schedule.
Implementation is hindered by a lack of long-term commitment, available investment capital and capacity and an absence of sustained support from implementing parties.
b) Changes to facilities and perhaps even the purchase of new furniture to provide appropriate physical access and meet occupational health and safety standards - including any special air- conditioning requirements, c) Provision of a reliable electrical supply.
d) Changes which may be required in physical layouts to accommodate more flexible adjustments.
This can have quite large impacts on the provision of appropriate spaces and technology support.
d) Establishment of special units for the development of ICT enhanced courses and materials, online development etc.
e) Rethinking how traditional support services will be provided.
For example, a common question that is often raised in discussion on online delivery is ‘can we do away with physical libraries’?
Perhaps a more relevant question is ‘how will traditional services need to be changed to meet the demands of ICT delivery of services?’  Inadequate capacity and affordability hinder access The main challenge to make effective use of the benefits offered by ICT is absent or inade-quate capacities at several levels, in particular on the individual level as regards literacy levels and ICT skills; at the institutional level as regards capacities for effective deployment and maintenance of ICT infrastructure, for the creation of relevant content and applications, as well 26 as for regula-tion.
This requires a mix of interventions in technical, legal, economic and socio- cultural is-sues.
For many media there are only limited applications which are relevant to the needs and capaci- ties of poor people.
Banking services or market information communicated via mobile phones have proven that poor people can benefit from ICT if the applications meet their needs and can be accessed by them.
An important aspect of relevant applications is the provision of locally relevant information in local languages.
African languages are seriously under-represented on the Internet and software ap-plications in African languages are only slowly developing.
Affordability and public access, which are of fundamental importance especially for rural Af- rica, are challenged by issues of financial sustainability.
A lack of a competition keeps prices for ICT services unattractively high.
Local radio stations have difficulties paying qualified staff and high li-censing fees.
Telecentres cannot always cover their costs from the income generated by their clients.
 Deficiencies in the regulatory and wider policy environment reduce potential benefits One of the most important limiting factors to affordable and efficient ICT in sub-Saharan Af- rica is often ineffective regulation combined with the large share of the market being held by monopo-lies or duopolies, which results in ineffective competition, high prices and limited investment from the private sector.
The dominance of monopolies together with the lack of independent telecommunication sector regulators which are able to encourage effective competition in the sector are two key chal-lenges in ICT policies.
The rationale for establishing independent regulatory institutions is based on ensuring non-discriminatory treatment of all players in the liberalized market.
The UN Task Force on Financing ICT stated: “The introduction and strengthening of independent, neutral sector regulation has helped to reinforce investor confidence and market performance, while enhancing consumer bene-fits.” In competing for scarce government financial resources, ICT policies and development priori- ties are often at odds with one another.
In addition, ICT policies are not always integrated into other national processes, namely the budget, general budget support (GBS) and poverty reduction strategies (PRS).
Policies with respect to universal access, education, electricity and micro-finance need to be closely interlinked to offer real benefits to the rural population.
But also other policy areas, such as taxes, need to integrate ICT concerns (for example the impact that high import taxes on computer equipment have on broad access to computer hardware).
Policy incoherence across relevant public policy areas can undermine the outreach and effectiveness of ICT by limiting the extent to which they can be used for low-cost communications and timely information-sharing.
The wider policy environment in many African countries is characterised by unfavourable business conditions as well as by the limited circulation of information.
Difficulties with respect to information access not only hamper private sector stakeholders, it also hinders transparency, account-ability and participation from the bottom, with a negative impact on governance in general.
Improved governance to give the poor a voice is essential to stimulate private investment and technological change for poverty reducing and sustainable economic growth.
Given the small size of many African countries and markets, the lack of a regionally harmonized regulatory environment is a serious hindrance to cheaper ICT services and greater geographical coverage.
New challenges for regulation are emerging in the wake of technological developments, in-cluding trends towards converging telecommunications and broadcasting.
27  CONCLUSION In this Unit we discussed what ICT means, the difference between ICT and IT.
We also discussed the general structure of ICT systems, as well as various application of ICT in different fields of life and various problems militating against the implementation of ICT.
SUMMARY ICT is an acronym that stands for Information Communications Tecnology.
It is concerned with the storage, retrieval, manipulation, transmission or receipt of digital data.
An ICT system is a set-up consisting of hardware, software, data and the people who use them.The integration of ICT devices and ICT information systems is greatly applied in addressing human problems such as in Health, Agriculture, commerce e.t.c.
Several factors are also militating against the implementation of ICT expecially in developing Countries were also taught; such factors includes finance, illiteracy, infrastructure e.t.c.
REFERENCE 1.http://www.bbc.co.uk/schools/revision.
2.
Using ICT for Quality Teaching, Learning and Effective Management Report of the Seventh UNESCO-ACEID.
International Conference on Education,Bangkok, Thailand, 11-l 4 December 2001.
3.
Global Journal of Business Research, Vol.2, No.2, 2008 BBBBB© 4.
Journal of Information Technology Impact Vol.
10, No.
1, pp.
25-34, 2010 5.
(OECD 2001 :84).
6.
GESCI ICT in Education.htm.
28  UNIT 3 INFORMATION SYSTEM Contents 1.0 Introduction 2.0 Objectives 3.0 Main Contents(categories of Information System) 3.1 Management Information System 3.2 Decision Support System 3.3 Executive Information System 3.4 Transaction Processing System 4.0 Conclusion 5.0 References 1.0 Introduction An information system (IS) – is any combination of information technology and people's activities that support operations, management, and decision making [2].In a very broad sense, the term information system is frequently used to refer to the interaction between people, processes, data, and technology.
In this sense, the term is used to refer not only to the information and communication technology (ICT) an organization uses, but also to the way in which people interact with this technology in support of business processes [3].
Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology.
Information systems are also different from business processes.
Information systems help to control the performance of business processes [4].
29 Information systems inter-relate with data systems on the one hand and activity systems on the other.
An information system is a form of communication system in which data represent and are processed as a form of social memory.
An information system can also be considered a semi- formal language which supports human decision making and action.
ISs can be categorized in four parts: 1.
Management Information System (MIS) 2.
Decision Support System (DSS) 3.
Executive Information System (EIS) 4.
Transaction Processing System (TPS) 2.0 Objectives of Information system After studying this unit the student should be able to do the following: 1.
Define Information System and know the types of Information system 2.
Explain the composition of the four major Information system 3.
Know the benefits and Limitations of Information Systems 4.
Understand, develop, and implement an information system infrastructure 3.0 Categories of Information System 3.1 Management Information System A management information system (MIS) provides information needed to manage organizations efficiently and effectively [5].
Management information systems involve three primary resources: people, technology, and information.
Management information systems are distinct from other information systems in that they are used to analyze operational activities in the organization [6].
Academically, the term is commonly used to refer to the group of information management methods tied to the automation or support of human decision making, e.g.
decision support systems, expert systems, and executive information systems[6].
A successful MIS supports a business's long range plans, providing reports based upon performance analysis in areas critical to those plans, with feedback loops that allow for titivation of every aspect of the enterprise, including recruitment and training regimens.
MIS not only indicate how things are going, but why and where performance is failing to meet the plan.
These reports include near-real-time performance of cost centers and projects with detail sufficient for individual accountability.
Types of management information system Most management information systems specialize in particular commercial and industrial sectors, aspects of the enterprise, or management substructure.
30  Management information systems (MIS), per se, produce fixed, regularly scheduled reports based on data extracted and summarized from the firm’s underlying transaction processing systems [7] to middle and operational level managers to identify and inform structured and semi-structured decision problems.
 Decision support systems (DSS) are computer program applications used by middle management to compile information from a wide range of sources to support problem solving and decision making.
 Executive information systems (EIS) is a reporting tool that provides quick access to summarized reports coming from all company levels and departments such as accounting, human resources and operations.
 Marketing information systems are MIS designed specifically for managing the marketing aspects of the business.
Office automation systems (OAS) support communication and productivity in the enterprise by automating work flow and eliminating bottlenecks.
OAS may be implemented at any and all levels of management Advantages The following are some of the benefits that can be attained for different types of management information systems [8].
 The company is able to highlight their strength and weaknesses due to the presence of revenue reports, employee performance records etc.
The identification of these aspects can help the company to improve their business processes and operations.
 Giving an overall picture of the company and acting as a communication and planning tool.
 The availability of the customer data and feedback can help the company to align their business processes according to the needs of the customers.
The effective management of customer data can help the company to perform direct marketing and promotion activities.
Information is considered to be an important asset for any company in the modern competitive world.
The consumer buying trends and behaviors can be predicted by the analysis of sales and revenue reports from each operating region of the company 3.2 Decision Support System A decision support system (DSS) is a computer-based information system that supports business or organizational decision-making activities.
DSSs serve the management, operations, and planning levels of an organization and help to make decisions, which may be rapidly changing and not easily specified in advance.
DSSs include knowledge-based systems.
A properly designed DSS is an interactive software- based system intended to help decision makers compile useful information from a combination of raw data, documents, personal knowledge, or business models to identify and solve problems 31 Components of DSS Design of a Drought Mitigation Decision Support System.
Three fundamental components of a DSS architecture are[9-13].
1. the database (or knowledge base), 2. the model (i.e., the decision context and user criteria), and 3. the user interface.
DSS components may be classified as: 1.
Inputs: Factors, numbers, and characteristics to analyze 2.
User Knowledge and Expertise: Inputs requiring manual analysis by the user 3.
Outputs: Transformed data from which DSS "decisions" are generated 4.
Decisions: Results generated by the DSS based on user criteria DSSs which perform selected cognitive decision-making functions and are based on artificial intelligence or intelligent agents technologies are called Intelligent Decision Support Systems (IDSS).
The nascent field of Decision engineering treats the decision itself as an engineered object, and applies engineering principles such as Design and Quality assurance to an explicit representation of the elements that make up a decision.
Benefits of DSS 1.
Improves personal efficiency 2.
Speed up the process of decision making 3.
Increases organizational control 4.
Encourages exploration and discovery on the part of the decision maker 5.
Speeds up problem solving in an organization 6.
Facilitates interpersonal communication 32 7.
Promotes learning or training 8.
Generates new evidence in support of a decision 9.
Creates a competitive advantage over competition 10.
Reveals new approaches to thinking about the problem space 11.
Helps automate managerial processes 3.3 Executive Information System An executive information system (EIS) is a type of management information system intended to facilitate and support the information and decision-making needs of senior executives by providing easy access to both internal and external information relevant to meeting the strategic goals of the organization.
It is commonly considered as a specialized form of decision support system (DSS)[14].
The emphasis of EIS is on graphical displays and easy-to-use user interfaces.
They offer strong reporting and drill-down capabilities.
In general, EIS are enterprise-wide DSS that help top-level executives analyze, compare, and highlight trends in important variables so that they can monitor performance and identify opportunities and problems.
EIS and data warehousing technologies are converging in the marketplace.
In recent years, the term EIS has lost popularity in favor of business intelligence (with the sub areas of reporting, analytics, and digital dashboards, v.gr.
Micro Strategy).
Components of EIS The components of an EIS can typically be classified as: Hardware When talking about hardware for an EIS environment, we should focus on the hardware that meet the executive’s needs.
The executive must be put first and the executive’s needs must be defined before the hardware can be selected.
The basic computer hardware needed for a typical EIS includes four components: 1.
Input data-entry devices.
These devices allow the executive to enter, verify, and update data immediately 2.
The central processing unit (CPU), which is the kernel because it controls the other computer system components 3.
Data storage files.
The executive can use this part to save useful business information, and this part also help the executive to search historical business information easily 4.
Output devices, which provide a visual or permanent record for the executive to save or read.
This device refers to the visual output device such as monitor or printer 33 In addition, with the advent of local area networks (LAN), several EIS products for networked workstations became available.
These systems require less support and less expensive computer hardware.
They also increase access of the EIS information to many more users within a company.
Software Choosing the appropriate software is vital to design an effective EIS.Therefore, the software components and how they integrate the data into one system are very important.
The basic software needed for a typical EIS includes four components: 1.
Text base software.
The most common form of text are probably documents 2.
Database.
Heterogeneous databases residing on a range of vendor-specific and open computer platforms help executives access both internal and external data 3.
Graphic base.
Graphics can turn volumes of text and statistics into visual information for executives.
Typical graphic types are: time series charts, scatter diagrams, maps, motion graphics, sequence charts, and comparison-oriented graphs (i.e., bar charts) 4.
Model base.
The EIS models contain routine and special statistical, financial, and other quantitative analysis Advantages and disadvantages of EIS Advantages of EIS  Easy for upper-level executives to use, extensive computer experience is not required in operations  Provides timely delivery of company summary information  Information that is provided is better understood  Filters data for management  Improves tracking information  Offers efficiency to decision makers Disadvantages of EIS  System dependent  Limited functionality, by design  Information overload for some managers  Benefits hard to quantify  High implementation costs  System may become slow, large, and hard to manage  Need good internal processes for data management 34  May lead to less reliable and less secure data 3.4 Transaction Processing System A transaction processing system is a type of information system.
TPSs collect, store, modify, and retrieve the transactions of an organization.
A transaction is an event that generates or modifies data that is eventually stored in an information system.
To be considered a transaction processing system the computer must pass the ACID test.
The essence of a transaction program is that it manages data that must be left in a consistent state, e.g.
if an electronic payment is made, the amount must be both withdrawn from one account and added to the other; it cannot complete only one of those steps.
Either both must occur, or neither.
In case of a failure preventing transaction completion, the partially executed transaction must be 'rolled back' by the TPS.
While this type of integrity must be provided also for batch transaction processing, it is particularly important for online processing: if e.g.
an airline seat reservation system is accessed by multiple operators, after an empty seat inquiry, the seat reservation data must be locked until the reservation is made, otherwise another user may get the impression a seat is still free while it is actually being booked at the time.
Without proper transaction monitoring, double bookings may occur.
Other transaction monitor functions include deadlock detection and resolution (deadlocks may be inevitable in certain cases of cross-dependence on data), and transaction logging (in 'journals') for 'forward recovery' in case of massive failures.
Components of TPS 1.
Input 2.
Processing 3.
Storage 4.
Output Storing and retrieving Storing and retrieving information from a TPS must be efficient and effective.
The data are stored in warehouses or other databases, the system must be well designed for its backup and recovery procedures.
Databases and files The storage and retrieval of data must be accurate as it is used many times throughout the day.
A database is a collection of data neatly organized, which stores the accounting and operational records in the database.
Databases are always protective of their delicate data, so they usually 35 have a restricted view of certain data.
Databases are designed using hierarchical, network or relational structures; each structure is effective in its own sense.
 Hierarchical structure: organizes data in a series of levels, hence why it is called hierarchal.
Its top to bottom like structure consists of nodes and branches; each child node has branches and is only linked to one higher level parent node.
 Network structure: Similar to hierarchical, network structures also organizes data using nodes and branches.
But, unlike hierarchical, each child node can be linked to multiple, higher parent nodes.
 Relational structure: Unlike network and hierarchical, a relational database organizes its data in a series of related tables.
This gives flexibility as relationships between the tables are built.
A relational structure.
A hierarchical structure.
A network structure.
The following features are included in real time transaction processing systems:  Good data placement: The database should be designed to access patterns of data from many simultaneous users.
 Short transactions: Short transactions enables quick processing.
This avoids concurrency and paces the systems.
 Real-time backup: Backup should be scheduled between low times of activity to prevent lag of the server.
 High normalization: This lowers redundant information to increase the speed and improve concurrency, this also improves backups.
 Archiving of historical data: Uncommonly used data are moved into other databases or backed up tables.
This keeps tables small and also improves backup times.
 Good hardware configuration: Hardware must be able to handle many users and provide quick response times.
In a TPS, there are 5 different types of files.
The TPS uses the files to store and organize its transaction data: 36  Master file: Contains information about an organization’s business situation.
Most transactions and databases are stored in the master file.
 Transaction file: It is the collection of transaction records.
It helps to update the master file and also serves as audit trails and transaction history.
 Report file: Contains data that has been formatted for presentation to a user.
 Work file: Temporary files in the system used during the processing.
 Program file: Contains the instructions for the processing of data.
4.0 Conclusion Information Systems are indispensable to the business, industry, academia and any organization to meet the future challenges.
It Enhanced global competitiveness, Captures market opportunities; supports corporate strategy, enhances worker productivity and improves quality of goods and services.
5.0 Referees [1] "Definition of Application Landscape".
Software Engineering for Business Information Systems (sebis).
Jan 21, 2009. http://wwwmatthes.in.tum.de/wikis/system- cartography/application-landscape.
Retrieved January 14, 2011.
[2] Kroenke, D M. (2008).
Experiencing MIS.
Prentice-Hall, Upper Saddle River, NJ [3] O'Brien, J A.
(2003).
Introduction to information systems: essentials for the e-business enterprise.
McGraw-Hill, Boston, MA [5] http://www.occ.treas.gov/handbook/mis.pdf [6] O’Brien, J (1999).
Management Information Systems – Managing Information Technology in the Internetworked Enterprise.
Boston: Irwin McGraw-Hill.
ISBN 0071123733 [7] Transaction processing systems (TPS) collect and record the routine transactions of an organization.
Examples of such systems are sales order entry, hotel reservations, payroll, employee record keeping, and shipping.
[8] Pant, S., Hsu, C., (1995), Strategic Information Systems Planning: A Review, Information Resources Management Association International Conference, May 21–24, Atlanta.
37 [9] Gutes Entscheiden in Wirtschaft, Politik und Gesellschaft.
Zurich, vdf Hochsch Haettenschwiler, P. (1999).
Neues anwenderfreundliches Konzept der Entscheidungsunterstützung ulverlag AG: 189-208.
[10] Power, D. J.
(2002).
Decision support systems: concepts and resources for managers.
Westport, Conn., Quorum Books.
[11] Sprague, R. H. and E. D. Carlson (1982).
Building effective decision support systems.
Englewood Cliffs, N.J., Prentice-Hall.
ISBN 0-130-86215-0 [12] Haag, Cummings, McCubbrey, Pinsonneault, Donovan (2000).
Management Information Systems: For The Information Age.
McGraw-Hill Ryerson Limited: 136-140.
ISBN 0-072- 81947-2 [13] Marakas, G. M. (1999).
Decision support systems in the twenty-first century.
Upper Saddle River, N.J., Prentice Hall 38  UNIT 4 BASIC CONCEPT OF COMMUNICATION THEORY TABLE OF CONTENTS 1.0 Introduction 2.0 Objective 3.0 Main content 3.1 Communication theory 3.2 Means of Communication 3.3 Interpersonal communication 3.4 Communication at a distance 3.5 Developmental trend in Distance communication 3.6 Conclusions 3.7 Summary 1.0 INTRODUCTION Since the time of writing, communication technologies have had a major influence on society.
The Canadian historian Harold Innis wrote that communication technologies were the key elements in the development of all the great ancient societies: Egypt was transformed by papyrus and written hieroglyphics; ancient Babylonia used cuneiform writing, impressed indelibly into clay tablets to develop a great economic system; the ancient Greeks' love of the spoken word led them to perfect public speaking, persuasive rhetoric, drama, and philosophy; for administering their empire, the Romans developed an unparalleled system of government that depended on the Roman alphabet; and of course paper and the printing press extended new ways of thinking across Europe and paved the way for the European Renaissance and the Protestant Reformation.
Later Marshall McLuhan (a student of Innis's) argued that radio, movies, and television had just as much impact, if not more, on modern society as printing.
The difference between these media is that while printing encourages logical thinking that lays out ideas one step at a time, electronic media disrupt logical, linear thinking.
Electronic media create the sense of experiencing everything at once, in no particular order and disconnected from the sources of the messages.
39 Most observers agree that communication media and technologies have contributed to a society that is changing very rapidly.
Three key issues have arisen in the tide of this rapid change: individual privacy, coverage of politics in the media, and the availability of information.
2.0 OBJECTIVE After studying this unit, the student should be able to:  To define and explain the difference between communication and Communication theory.
 To understand and identify the various means/methods of communication.
 To understand and explain the meaning and basic forms of interpersonal communication.
 To explain the meaning of Communication at a distance.
 To understand and explain the various developmental trend in Distance communication 3.1 COMMUNICATION THEORY What is communication theory : it is the study of all forms of human communication, including branches of linguistics such as semantics as well as telecommunications and other nonlinguistic forms.
What is communication: it is the exchange of information between people, e.g.
by means of speaking, writing, or using a common system of signs or behavior Communication, the process of sharing ideas, information, and messages with others in a particular time and place.
Communication includes writing and talking, as well as nonverbal communication (such as facial expressions, body language, or gestures), visual communication (the use of images or pictures, such as painting, photography, video, or film), and electronic communication (telephone calls, electronic mail, cable television, or satellite broadcasts).
Communication is a vital part of personal life and is also important in business, education, and any other situation where people encounter each other.
Businesses are concerned with communication in several special ways.
Some businesses build and install communication equipment, such as fax (facsimile) machines, video cameras, CD players, printing presses, personal computers, and telephones.
Other companies create some of the messages or content that those technologies carry, such as movies, books, and software.
These companies are part of the media or telecommunications industries.
Organizational communication is important in every business.
People in organizations need to communicate to coordinate their work and to inform others outside the business about their products and services (these kinds of communication are called advertising or public relations).
3.2 MEANS/METHODS OF COMMUNICATION 3.2.1.
COMMUNICATION AMONG ANIMALS Humans are not the only creatures that communicate; many other animals exchange signals and signs that help them find food, migrate, or reproduce.
The 19th-century biologist Charles Darwin showed that the ability of a species to exchange information or signals about its environment is an important factor in its biological survival.
For example, honey bees dance in specific patterns that tell other members of the hive where to find food.
Insects regularly use pheromones, a special kind of hormone, to attract mates.
Elephants emit very low-pitched sounds, below the level of human hearing, that call other members of the herd over many miles.
Chimpanzees use 40 facial expressions and body language to express dominance or affection with each other.
Whales and dolphins make vocal clicks, squeals, or sing songs to exchange information about feeding and migration, and to locate each other 3.2.2 LANGUAGE IN COMMUNICATION While other animals use a limited range of sounds or signals to communicate, humans have developed complex systems of language that are used to ensure survival, to express ideas and emotions, to tell stories and remember the past, and to negotiate with one another.
Oral (spoken) language is a feature of every human society or culture.
Anthropologists studying ancient cultures have several theories about how human language began and developed.
The earliest language systems probably combined vocal sounds with hand or body signals to express messages.
Some words may be imitative of natural sounds.
Others may have come from expressions of emotion, such as laughter or crying.
Language, some theorists believe, is an outgrowth of group activities, such as working together or dancing.
Over 6000 languages and major dialects are spoken in the world today.
As some languages grow, others disappear.
Languages that grow also evolve and change due to class, gender, profession, age group, and other social forces.
The Latin language is no longer spoken but survives in written form.
Hebrew is an ancient language that became extinct, but has now been brought back to life and is spoken today.
Others, such as the ancient languages of native peoples in Central and South America, the Pacific Islands, and some of the Native American peoples of North America, which had no written form, have been lost as the speakers died.
Today anthropologists are trying to record and preserve ancient languages that are still spoken in remote areas or by the last remaining people in a culture 3.2.3 ALPHABETS AND SYMBOL IN COMMUNICATION Most languages also have a written form.
The oldest records of written language are about 5000 years old.
However, written communication began much earlier in the form of drawings or marks made to indicate meaningful information about the natural world.
The earliest artificially created visual images that have been discovered to date are paintings of bears, mammoths, woolly rhinos, and other Ice Age animals on cave walls near Avignon, France.
These paintings are over 30,000 years old.
The oldest known animal carving, of a horse made from mammoth ivory, dates from approximately 30,000 years BC and was found in present-day Vogelhard, Germany.
Other ancient symbol-recording systems have been discovered.
For example, a 30,000-year-old Cro- Magnon bone plaque discovered in France is engraved with a series of 29 marks; some researchers believe the plaque records phases of the moon.
A piece of reindeer antler approximately 15,000 years old was also found in France, carved with both animal images and “counting” marks.
The ancient Incas in Peru, who lived from about the 11th century to the 15th century AD, used a system of knotted and colored strings called quipu to keep track of population, food inventories, and the production of gold mines.
Perhaps the earliest forerunner of writing is a system of clay counting tokens used in the ancient Middle East.
The tokens date from 8000 to 3000 BC and are shaped like disks, cones, spheres and other shapes.
They were stored in clay containers marked with an early version of cuneiform writing, to indicate what tokens were inside.
Cuneiform was one of the first forms of writing and was pictographic, with symbols representing objects.
It developed as a written language in Assyria (an ancient Asian country in present-day Iraq) from 3000 to 1000 BC.
Cuneiform eventually acquired ideographic elements— that is, the symbol came to represent not only the object but also ideas and qualities associated 41 with it.
The oldest known examples of script-style writing date from about 3000 BC; papyrus sheets (a kind of early paper made from reeds) from about 2700 to 2500 BC have been found in the Nile Delta in Egypt bearing written hieroglyphs, another pictographic-ideographic form of writing.
Chinese began as a pictographic-ideographic written language perhaps as early as the 15th century BC.
Today written Chinese includes some phonetic elements (symbols indicating pronunciation) as well.
The Chinese writing system is called logographic because the full symbols, or characters, each represent a word.
Cuneiform and Egyptian hieroglyph eventually incorporated phonetic elements.
In syllabic systems, such as Japanese and Korean, written symbols stand for spoken syllable sounds.
The alphabet, invented in the Middle East, was carried by the Phoenicians (people from a territory on the eastern coast of the Mediterranean, located largely in modern Lebanon) to Greece, where vowel sounds were added to it.
Alphabet characters stand for phonetic sounds and can be combined in an almost infinite variety of words.
Many modern languages, such as English, German, French, and Russian, are alphabetic languages.
3.3 INTERPERSONAL COMMUNICATION In every society, humans have developed spoken and written language as a means of sharing messages and meanings.
The most common form of daily communication is interpersonal that is, face-to-face, at the same time and in the same place.
The most basic form of interpersonal communication is a dyad (an encounter or conversation between two people).
Some dyads exist over a long period of time, as in a marriage or partnership.
Communicating well in a dyad requires good conversational skills.
Communicators must know how to start and end the conversation, how to make themselves understood, how to respond to the partner's statements, how to be sensitive to their partner's concerns, how to take turns, and how to listen.
Together, these abilities are called communication competence.
Shyness or reluctance to interact is called communication apprehension.
Persuasion is the process of convincing others that one's ideas or views are valuable or important.
Communication may also occur in small groups, such as families, clubs, religious groups, friendship groups, or work groups.
Most small-group interaction involves fewer than ten people, and the communicators need the same communication skills as in a dyadic conversation.
However, additional factors called group dynamics come into play in a small group.
A group may try to work toward a consensus, a general sense of understanding or agreement with others in the group.
Groupthink may occur, in which a group reaches consensus so quickly that its members mistakenly ignore other good ideas.
Small-group members may experience disagreement or even conflict.
Some members may be more persuasive than others and form sides, or cliques, within the group.
A special case of small-group interaction occurs in organizations where there is work to do or a task for the group to perform.
Or several small groups may need to interact among each other within a single organization.
In these cases, the groups must communicate well, both among themselves and with other groups, so that their members can perform their work effectively and make good decisions.
Problems sometimes arise in organizational communication between supervisors and workers, or between different groups of workers who are responsible for different parts of a task.
Therefore, small-group communication skills can be as necessary as conversation skills in the workplace or other organizational activities.
42 Interpersonal communication occurs with larger groups as well, such as when a speaker gives a talk to a large crowd (a political candidate giving a speech at a campaign rally, or a teacher lecturing to a large class).
However, the audience can respond in only limited ways (such as with applause, nodding, whistles, boos, or silence).
The speaker usually wants to be persuasive or informative, so the words chosen and the style of delivery or performance are very important.
A speaker who wants to reach an even larger audience than the people who can physically hear the speech in one place must use communication technology or media to get the message across distance and even time.
3.4 COMMUNICATION AT A DISTANCE From the earliest times, people have needed to communicate across distance or over time.
Since the beginnings of writing, communication media have allowed messages to travel over distance and time.
A communication medium is a means for recording and transporting a message or information.
The word medium comes from the Latin word medius, meaning middle or between.
It is a channel or path for sending a message between communicators.
A single channel—such as radio, or a book, or the telephone—is called a medium; media is plural, meaning more than one medium.
Entertainment are delivered to the public in virtually every nation around the world.
The term broadcasting refers to the airborne transmission of electromagnetic audio signals (radio) or audiovisual signals (television) that are accessible to a wide population via standard, readily available receivers.
The term has its origins in the medieval agricultural practice of “broadcasting,” which refers to planting seeds by scattering them across a field.
Broadcasting is a crucial instrument of modern social and political organization.
At its peak of influence in the mid-20th century, radio and television broadcasting was employed by political leaders to address entire nations.
Because of radio and television’s capacity to reach and influence large numbers of people, and owing to the limited spectrum of frequencies available, governments have commonly regulated broadcasting wherever it has been practiced.
In the early 1980s, new technologies—such as cable television and videocassette players—began eroding the dominance of broadcasting in mass communication, splitting audiences into smaller, culturally distinct segments.
Previously the only means of delivering radio and television to home receivers, broadcasting is now just one of several delivery systems available to listeners and viewers.
Sometimes broadcasting is used in a broader sense to include delivery methods such as wire-borne (cable) transmission, but these are more accurately called “narrowcasting” because they are generally limited to paying subscribers.
3.5 DEVELOPMENTAL TREND IN DISTANCE COMMUNICATION 3.5.1.EARLY METHODS OF COMMUNICATION Early societies developed systems for sending simple messages or signals that could be seen or heard over a short distance, such as drumbeats, fire and smoke signals, or lantern beacons.
Messages were attached to the legs of carrier pigeons that were released to fly home (this system was used until World War I, which started in 1914).
Semaphore systems (visual codes) of flags or flashing lights were employed to send messages over relatively short but difficult-to-cross distances, such as from hilltop to hilltop, or between ships at sea.
In the early 1790s the French scientist and engineer Claude Chappe persuaded the French government to install a system of towers that used semaphore signals to send visual telegraphs along approved routes throughout the country.
The system was copied in Great Britain and the United States.
43 Some ancient societies, such as the Roman or Byzantine empires, expanded their territorial control far beyond their original boundaries, and traded with distant neighbors.
To hold on to their far-flung territories, they needed two technologies that have remained closely tied ever since: transportation and the ability to record information.
Recorded messages had to be carried easily; therefore, lightweight forms of recording (such as papyrus or animal skins) were desirable.
In Nigeria, town criers, beating of metal gong , e.tc.
are used for communication purposes 3.5.2.PAPER AND PRINTING The first lightweight medium was papyrus, an early form of paper used by the Egyptians that was made from grasses called reeds.
Later, in the 2nd century AD, the Chinese wrote on silk fabric instead of wood, and developed paper made from silk fibers.
(Today paper made from cotton or linen fibers is still called rag paper.)
From as early as the 2nd century BC, Europeans wrote on thin layers of tanned and scraped animal skins called parchment or vellum, with quill pens made from bird feathers.
Parchment is not as light as papyrus but is very durable; many parchment manuscripts and books from the Middle Ages still exist.
The Arabs brought papermaking to Europe from China in the 11th century AD.
Paper gave European merchants, who traveled across the continent, a portable and inexpensive way to keep records.
Until the 1400s in Europe, all documents were handwritten.
Copyists and editors called scribes recorded commercial transactions, legal decisions and pronouncements, and manuscript copies of religious books—many scribes were monks working in monasteries.
By the 15th century, however, the need arose for an easier way to duplicate documents.
In Asia, block printing had already been developed by Buddhist monks in China in about the 8th century .
A similar technique was later used in the 15th century by Europeans to make illustrations for printed books.
An early version of movable type was first developed in China around 1045, and was independently developed by Koreans in the 13th century AD.
In 1450 the German printer Johannes Gutenberg perfected movable metal type and introduced the first reliable system of typesetting, a key invention in the development of printing.
With movable type, a raised, reversed image of each letter can be hand-set, word by word, into a frame that holds the pieces together.
The raised letters are inked, a sheet of paper laid over them and pressed down on the letters with a screw-driven press, creating a correct image of the text.
When enough copies are printed, the letters can be taken apart and reused.
The technique made printing numerous copies of textual material much easier, and the number of printing shops grew dramatically over the next century.
As more books became available, more people learned to read.
Books were printed in the local, or vernacular, languages as well as classical Greek and Latin.
With literacy came exposure to new ideas; some historians believe that the 16th-century Protestant Reformation (a revolution in the Christian church that divided it into factions) might not have occurred if European thought had not been prepared by ideas introduced and circulated in printed books.
Printers published other things besides books, including newspapers, pamphlets, and broadsides (sheets of paper printed on one or both sides).
These cheaper works helped spread news throughout Europe and, in the 17th and 18th centuries, throughout the British colonies During the Industrial Revolution of the late 18th and early 19th centuries, printing technologies evolved rapidly.
The steam-powered press was invented in Germany in the 19th century, and the rotary press, which prints images onto a continuous sheet of paper from a rotating drum, was 44 introduced in the United States in 1846.
The Linotype typesetting machine was patented by the German-born American inventor Ottmar Mergenthaler in 1884.
It permitted typesetters to set text by typing on a keyboard rather than hand-setting each letter individually.
Together, the Linotype machine and the rotary press transformed the speed of printing.
These so-called hot- metal or letterpress printing technologies dominated the industry until the 1950s, when phototypesetting and photo-offset printing were introduced.
Photocopying was another technology that made document duplication easier.
Invented by American physicist and inventor Edwin Land in the 1950s, photocopying transfers an image from one sheet of paper to another very rapidly.
A more recent advance is computer typesetting and printing.
Computers and word-processing and graphics software are used today to set type and compose pages on the screen just as they will look in the final print, in either black and white or color.
Page layouts can also be transmitted digitally (numerically coded into electronic pulses) via fax machines, computer modems, telephone networks, and satellite systems to other locations for editing, redesign, or printing.
The spread of computer-based word processing and graphic design has led to the growth of desktop publishing.
Today almost anyone can publish newsletters, newspapers, or magazines for medium-sized audiences.
Business communication has been transformed by computer and information technologies: letters, memos, reports, or other documents can be transmitted almost anywhere at the speed of light.
Early advocates of business computers predicted the paperless office, an office where paper would be made obsolete by computer technology.
Experience, however, has shown that the ease of copying, printing, and document transmission made possible by computer technology has produced more demand for paper, not less.
3.5.3.POSTAL SERVICES Different societies have also devised systems for transporting messages from place to place and from person to person.
The earliest were courier-type services; messengers carried memorized or written messages from one person to another, and returned with the reply.
The Persian and Roman empires and some Asian societies sent couriers regularly along planned routes to retrieve reliable and timely information about trade and military affairs from distant areas.
In Europe, similar systems were established by commercial concerns and merchants who needed to exchange information about trade routes and goods.
The ruling aristocracy used trusted messengers to carry confidential or sensitive information from capital to capital or kingdom to kingdom, but they were typically soldiers or servants.
Over time, these arrangements evolved into government-operated systems for any citizen or subject to post messages to any other, financed by charging users a tax or fee for postage (verified by postage stamps).
In the United States, the postal service was established by the government in 1789, and the postmaster general's office was created to supervise the mail service.
The first postmaster general of the United States was Samuel Osgood.
In the late 19th century, as the United States expanded its territory west beyond reliable roads or rail lines, the U.S. Post Office started the Pony Express, reviving courier-style services in the new territories.
Pony Express riders carried sacks of mail through rugged and remote territory, relaying their loads from one rider to the next.
The Pony Express quickly became renowned for its speed of delivery.
Over time, the U.S. Post Office took advantage of new transportation systems.
Huge volumes of mail were sent across the country on trains, and the Post Office started its own postal security 45 force to prevent the mail from being stolen in railroad holdups.
They were also the first postal service to hire pilots to fly mail to distant or rural locations within the United States and overseas.
By the 1930s every small town and rural route had carrier service; in many places, deliveries were made twice a day.
As demand for postal services grew, the U.S. Post Office developed systems for coding and sorting the mail more quickly, notably the neighborhood ZIP Code system in the 1960s.
The U.S. Post Office became a private operation in the 1970s under the supervision of the U.S. federal government, and was renamed the U.S.
Postal Service (USPS).
Today the USPS is self- supporting, and is exploring a number of new technologies that will allow it to offer better service at lower cost, including electronic document delivery services and new electronic sorting systems.
3.5.4.THE TELEGRAPH For most of human history, messengers were the chief method of relaying information across long distances.
The marathon in athletic contests is said to commemorate the time in ancient Greece when a messenger ran 42 km (26 mi) with the news of a great battle victory.
According to the legend, the man collapsed and died just after delivering the good news.
First horses and later the steam engine shortened the time that information spent in transit, but these were incremental improvements.
Then came the telegraph, and the world has never been the same since.
Telegraph was the first truly electronic medium, which sent and received electrical signals over long-distance wires The principle of telegraphy is simple: Pulses of electrical current are sent through a wire by manually tapping on a key to operate a simple switch.
At the receiving end, the pulses create a magnetic field that causes a needle to punch holes in a strip of paper or that creates an audible click as a contact closes.
When relayed in a coded fashion, these pulses can transmit a message, potentially over great distances.
In the 1830s and 1840s several scientists and tinkerers built telegraph prototypes.
However, the technology did not take hold until an American painter and sculptor named Samuel F. B. Morse convinced the United States government to fund a telegraph line between Washington, D.C., and Baltimore, Maryland, in 1843.
Morse also invented a system of dots and dashes—the code that now bears his name—to help standardize transmissions by wire.
After the Washington-to- Baltimore line was completed in May 1844, Morse tapped out the first message: “What hath God wrought!” When the message was successfully received almost instantaneously 60 km (40 mi) away, the telecommunications revolution had begun.
Early telegraph operators soon found that the dots and dashes of Morse code could be distinguished by sound, and the paper tapes were discarded.
The telegraph made it possible for many companies to conduct their business globally for the first time.
Because price changes could be communicated almost instantaneously, the telegraph also prompted the reorganization of American commodities markets.
Prices became uniform from city to city, and futures (agreements to buy a commodity at a fixed price on a fixed date in the future) markets were established.
In addition, standard time zones across the United States were established so that railroads could set regular and consistent schedules as trains moved across the country, enabling the railroads to check on schedules, passengers, and freight via telegraph.
46 Telegraph technology became more sophisticated, especially after its competitor, the telephone, was introduced in the 1890s.
Telegraph systems evolved into telex systems, in which machines eliminated the need for coding and decoding the messages.
Users could type in a message, and the identical message would appear at the recipient's end, carried over telegraph and telephone lines (and eventually satellite systems) to telex machines anywhere in the world.
In remote areas where long-distance telephone service was unavailable or impractical, telex machines were widely used (much like an early version of electronic mail).
Telegraph and telephone lines were also used to transmit pictures via an early version of facsimile called telefacsimile or Wirephoto service; newspapers used Wirephoto to transmit photographs as early as the 1930 3.5.5.TELEPHONE Early devices capable of transmitting sound vibrations and even human speech appeared in the 1850s and 1860s.
The first person to patent and effectively commercialize an electric telephone was Scottish-born American inventor Alexander Graham Bell.
His patent, granted in 1876, was titled Improvement in Telegraphy, and contained the design of a device that would transmit the human voice over wires instead of electrical clicks or other signals, like the telegraph.
Originally, Bell thought that the telephone would be used to transmit musical concerts, lectures, or sermons.
The American inventor Elisha Gray filed an intent to patent at the same time, but after many court battles, Bell was given the rights to the invention.
Another court case, on behalf of Italian- American inventor Antonio Meucci, was headed for the Supreme Court of the United States when Meucci died and the case was abandoned.
Bell and his financial backers established the Bell Telephone Company.
In an extraordinary business move, Bell decided to lease telephones rather than sell them.
His next step would be to build the connecting networks and sell services on those networks to customers.
Bell began by leasing pairs of telephones that would connect two locations, such as a businessman's home and office, or between two partners' offices.
However, the real appeal of telephone service emerged with the opening of the first telephone exchange—a switchboard connecting any member of a group of subscribers to any other member—in 1878.
The Bell Telephone Company established as many exchanges as possible, especially high-quality voice lines for wealthy city customers.
Some customers resisted using the telephone at first because it did not leave a written record of transactions or orders; however, others saw this feature as an advantage.
By 1894 roughly 260,000 Bell telephones were in use in the United States, about one for every 250 people.
After Bell's patents expired in 1893 and 1894, other companies began manufacturing telephones, wiring new networks, and installing exchanges.
The new exchanges connected people in rural communities and residential households.
Some were rural cooperatives owned and operated by the customers.
The American Telephone and Telegraph Company (AT&T), which bought the Bell Telephone Company in 1900, developed switching systems to connect calls between exchanges, and eventually began experimenting with long-distance connections.
Between the 1880s and the 1980s the telephone system in the United States had an enormous effect on the quality of life and work.
In rural communities, telephone service meant an end to the isolation and loneliness experienced by many farm and ranch families.
Families whose members moved away to school or new jobs could stay in contact with each other over the phone.
For ill or disabled people, the telephone became an indispensable link to the outside world.
Telephone service also enabled immediate contact with emergency services, such as the police, fire department, or emergency medical services.
By the 1960s the telephone was 47 considered so essential that telephone companies provided basic services at reduced rates to elderly and disabled people.
The telephone network has also provided the electronic network for new computer-based systems like the Internet (a worldwide interconnection of computers and computer networks), facsimile transmissions (copies sent electronically by fax machines through telephone lines), and the World Wide Web (library of resources stored on computers and accessed through the Internet).
The memory and data-processing power of individual computers can be linked together and data transmitted over telephone lines, even internationally via satellite, by connecting computers to the telephone network through telephone-like devices called modems (modulator- demodulators).
The telephone network itself now relies extensively on computer-based switches and exchanges that have made all kinds of new telephone-related services possible, such as call waiting, call forwarding, call return, voice-mail services, and caller ID.
The relationship today between computers and the telephone system is inseparable 3.5.6.RADIO The telegraph and telephone were systems for distance communication that sent electrical signals through wires.
The earliest system for sending electrical signals through the air via electromagnetic waves was called wireless, and later radio.
Radio technology was based on the discoveries of James Clerk Maxwell.
In 1864 he proposed a theory that electromagnetic signals did not need wires to be transmitted, but could be carried on electromagnetic waves.
He demonstrated that light, electricity, and heat are all part of a band of radiant energy we now call the radio or electromagnetic spectrum.
The Italian electrical engineer Guglielmo Marconi was the first person to invent a true wireless radio.
In 1895 he built a system that could send and receive a signal at a distance of close to 3 km (close to 2 mi).
He moved to England, and by 1899 the British Marconi Company had sent signals across the English Channel.
In 1901 Marconi received the Morse code signal for the letter S sent across the Atlantic Ocean to Canada.
Marconi's radio system used a spark-gap technology that could transmit only simple on-off signals—so radio signaling used an on-off system like Morse code.
This type of radio technology is called radiotelegraphy.
Wireless was especially valuable for ships in distress, so that other ships could be dispatched to save their passengers and crews in times of emergency.
In 1901 the Canadian-born American physicist Reginald Fessenden patented an alternator that would use continuous waves instead of on-off spark-gap signals.
This system could also send signals much farther and with much less background noise, so it could carry the sound of the human voice.
This new approach to radio was called radiotelephony.
On Christmas Eve and New Year's Eve in 1906, Fessenden produced the first radio broadcasts from Brant Rock, Massachusetts, which were picked up as far away as New York and by ships in the Atlantic.
Another American, Lee De Forest, is best known for his invention of the triode vacuum tube, called the Audion, which amplified radio signals so that musical concerts, dramatic performances, and speeches could be heard clearly over the radio.
Radio technology improved rapidly throughout the 20th century.
The first breakthrough was the invention of the cat's-whisker receiver, or crystal set, which used a silicon crystal and a small metal wire to detect radio waves clearly.
Later improvements were made in the valves, or tubes, such as De Forest's Audion, which amplified the signal once it was received.
Radio transmissions initially used amplitude modulation (AM) to superimpose audio signals onto radio waves.
The invention of frequency modulation (FM) radio provided much more sensitive and 48 clear radio transmission and reception.
Tuners became more sensitive, and more broadcast signals were sent over the air at different frequencies.
In the 1950s and 1960s radio manufacturers began replacing the bulky and heat-generating vacuum tubes in radios with transistors, and radios became smaller.
In the United States, the first regularly scheduled public broadcasts were made in 1920 from station KDKA in Pittsburgh, Pennsylvania.
Other stations were soon established across the country, and companies like the Radio Corporation of America (RCA) and Westinghouse, which owned many stations, formed radio networks that would produce and share programming.
Radio programming soon filled the airwaves.
Competitors using the same frequencies jammed each other's signals.
Eventually the radio industry asked the federal government to intervene in their disputes over frequencies and signal power.
The Federal Radio Commission (FRC) was created in 1927 and was given the task of allocating frequencies to different users.
However, the FRC was a somewhat ineffective body until the Communications Act of 1934, when it was renamed the Federal Communications Commission (FCC) and given a budget and a staff.
FCC rulings had the power of law, and the agency was responsible for issuing licenses to radio broadcasters for particular bandwidths, frequencies, and signal powers.
License holders had to demonstrate that they operated their radio stations “in the public interest, convenience, and necessity”.
Most large cities and many small towns have a number of local radio stations, on both the AM and the FM frequencies.
Other radio frequencies are used for other purposes, especially television.
Certain frequencies are used to relay wireless telephone calls across small defined geographic areas called “cells”.
In the United States, some frequencies are dedicated to citizens-band (CB) radio, which long-distance truck drivers use to check on road conditions, report problems, or just to chat.
Special frequencies are devoted to emergency use, such as police, fire, or emergency medical dispatching, or to aviation radio.
An important part of the radio spectrum is shortwave, which can carry radio signals around the world.
International shortwave broadcasts are very popular 3.5.7.TELEVISION Just as inventors had sought ways to transmit sound using electromagnetic waves, they worked to develop similar methods for transmitting pictures.
By the first decade of the 20th century, the basic ideas of television technology were understood, although it took several more decades to work out the necessary improvements in existing technology.
Two pioneers independently created the first workable television systems—American inventor Philo T. Farnsworth and Russian-born American engineer Vladimir K. Zworykin.
Farnsworth used an electronic camera he called an image dissector to transmit a picture of a dollar sign in 1927.
He patented aspects of his system, and developed his television further in the 1930s, but lost his financial backing when World War II (1939-1945) began.
In 1923 Zworykin first demonstrated an electronic television camera he called the iconoscope.
At the time, he was working for Westinghouse Electronic Corporation, but Zworykin moved to RCA when David Sarnoff, vice president of RCA, became interested in his invention.
Sarnoff supported the development of the iconoscope when RCA obtained the rights to Westinghouse's radio research projects in 1930.
In 1932 RCA was transmitting 120-line pictures, and by 1935 had a 343-line image.
The first television sets were offered for sale in the United States in 1938, although they had been available in England for two years before that.
Standards for television broadcasting and 49 television receivers were put in place by the FCC in 1939; in 1941 the commission accepted industry recommendations for television technology standards that are still in place today.
RCA started the first regularly scheduled television programming on July 1, 1941, in New York City.
World War II put television technology on “hold.” After the war, however, technical improvements and American prosperity created a great demand for radio and television systems.
At the end of the war, only six television stations were broadcasting in the United States (in New York City; Washington, D.C.; Chicago, Illinois; Philadelphia, Pennsylvania; Schenectady, New York; and Los Angeles, California), and for only a few hours each day.
By 1948, 34 stations were broadcasting television signals in 21 major cities, and about 1 million television sets had been sold.
By the end of the 1950s television was on the air almost everywhere in the country.
The FCC set aside channels for public or educational television to ensure quality programming and to reach remote communities.
Three national television networks emerged—NBC, then owned by RCA; CBS; and ABC.
Since the 1950s many improvements have been made in television technology, particularly the introduction of color television in the 1960s.
Image reception has become clearer, and screens have become larger.
Most televisions can now receive stereo sound.
The widespread growth of cable television since the 1960s has introduced many new channels and types of programming into American homes.
And today direct-broadcast-satellite (DBS) services allow individual households to receive hundreds of channels carried by satellites directly into their homes.
There is no doubt that television has been one of the most important communication technologies in history.
Televisions are switched on an average of seven hours a day in American households.
Debates continue about the medium's effects on children, culture, education, politics, and community life.
Critics say that television feeds a constant stream of simplified ideas and sensationalistic images, that it has a negative effect on political campaigns and voting patterns, that it destroys local cultures in favor of a bland national culture, and that it has encouraged the growth of an uncritical and passive audience.
Defenders say that television provides a great deal of high-quality educational and cultural programming, and that it is the major source of national and international news and information for most U.S. citizens.
Television can be a very effective teaching tool in the classroom and at home.
And, as the Canadian writer Marshall McLuhan pointed out, perhaps nothing has been more responsible for creating the global village—the sense that we can see and hear events anywhere in the world as they happen, and so can feel more connected to other places.
3.5.8.COMPUTERS By the 1890s calculating machines were used to tabulate the U.S. Census with a punched-card system invented by Herman Hollerith.
Electromechanical calculators were being built by the 1930s, especially by a new company called the International Business Machines Company (IBM).
The first truly electronic memory and processors were built by John Vincent Atanasoff in 1939 at the Iowa State College, and the first fully functioning electronic computers, a series of ten called Colossus, were built by the British Secret Service during World War II to help them crack the Germans' secret military codes.
The first general-purpose electronic computer in America, called the Electronic Numerical Integrator and Computer (ENIAC), was introduced at the University of Pennsylvania in 1946.
Two of its inventors, American engineers John Presper Eckert, Jr., and John Mauchly, moved on to build the first electronic computer for commercial use, the UNIVAC, at the Remington Rand Corporation.
50 While computers continued to improve, they were used primarily for mathematical and scientific calculations, and for encoding and deciphering encoded messages.
Computer technology was finally applied to printed communication in the 1970s when the first word processors were created.
These machines had a single purpose and were not what would be considered full computers, but they added computer processing to typewriters to make writing and changing text significantly easier.
In 1975 the first microcomputer was introduced, which had the power of many larger machines but could fit onto a desktop.
This miniaturization was accomplished by using new microprocessor technologies, which compressed the memory and processing power of many hundreds and then thousands of circuits onto tiny chips of materials called semiconductors.
The invention was soon followed by the introduction of the first word-processing software in 1978, which enabled people to use the computer to write and change text and graphics.
At the same time that computers were becoming faster, more-powerful, and smaller, networks developed for interconnecting computers.
In the 1960s the Advanced Research Projects Agency (ARPA) of the U.S. Department of Defense, along with researchers working on military projects at research centers and universities across the country, developed a network called the ARPANET for sharing data and mainframe computer processing time over specially equipped telephone lines and satellite links.
The network was designed to survive the attack or destruction of some of its parts and continue to work.
Soon, however, scientists using the ARPANET realized that they could send and receive messages as well as data and programs over the network.
The ARPANET became the first major electronic-mail network; soon thousands of researchers all over the world used it.
Later the National Science Foundation (NSF) helped connect more universities and nonmilitary research sites to the ARPANET, and renamed it the Internet because it was a network of networks among many different organizations.
Today the Internet is the foundation of computer networks in the United States.
It is interconnected by both wire and over-the-air microwave and satellite telephone lines.
Commercial online service providers—such as America Online, CompuServe, and the Microsoft Network—sell Internet access to individual computer users and companies.
Smaller networks of computers, called Local Area Networks (LANs), can be installed in a single building or for a whole organization.
Wide Area Networks (WANs) can be used to span a large geographical area.
LANs and WANs use telephone lines, computer cables, and microwave and laser beams to carry digital information around a smaller area, such as a single college campus.
In turn, they can interconnect to the Internet.
Computer networks can carry any digital signals, including video images, sounds, graphics, animations, and text.
Since the 1970s personal computers have transformed American business, education, and entertainment.
The typical home or business computer today has many times the computing power of a single early mainframe.
People can use computers to design graphics and full-motion video, compose music, send electronic mail, make airline or hotel reservations, or search the Library of Congress over the World Wide Web.
They can play games and even visit electronic rooms or parties to talk to other people.
These activities are made possible by multimedia computer programs that employ still and motion pictures, sounds, graphics, and text together.
Computers are used in all aspects of business and education.
Self-instructional computer programs help people learn new information or skills .
Some programs are simulations, which imitate tasks that require the learner to perform in certain ways, and give the learner feedback 51 about that performance.
For example, airline pilots sharpen their flying skills in computer- generated flight simulators, which exactly duplicate the experience of flying in different types of aircraft.
CONCLUSION In this Unit we discussed the basic concept of communication theory, we considered the various means/methods of communication by both animals and humans, we also discussed the developmental trends in communication.
SUMMARY Communication media and technologies have contributed to a society that is changing very rapidly.
Communication, the process of sharing ideas, information, and messages with others in a particular time and place.
Communication includes writing and talking, as well as nonverbal communication (such as facial expressions, body language, or gestures), visual communication (the use of images or pictures, such as painting, photography, video, or film), and electronic communication (telephone calls, electronic mail, cable television, or satellite broadcasts) The study of this various forms of communication is called communication theory; the forms includes branches of linguistics such as semantics as well as telecommunications and other nonlinguistic forms.
REFERENCES 1.
McEliece, Robert J.
"Information Theory."
Microsoft® Encarta® 2009 [DVD].
Redmond, WA: Microsoft Corporation,.
2.
Lievrouw, Leah A.
"Communication."
Microsoft® Encarta® 2009 [DVD].
Redmond, WA: Microsoft Corporation, 3.
Watzlawick, P., Beavin, J., and Jackson, D. Pragmatics of Human Communication.
New York: Norton, 1967.
Unit 5 Communication System 52 Contents 1.0 Introduction 2.0 Objectives 3.0 Main contents 3.1 Concept of communication 3.2 Modulation/Demodulation process 3.21 Need for modulation 3.3 Limitation of communication process 3.31 Noise 3.32 Information and Bandwidth 3.4 Components of communication system 3.41 Transmitter 3.42 Communication channel 3.43 Receiver 3.44 Signal 3.5 Digital Modulation Technique 3.51 Introduction 3.52 Digital Modulation Formats 4.0 Conclusion 5.0 Reference 1.0 Introduction Community is simply the process of conveying message at a distance.
The electronic equipments, which are used for communication purpose are called communication equipments.
Different communication equipments when assembled together form a communication system.
Typical examples of communication system are line telephony and line telegraphs, radio 53 telephony and radio telegraphs, radio broadcasts, point-to-point communication and mobile communication, computer communication, radio communication, television broadcasting, radio elements, radio aids to navigation, radio aids to air craft handy, etc.
The earliest communication system namely, line telegraphy originated in eighteen forties (1840s).
in addition to this, line telephony came a few decades later where as radio-communication could become possible in the beginning of twentieth century on invention of triode valve.
Radio communication was further greatly improved during World War II.
It became more wildly used following the invention of transistor, integrated circuits (ICs) and other semiconductor devices in the subsequent years.
Also, in recent years, communication has become more widespread with the use of satellites and fiber optics.
Today, there has been an increasing emphasis on the use of computers in communication.
2.0 Objectives (1) Understand the concept of Electronic Communication (2) The reasons for Modulation and Demodulation Techniques (3) Sources of noise in radio communication system (4) Components of communication systems (5) Understanding the concept of digital communication 3.1 Concept of Communication Telecommunication (or simply communication) systems are concerned with the transmission of information form one point called the source to another point called the destination.
The exact nature of the wave form present at the sink or destination is unknown until after it is received – otherwise, no information was transmitted.
Information is said to be communicated to the user when on receiving such information, the user is surprised by the massage received.
Therefore, information transition involves the presentation of the receiving end of messages quite unknown 54 or strange to the user.
Let us consider the following information as being transmitted to a given user: LADIES AND GENTLEMEN, YOU ALL KNOW THAT TODAY IS FRIEDY AND THEREFORE, TOMORROW WILL BE SATURDAY.
LADIES AND GENTLEMEN, THE AMERICAN PRESIDENT WHO WAS HERE YESTERDAY NIGHT HAD A PLANE CRASH ON HIS WAY HOME AND DIED.
Surely, the second item contains more information than the first.
For information to be transferred from one point to another, a link is required between the two points.
The totality of equipments and facilities that transfer the information between the source of the information and the destination is referred to as the communication system.
We shall concern ourselves here, as a matter of fact, with an electrical communication system, which now achieves this function through the use of electrical principles and devices.
The main objectives of all communication systems, electrical or otherwise, are to provide at the destination an acceptable replica of the message sent from the source.
In other words, the message received at the destination should be identical to the original message or so similar to it that it is unmistakable for any other potential message.
The extent to which this primary objective is achieved depends on the equipment sophistication and this in turn is contingent upon the level of industrial development available.
There are many types of information sources, including men and machine.
Invariably, messages are in a variety of forms including:  Words or group of words  A sequence of discrete symbols or letters.
 Single time- changing quantity like the acoustic pressure produced by speech or music.
 Angular position of an aircraft  A pre-given code.
55 Now, let us consider the various elements of a typical communication system with a view to appreciating how the various components play their role in achieving successful communication.
Fig 1 shows the various components of a typical communication system.
We can analyze fig 1 using an example, a TV news bulletin from NTA.
The source is the news caster right inside the news studio.
The newscaster sends across to the audience in the homes, messages selected from the news bulletin.
These messages – in form of sentences are so to speak.
“raw” messages.
Also, the physical form or appearance of the newscaster is also referred to as “raw” messages.
The image of the newscaster is picked up by the TV camera and converted to a non-physical form- electrical signal called the video signal.
Similarly, the sound waves from the newscaster are picked up by a microphone and converted to yet another non physical form called audio signal.
Hence, the input transducers here are the TV camera and the microphone.
The video and audio signals are both fed into the transmitter for further processing so as to produce a new signal ( called the TV composite signal) containing the sound and picture information, but now having properties better suitable for transmission by radio wave propagation.
In this example, the channel is space with its attendant interference and noise.
After traveling a long distance, the transmitted signal in the channel is pick up by a home antenna.
The signal by now has suffered from attenuation among some other factors.
The antenna delivers the signal to the TV receiver in this example whose function is the reversal of the operations performed by the transmitter.
The video and audio signals are separated by the receiver and are subsequently applied to the output transducers, namely; the picture tube and the loudspeaker which recreate the image and sound information.
The destination is the person watching the TV set in the home.
3.2 Modulation/ Demodulation Process 3.2.1 Introduction Baric to the field of communication is the concept of modulation.
Modulation is the process of putting information onto a high frequency carrier for transmission.
In essence, then, the transmission takes place at the high frequency (the carrier) which has been modified to 56 “carry” the lower-frequency information.
The low - frequency information is often called the intelligent signal, modulating signal or base band signal it follow that once this information is received, the intelligence signal must be removed from the high - frequency carrier – a process known as demodulation or detection.
At this point, you may be thinking, why bother to go through this modulation/demodulation process?
Why not just transmit the information directly?
The problem is that frequency of the human voice ranges from about 20 to 3000 Hz.
If everyone transmits these frequencies directly as radio wave, interference would cause them to be ineffective.
Another limitation of equal importance is the virtual impossibility of transmitting such low – frequencies since the required antenna for efficient propagation would be kilometers in length.
The solution is modulation, which allows propagation of the low – frequency modulating signal with a high – frequency carrier.
The high-frequency carriers are chosen such that only one transmitter in an area operates at the same frequency to minimize interference, and that frequency is high enough so that efficient antenna size is manageable.
There are three basic methods of putting low-frequency information into a higher frequency.
Equation (1) is the mathematical representation of a sine wave, which we shall assume to be the high-frequency carrier.
V = Vp sin (wt + Ø) 1 Where V = instantaneous value Vp = peak value (Amplitude) W = angular velocity Ø = phase angle Any one of the three terms could be varied in accordance with the low- frequency information signal to produce a modulated carrier signal that contains the intelligence.
If the amplitude term, Vp is the parameter varied, I is called amplitude modulation (AM).
If the frequency is varied, it is frequency modulation (FM).
Varying the phase angle Ø, results in phase modulation.
57 Communication systems are often categorized by the frequency of the carrier.
Table (1) provides the names of various frequency ranges in the radio spectrum.
The extra higher frequency range begins at the starting point of the infrared frequency, but the infrared extend considerably beyond 300 HZ (300 x 109 HZ).After the infrared in the electromagnetic spectrum( of which the radio waves are a very small portion) come light waves, ultraviolet rays, X-rays, gamma rays and cosmic rays.
Table 1: Classification of Radio spectrum Frequency Destination Abbreviation 30-300Hz Extremely low frequency ELF 300–000Hz Voice frequency VF 3-30KHz Very low frequency VLF 30-300KHz Low frequency LF 300Khz-3MHz Medium frequency MF 3-30MHz High frequency HF 30-300MHz Very high frequency VHF 300MHz-3GHz Ultra high frequency UHF 3-30GHz Super high frequency SHF 30-300GHz Extremely high frequency EHF 3.21 Need for Modulation 58  The need for modulation can be summarized as follows: (i) The input signal from the transducer (which is the base band signal) is usually too low in terms of amplitude and frequency to be trimester directly over a communication channel.
Hence, a carrier of high frequency is needed and this carrier has got to be modulated by the base band signal nearing any modulation technique (AM, FM and PM).
(ii) Ease of Radiation: Using space as the channel, antennas are required to radiate and receive radio frequency signals.
For best results, antenna dimensions must be at least 1/10 of the wavelength of the frequency being used.
For example, to transmit an audio signal of frequency 3.5KHz over the channel requires an antenna of length: λ = C/F = (3 X 108)/3.5 KHz = 86Km long; Whereas, the use of carrier frequency of 100MHz requires an antenna size of 3meters only.
(iii) Frequency Assignment: It is true that all sounds are within 20Hz – 20 KHz (the audio frequency range), and they are available in space.
There is need for sounds top be separated.
For example, sounds from radio, tv, walkie-talkie, telephone, etc need to be separated from each other.
We need to separate AIT from NTA and Radio Kaduna from Radio Nigeria.
This is achieved by assigning different frequencies (carrier frequencies) to each of the operators.
MTN, glo, Visafone wireless operate at different assigned frequencies as given by the Nigerian Communication Commission (NCC).
(iv) Multiplexing: This is the transmission of signals of different characteristics together under a single channel.
The different signals are transmitted at the same time.
Multiplexing techniques include Time Division Multiplexing (TDM) and Frequency Division Multiplexing (FDM) 3.3 Limitation of communications systems There are two basic limitations on the performance of a communications system: Noise and the bandwidth of frequencies allocated for the transmitted signal 59 3.3.1 Noise Noise is a constant concern in communication system.
An understanding of the sources and types of noise that may be present will help in determining how the system should be designed,how well it will operate, and which system malfunctions are unavoidable.
In some cases the best thing to do is to try to identify the noise source and minimize or remove it, if possible.
Noise can be divided into two main sources: external and internal  External noise External noise can be man-made or can arise from natural origins.
Virtually all electrical and electronic devices emit electromagnetic waves as a by-product of their operation.
Motors, switches and powerlines are all sources of this noise, which is often called radio- frequency interference (RFI).
Every time an ordinary household light switch is turned on or off, the flow of electricity is suddenly started or stopped.
This sudden change in voltage and current contains a wide range of frequencies.
Some of these frequency components then radiate from the switch and house wiring, which act as a miniature transmitter and antenna.
This noise is often called impulse noise, because it comes primarily from sudden, on/off events.
Another major source of external noise is the ac power line.
Power lines inductively couple 50Hz or 60Hz signals (in some countries) onto adjacent wires.
This interfering 50Hz then enters the communication system and corrupts the desired signals.
In contrast to most types of man-made noise, this noise has a single frequency component at 50Hz, so it can usually be eliminated within the system by carefully placed filters.
The only problem with the filter approach is that the communication system may be carrying a signal that contains information at 50Hz and any filtering at this frequency will also eliminate some of the desired information.
This is the case in a system that is transmitting music with a bandwidth of 20 to 20000Hz, for these applications, the proper solutions is shielding and other techniques to prevent the 50Hz radiated signal from getting into the system in the first place.
60  Any transmitter or any signal source can be a source of noise to other systems.
A transmitter that is operating perfectly can affect a nearby system when some of the transmitter signal enters the circuitry of the nearby system.
The system circuitry then adds this circuitry of this nearby system.
The system circuitry then adds this “foreign” signal to its own , and the result is an undesired signal corrupting the desired ones.
This phenomenon often called RF saturation or front-end overhead,when a strong signal at another frequency overheads and saturates the front end of a receiver that is designed for very weak signals and providing a large amount of amplication of them.
Many sources of external noise are not man made.
They come from natural events in the atmosphere and outspace.
Lighting and static discharges in the air cause wideband noise, up to about 20 or 30MHz, similar to the on/off switch action, but more powerful.
Another source of noise that affects system performance originates in space.
The sun (and all stars) is sources of wideband, low-level noise that begins at approximately 8MHz and extends out to 1.439Hz and beyond.
The sun itself has a cycle of generating greater and smaller amounts of noise, related to the 11-year sun-spot cycle.
In 1957, for, the sunspot cycle was at its peak and long-distance radio communications were often unusable d to the large noise values Internal Noise Internal noise is also a significant factor in all communications system and circuits.
In an ideal (noise-free) system, an signal that arrived could be amplified or transformed as needed by the receivers, and there would be no loss of signal quality.
The movement of the electrons is the circuit generates noise.
Different noise mechanisms, related to various aspects of the electron motion, are observed in circuits.
The result is that electronic circuitry and component add some noise to any signal as it passes through circuit.
For larger amplitude signals, this additional noise may insignificant, but for low- level signals such as those from satellite or distant transmitter this may be a major factor 61 The internal noise is generated in any power-dissipating component in the circuit.
The simplest type of internal noise called thermal noise (or Johnson noise).
The amount of noise generated increase with the temperature, since the random motion of electrons is directly related to the absolute temperature.
The equation that shows the amount of noise is: P = KT∆F ( 2 ) Where p is measured in watts; K is the Boltzmann’s constant = 1.38x 10-23J/K; T is the absolute temperature in Kelvin (K= C+273); and ∆f is the bandwidth in hertz.
This equation shows that the noise power increases with both the temperature and the bandwidth.
The noise spectrum extends across all frequencies and has equal amount of power in each part of the frequency spectrum.
For this reason this is also referred to as white noise, just as white light contains all colours (light Frequencies).Not all sources of circuit and system noise come from the random motion of electrons.
Imperfections in the circuit design or in the practical limitations of components can cause effects that are similar to noise.
These imperfections cause effects that are similar to noise.
These imperfections cause unwanted and unknown variations in the desired signal.
3.3.2 Information and Bandwidth It may seem possible to squeeze as many users as desired into a band or part of the spectrum.
For example, why not assign FM broadcast stations with 2-KHz spacing, to 88, 100, 101, 88,102, and so on, MHz, instead of spacing the channel assignment 200 KHz apart?
The answer has to do with the need for bandwidth.
Bandwidth is the span of frequencies within the spectrum occupied by a signal and used by the signal for conveying information.
The function of any communication system is to carry information.
This information may be in the form of voice signals, video signals for TV, digital signals from computers or on/off signal as from Morse code.
Regardless of the type of information there is always one simple fact to remember: carrying information requires 62 bandwidth.
To pass information, the system must make use of a specific amount of the spectrum.
Music which is a form of information, use the range of frequencies from 0 to 20 KHz and so has a 20KHz bandwidth.
The next issue is how much bandwidth is required to convey the information.
The answer depends both on how much information must be transmitted and in much (or little) time.
To send more information in a short time requires more bandwidth.
The same quantity of information can be sent in a longer period using less bandwidth.
It might be asked: why is efficient channel utilization so important?
The band of frequencies is limited and we are living in a world increasingly dependent on electrical communication.
Regulating agencies (the NCC in Nigeria) allocates the channel that may be used for a given application in a given area.
This is done to minimize interference possibilities that will exist with two different signal working at the same frequency.
The information explosion of recent years has taxed the total available frequency spectrum to the point where getting the most information from yhe smallest range of frequencies is infact quite important.
A formal relationship between bandwidth and information was developed by R.Hartley of Bell laboratories in 1928 and is called Harleys law.
It states that the information that can be transmitted is proportional to the product of the bandwidth utilized times the time of transmission.
In simpler terms it means the greater the bandwidth, the more information that can be transmitted.
Expressed as an equation, Hartley’s law is: information ∞ bandwidth x time of transmission (3) 3.4 Components of communication system 3.4.1 Transmitter The function of the transmitter is to process electrical signals from different aspects.
For example, in a radio broadcast the electrical signal from sound signal 63 is process to restrict its range of audio frequencies, (up to 5Kz in amplitude modulation radio broadcast) and is often amplified.
In wired telephony, no real processing is needed.
However, in long-distance radio communication or broadcast, signal amplification is necessary before modulation.
Modulation is the main function of the transmitter.
In modulation, the message signal is superimpose upon the high frequency carrier frequency signal.
in short, we can say that inside the transmitter, signal processing such as restriction of range of audio frequencies, amplification and modulation are achieved which are done just to ease the transmission of the signal through the channel.
The following classifications are common for radio transmitters.
(1) Depending on the type of operation; transmitters can be classified as amplitude modulated (AM), Frequency modulated (FM), Phase Modulated (PM) transmitters.
(2) The use for which the transmitter is design; telegraphy(intelligence transmitted in form of code), telephony(intelligence in direct audio speech), broadcast (intelligence in speech and music), or facsimile- intelligence to produce a video picture still(newspaper) or moving(TV) (3) Transmitters are also classified according to the range it is meant to serve.
Essentially it is impracticable or impossible to transmit a low frequency (size of equipment and power requirement being prohibitive thus, a much higher frequency must be used.
Intelligence whether in the coded form or direct audio is of low frequency and in other to be transmitted must be modulated by a carrier signal of higher frequency.
In other to to cover a better range more power is required.
Now to increase the power , the oscillator section will tend to make it more difficult to produce stabilize, therefore generally a low power oscillator is used and the necessary power is produced by separate amplifier stages(more stages- more power –longer ranges) known as radio frequency(RF) amplifiers.
An increase in operating frequency of the carrier oscillator is desirable because of the congestion of the radio band at lower frequencies and also because the higher 64 the operating frequency, the less power is required (a VHF transmitter is considerably smaller than a LF transmitter).
Generally speaking a carrier oscillator needs to be very stable and therefore crystal controlled.
The dimension of the crystal controlled oscillator at above about 10MHz is impracticable to overcome this difficulty and still maintain a stable output low (relatively) frequency crystal controlled oscillator is used and followed by one or more stages of either frequency doublers or harmonic generators to obtain the necessary high operating frequency.
Further improvement in stability a buffer amplifier can be introduced to isolate the oscillator from the load.
The basic design of a transmitter would be of the form shown in the figure (1) Modulator RF Amplifier Audio Master Harmonic Buffer RF Amplifier mic Amplifier oscillator Generator Amplifier Fig (1) 3.4.2 Communication Channel The channel means the medium through which the message travels from the transmitter to the receiver.
In another word, we can say that the function of channel is to provide a physical connection between the transmitter and the receiver.
There are two types of channels, mainly point to point channels and broadcast channels.
Example of point to point channels are wire lines, microwave link and optical fibers.
Wire lines operate by guided electromagnetic waves and they are used for local telephone transmission.
In case of microwave link, the transmitted signal is 65 radiated as magnetic waves in free space.
Microwave links are used in long distance telephone transmission.
An optical fiber is a low loss well controlled, guided optical medium.
Optical fibers are used in optical communications.
Broadcast channel produce a capability where several recent stations can be reached simultaneously from a single transmit.
Depending on the mode of transmission, we may classify the communication channels into the following 2 categories;  Channels based on guided propagator  Channels based on free propagation.
The classification of channels has been shown in fig (2) Communication channel Channels based on guided propagator Channels based on free propagation 66  Telephone channel Covered Case Optical Fiber Wireless channel Satellite channel Noise channel Fig (2) classification of communication channels Some of the important characteristics of a channel are as under: Power required achieving the desired S/N ratio Bandwidth data of the channel Types of channel (linear or non linear) Effects of External interface on the channel 3.4.3 Radio Receivers A receiver is device which connected to the receiving antenna, amplified the signal received from the transmitter and converts it into the correct form for the interpretation of the information.
At the radio transmitter the carrier frequency is modulated by the desired signal, which may consist of coded characters, voice, music or other type of signal.
Amplitude modulation occurs if the signal causes the amplitude of the carrier to vary.
Frequency modulation occurs if the signal causes the frequency of of the carrier, or center frequency to vary.
The function of the receiver is to select the desired carrier frequency from those present in the antenna circuit and to amplified the small received ac-signal voltage (small ac- voltage induced in the antenna by the received signal).
The Receiver then removes the carrier by the process of demodulation or detection; amplify the 67 resultant audio signal to the proper magnitude to operate the loudspeaker or earphones.
A typical radio receiver is of the super heterodyne type which provides tuning to any desired frequency (stations) using a single tuned or resonant circuit.
Signal from the antenna passes first through the RF amplifier (pre-selector) where the amplitude of the signal is increased.
A locally generated unmodulated RF signal of constant amplitude is then mixed with the received frequency signal in the mixer stage.
The mixing or heterodyning of these two frequencies produces an intermediate frequency (IF) signal which contains all of the modulation characteristics of the original signal.
The IF is equal to the differences between the station frequency and the oscillator frequency is then amplified in one or more stages called IF amplifiers 3.4.4 Signal A signal may be defined as the single valued function of time.
Time play role of an independent variable.
This means that at every instant of time, the signal has a unique value.
Classification of signal The signal may be classified as under Signal 68 One dimensional signal Two dimensional signal Three dimensional signal Four dimensional signal Speech pictures video data volume data over time 1.
Speech Speech involves transfer of information from the speaker to the listener.
Such a transfer of information takes place in following & stages  Protection  Propagation and  Perception.
The band of frequencies considered to be essential for the speech comments is 300Hz to 3100Hz.
This band is actualized for the commercial telephone comment.
2.
Music Music signal is originated from the instrument such as the Piano, volume, flute etc.
The musical note may list for a long time depending upon the instrument being used for its origination.
Music signal has following two possible structures.
 Melodic Structure  Harmonic structure The melodic structure consists of a time sequence of a time sequence of sound; where as the harmonic structure consists of a set of simultaneous sounds.
Music signal is a bipolar signal and 69 requires a bandwidth of about 15 kHz.
Hence, the channel bandwidth required for the transmission of music larger than the required for the transmission of speech signal.
3.
Picture The picture can be either static or dynamic example of static picture is the picture sent by fax machine or that of a dynamic picture is the one produced on T.V.
The T.V camera produces a dynamic picture and picture tube produces it at the recovery end.
The electrical equivalent of dynamic picture is produce at the output of the camera and it is called as typical video signal.
4.
Computer Data Personal computers are used for used for electronic mail, exchange of software, and sherry of resources.
The text matter transmitted by a computer is encoded using American standard code for information interchange (ASCII) Each character in ASCII is represented by several data bits.
Hence total 27=128 characters can be represented using ASCII 3.5 Digital Modulation Techniques 3.5.1 Introduction Modulation is defined as the process by which some characteristics of a carrier is varied in accordance with a modulating signal [1].
In digital communications, the modulating signal consists of binary data or m-ary enclosed version of it.
This data is used to modulate a carrier wave with fixed frequency.
In fact, the input data may represent the digital computer outputs or PCM waves generated by digitizing voice or video signals.
The channel may be a telephone channel, microwave radio link, satellite channel or optical fiber.
In digital communication, the modulator process involves switching or keying the amplitude, frequency or phase of the carrier in accordance with the input data.
70 3.5.2 Digital Modulation Formats There are three basic modulation techniques for the transmission of digital data.
They are known as amplitude- shift keying (ASK), frequency shift keying (FSK) and phase- shift keying (PSK) which can be views as special cases of amplitude modulation frequency, modulation and phase modulation respectively.
When we have to transmit a digital signal over a long distance, we need continuous- wave (CW) modulation.
For this purpose, the transmission medium can be in form of radio, cable or other tying of channel.
Also a carrier signal having some frequency Fc is used for modulation.
Then the modulating digital signal modulates some parameters like frequency, phase or amplitude of the carried.
There is some deviation in carried frequency Fc.
This deviation is known as the bandwidth of the channel.
This means that the channel has to transit sane range or band of frequencies, such type of transmission is known as band pass channel.
When it is required to transmit digital signals on a band pass channel, the amplitude, frequency or phase of the sinusoidal carried is varied in accordance with the incoming digital data.
Since the digital data is in discrete steps.
The modulation of the band pass sinusoidal carried is also done in discrete steps.
Due to this reason, this type of modulation (Digital Modulation) is also known as switching or signaling Became of constant amplitude of FSK and PSK, the effect of non-linearity, noise interference is minimum on signal detection.
However, these effect are more pronounced on ASK.
Therefore, FSK & PSK are preferred over ASK.
71 In digital modulations, instead of transmitting one bit at a lime, we transmit two or more bits simultaneously.
This is known as M-ary transmission.
This type of transmission results in reduced channel bandwidth.
However, sometimes, we use two quadrative carriers for modulation.
This process is known as quadrature modulation.
Coherent digital modulation techniques are those techniques which employ coherent detection, in coherent detection, the local carrier gene4ayed at the receiver is phase locked with the carrier at the transmitter.
Thus, the detection is done by correlating received noisy signal & local generated carrier.
The coherent detection is a synchronous detected.
Non-coherent digital modulation techniques are those techniques in which the detection process data met need receiver carrier to be phase locked with transmitter carrier.
3.5.3 Coherent Binary Modulation Techniques The binary (i.e., digital) modulation has three basic forms amplitude- shift keying (ASK),phase- shift keying (PSK) and frequency-shift keying (FSK).
In this section let us discuss different coherent binary modulation techniques.
ASK signal may be generated by simply applying the incoming binary data represented in unipolar form) and the sinusoidal carrier to the two input of a product modulation.
The demodulation of binary, ASK waveform can be achieved with the help of coherent detector.
It consists of a product modulator which is followed by an integrator and a decision making device.
72   Binary Phase Shift Keying (BPSK) In binary phase shift keying (BPSK), binary symbolic 1 and ‘o’ modulation the phase of the carrier.
The BPSK signal may b e generated by applying carrier signal to a balanced modulation.
Here, the base band signal- b(t).
Infact.
This result in ambiguity in the output signal  Coherent Binary Frequency Shift Keying (BFSK) In binary frequency shift keying (BFSK), The frequency of the carrier is shifted according to the binary symbol.
However, the phase of the carrier is unaffected.
That is have two different frequency signals according to binary symbols.
Orthogonal carrier are used for M-ary PSK and Quadrature Amplitude Shift Keying (QASK).ease, even that it has many disadvantage compared to BPSK signal.
Firstly, its bandwidth is greater than 4fb, which is almost double the bandwidth of BPSK, also the distance between the signal points is less case of BFSK.
Therefore, the error rate of BFSK is more compared to BPSK.
 Differential Phase Shift Keying (DPSK) DPSK dose not need a synchronous (coherent) carrier at the demodulator.
DPSK dose not need carrier at the receiver end.
This means that the complicated circuitry for generation of local carrier is not required the Bandwidth requirement of DPSK is reduced as compared to that of BPSK.
Because DPSK uses two successive bit for its reception, error in the first bit creates errors 73 in the second DPSK is more.
On the other hand, in BPSK single bit can go in error since detection of each bit is independent.
 Quadrature Phase Shift Keying (QPSK) In communication systems, we have two main resources.
These are the transmission power and the channel bandwidth.
The channel bandwidth depends upon the bit rate or signaling rate Fb.
In digital band pass transmission, we use a combined in some symbols, than the signaling rate will be reduced.
Thus, the frequency of the carrier needed is also reduced.
This reduced the transmission channel bandwidth.
Hence, because of grouping of bits in symbols, the transmission channel bandwidth can be reduced.
In quadrature phase shift keying (QPSK), two successive bits in the data sequence are grouped together.
This reduces the bit rate or signaling rate (i.e., fb) and this reduces the bandwidth of the channel.
4.0 Conclusion Communication is the process of establishing connection or link between two points for information exchange.
The Electronic Equipments which are used for communication purpose are called communication equipments.
Different communication equipments were assembled together to form a communication system Typical examples of communication system are line telephony and line telegraphy, radio telephony and line telegraphy, radio broadcasting, point-to-point communication, and mobile 74 communication, computer communication, radio communication, television broadcasting, radio telephony, radio aids to navigation radio aids to aircraft landing etc.
5.0 Reference [1] Sanjay Sharma, (2010) “Communication Systems (Analog and Digital)” 5th completely Revised and Enlarged Edition [2] Idigo V.E., Nsionu II., Ohaneme C.O., (2004), “Telecommunication Engineering”, Mekmand Publication, Enugu.
UNIT 6 COMMUNICTION MODELS TABLE OF CONTENT 1.0 Introduction 2.0 Objectives 3.0 Main content 3.1 What is a Model?
3.2 Advantages of Models 3.3 Limitations of Models 3.4 Classical Communication Models 3.5 Early Linear Models 3.6 Non-linear Models 3.7 Multidimensional Models 75 3.8 Suggestions for Communication Models 3.9 Summary 3.10 Conclusion 3.11 References INTRODUCTION The communication model consists of a source that generates digital information.
This information is sent to a destination through a channel.
The communication can happen in the spatial domain (i.e., we need to send information over a physical distance on a channel) or in the time domain (i.e., we want to retrieve data that we stored at an earlier point of time).
The channel can be associated with noise.
So we have two cases : • Noiseless case: The channel in this case transmits symbols without causing any errors.
One would need to exploit the redundancy in the source to economize the length of the transmission.
This is done through data compression, also called source coding.
The information is decompressed at destination.
• Noisy case: The channel in this case introduces noise that causes errors in the received symbols at the destination.
To reduce the errors incurred due to noise, one should add systematic redundancy to the information to be sent.
This is done through channel coding.
It is known that reliable communication is possible in the above model if the entropy of the source, i.e.
the amount of non-redundant information it generates per unit of time, is less than the “capacity” of the channel, i.e., the maximum number of information bits that can be communicated reliable per channel use.
Fig 1 The modules of the communication model: 2.0 OBJECTIVES After studying this unit, the student should be able to:  Understand the general meaning of a model  Understand the meaning of a communication model.
 Understand and explain the Advantages of Models.
 Understand and explain the Limitations of Models.
 Understand and explain some Classical Communication Models.
76  Understand and explain the Early Linear Models.
 Understand and explain the Non-linear Models.
 Understand and explain Multidimensional Models.
 Understand and explain some Suggestions for Communication Models 3.1 WHAT IS A MODEL In the broadest sense, a model is a systematic representation of an object or event in idealized and abstract form.
Models are somewhat arbitrary by their nature.
The act of abstracting eliminates certain details to focus on essential factors.
The key to the usefulness of a model is the degree to which it conforms in point-by-point correspondence to the underlying determinants of communicative behavior.
Communication models are merely pictures; they’re even distorting pictures, because they stop or freeze an essentially dynamic interactive or transactive process into a static picture.
Models are metaphors.
They allow us to see one thing in terms of another.
3.2 THE ADVANTAGES OF MODELS  They should allow us to ask questions.
A good model is useful, then, in providing both general perspective and particular vantage points from which to ask questions and to interpret the raw stuff of observation.
The more complex the subject matter—the more amorphous and elusive the natural boundaries the greater are the potential rewards of model building.
 They should clarify complexity.
Models also clarify the structure of complex events.
They do this by reducing complexity to simpler, more familiar terms.
Thus, the aim of a model is not to ignore complexity or to explain it away, but rather to give it order and coherence.
 They should lead us to new discoveries-most important.
At another level models have heuristic value; that is, they provide new ways to conceive of hypothetical ideas and relationships.
This may well be their most important function.
With the aid of a good model, suddenly we are jarred from conventional modes of thought.
Ideally, any model, even when studied casually, should offer new insights and culminate in what can only be described as an “Aha!” experience.
3.3 LIMITATIONS OF MODELS  Can lead to oversimplifications.
There is no denying that much of the work in designing communication models illustrates the oft- repeated charge that anything in human affairs which can be modeled is by definition too superficial to be given serious consideration.
Some believe there is no value in models at all; We can guard against the risks of oversimplification by recognizing the fundamental distinction between simplification and over simplification.By definition, and of necessity, models simplify.
So do all comparisons.
As Kaplan (1964) noted, “Science always simplifies; its aim is not to reproduce the reality in all its complexity, but only to formulate what is essential for understanding, prediction, or control.
That a model is simpler than the subject-matter being inquired into is as much a virtue as a fault, and is, in any case, inevitable.” So 77 the real question is what gets simplified.
Insofar as a model ignores crucial variables and recurrent relationships, it is open to the charge of oversimplification.
If the essential attributes or particulars of the event are included, the model is to be credited with the virtue of parsimony, which insists-where everything is equal-that the simplest of two interpretations is superior.
Simplification, after all, is inherent in the act of abstracting.
For example, an ordinary orange has a vast number of potential attributes; it is necessary to consider only a few when one decides to eat an orange, but many more must be taken into account when one wants to capture the essence of an orange in a prize-winning photograph.
abstracting.
For example, an ordinary orange has a vast number of potential attributes; it is necessary to consider only a few when one decides to eat an orange, but many more must be taken into account when one wants to capture the essence of an orange in a prize-winning photograph.
Models can miss important points of comparison.
“A model can tolerate a considerable amount of slop [p. 118].”  Can lead of a confusion of the model between the behavior it portrays Critics also charge that models are readily confused with reality.
The problem typically begins with an initial exploration of some unknown territory.
Then the model begins to function as a substitute for the event: in short, the map is taken literally.
And what is worse, another form of ambiguity is substituted for the uncertainty the map was designed to minimize.
What has happened is a sophisticated version of the general semanticist’s admonition that “the map is not the territory.” Spain is not pink because it appears that way on the map, and Minnesota is not up because it is located near the top of a United States map.
“The proper antidote lies in acquiring skill in the art of map reading.”  Premature Closure The model designer may escape the risks of oversimplification and map reading and still fall prey to dangers inherent in abstraction.
To press for closure is to strive for a sense of completion in a system.
The danger is that the model limits our awareness of unexplored possibilities of conceptualization.
We tinker with the model when we might be better occupied with the subject-matter itself.
In many areas of human behavior, our knowledge is on the level of folk wisdom incorporating it in a model does not automatically give such knowledge scientific status.
The majority of our ideas is usually a matter of slow growth, which cannot be forced.
Closure is premature if it lays down the lines for our thinking to follow when we do not know enough to say even whether one direction or another is the more promising.
Building a model, in short, may crystallize our thoughts at a stage when they are better left in solution, to allow new compounds to precipitate.
One can reduce the hazards only by recognizing that physical reality can be represented in any number of ways.
3.4 CLASSICAL COMMUNICATION MODELS  Aristotle’s definition of rhetoric: One of the earliest definitions of communication came from the Greek philosopher-teacher Aristotle (384-322 B.C.).
“Rhetoric” is “the faculty of observing in any given case the available means of persuasion” (Rhetoric 1335b).
78 Aristotle’s speaker-centered model received perhaps its fullest development in the hands of Roman educator Quintilian (ca.
35-95 A.D.), whose Institutio Oratoria was filled with advice on the full training of a “good” speaker-statesman.
FIG 2 Aristotle Model of communication 79  FIG 3 Aristotle Model of proof 80  Bitzer’s Rhetorical Situation.
Lloyd Bitzer developed described the “Rhetorical Situation,” which, while not a model, identifies some of the classical components of a communication situation (“The Rhetorical Situation,” Philosophy and Rhetoric, 1 (Winter, 1968):1-15.).
Bitzer defines the “rhetorical situation” as “a complex of persons, events, objects, and relations presenting an actual or potential exigence which can be completely or partially removed if discourse, introduced into the situation, can so constrain human decision or action so as to bring about significant modification of the exigence.” 3.5 Early Linear Models  The Shannon-Weaver Mathematical Model, 1949 Claude Shannon, an engineer for the Bell Telephone Company, designed the most influential of all early communication models.
His goal was to formulate a theory to guide the efforts of engineers in finding the most efficient way of transmitting electrical signals from one location to another .
Later Shannon introduced a mechanism in the receiver which corrected for differences between the transmitted and received signal; this monitoring or correcting mechanism was the forerunner of the now widely used concept of feedback (information which a communicator gains from others in response to his own verbal behavior).
FIG 4 Shannon- weaver Mathematical Model Strengths This model, or a variation on it, is the most common communication model used in low-level communication texts.
81 Significant development.
“Within a decade a host of other disciplines—many in the behavioral sciences—adapted it to countless interpersonal situations, often distorting it or making exaggerated claims for its use.” “Taken as an approximation of the process of human communication.” Significant heuristic value.
With only slight changes in terminology, a number of nonmathematical schemas have elaborated on the major theme.
For example, Harold Lasswell (1948) conceived of analyzing the mass media in five stages: “Who?” “Says what?” “In which channel?” “To whom?” “With what effect?” In apparent elaboration on Lasswell and/or Shannon and Weaver, George Gerbner (1956) extended the components to include the notions of perception, reactions to a situation, and message context.
The concepts of this model became staples in communication research Entropy-the measure of uncertainty in a system.
“Uncertainty or entropy increases in exact proportion to the number of messages from which the source has to choose.
In the simple matter of flipping a coin, entropy is low because the destination knows the probability of a coin’s turning up either heads or tails.
In the case of a two-headed coin, there can be neither any freedom of choice nor any reduction in uncertainty so long as the destination knows exactly what the outcome must be.
In other words, the value of a specific bit of information depends on the probability that it will occur.
In general, the informative value of an item in a message decreases in exact proportion to the likelihood of its occurrence.” Redundancy-the degree to which information is not unique in the system.
“Those items in a message that add no new information are redundant.
Perfect redundancy is equal to total repetition and is found in pure form only in machines.
In human beings, the very act of repetition changes, in some minute way, the meaning or the message and the larger social significance of the event.
Zero redundancy creates sheer unpredictability, for there is no way of knowing what items in a sequence will come next.
As a rule, no message can reach maximum efficiency unless it contains a balance between the unexpected and the predictable, between what the receiver must have underscored to acquire understanding and what can be deleted as extraneous.” Noise-the measure of information not related to the message.
“Any additional signal that interferes with the reception of information is noise.
In electrical apparatus noise comes only from within the system, whereas in human activity it may occur quite apart from the act of transmission and reception.
Interference may result, for example, from background noise in the immediate surroundings, from noisy channels (a crackling microphone), from the organization and semantic aspects of the message (syntactical and semantical noise), or from psychological interference with encoding and decoding.
Noise need not be considered a detriment unless it produces a significant interference with the reception of the message.
Even when the disturbance is substantial, the strength of the signal or the rate of redundancy may be increased to restore efficiency.” Channel Capacity-the measure of the maximum amount of information a channel can carry.
“The battle against uncertainty depends upon the number of alternative possibilities the message eliminates.
Suppose you wanted to know where a given checker was located on a checkerboard.
If you start by asking if it is located in the first black square at the extreme left of the second row from the top and find the answer to be no, sixty-three possibilities remain-a high level of uncertainty.
On the other hand, if you first ask whether it falls on any square at the top half of the board, the alternative will be reduced by half regardless of the answer.
By following the first strategy it could be 82 necessary to ask up to sixty-three questions (inefficient indeed!
); but by consistently halving the remaining possibilities, you will obtain the right answer in no more than six tries.” Information is a measure of uncertainty, or entropy, in a situation.
The greater the uncertainty, the more the information.
When a situation is completely predictable, no information is present.
Most people associate information with certainty or knowledge; consequently, this definition from information theory can be confusing.
As used by the information theorist, the concept does not refer to a message, facts, or meaning.
It is a concept bound only to the quantification of stimuli or signals in a situation.
On closer examination, this idea of information is not as distant from common sense as it first appears.
We have said that information is the amount of uncertainty in the situation.
Another way of thinking of it is to consider information as the number of messages required to completely reduce the uncertainty in the situation.
For example, your friend is about to flip a coin.
Will it land heads up or tails up?
You are uncertain, you cannot predict.
This uncertainty, which results from the entropy in the situation, will be eliminated by seeing the result of the flip.
Now let’s suppose that you have received a tip that your friend’s coin is two headed.
The flip is “fixed.” There is no uncertainty and therefore no information.
In other words, you could not receive any message that would make you predict any better than you already have.
In short, a situation with which you are completely familiar has no information for you [emphasis added].
Weaknesses :Not analogous to much of human communication.“Only a fraction of the information conveyed in interpersonal encounters can be taken as remotely corresponding to the teletype action of statistically rare or redundant signals.” 83 “Though Shannon’s technical concept of information is fascinating in many respects, it ranks among the least important ways of conceiving of what we recognize as “information.” “ Only formal—does not account for content “Shannon and Weaver were concerned only with technical problems associated with the selection and arrangement of discrete units of information—in short, with purely formal matters, not content.
Hence, their model does not apply to semantic or pragmatic dimensions of language.
“ Theodore Roszak provides a thoughtful critique of Shannon’s model in The Cult of Information.
Roszak notes the unique way in which Shannon defined information: Once, when he was explaining his work to a group of prominent scientists who challenged his eccentric definition, he replied, “I think perhaps the word ‘information’ is causing more trouble .
.
.
than it is worth, except that it is difficult to find another word that is anywhere near right.
It should be kept solidly in mind that [information] is only a measure of the difficulty in transmitting the sequences produced by some information source” [emphasis added] As Roszak points out, Shannon’s model has no mechanism for distinguishing important ideas from pure non-sense: In much the same way, in its new technical sense, information has come to denote whatever can be coded for transmission through a channel that connects a source with a receiver, regardless of semantic content.
For Shannon’s purposes, all the following are “information”: E = mc2 Jesus saves.
Thou shalt not kill.
I think, therefore I am.
Phillies 8, Dodgers 5 ‘Twas brillig and the slithy roves did gyre and gimble in the wabe.
And indeed, these are no more or less meaningful than any string of haphazard bits (x!9#44jGH?566MRK) I might be willing to pay to have telexed across the continent.
As the mathematician Warren Weaver once put it, explaining “the strange way in which, in this theory, the word ‘information’ is used ....
It is surprising but true that, from the present viewpoint, two messages, one heavily loaded with meaning and the other pure nonsense, can be equivalent as regards information” Static and Linear Mortensen: “Finally, the most serious shortcoming of the Shannon-Weaver communication system is that it is relatively static and linear.
It conceives of a linear and literal transmission of information from one location to another.
The notion of linearity leads to misleading ideas when transferred to human conduct; some of the problems can best be underscored by studying several alternative models of communication.”  Berlo’s S-M-C-R, 1960 The simplest and most influential message-centered model of our time came from David Berlo (Simplified from David K. Berlo, The Process of Communication (New York: Holt, Rinehart, and Winston, 1960)):” Essentially an adaptation of the Shannon-Weaver model.
84  FIG 5 Berlo’s Model of Communication Significant after World War II because: The idea of “source” was flexible enough to include oral, written, electronic, or any other kind of “symbolic” generator-of-messages.
“Message” was made the central element, stressing the transmission of ideas.
The model recognized that receivers were important to communication, for they were the targets.
The notions of “encoding” and “decoding” emphasized the problems we all have (psycho- linguistically) in translating our own thoughts into words or other symbols and in deciphering the words or symbols of others into terms we ourselves can understand.
Weaknesses: Tends to stress the manipulation of the message—the encoding and decoding processes it implies that human communication is like machine communication, like signal-sending in telephone, television, computer, and radar systems.
It even seems to stress that most problems in human communication can be solved by technical accuracy-by choosing the “right” symbols, preventing interference, and sending efficient messages.
85 But even with the “right” symbols, people misunderstand each other.
“Problems in “meaning” or “meaningfulness” often aren’t a matter of comprehension, but of reaction, of agreement, of shared concepts, beliefs, attitudes, values.
To put the com- back into communication, we need a meaning- centered theory of communication.”  Schramm’s Interactive Model, 1954 Wilbur Schramm (1954) was one of the first to alter the mathematical model of Shannon and Weaver.
He conceived of decoding and encoding as activities maintained simultaneously by sender and receiver; he also made provisions for a two-way interchange of messages.
Notice also the inclusion of an “interpreter” as an abstract representation of the problem of meaning.
(From Wilbur Schramm, “How Communication Works,” in The Process and Effects of Communication, ed.
Wilbur Schramm (Urbana: University of Illinois Press, 1954) FIG 6 Schramm’s model of communication Strengths Schramm provided the additional notion of a “field of experience,” or the psychological frame of reference; this refers to the type of orientation or attitudes which interactants maintain toward each other.
Included Feedback Communication is reciprocal, two-way, even though the feedback may be delayed.
Some of these methods of communication are very direct, as when you talk in direct response to someone.
86 Others are only moderately direct; you might squirm when a speaker drones on and on, wrinkle your nose and scratch your head when a message is too abstract, or shift your body position when you think it’s your turn to talk.
Still other kinds of feedback are completely indirect.
For example, politicians discover if they’re getting their message across by the number of votes cast on the first Tuesday in November; teachers measure their abilities to get the material across in a particular course by seeing how many students sign up for it the next term.
Included Context A message may have different meanings, depending upon the specific context or setting.
Shouting “Fire!” on a rifle range produces one set of reactions-reactions quite different from those produced in a crowded theater.
Included Culture A message may have different meanings associated with it depending upon the culture or society.
Communication systems, thus, operate within the confines of cultural rules and expectations to which we all have been educated.
Weaknesses Schramm’s model, while less linear, still accounts for only bilateral communication between two parties.
The complex, multiple levels of communication between several sources is beyond this model.
3.6 Non-linear Models  Dance’s Helical Spiral, 1967 Depicts communication as a dynamic process.
The helix represents the way communication evolves in an individual from his birth to the existing moment.
Dance: “At any and all times, the helix gives geometrical testimony to the concept that communication while moving forward is at the same moment coming back upon itself and being affected by its past behavior, for the coming curve of the helix is fundamentally affected by the curve from which it emerges.
Yet, even though slowly, the helix can gradually free itself from its lower-level distortions.
The communication process, like the helix, is constantly moving forward and yet is always to some degree dependent upon the past, which informs the present and the future.
The helical communication model offers a flexible communication process”.
87  88 FIG 7 helical model of communication Strengths As a heuristic device, the helix is interesting not so much for what it says as for what it permits to be said.
Hence, it exemplifies a point made earlier: It is important to approach models in a spirit of speculation and intellectual play.
The helix implies that communication is continuous, unrepeatable, additive, and accumulative; that is, each phase of activity depends upon present forces at work as they are defined by all that has occurred before.
All experience contributes to the shape of the unfolding moment; there is no break in the action, no fixed beginning, no pure redundancy, no closure.
All communicative experience is the product of learned, nonrepeatable events which are defined in ways the organism develops to be self- consistent and socially meaningful.
In short, the helix underscores the integrated aspects of all human communication as an evolving process that is always turned inward in ways that permit learning, growth, and discovery.
Weaknesses May not be a model at all: too few variables.
“If judged against conventional scientific standards, the helix does not fare well as a model.
Indeed, some would claim that it does not meet the requirements of a model at all.
More specifically, it is not a systematic or formalized mode of representation.
Neither does it formalize relationships or isolate key variables.
It describes in the abstract but does not explicitly explain or make particular hypotheses testable.” Generates Questions, but leaves much unanswered.
“For example, does not the helix imply a false degree of continuity from one communicative situation to another?
Do we necessarily perceive all encounters as actually occurring in an undifferentiated, unbroken sequence of events?
Does an unbroken line not conflict with the human experience of discontinuity, intermittent periods, false starts, and so forth?
Is all communication a matter of growth, upward and onward, in an ever-broadening range of encounters?
If the helix represents continuous learning and growth, how can the same form also account for deterioration and decay?
What about the forces of entropy, inertia, decay, and pathology?
And does not the unbroken line of a helix tacitly ignore the qualitative distinctions that inevitably characterize different communicative events?
Also, what about movements which we define as utterly wasted, forced, or contrived?
Along similar lines, how can the idea of continuous, unbroken growth include events we consider meaningless, artificial, or unproductive?
Countless other questions could be raised.
And that is the point.
The model brings problems of abstraction into the open.
“rtificial, or unproductive?
Countless other questions could be raised.
And that is the point.
The model brings problems of abstraction into the open.
“  Becker’s Mosaic Model, 1968 Becker assumes that most communicative acts link message elements from more than one social situation.
In the tracing of various elements of a message, it is clear that the items may result in part from a talk with an associate, from an obscure quotation read years before, from a recent TV commercial, and from numerous other dissimilar situations—moments of introspection, public debate, coffee-shop banter, daydreaming, and so on.
In short, the elements that make up a message ordinarily occur in bits and pieces.
Some items are separated by gaps in time, others by gaps in modes of presentation, in social situations, or in the number of persons present.
89 Becker likens complex communicative events to the activity of a receiver who moves through a constantly changing cube or mosaic of information .
The layers of the cube correspond to layers of information.
Each section of the cube represents a potential source of information; note that some are blocked out in recognition that at any given point some bits of information are not available for use.
Other layers correspond to potentially relevant sets of information.” FIG 8Becker’s Model of communication Strengths It depicts the incredible complexity of communication as influenced by a constantly changing milieu.
It also accounts for variations in exposure to messages.
In some circumstances receivers may be flooded by relevant information; in others they may encounter only a few isolated items.
Individual differences also influence level of exposure; some people seem to be attuned to a large range of information, while others miss or dismiss much as extraneous.
Different kinds of relationships between people and messages cut through the many levels of exposure.
Some relationships are confined to isolated situations, others to recurrent events.
Moreover, some relationships center on a particular message, while others focus on more diffuse units; that is, they entail a complex set of relationships between a given message and the larger backdrop of information against which it is interpreted.
It may be useful to conceive of an interaction between two mosaics.
One comprises the information in a given social milieu, as depicted in the model; the other includes the private mosaic of information that is internal to the receiver.
The internal mosaic is every bit as complex as the one shown in the model, but a person constructs it for himself.
90 Weaknesses Even though this model adds a third dimension, it does not easily account for all the possible dimensions involved in a communication event.
3.7 Multidimensional Models  Ruesch and Bateson, Functional Model, 1951 Ruesch and Bateson conceived of communication as functioning simultaneously at four levels of analysis.
One is the basic intrapersonal process (level 1).
The next (level 2) is interpersonal and focuses on the overlapping fields of experience of two interactants.
Group interaction (level 3) comprises many people.
And finally a cultural level (level 4) links large groups of people.
Moreover, each level of activity consists of four communicative functions: evaluating, sending, receiving, and channeling.
Notice how the model focuses less on the structural attributes of communication-source, message, receiver, etc.—and more upon the actual determinants of the process.
A similar concern with communicative functions can be traced through the models of Carroll (1955), Fearing (1953), Mysak (1970), Osgood (1954), and Peterson (1958).
Peterson’s model is one of the few to integrate the physiological and psychological functions at work in all interpersonal events.
FIG 9 Ruesch and bateson, Functional model  Barnlund’s Transactional Model, 1970 Mortensen: “By far the most systematic of the functional models is the transactional approach taken by Barnlund, one of the few investigators who made explicit the key assumptions on which his model was based.
Its most striking feature is the absence of any simple or linear directionality in the interplay between self and the physical world.
The spiral lines connect the functions of encoding and decoding and give 91 graphic representation to the continuous, unrepeatable, and irreversible assumptions mentioned earlier.
Moreover, the directionality of the arrows seems deliberately to suggest that meaning is actively assigned or attributed rather than simply passively received.
Any one of three signs or cues may elicit a sense of meaning.
Public cues (Cpu) derive from the environment.
They are either natural, that is, part of the physical world, or artificial and man-made.
Private objects of orientation (Cpr) are a second set of cues.
They go beyond public inspection or awareness.
Examples include the cues gained from sunglasses, earphones, or the sensory cues of taste and touch.
Both public and private cues may be verbal or nonverbal in nature.
What is critical is that they are outside the direct and deliberate control of the interactants.
The third set of cues are deliberate; they are the behavioral and nonverbal (Cbehj cues that a person initiates and controls himself.
Again, the process involving deliberate message cues is reciprocal.
Thus, the arrows connecting behavioral cues stand both for the act of producing them-technically a form of encoding- and for the interpretation that is given to an act of others (decoding).
The jagged lines (VVVV ) at each end of these sets of cues illustrate the fact that the number of available cues is probably without limit.
Note also the valence signs (+, 0, or -) that have been attached to public, private, and behavioral cues.
They indicate the potency or degree of attractiveness associated with the cues.
Presumably, each cue can differ in degree of strength as well as in kind.
“t each end of these sets of cues illustrate the fact that the number of available cues is probably without limit.
Note also the valence signs (+, 0, or - ) that have been attached to public, private, and behavioral cues.
They indicate the potency or degree of attractiveness associated with the cues.
Presumably, each cue can differ in degree of strength as well as in kind."
FIG 10 Barnlund’s transactional Model 92  FIG 10 Barnlund’s transactional Model Strengths The assumptions posit a view of communication as transactions in which communicators attribute meaning to events in ways that are dynamic, continuous, circular, unrepeatable, irreversible, and complex.
Weaknesses The exception is the assumption that communication describes the evolution of meaning.
In effect, the model presupposes that the terms communication and meaning are synonymous and interchangeable.
Yet nowhere does the model deal in even a rudimentary way with the difficult problem of meaning.
The inclusion of decoding and encoding may be taken as only a rough approximation of the “evolution of meaning,” but such dualistic categories are not particularly useful in explaining the contingencies of meaning.
3.8 Suggestions for Communication Models  A Systemic Model of Communication, 1972 93 Some communication theorists have attempted to construct models in light of General Systems Theory.
The “key assumption” of GST “is that every part of the system is so related to every other part that any change in one aspect results in dynamic changes in all other parts of the total system (Hall and Fagen, 1956).
It is necessary, then, to think of communication not so much as individuals functioning under their own autonomous power but rather as persons interacting through messages.
Hence, the minimum unit of measurement is that which ties the respective parties and their surroundings into a coherent and indivisible whole.” A Systemic Communication Model would have to address the following axioms by Watzlawick and his associates (1967).
 The Impossibility of Not Communicating Interpersonal behavior has no opposites.
It is not possible to conceive of non-behavior.
If all behavior in an interactional situation can be taken as having potential message value, it follows that no matter what is said and done, “one cannot not communicate.” Silence and inactivity are no exceptions.
Even when one person tries to ignore the overtures of another, he nonetheless communicates a disinclination to talk.
 Content and Relationship in Communication All face-to-face encounters require some sort of personal recognition and commitment which in turn create and define the relationship between the respective parties.
“Communication,” wrote Watzlawick (1967), “not only conveys information, but at the same time .
.
.
imposes behavior ” Any activity that communicates information can be taken as synonymous with the content of the message, regardless of whether it is true or false, valid or invalid.
Each spoken word, every movement of the body, and all the eye glances furnish a running commentary on how each person sees himself, the other person, and the other person’s reactions.
 The Punctuation of the Sequence of Events Human beings set up between them patterns of interchange (about which they may or may not be in agreement) and these patterns will in fact be rules of contingency regarding the exchange of reinforcement.
 Symmetrical and Complementary Interaction A symmetrical relationship evolves in the direction of heightening similarities; a complementary relationship hinges increasingly on individual differences.
The word symmetrical suggests a relationship in which the respective parties mirror the behavior of the other.
Whatever one does, the other tends to respond in kind.
Thus, an initial act of trust fosters a trusting response; suspicion elicits suspicion; warmth and congeniality encourage more of the same, and so on.
In sharp contrast is a complementary relationship, where individual differences complement or dovetail into a sequence of change.
Whether the complementary actions are good or bad, productive or injurious, is not relevant to the concept.
 Brown’s Holographic Model, 1987 Rhetorical theorist, William Brown, proposed “The Holographic View of Argument”.
Arguing against an analytical approach to communication that dissects the elements of communication, Brown argued for seeing argument or communication as a hologram “which as a 94 metaphor for the nature of argument emphasizes not the knowledge that comes from seeing the parts in the whole but rather that which arises from seeing the whole in each part.” “The ground of argument in a holographic structure is a boundaryless event.” A model of communication based on Brown’s holographic metaphor would see connections between divided elements and divisions between connections.
 A Fractal Model Polish-born mathematician, Benoit Mandelbrot, while working for IBM in the 1960s and 70s, became intrigued with the possibility of deriving apparently irregular shapes with a mathematical formula.
"Clouds are not spheres," he said, "mountains are not cones, coastlines are not circles, and bark is not smooth, nor does lightning travel in a straight line."
So if these regular geometric forms could not account for natural patterns, what could?
To solve the problem, Mandelbrot developed the fractal, a simple, repeating shape that can be created by repeating the same formula over and over.
“I coined fractal from the Latin adjective fractus.
The corresponding Latin verb frangere means ‘to break’: to create irregular fragments.
It is therefore sensible—and how appropriate for our needs!— that, in addition to ‘fragmented’ fractus should also mean ‘irregular,’ both meanings being preserved in fragment.” Benoit Mandelbrot FIG 11 A Fractal Model Construction of a Fractal Snowflake A Koch snowflake is constructed by making progressive additions to a simple triangle.
The additions are made by dividing the equilateral triangle’s sides into thirds, then creating a new triangle on each middle third.
Thus, each frame shows more complexity, but every new triangle in the design looks 95 exactly like the initial one.
This reflection of the larger design in its smaller details is characteristic of all fractals.
Fractal shapes occur everywhere in nature: a head of broccoli, a leaf, a snowflake—almost any natural form.
Mandelbrot’s discovery changed computer graphics—by using fractal formulas, graphic engines could create natural-looking virtual landscapes.
More importantly, fractal formulas can account for variations in other natural patterns such as economic markets and weather patterns.
Mandelbrot Set Polish-born French mathematician Benoit Mandelbrot coined the term “fractal” to describe complex geometric shapes that, when magnified, continue to resemble the shape’s larger structure.
This property, in which the pattern of the whole repeats itself on smaller and smaller scales, is called self similarity.
The fractal shown here, called the Mandelbrot set, is the graphical representation of a mathematical function.
96 Fractals allow for almost infinite density.
For example, Mandelbrot considered the deceptively simple question: “How long is the coast line of Britain?” A typical answer will ignore inlets and bays smaller than a certain size.
But if we account for these small coastline features, and then those smaller still, we would soon find ourselves with a line of potentially infinite and constantly changing length.
A fractal equation could account for such a line.
vi.
Fractal geometry is in some ways related to chaos theory, the science of finding pattern in apparently random sequences, like a dripping faucet or weather patterns.
Chaos theory has been applied to computer-generated landscapes, organizational structures and even washing machines.
Of course, it has also been applied to economics and the stock market, in particular: The stock markets are said to be nonlinear, dynamic systems.
Chaos theory is the mathematics of studying such nonlinear, dynamic systems.
Does this mean that chaoticians can predict when stocks will rise and fall?
Not quite; however, chaoticians have determined that the market prices are highly random, but with a trend.
The stock market is accepted as a self-similar system in the sense that the individual parts are related to the whole.
Another self-similar system in the area of mathematics are fractals.
Could the stock market be associated with a fractal?
Why not?
In the market price action, if one looks at the market monthly, weekly, daily, and intra day bar charts, the structure has a similar appearance.
However, just like a fractal, the stock market has sensitive dependence on initial conditions.
This factor is what makes dynamic market systems so difficult to predict.
Because we cannot accurately describe the current situation with the detail necessary, we cannot accurately predict the state of the system at a future time.
Stock market success can be predicted by chaoticians.
Short-term investing, such as intra day exchanges are a waste of time.
Short-term traders will fail over time due to nothing more than the cost of trading.
However, over time, long-term price action is not random.
Traders can succeed trading from daily or weekly charts if they follow the trends.
A system can be random in the short-term and deterministic in the long term.
One key premise in both chaos theory and fractals is "sensitive dependence on initial conditions."
One early chaos theorist studying weather patterns stumbled on this when he was using a simple computer program to plot the course of only 12 weather variables.
The computer printout ran out of paper, so he noted the status of the variables at an earlier point, stopped the process, replaced the paper and restarted the process at the earlier point.
Even though the variables started at the same point, the patterns quickly diverged, demonstrating the similar or even identical initial conditions can lead radically different outcomes.
This phenomenon led researchers to talk about "the butterfly effect" to illustrate how a very small change can produce significant changes in a system.
The butterfly effect refers to the fact that a butterfly flapping its wings over Beijing can result in a change in the weather patterns in New York two months later.
Applying Fractals to Communication Like Dance’s Helix, seeing communication as a fractal form allows us to conceptualize the almost infinite density of a communication event.
Margaret J. Wheatley has attempted to apply Fractal theory and the science of chaos to management.
(Leadership and the New Science: Learning about Organization from an Orderly Universe.
San Francisco, CA: Berrett-Kohler Publishers, 1992.)
You can read some of Wheatley's ideas here.
iii.
The significance of this for the topic at hand is this: First, the patterns of complexity in natural systems, of which human beings are a part, is profoundly complex and not easily captured in any formula.
Therefore, any predictions about the outcome of these systems are necessarily limited 97 because of the difficulty of being sensitive to initial conditions.
A model of communication drawn from fractals and chaos theory would have to reflect this complexity and respond to variations in initial conditions.
In addition, if we marry the fractal to other mathematical constructs, we can develop an even richer heuristic.
The mathematician Rudy Rucker, in a way that only mathematicians can, said “Life is a fractal in Hilbert space.” (Mind Tools: The Five Levels of Mathematical Reality (Boston : Houghton Mifflin, 1987) 248.)
Hilbert Space is a theoretical multi-dimensional space.
Rucker is saying that life is an infinitely variegated entity that exists in multiple dimensions.
So, we can borrow Rucker’s phrase and say that communication is a fractal in Hilbert space.
3.9 CONCLUSION In this Unit we discussed the general meaning of a model as well as communication model.
We discussed advantages of Models as well as the Limitations of Models.
We also look at the some Classical Communication Models, Early Linear Models, Non-linear Models as well as Multidimensional Models and some Suggestions for Communication Models.
3.10 SUMMARY In the broadest sense, a model is a systematic representation of an object or event in idealized and abstract form.
Communication models involves the use of pictures.
Models have their limitations as well as their advantage, the aim of a model is not to ignore complexity but rather to give it order and coherence.
There are several Classical Communication Models such as the model by Aristotle’s definition of rhetoric and others, we have also the Early Linear Models which includes model by Berlo’s S-M-C-R. there are other models group under the Non-linear Models and Multidimensional Models as well as Suggestions for Communication Models 3.11 REFERENCES 1.Barnlund, D. C. Interpersonal Communication: Survey and Studies.
Boston: Houghton Mifflin, 1968.
2.
Chapanis, A.
“Men, Machines, and Models,” American Psychologist, 16:113131, 1961.
3.
Deutsch, K. “On Communication Models in the Social Sciences,” Public Opinion Quarterly, 16:356-380, 1952.
4.
Gerbner, G. “Toward a General Model of Communication,” Audio-Visual Communication Review, 4:171-199, 1956.
5.
Kaplan, A.
The Conduct of Inquiry: Methodology for Behavioral Science.
San Francisco: Chandler, 1964.
6.
Lackman, R. “The Model in Theory Construction,” Psychological Review, 67:113-129, 1960.
7.
Sereno, K. K., and Mortensen, C. D. Foundations of Communication Theory.
New York: Harper & Row, 1970.
98  UNIT 7 COMPUTER APPLICATION IN DATA TRANSMISSION TABLE OF CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main content 3.1 Computer storage devices 3.2 Data processing.
3.2.2 Data processing cycle 3.3 Communication system components 3.3.2 Modem 3.4 Net working 5.5 Application of computer in data transmissiom 3.6 Conclusion 3.7 Summary 1.0 INTRODUCTION Computer is a machine that performs tasks, such as calculations or electronic communication, under the control of a set of instructions called a program.
Programs usually reside within the computer and are retrieved and processed by the computer’s electronics.
The program results are stored or routed to output devices, such as video display monitors or printers.
Computers perform a wide variety of activities reliably, accurately, and quickly.
A typical computer system consists of a central processing unit (CPU), input devices, storage devices, and output devices.
The CPU consists of an arithmetic/logic unit, registers, control section, and internal bus.
The arithmetic/logic unit carries out arithmetical and logical operations.
The registers store data and keep track of operations.
The control unit regulates and controls various operations.
The internal bus connects the units of the CPU with each other and with external components of the system.
For most computers, the principal input devices are a 99 keyboard and a mouse.
Storage devices include hard disks, CD-ROM drives, and random access memory (RAM) chips.
Output devices that display data include monitors and printers.
The physical computer and its components are known as hardware.
Computer hardware includes the memory that stores data and program instructions; the central processing unit (CPU) that carries out program instructions; the input devices, such as a keyboard or mouse, that allow the user to communicate with the computer; the output devices, such as printers and video display monitors, that enable the computer to present information to the user; and buses (hardware lines or wires) that connect these and other computer components.
The programs that run the computer are called software.
Software generally is designed to perform a particular type of task—for example, to control the arm of a robot to weld a car’s body, to write a letter, to display and modify a photograph, or to direct the general operation of the computer.
Data consists of information coming from observations, counts, measurements, or responses.
Data Transmission is the movement of data/information from one location to another, either within a computer (as from a disk drive to random access memory) or between a computer and an external device (as between two computers or a file server and a computer on a network).
The speed of transfer is called the data rate, or data transfer rate, and is usually measured in bits per second, or bps.
The raw data rate (the maximum transfer speed) is usually considerably higher than the actual rate at which meaningful data is transferred because of idle time, error-checking procedures, and other overhead.
Also, data transfers from different sources to different destinations often compete with each other if they use the same data path—for example, on a network, or on a bus in a computer system 2.0 OBJECTIVES After studying this unit, the student should be able to:  Define a computer and explain parts of the computer.
 Define data and data transmission.
 Explain database processing and transaction processing.
 Data processing cycle and stages in data processing.
 Describe communication system components.
 Define computer networking and types of network.
 Describe various application of computer in data transmission 3.1 COMPUTER STORAGE DEVICES Storage hardware provides permanent storage of information and programs for retrieval by the computer.
The two main types of storage devices are disk drives and memory.
There are several types of disk drives: hard, floppy, magneto-optical, magnetic tape, and compact.
Hard disk drives store information in magnetic particles embedded in a disk.
Usually a permanent part of the computer, hard disk drives can store large amounts of information and retrieve that information very quickly.
Floppy disk drives also store information in magnetic particles embedded in removable disks that may be floppy or rigid.
Floppy disks store less information than a hard disk drive and retrieve the information at a much slower rate.
While most computers still include a floppy disk drive, the technology has been gradually phased out in favor of newer technologies.
Magneto-optical disk drives store information on removable disks that are sensitive to both laser light and magnetic fields.
They can store up to 9.1 gigabytes (GB) of data, but they have slightly slower retrieval speeds as opposed to hard drives.
They are much more rugged than floppy disks, 100 making them ideal for data backups.
However, the introduction of newer media that is both less expensive and able to store more data has made magneto-optical drives obsolete.
Magnetic tape drives use magnetic tape similar to the tape used in VCR cassettes.
Tape drives have a very slow read/write time, but have a very high capacity; in fact, their capacity is second only to hard disk drives.
Tape drives are mainly used to back up data.
Compact disc drives store information on pits burned into the surface of a disc of reflective material.
CD-ROMs can store up to 737 megabytes (MB) of data.
A Compact Disc-Recordable (CD-R) or Compact Disc-ReWritable (CD-RW) drive can record data onto a specialized disc, but only the CD-RW standard allows users to change the data stored on the disc.
A digital versatile disc (DVD) looks and works like a CD-ROM but can store up to 17.1 GB of data on a single disc.
Like CD-ROMs, there are specialized versions of DVDs, such as DVD-Recordable (DVD- R) and DVD-ReWritable (DVD-RW), that can have data written onto them by the user.
More recently Sony Electronics developed DVD technology called Blu-ray.
It has much higher storage capacities than standard DVD media.
Another type of storage media, called a flash memory, traps small amounts of electric charge in “wells” on the surface of a chip.
Side effects of this trapped charge, such as the electric field it creates, are later used to read the stored value.
To rewrite to flash memory, the charges in the wells must first be drained.
Such drives are useful for storing information that changes infrequently.
Memory refers to the computer chips that store information for quick retrieval by the CPU.
Random access memory (RAM) is used to store the information and instructions that operate the computer's programs.
Typically, programs are transferred from storage on a disk drive to RAM.
RAM is also known as volatile memory because the information within the computer chips is lost when power to the computer is turned off.
Read-only memory (ROM) contains critical information and software that must be permanently available for computer operation, such as the operating system that directs the computer's actions from start up to shut down.
ROM is called nonvolatile memory because the memory chips do not lose their information when power to the computer is turned off.
3.2 DATA PROCESSING Data Processing, in computer science, the analysis and organization of data by the repeated use of one or more computer programs.
Data processing is used extensively in business, engineering, and science and to an increasing extent in nearly all areas in which computers are used.
Businesses use data processing for such tasks as payroll preparation, accounting, record keeping, inventory control, sales analysis, and the processing of bank and credit card account statements.
Engineers and scientists use data processing for a wide variety of applications, including the processing of seismic data for oil and mineral exploration, the analysis of new product designs, the processing of satellite imagery, and the analysis of data from scientific experiments.
Data processing is divided into two kinds of processing: database processing and transaction processing.
A database is a collection of common records that can be searched, accessed, and modified, such as bank account records, school transcripts, and income tax data.
In database 101 processing, a computerized database is used as the central source of reference data for the computations.
Transaction processing refers to interaction between two computers in which one computer initiates a transaction and another computer provides the first with the data or computation required for that function.
Most modern data processing uses one or more databases at one or more central sites.
Transaction processing is used to access and update the databases when users need to immediately view or add information; other data processing programs are used at regular intervals to provide summary reports of activity and database status.
Examples of systems that involve all of these functions are automated teller machines, credit sales terminals, and airline reservation systems.
3.2.2 DATA PROCESSING CYCLE The data-processing cycle represents the chain of processing events in most data-processing applications.
It consists of data recording, transmission, reporting, storage, and retrieval.
The original data is first recorded in a form readable by a computer.
This can be accomplished in several ways: by manually entering information into some form of computer memory using a keyboard, by using a sensor to transfer data onto a magnetic tape or floppy disk, by filling in ovals on a computer-readable paper form, or by swiping a credit card through a reader.
The data are then transmitted to a computer that performs the data-processing functions.
This step may involve physically moving the recorded data to the computer or transmitting it electronically over telephone lines or the Internet.. Once the data reach the computer, the computer processes it.
The operations the computer performs can include accessing and updating a database and creating or modifying statistical information.
After processing the data, the computer reports summary results to the program’s operator.
As the computer processes the data, it stores both the modifications and the original data.
This storage can be both in the original data-entry form and in carefully controlled computer data forms such as magnetic tape.
Data are often stored in more than one place for both legal and practical reasons.
Computer systems can malfunction and lose all stored data, and the original data may be needed to recreate the database as it existed before the crash.
The final step in the data-processing cycle is the retrieval of stored information at a later time.
This is usually done to access records contained in a database, to apply new data-processing functions to the data, or in the event that some part of the data has been lost—to recreate portions of a database.
Examples of data retrieval in the data-processing cycle include the analysis of store sales receipts to reveal new customer spending patterns and the application of new processing techniques to seismic data to locate oil or mineral fields that were previously overlooked Data Data Data Data Data recording Transmission reporting storage retrieval Fig 1 Data processing cycle 102  3.3 COMMUNICATION SYSTEM COMPONENTS Any time a message is sent from a sender to a receiver, the different parts of the communication system can be represented by the accompanying schematic figure titled 'Elements of a Communication System,' adapted from Shannon’s work on information theory.
Information messages sig nTaral nsmitter signal Received Receiver message signal source Destination Noise source Fig 2 Elements of a Communication System The model he devised to represent a communication system always consists of five major parts: the information source, the transmitter, the channel, the receiver, and the destination.
The information source produces (or selects) the message or the sequence of messages to be transmitted to the destination.
For example, the information source could be a distant spacecraft and the message could be an image of a planet, or the information source could be a rock-and- roll band and the message could be a new song.
The transmitter converts the message into a signal suitable for transmission over the channel.
For example, the transmitter could be the spacecraft telecommunication system that converts a photograph of Jupiter into a television signal.
Another example would be the recording studio’s audio equipment, which converts the rock-and-roll song into a sequence of tiny pits on the mirror like surface of a compact disc (CD).
It could be an eletro-mechanical device like a computer or handsets or even a Tv/ radio station transmitter.
The channel is the medium that is used to transmit the signal/data.
It could be wired(transmission cable) e.g telephone lines , printer cables e.t.c or wireless(using, microwave links, communications satellites, sensors, e.t.c) or Fiber optics.
Data and messages can be transmitted from one computer to another using, microwave links, communications satellites The channel is often noisy, in the sense that when the signal arrives at the receiver, it may contain noise or static, or it may be slightly garbled.
For example, the channel could be the millions of kilometers of empty space between Jupiter and Earth, with noise arising because the received signal is so weak.
Or it could be the surface of a CD, with noise occurring because of fingerprints, dust, or scratches on the surface.
The receiver is a device that reconstructs (either exactly or approximately) the message from the received signal.
It could be a large dish antenna or the electronics in a CD player.
The destination is the person (or thing) for which the message is intended.
For example, the destination could be a teenager interested in planetary science or an astronomer interested in 3.3.2 MODEM Modem, device that enables computers, facsimile machines, and other equipment to communicate with each other across telephone lines or over cable television network cables.
In 103 the strictest sense, a modem is a device that converts between analog signals, such as sound waves, and digital signals, which are used by computers.
However, the term has also come to include devices that permit the transmission of entirely digital signals.
The need for computer “connectivity” has established the usefulness of the modem.
Modems permit two computers to communicate in order to access databases, transmit files, upload and download facsimile transmissions, and send and receive electronic mail An analog modem converts the digital signals of the sending computer to analog signals that can be transmitted through telephone lines.
When the signal reaches its destination, another modem reconstructs the original digital signal, which is processed by the receiving computer.
A standard analog modem has a maximum speed of 33.6 Kbps.
The word modem is an acronym formed from the two basic functions of an analog modem: modulation and demodulation.
To convert a digital signal to an analog one, the modem generates a carrier wave and modulates, or adjusts, it according to the digital signal.
The kind of modulation used depends on the application and the speed of operation for which the modem is designed.
For example, many high-speed modems use a combination of amplitude modulation, in which the amplitude of the carrier wave is changed to encode the digital information, and phase modulation, in which the phase of the carrier wave is changed to encode the digital information.
The process of receiving the analog signal and converting it back to a digital signal is called demodulation.
3.4 NET WORKING Network is a system used to link two or more computers.
Network users are able to share files, data, applications, and resources, printers, and other resources; send electronic messages; and run programs on other computers.
A network enables the fast and effective transfer of information within a group of users and reduces operational costs.
Types of computer networks include: PAN (Personal Area Network), such as Bluetooth, Infrared links computers within a few meters from each other.
Local Area Network (LAN), Computers in a LAN are separated by distances of up to a few kilometers and are typically used in offices or across university campuses.
Metropolitan Network (MAN) is a netework connecting computers within a city that covers longer distance than LAN Wide Area Network (WAN), are networks that span large geographical areas in which computers are linked to networks to use facilities in another city or country.
The Internet is an interconnected web of wide area networks.
104  Fig 3 Diagram for network description 3.5 PRACTICAL COMPUTER NETWORKS 3.5.1 INTERNET Electronic mail, or e-mail, is a key attraction of the Internet and a common form of computer telecommunications.
E-mail is a text-based message delivery system that allows information such as typed messages and multimedia to be sent to individual computer users.
Local e-mail messages (within a building or a company) typically reach addressees by traveling through wire- based internal networks.
E-mail that must travel across town or across a country to reach the final destination usually travels through the telephone network.
Instant messaging is another key feature of computer telecommunications and involves sending text, audio, or video data in real time.
Other computer telecommunications technologies that businesses frequently use include automated banking terminals and devices for credit card or debit card transactions.
These transactions either bill charges directly to a customer’s credit card account or automatically deduct money from a customer’s bank account.
The same message can be sent to a number of different addresses.
E-mail is sent through a company's own local area network or beyond, through a nationwide or worldwide communications network.
E-mail services use a central computer to store messages and data and to route them to their intended destination.
With a subscription to a public e-mail network, an individual PC user needs only a modem and a telephone to send and receive written or vocal messages.
Because of the huge amount of e-mail that can be generated, systems have been developed to screen mail for individual users.
A specialized type of e-mail system, voice mail, is a relatively simple, computer-linked technology for recording, storing, retrieving, and forwarding phone messages.
It is called voice mail, or voice-messaging, because the messages are spoken and left in a “voice mailbox.” The telephone doubles as a computer terminal, but instead of presenting the information on a computer screen, the system reads it over the phone line, using prerecorded voice vocabulary.
The systems are based on special-purpose computer chips and software that convert human 105 speech into bits of digital code.
These digitized voices are stored on magnetic disks, from which they can be instantaneously retrieved.
Callers are offered a menu of choices, and the messages they select are played; they can leave messages in “voice mailboxes,” or they can access huge computer databases 3.5.2 ELECTRONIC FUNDS TRANSFER (EFT), Electronic funds transfer is a method of transferring funds automatically from one account to another by electronic means.
One example is electronic funds transfer at point of sale (EFTPOS), which provides for the automatic transfer of money from buyer to seller at the time of a sale.
A customer inserts a plastic card into a point-of-sale computer terminal in a supermarket, for example.
Telephone lines are then used to make an automatic debit from the customer's bank account to pay the bill.
3.5.3 ELECTRONIC POINT OF SALE (EPOS), Electronic Point of Sale (EPOS), system used in retailing in which a bar code on a product is scanned at a cashier’s station and the information is relayed to the store computer.
The computer relays back the price of the item to the cashier’s station.
The customer can then be given an itemized receipt while the computer removes the item from stock figures.
EPOS allows for efficient computer stock control and reordering, as well as giving a wealth of information about turnover, profitability on different lines, stock ratios, and other important financial indicators 3.5.4 ELECTRONIC BANKING In recent years, banks have made their services increasingly convenient through electronic banking.
Electronic banking uses computers to carry out transfers of money.
For example, automated teller machines (ATMs) enable bank customers to withdraw money from their checking or savings accounts by inserting an ATM card and a private electronic code into an ATM.
The ATMs enable bank customers to access their money 24 hours a day and seven days a week wherever ATMs are located, including in foreign countries.
Banks also offer debit cards that directly withdraw funds from a customer’s account for the amount of a purchase, much like writing a check.
Banks also use electronic transfers to deposit payroll checks directly into a customer’s account and to automatically pay a customer’s bills when they are due.
Many banks also use the Internet to enable customers to pay bills, move money between accounts, and perform other banking functions.
3.5.5 COMPUTER-AIDED INSTRUCTION (CAI), Computer-Aided Instruction (CAI), diverse and rapidly expanding spectrum of computer technologies that assist the teaching and learning process.
CAI is also known as computer- assisted instruction.
Examples of CAI applications include guided drill and practice exercises, computer visualization of complex objects, and computer-facilitated communication between students and teachers.
By connecting millions of computers worldwide, these networks enable students to access huge stores of information, which greatly enhances their research capabilities.
3.5.6 EXCHANGE (BUSINESS) Computer application in data transfer has helped a lot Exchange (business), it has helped to reduce the number of person at the central office or place of business at every given time; such 106 in the floor of stock exchange or labor exchange.
People can now trade in securities or commodities at the comfort of their homes.
3.5.7 ELECTRONIC COMMERCE Electronic Commerce or e-commerce, the exchange of goods and services by means of the Internet or other computer networks.
E-commerce follows the same basic principles as traditional commerce—that is, buyers and sellers come together to exchange goods for money.
But rather than conducting business in the traditional way—in stores and other “brick and mortar” buildings or through mail order catalogs and telephone operators—in e-commerce buyers and sellers transact business over networked computers.
E-commerce offers buyers convenience.
They can visit the World Wide Web sites of multiple vendors 24 hours a day and seven days a week to compare prices and make purchases, without having to leave their homes or offices.
In some cases, consumers can immediately obtain a product or service, such as an electronic book, a music file, or computer software, by downloading it over the Internet.
For sellers, e-commerce offers a way to cut costs and expand their markets.
They do not need to build, staff, or maintain a store or print and distribute mail order catalogs.
Automated order tracking and billing systems cut additional labor costs, and if the product or service can be downloaded, e-commerce firms have no distribution costs.
Because they sell over the global Internet, sellers have the potential to market their products or services globally and are not limited by the physical location of a store.
Internet technologies also permit sellers to track the interests and preferences of their customers with the customer’s permission and then use this information to build an ongoing relationship with the customer by customizing products and services to meet the customer’s needs.
3.5.8 HEALTH MANAGEMENT Telemedicine is a rapidly developing application of clinical medicine where medical information is transferred through interactive audiovisual media for the purpose of consulting, and sometimes for remote medical procedures or examinations.
Telemedicine may be as simple as two health professionals discussing a case over the telephone, or as complex as using satellite technology and videoconferencing equipment to conduct a real-time consultation between medical specialists in two different countries.
Telemedicine generally refers to the use of communications and information technologies for the delivery of clinical care.
A key difference between traditional in-person patient meetings and telemedicine encounters is the omission of an actual physical examination and history.
This process requires the clinician to rely on history report and audio/video information in lieu of a physical examination.
In most developing nations, people living in rural and remote areas struggle to access timely, quality specialty medical care primarily because the few specialist physicians available are more likely to be located in areas of concentrated population.
Even when they are available, it may just be one specialist covering many clinics.
Medicine is a very sensitive field that requires collaboration by different specialists in a particular field for more effective outcome.
Providing top quality care necessitates close collaboration, secure, easy and timely exchange of information, and coordination of the medical team’s activities [3], the geographic proximity or geographic separation between the team members and the patient notwithstanding.
This should 107 be achieved irrespective of the geographic location of the individual members of the team, or even if the different doctors treating the patient for possibly different symptoms are at different hospitals.
It is of course obvious, that the concurrent physical presence at the point of care of all members of the team is rarely possible.
This creates serious difficulties for providing the quality care that patients deserve to obtain around their vicinity.
This system is aimed at providing more effective health- care delivery by offering health-care team services aimed at achieving a seamless continuum of health and health-related services, despite the structural problems of some of our remote clinics, as compared to facilities available in major health institutions like the Teaching hospitals.
CONCLUSION In this Unit we discussed the various parts of a computer, how a computer is applied in data transmission from the information source to the final destination of the information through networks and the processes involved.
We also discussed the various practical ways we make us of computer in date transmission and explain parts of the computer.
SUMMARY There are four basic parts of any communication system, these are the Information source, the transmitter, the Channel, the receiver and the destination.
For data to be transmitted it must undergo what is called Data processing cycle, this processing cycle includes data recording, transmission, reporting, storage, and retrieval.
There are several practical application of computer in data transmission which includes: Internet services, electronic fund transfer e.t.c REFERENCES 1.
Www.
VOIP Industry Dictionary.
2.
McEliece, Robert J.
"Information Theory."
Microsoft® Encarta® 2009 [DVD].
Redmond, WA: Microsoft Corporation, 2008.
3. http://www.bbc.co.uk/schools/revision 4.Journal of American medical information Association Volume 2 Number 5 1995 108  UNIT 8 LAN, LAM AND WIRELESS TRANSMISSION CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main content 3.1 Principles of wireless transmission 3.2.
Transmission Bandwidth 3.3 Modes of wireless transmission 3.4 Net working 3.4.1 Components of a network 3.4.2 Network connections 3.4.3 Network media 3.4.4 Network operation and management 3.4.5 Types networks 3.5 LAN overview 3.6 LAM overview 3.7 Application of computer in data transmissiom 4.0 Conclusion 5.0 Summary 6.0.
References.
1.0 INTRODUCTION Wireless Communications, various telecommunications systems that use radio waves to carry signals and messages across distances.
Wireless communications systems include cellular telephones, pagers, radio telegraphs, satellite telephones, laptop computers, personal digital assistants (PDAs), shortwave radios, and two-way radios.
They are used primarily to transmit private communications.
Commercial radio and television are also wireless telecommunications systems, but radio and television are mainly public broadcast services rather than private communications systems (see Radio and Television Broadcasting).
This article focuses on wireless communications systems that are used primarily for private communications.
109 Wireless communications allow people greater flexibility while communicating, because they do not need to remain at a fixed location, such as a home or office, but instead can communicate with other people while traveling in a car or walking along a street.
Wireless technologies make communications services more readily available than traditional wire-based services (such as ordinary telephones), which require the installation of wires in fixed locations.
Wireless communications devices are useful in places where communications services are only temporarily needed, such as at outdoor festivals or large sporting events.
These technologies are also useful for communicating in remote locations, such as mountains, jungles, or deserts, where wire-based telephone service might not exist.
Police, fire, and other emergency departments use wireless devices, such as two-way radio, to communicate information between vehicles that are already responding to emergency calls.
Construction and utility workers frequently use handheld radios for short-range communication and coordination.
Many businesspeople use wireless devices, such as cellular radio telephones, also known as cell phones, to stay in contact with colleagues and clients while traveling.
Increasingly, people are using wireless devices for a variety of everyday purposes, such as scheduling appointments, arranging meeting places, shopping for food, or agreeing on home video selections while in a video store.
All wireless communications devices use radio waves to transmit and receive signals.
These devices operate on different radio frequencies so that signals from one device will not overlap and interfere with nearby transmissions from other devices.
The number of companies offering wireless communications services has grown steadily in recent years.
For example, in 1988 about 500 companies offered cell phone services.
By 2001 that number had grown to more than 2,500 companies serving about 120 million subscribers.
Currently, telecommunications companies throughout the world are activating more wireless service subscriptions than they are conventional wire-based service subscriptions.
Wireless communication is becoming increasingly popular because of the convenience and mobility it affords; the expanded availability of radio frequencies for transmitting, which makes it possible to handle a larger volume of calls; and improvements in technology, which have added other services such as Internet access and improved the clarity of voice transmissions.
2.0 OBJECTIVES After studying this unit, you should be able to:  Understand and explain what wireless transmission is all about.
 Understand and explain the principles of wireless transmission.
 Understand and Explain transmission bandwidth.
 Understand and explain various modes of wireless transmission.
 Understand and Describe what is a Net working, Components of a network, Network connections, Network media, Network operation & management and types networks  Understand and Describe a LAN network.
 Understand and Describe a LAM network.
3.
1 PRINCIPLES OF WIRELESS TRANSMISSION Wireless communications begin with a message that is converted into an electronic signal by a device called a transmitter.
There are two types of transmitters: analog and digital.
An analog transmitter sends electronic signals as modulated radio waves.
The analog transmitter modulates the radio wave to carry the electronic signal and then sends the modified radio signal through 110 space.
A digital transmitter encodes electronic signals by converting messages into a binary code, the series of zeros and ones that are the basis of all computer programming.
The encoded electronic signal is then sent as a radio wave.
Devices known as receivers decode or demodulate the radio waves and reproduce the original message over a speaker.
Wireless communications provide more flexibility than wire-based means of communication.
However, there are some drawbacks.
Wireless communications are limited by the range of the transmitter (how far a signal can be sent), and since radio waves travel through the atmosphere they can be disturbed by electrical interferences (such as lightning) that cause static Wireless communications systems involve either one-way transmissions, in which a person merely receives notice of a message, or two-way transmissions, such as a telephone conversation between two people.
An example of a device that only receives one-way transmission is a pager, which is a high-frequency radio receiver.
When a person dials a pager number, the pager company sends a radio signal to the desired pager.
The encoded signal triggers the pager’s circuitry and notifies the customer carrying the pager of the incoming call with a tone or a vibration, and often the telephone number of the caller.
Advanced pagers can display short messages from the caller, or provide news updates or sports scores.
Two-way transmissions require both a transmitter and a receiver for sending and receiving signals.
A device that functions as both a transmitter and a receiver is called a transceiver.
Cellular radio telephones and two-way radios use transceivers, so that back-and-forth communication between two people can be maintained.
Early transceivers were very large, but they have decreased in size due to advances in technology.
Fixed-base transceivers, such as those used at police stations, can fit on a desktop, and hand-held transceivers have shrunk in size as well.
Several current models of handheld transceivers weigh less than 0.2 kg (0.5 lb).
Some pagers also use transceivers to provide limited response options.
These brief return- communication opportunities allow paging users to acknowledge reception of a page and to respond using a limited menu of options.
3.2 TRANSMISSION BANDWIDTH Bandwidth, in computer science, the amount of information that can be sent through a connection between two computers in a given amount of time.
Computers may be connected by telephone wires, by coaxial cable, or through radio waves or microwaves.
A connection that can transmit more data in a shorter period of time is said to have more bandwidth than another, slower connection.
The term bandwidth originated with radio broadcasting and in that context refers to the amount of the electromagnetic spectrum (i.e.
the range of frequencies) that is allocated for a specific use.
A band consists of a range of frequencies, and its width is determined by the difference between its highest and lowest frequencies.
For instance, FM radio stations are allotted 200 kilohertz (200,000 cycles per second) of bandwidth.
Since FM broadcasting relies on changes in frequency to transmit information, a station at 102.5 on the FM dial is using frequencies between 102.4 MHz and 102.6 MHz, with a small buffer at either end, to broadcast its information.
In contrast, AM radio stations, which use changes in amplitude rather than frequency for information transmission, do not require as much bandwidth and are alloted only 10kHz.
Bandwidth in computers is measured not in cycles per second but by the number of bits per second (bps) that can be sent over a connection.
A bit is the smallest unit of information a computer uses and it may have a value of either 0 or 1.
Bits are usually combined into groups of eight to form bytes that can represent visual information, such as a letter, number, punctuation 111 mark, or symbol.
Computers manipulate bits by turning small switches, called transistors, off and on.
When a transistor is on, bits can pass through the computer connection.
When a transistor is off, the bits stop traveling through the connection.
Early telecommunication systems had low bandwidths that sent information at a relatively slow speed.
The first teletype machines, used by newspapers to send news stories, operated at just 110 bps, or about 11 characters per second.
At this speed it took a full second to transmit the word characters and the space that follows it.
That is extremely slow by modern standards—today information can be sent at speeds of millions or even billions of bits per second.
The text of this article would reach your computer in less than a second using some of today’s high-speed data connections.
To simplify the expression of these larger bandwidth speeds, bandwidth is usually expressed in units of kbps (where k stands for kilo, the metric term for thousand), mbps (where m stands for mega, a million), or gbps (in which g stands for giga, a billion).
Using this shorthand, you can shorten 56,000 bps to 56 kbps and 1,500,000 bps to 1.5 mbps.
Bandwidth directly affects the quality of transmitted information.
For example, when a caller telephones into a radio show, the caller’s voice is not as clear to a listener as the radio host’s voice because the bandwidth of a telephone connection is smaller than the bandwidth of radio signals.
The larger bandwidth of radio signals can carry a broader range of sound frequencies, and this improves the sound quality of the human voice.
The telecommunications industry, including telephone companies, cable companies, and Internet service providers, continually seeks new technologies that will increase the amount of bandwidth it can provide in order to improve the quality of information flow and attract more customers.
3.3 MODES OF WIRELESS TRANSMISSION Wireless communications systems have grown and changed as technology has improved.
Several different systems are used today, all of which operate on different radio frequencies.
New technologies are being developed to provide greater service and reliability.
Some of such technology include: SEA AND AIR TRANSCEIVERS The first wireless communications devices were radio telegraphs.
A telegraph is a device that sends simple electrical pulses along copper wires or through the air as radio waves.
The pulses were caused by contact between two metal surfaces, and receivers interpreted these electrical pulses as tones or beeps.
A code of long and short signals was developed to represent the letters of the alphabet (see Morse Code, International), and in this way coded messages could be sent between telegraphs.
Radio telegraphs used radio waves rather than wire telegraph lines to send and receive messages.
Radio telegraphs sent telegraph signals over long distances and were ideal for ship-to-shore communication.
Bulky radio telegraphs were installed on ships as early as 1899 and were widely used by 1905.
Maritime and aviation telecommunications systems now use high-frequency radios and satellites capable of transmitting speech and text, rather than wireless telegraphy, to send messages.
Aircraft pilots use radios to communicate with air traffic controllers at airports and also to communicate with other pilots.
Navigation beacons are equipped with transmitters that send automated signals to help ships and aircraft in distress determine their positions.
While high- frequency radio can transmit signals over long distances, the quality of these signals can be 112 diminished by bad weather or by electrical interference in the atmosphere, which is often caused by radiation from the Sun.
HANDHELD RADIO TRANSCEIVERS Police, fire, and other emergency organizations, as well as the military, have used two-way wireless radio communication since the 1930s.
Early vehicle-based radios were large, heavy units.
After the invention of the transistor in 1948, radios shrank in size to small handheld radio transceivers.
Public two-way radios with several frequency options are widely available as well.
Usually limited in range to a few miles, these units are great aids for such mobile professionals as construction workers, film crews, event planners, and security personnel.
Simpler two-way radios, called walkie-talkies, have been popular children’s toys for years.
Most walkie-talkies broadcast on channel 14 of the citizens band (CB), a range of frequencies grouped into channels and allocated for public use.
CB radios can transmit and receive on 40 different channels.
An unlicensed radio service, the Family Radio Service, allows individuals to use high-frequency wireless devices with a range of up to 3.2 km (2 mi).
SHORTWAVE TRANSCEIVERS Long-range broadcast services and frequencies, in what is known as the shortwave radio band (with frequencies of 3 to 30 megahertz), are available for amateur or ham radio operators.
Shortwave radio broadcasts can travel long distances because of the concentration of ionized, or electrically charged, particles in the layer of the atmosphere known as the ionosphere.
The ionoshere reflects radio signals, so that signals transmitted upward are reflected back to the surface of Earth.
This skipping of waves against the ionosphere can greatly increase the range of the transmitter.
These broadcasts can travel thousands of kilometers.
Under certain conditions and on special “clear channel” frequencies, listeners of AM radio can receive a signal from several time zones away.
Shortwave radio listeners sometimes can receive signals from the other side of the world.
The degree of reflectivity of the ionosphere depends on the time of day.
During daylight hours, the ionosphere has the concentration of ions necessary for reflecting radio waves only at the higher frequencies of the shortwave band.
At night, the ionosphere has the concentration necessary for reflecting lower frequencies within the lower parts of the shortwave band.
If there is an inadequate concentration of ions, the radio waves simply continue through the ionosphere into space.
CELLULAR RADIO TELEPHONES Cellular radio telephones, or cell phones, combine their portable radio capability with the wired, or wire-based, telephone network to provide mobile users with access to the rest of the public telephone system used by non mobile callers.
An early form of radio telephone communicated with a single powerful antenna within a given geographic or metropolitan area.
This large antenna was wired to the telephone system.
With only one antenna for a large metro area, this limited the number of frequencies that could be used, because radio telephone frequencies would often overlap and cause interference.
As a result, only a limited number of simultaneous calls could be handled, because only a small block of channels could be generated over the available radio spectrum allocated for the service.
Modern cellular telephones use a network of several short-range antennas known as towers that connect to the telephone system.
Because the antennas have a shorter range and cover a smaller area, often as short as 1.5 to 2.4 km (1.0 to 1.5 113 mi), frequencies can be reused a short distance away without overlapping and causing interference.
Cell phone towers pick up requests from cell phones for a dial tone and also deliver inbound calls to the appropriate cell phone or deliver calls to people using regular telephones on the wire- based system.
To do any of these things, the cell phone must have a singular identity that can be recognized by computers housed in a central mobile telephone switching office (MTSO).
When a cell phone is turned on, it connects by radio waves to the nearest cell tower (tower receiving the strongest signal).
The cell towers are spaced so their receiving ranges slightly overlap.
This continuous contact makes it possible for the MTSO to transfer a call from tower to tower as a mobile cell phone user (in a moving vehicle, for instance) moves from one cell area to another.
SATELLITE COMMUNICATIONS Satellite communications services connect users directly to the telephone network from almost anywhere in the world.
Special telephones are available to consumers that communicate directly with communications satellites orbiting Earth.
The satellites transmit these signals to ground stations that are connected to the telephone system.
These satellite services, while more expensive than cellular or other wireless services, give users access to the telephone network in areas of the world where no wired or cellular telephone service exists.
Satellite phones are also able to deliver video images through videophones that use tiny cameras and transmit their images via the satellite phone.
RADIO MODEMS Wi-Fi, an abbreviation for wireless fidelity, is a wireless communication technology that can provide connections between portable computers and wired connections to the Internet.
To connect users with the Internet, Wi-Fi devices use low-power transmitters and receivers equipped with special computer chips containing radio modems.
The chips can be installed in laptop computers, personal digital assistants (PDAs), and cellular telephones.
Radio modems provide the same functions as modems that operate with conventional wire-based networks: They modulate and demodulate signals to mimic digital bit streams, the same format used by computers.
Wi-Fi-equipped computers, cell phones, and PDAs provide mobile, wireless access to e-mail and Internet sites.
The radio modems must be in range of a Wi-Fi device containing a transmitter and receiver that is connected to a landline providing Internet access.
Areas within range of a Wi-Fi transmitter and receiver are known as hot spots.
Current technical standards limit the range to distances of about 90 m (300 ft).
Many transmitters, however, can be linked to cover a wider area, such as an airport or hotel.
Current Wi-Fi standards enable data to be sent at high speeds ranging from 11 to 54 megabits per second.
This is known as a broadband connection because a vast amount of data can be sent quickly.
A new technology known as WiMax promises to extend the range of a transmitter and receiver to about 48 km (30 mi).
The WiMax technology also expands the capabilities of broadband connections by enabling users to remain connected to Internet hot spots even when traveling in an automobile or train at speeds up to 250 km/h (155 mph).
ULTRAWIDEBAND (UWB) Wi-Fi may eventually give way to another radio technology known as ultrawideband (UWB), according to some experts.
Unlike Wi-Fi, UWB does not use a single radio frequency but sends its radio signals in short pulses across the entire radio spectrum.
This technology reduces 114 interference and enables UWB to send larger amounts of data than Wi-Fi.
UWB is expected to be used to connect all types of electronic equipment within a home without the use of wires.
For example, stereo speakers could be connected to a high-definition television set, and the television could receive signals from a DVD player, and the DVD player could be connected to a personal computer, and all these connections could be done wirelessly.
3.4 NET WORKING Network is a system used to link two or more computers.
Network users are able to share files, data, applications, and resources, printers, and other resources; send electronic messages; and run programs on other computers.
A network enables the fast and effective transfer of information within a group of users and reduces operational costs.
COMPONENTS OF A NETWORK A network has three layers of components: application software, network software, and network hardware.
Application software consists of computer programs that interface with network users and permit the sharing of information, such as files, graphics, and video, and resources, such as printers and disks.
One type of application software is called client-server.
Client computers send requests for information or requests to use resources to other computers, called servers, that control data and applications.
Another type of application software is called peer-to-peer.
In a peer-to-peer network, computers send messages and requests directly to one another without a server intermediary.
Network software consists of computer programs that establish protocols, or rules, for computers to talk to one another.
These protocols are carried out by sending and receiving formatted instructions of data called packets.
Protocols make logical connections between network applications, direct the movement of packets through the physical network, and minimize the possibility of collisions between packets sent at the same time.
Network hardware is made up of the physical components that connect computers.
Two important components are the transmission media that carry the computer's signals, typically on wires or fiber-optic cables, and the network adapter, which accesses the physical media that link computers, receives packets from network software, and transmits instructions and requests to other computers.
Transmitted information is in the form of binary digits, or bits (1s and 0s), which the computer's electronic circuitry can process.
NETWORK CONNECTIONS A network has two types of connections: physical connections that let computers directly transmit and receive signals and logical, or virtual, connections that allow computer applications, such as e-mail programs and the browsers used to explore the World Wide Web, to exchange information.
Physical connections are defined by the medium used to carry the signal, the geometric arrangement of the computers (topology), and the method used to share information.
Logical connections are created by network protocols and allow data sharing between applications on different types of computers, such as an Apple Macintosh or a personal computer (PC) running the Microsoft Corporation Windows operating system, in a network.
Some logical connections use client-server application software and are primarily for file and printer sharing.
The Transmission Control Protocol/Internet Protocol (TCP/IP) suite, originally developed by the United States Department of Defense, is the set of logical connections used by the Internet, the worldwide consortium of computer networks.
TCP/IP, based on peer-to-peer application software, creates a connection between any two NETWORK MEDIA 115 The medium used to transmit information limits the speed of the network, the effective distance between computers, and the network topology.
Copper wires and coaxial cable provide transmission speeds of a few thousand bits per second for long distances and about 100 million bits per second for short distances.
(A million bits is equal to one megabit, and one megabit per second is abbreviated Mbps.)
Optical fibers carry 100 million to 40 billion bits of information per second over long distances.
(A billion bits is equal to one gigabit, and a billion bits per second is abbreviated Gbps.)
Wireless networks, often used to connect mobile, or laptop, computers, send information using infrared or radio-frequency transmitters.
Infrared wireless local area networks (LANs) work only within a room, while wireless LANs based on radio-frequency transmissions can penetrate most walls.
Wireless LANs using Wi-Fi technology have capacities of around 54 Mbps and operate at distances up to a few hundred meters.
Wireless communications for wide area networks (WANs) use cellular radio telephone networks, satellite transmissions, or dedicated equipment to provide regional or global coverage.
Although transmission speeds continue to improve, today’s wide area cellular networks run at speeds ranging from 14 to 230 kilobits per second.
(A kilobit is equal to 1,000 bits, and one kilobit per second is abbreviated Kbps.)
Some networks use a home’s existing telephone and power lines to connect multiple machines.
HomePNA networks, which use phone lines, can transmit data as fast as 128 Mbps, and similar speeds are available on Power Line or HomePlug networks.
NETWORK OPERATION AND MANAGEMENT Network management and system administration are critical for a complex system of interconnected computers and resources to remain operating.
A network manager is the person or team of people responsible for configuring the network so that it runs efficiently.
For example, the network manager might need to connect computers that communicate frequently to reduce interference with other computers.
The system administrator is the person or team of people responsible for configuring the computer and its software to use the network.
For example, the system administrator may install network software and configure a server's file system so client computers can access shared files.
Networks are subject to hacking, or illegal access, so shared files and resources must be protected.
A network intruder could eavesdrop on packets being sent across a network or send fictitious messages.
For sensitive information, data encryption (scrambling data using mathematical equations) renders captured packets unreadable to an intruder.
Most servers also use authentication schemes to ensure that a request to read or write files or to use resources is from a legitimate client and not from an intruder TYPES NETWORKS Types of networks includeS: PAN (Personal Area Network), such as Bluetooth, Infrared links computers within a few meters from each other.
Local Area Network (LAN), This covers distances of up to a few kilometers and are typically used in offices or across university campuses.
Metropolitan Network (MAN) is a netework connecting computers within a city that covers longer distance than LAN less than WAN Wide Area Network (WAN), are networks that span large geographical areas in which computers are linked to networks to use facilities in another city or country.
The Internet is an interconnected web of wide area networks.
116  LAM (Local Area Multicomputer) is an message passing interface (MPI) programming environment and development system for heterogeneous computers on a network Campus Area Network - a network spanning multiple LANs but smaller than a MAN, such as on a university or local business campus.
Storage Area Network - connects servers to data storage devices through a technology like Fibre Channel.
System Area Network - links high-performance computers with high-speed connections in a cluster configuration.
Also known as Cluster Area Network.
PRIVATE BRANCH EXCHANGES (PBXS) provide continuous computer connections for the transfer of specialized data such as telephone transmissions, but they are not ideally suited to send and receive the short bursts of data used by most computer applications.
LAN OVERVIEW The Primary Function of a Local Area Network includes the following applications: file serving, database and application serving, print serving, electronic mail, remote links, video transfers, process control and monitoring, and distributed processing.
INTERNAL LAN CONNECTIONS A LAN connects network devices over a relatively short distance.
A networked office building, school, or home usually contains a single LAN, though sometimes one building will contain a few small LANs (perhaps one per room), and occasionally a LAN will span a group of nearby buildings A LAN device can send and receive signals from all other devices in the network.
Alternatively, each device may be linked to a repeater or hub, specialized equipment that selectively pass information from one device to one or more destinations in the network.
Networks use protocols, or rules, to exchange information through a single shared connection.
These protocols prevent collisions of data caused by simultaneous transmission between two or more computers.
Computers on most LANs use protocols known as Ethernet or Token Ring.
An Ethernet-linked computer checks if a shared connection is in use.
If not, the computer transmits data.
Since computers can sense an idle connection and send data at the same time, transmitting computers continue to monitor their shared connection and stop transmitting if a collision occurs.
Token Ring protocols pass a special message called a token through the network.
A computer that receives the token is given permission to send a packet of information or, if the computer has no packet to send, it passes the token to the next computer.
EXTERNAL LAN CONNECTIONS Connections that link LANs to external resources, such as other LANs or remote databases, are called bridges, routers, and gateways.
A bridge creates an extended LAN by passing information between two or more LANs.
A router is an intermediary device that connects a LAN to a larger LAN or to a WAN by interpreting protocol information and selectively forwarding packets to different LAN or WAN connections through the most efficient route available.
A gateway connects and translates between networks that use different communications protocols.
LAN computers use a gateway or router to connect to a WAN such as the Internet, the worldwide consortium of computer networks.
Such connections are a security risk because the LAN has no control over users on the Internet.
Applications transferred from the Internet to the LAN may contain computer viruses that can harm the components of the LAN, or external and 117 unauthorized users may gain access to sensitive files or erase or alter files.
A special type of gateway called a firewall keeps external users from accessing resources on the LAN while letting LAN users access the external information.
LAM OVERVIEW LAM (Local Area Multicomputer) is an MPI programming environment and development system for heterogeneous computers on a network.
With LAM, a dedicated cluster or an existing network computing infrastructure can act as one parallel computer solving one problem.
LAM features a full implementation of Message-Passing Interface.
Compliant applications are source code portable between LAM and any other implementation of MPI.
In addition to meeting the standard in a high quality manner, LAM offers extensive monitoring capabilities to support debugging.
Monitoring happens on two levels.
LAM has the hooks to allow a snapshot of process and message status to be taken at any time during an application run.
The status includes all aspects of synchronization plus data type map / signature, communicator group membership and message contents.
On the second level, the MPI library is instrumented to produce a cummulative record of communication, which can be visualized either at runtime or post-mortem.
The Foundation LAM runs on each computer as a single UNIX daemon uniquely structured as a nano-kernel and hand-threaded virtual processes.
The nano-kernel component provides a simple message-passing, rendez-vous service to local processes.
Some of the in-daemon processes form a network communication subsystem, which transfers messages to and from other LAM daemons on other nodes.
Inter-node packet exchange is via a scalable UDP-based protocol.
The network subsystem adds features like packetization and buffering to the base synchronization.
Other in-daemon processes are servers for remote capabilities, such as program execution and file access.
The layering is quite distinct: the nano-kernel has no connection with the network subsystem, which has no connection with the servers.
Users can configure in or out services as necessary.
Application management and debugging are provided by a uniquely structured extensible daemon.
The unique software engineering of LAM is transparent to users and system administrators, who only see a conventional daemon.
System developers can de-cluster the daemon into a daemon containing only the nano-kernel and several full client processes.
This developer's mode is still transparent to users but exposes LAM's highly modular components to simplified individual debugging.
It also reveals LAM's evolution from Trollius, which ran natively on scalable multicomputers and joined them to the UNIX network through a uniform programming interface.
The network layer in LAM is a documented, primitive and abstract layer on which to implement a more powerful communication standard like MPI .
MPI Tools An important feature of LAM is hands-on control of the multicomputer.
There is very little that cannot be seen or changed at runtime.
In particular, unreceived messages and the synchronization state of processes can be examined with the mpimsg and mpitask tools.
The key synchronization variables: source rank, destination rank, tag and communicator, are all displayed.
Within a communicator, an opaque MPI object, the user can examine the group membership.
Within a data type, the type map and signature are exposed.
Additionally, the 118 contents of a message can be displayed.
Again, these capabilities are available at any moment during an application's run.
Message queues and blocked processes can be examined at runtime.
The mpirun tool finds and loads the program(s) which constitute the application.
A simple SPMD application can be specified on the mpirun command line while a more complex configuration is described in a separate file, called an application schema.
MPI Library A strength of this MPI implementation is the movement of non-blocking communication requests, including those that result from buffered sends.
This is the real challenge of implementing MPI; everything else is mostly a straight forward wrapping of an underlying communication mechanism.
LAM / MPI allows messages to be buffered on the source end in any state of progression, including partially transmitted packets.
This capability leads to great portability and robustness.
The sophisticated message-advancing engine at the heart of the MPI library uses only a handful of routines to drive the underlying communication system.
Runtime flags decide which implementation of these low-level routines is used, so recompilation is not necessary.
The default implementation uses LAM's network message-passing subsystem, including its buffer daemon.
In this "daemon" mode, LAM's extensive monitoring features are fully available.
The main purpose of daemon-based communication is development, but depending on the application's decomposition granularity and communication requirement, it may also be entirely adequate for production execution.
MPI communication can bypass the LAM daemon for peak performance.
The other implementation of the MPI library's low-level communication intends to use the highest performance underlying mechanism, certainly bypassing the LAM daemon and connecting directly between application processes.
This is the "client to client" mode (C2C).
The availability of optimal C2C implementations will continue to change as architectures come and go.
At the least, LAM includes a TCP/IP implementation of C2C that bypasses the LAM daemon.
Guaranteed Envelope Resources Applications may fail, legitimately, on some implementations but not others due to an escape hatch in the MPI Standard called "resource limitations".
Most resources are managed locally and it is easy for an implementation to provide a helpful error code and/or message when a resource is exhausted.
Buffer space for message envelopes is often a remote resource (as in LAM) which is difficult to manage.
An overflow may not be reported (as in some other implementations) to the process that caused the overflow.
Moreover, interpretation of the MPI guarantee on message progress may confuse the debugging of an application that actually died on envelope overflow.
LAM advertises and guarantees a minimum level of internal resources for message delivery.
LAM 6.1 has a property called "Guaranteed Envelope Resources" (GER) which serves two purposes.
It is a promise from the implementation to the application that a minimum amount of envelope buffering will be available to each process pair.
Secondly, it ensures that the producer of messages that overflows this resource will be throttled or cited with an error as necessary.
Process Spawning A group of MPI processes can collectively create a group of new processes.
An intercommunicator is established for communication.
Dynamic Nodes and Fault Tolerance 119 An initial set of LAM nodes are started with the lamboot tool.
Afterwards, nodes can be added or deleted with the lamgrow and lamshrink tools.
Thus, a resource manager can adjust the resources allocated to a LAM session at runtime.
Both LAM nodes and MPI processes can be dynamically added and removed at runtime in a dynamic resource environment.
The unplanned removal of a node due to a machine crash or other fault is detected by LAM and handled in much the same way as a planned adjustment with lamshrink.
All surviving application processes are informed, via asynchronous signal, that a node has been removed.
The MPI library reacts by invalidating all communicators that include processes on the dead node.
Pending communication requests are marked with an error and future attempts to use invalid communicators also raise an error.
The application can detect these errors, free the invalid communicators and possibly create new processes.
Installation and Session Initiation LAM installs anywhere and uses the shell's search path at all times to find LAM and application executables.
A multicomputer is specified as a simple list of machine names in a file, which LAM uses to verify access (recon), start the environment (lamboot), and remove it (wipe).
CONCLUSION SUMMARY REFERENCES Microsoft ® Encarta ® 2009.
© 1993-2008 Microsoft Corporation.
All rights reserved.
LAM / MPI Parallel Computing Laboratory for Scientific Computing University of Notre Dame 120
