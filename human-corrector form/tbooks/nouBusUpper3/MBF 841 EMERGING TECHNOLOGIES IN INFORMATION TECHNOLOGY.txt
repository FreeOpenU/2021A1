 NATIONAL OP EN UNIVERSITY OF NIGERIA CO URSE CODE: MBF 841 COURSE TITLE: EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY  CO URSE GUIDE MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Course Team Gerald C. Okereke (Writer) - ECO COMM Dr. O. J. Onwe (Programme Leader) - NOUN Abimbola E. Adegbola (Coordinator) - NOUN NATIONAL O PEN UNIVERSITY OF NIGERIA ii  MBF 841 COURSE GUIDE National Open University of Nigeria Headquarters 14/16 Ahmadu Bello Way Victoria Island Lagos Abuja Office 5, Dar es Salaam Street Off Aminu Kano Crescent Wuse II, Abuja Nigeria E-mail: centralinfo@nou.edu.ng URL: www.nou.edu.ng Published By: National Open University of Nigeria Printed 2009 ISBN: 978-058-751-9 All Rights Reserved iii  CONTENTS PAGE Introdcution .. .. 1 Course Aim .. .. .. 1 Course Objectives .
.. 2 The Course Materials .
2 Study Units .. 3 The Assigmnet File .
4 Assessment .
.. 4 Final Examination and Grading .
4 Credit Units .
4 Presentation Schedule .
5 Course Overview .
5 iv  Introductiom This course, Emerging Technologies in Information Technology, is a compulsory course in the School of Business and Human Resources Management for those students pursuing Masters Degree (MBA) in business, finance and related subjects.
It is designed to aid business and financial managers in knowing information technologies readily available to enhance speed and accuracy in processing business information.
This course is also relevant for undergraduate students.
This course examined most recent developments in information technology world, especially the infrastruture/equipment.
But unit one (1) examined conceptual developments in information technology.
For each of the technologies examined, the history, advantages, disadvantages to life and business were discussed.
This course guide takes you through the nature of the course, the materials you are going to use and how you are to use materials to your maximum benefit.
It is expected that at least two hours should be devoted to the study of each course unit.
For each unit there assessments in the form of tutor-marked assignment.
You are advised to carry out the exercises immediatel y after studying the unit.
There will be tutorial lectures to organised for this course, this serves as an avenue to interact with course instructors who will communicate more clearly with you regarding the course.
You are advised to attend the tutorial lectures because it will enhance your understanding of the course.
Note that it is also through these tutorial lectures that you will submit your tutor-marked assignment and be assessed accordingly.
Course Aim This course is designed to unveil emerging technologies in information technology.
The term emerging does not necessarily mean brand new technology.
A technology is considered emerging if it is either a technology that is so new that businesses and individuals have not exploited it or if it is a technology that is fairly well established but businesses and individuals have not fully exploited it.
Regardless of what technologies are described as emerging or why, the emphasis of the course is to let the business managers know and learn about them to determine how best to use them.
The focus is on what they are, how they are being used, and how these technologies may be used in the future.
MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Course O bjectives A summary of the objectives of this course includes: • Have knowledge of the concept of management information system and how it evolved.
• Know the transition points of the information system triad.
• Identify the various applications of speech and speaker recognition.
• Trace the history and the development of the concept of video conferencing.
• Know the basic features of web conference.
• Define and know the different types of electronic s ystems.
• Identify the drawbacks associated with digital and electronic signatures.
• Identify the uses of multimedia.
• Know how World Wide Web grew to what it is today.
• Trace the trend in the development of personal digital assistants and sub notebooks.
• Know the health concerns in using bluetooth.
• Know the software and hardware support for iPod.
• Identify the advantages and disadvantages associated with the use of flash drive.
• Trace the trend and history in the development of MP3.
• Know the technology behind Internet radio and television.
• Identify the benefits of adopting blended method of learning.
• Identify the various uses of VoIP in business and other facets of life.
The Course Materials 1.
Course Guide 2.
Study Units 3.
Textbooks 4.
The Assignment File 5.
Tutorials ii  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Study Units This course consists of three (3) modules divided into sixteen (16) units.
Each module deals with major aspect of he course.
Module 1 Unit 1 Emergence of Information Systems and Technology (Information Triad) Unit 2 Speech and Speaker Recognition Unit 3 Electronic Video Conferencing Unit 4 Web Conferencing and WebCasting Unit 5 Electronic Payment Systems Module 2 Unit 1 Digital Signature and Electronic Signature Unit 2 Multimedia Unit 3 The World Wide Web Unit 4 Personal Digital Assistant and Sub Notebooks Unit 5 Bluetooth Module 3 Unit 1 The Ipod Technology Unit 2 USB Flash Drive Unit 3 MP3 Technology Unit 4 Internet Radio and Television Unit 5 Blended Learning Unit 6 Voice over Internet Protocol In studying the units, a minimum of 2 hours is expected of you.
Statrt by going through the unit objectives for you to know what you need to learn and know in the course of studying the unit.
At the end of the study of the unit, evaluate yourself to know if you ahave acheieved the objectives of the unit.
If not, you need to go through the unit again.
To help you ascertain how well how well you understood the course, there will be exercises mainly in the form of tutor-marekd assignments at the end of each unit.
At first attempt, try to answer the questions without necessarily having to go through the unit.
However, if you cannot proffer solutions offhand, then go through the unit to answer the questions.
iii  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY The Assignment F ile For each unit, you find one (1) or two (2) tutor-marked assignments.
These assignments serve two purposes: 1.
Self Eva luatio n: The tutor-marked assignment will assist you to thoroughly go through each unit, because you are a dvised to attempt to answer the questions immediately after studying each unit.
The questions are designed in such a way that at least one qstion must prompt a typical self-asessment test.
2.
Obatin Va luable Marks: The tutor-marked assignment is also valid means to obtain marks that will form part of your total score in this course.
It constitutes 30% of total marks obtainable in this course.
You are advised to go through the units thoroughly for you to be ablee to proffer correct solution to the turtor-marked assignment.
Asses sment You will be assessed and graded in this course through tutor-marked assignment and formal written examination.
The allocation of marks is as indicated below.
• Assignments = 30% • Examination = 70% Final Examination and Grading The final examination will consist of two (2) sections: 1.
Secto n 1: This is compulsory and weighs 40marks 2.
Section 2: This consists of six (6) questions out of which you are to answer (4) questions.
It weighs 60 marks.
The duration of the examination will be 3 hours.
Credit Units This course attracts three (3) credit units only.
iv  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY P resentation Schedule This constitutes the schedules dates and veneues for the tutorial classes, as well as ho and when to submit the tutorials.
All this will be communicated to you in due course.
Course Over view This indicates the units/topic, issues to be studied each week.
It is also included the duration of the course, revision week and examination week.
The details are as provided below: Unit Titke of Wo rk Week Assessment/ Activity Examina tion Course Guide Mo du le 1 1 Emergence of Information 1 Tutor-Marked Systems and Technology Asssignment (Information Triad) 2 Speech and Speaker Recognition 2 Tutor-Marked Asssignment 3 Electronic Video Conferencing 3 Tutor-Marked Asssignment 4 Web Conferencing and 4 Tutor-Marked WebCasting Asssignment 5 Electronic Payment Systems 5 Tutor-Marked Asssignment Mo dule 2 1 Digital Signature and Electronic 6 Tutor-Marked Signature Asssignment 2 Multimedia 7 Tutor-Marked Asssignment 3 The World Wide Web 8 Tutor-Marked Asssignment 4 Personal Digital Assistant and 9 Tutor-Marked Sub Notebooks Asssignment 5 Bluetooth 10 Tutor-Marked Asssignment Mo dule 3 1 The Ipod Technology 11 Tutor-Marked Asssignment 2 USB Flash Drive 12 Tutor-Marked Asssignment 3 MP3 Technology 13 Tutor-Marked Asssignment v  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 4 Internet Radio and Television 14 Tutor-Marked Asssignment 5 Blended Learning 15 Tutor-Marked Asssignment 6 Voice over Internet Protocol 16 Tutor-Marked Asssignment Revisio n a nd Exa minatio n 14 vi  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY MAIN COURSE Course Code MBF 841 Course Title Emerging Technologies in Information Technology Course Team C.A.C.
Chukwunka Developer/Writer - (Pentagon) Dr. Godwin I. Oyakhiromen (Editor) - NOUN Dr. O.A.
Adewale (Programme Leader) - NOUN Dr. A.T. Adegoke (Coordinator) - NOUN NATIO NAL OPEN UNIVERSITY OF NIG ERIA vii  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY National Open Universit y of Nigeria Headquarters 14/16 Ahmadu Bello Way Victoria Island Lagos Abuja Office No.
5 Dar es Salaam Street Off Aminu Kano Crescent Wuse II, Abuja Nigeria e-mail: centralinfo@nou.edu.ng URL: www.nou.edu.ng Published By: National Open Universit y of Nigeria Printed 2009 ISBN: 978-058-739-X All Rights Reserved viii  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY CONTENTS PAGE Module 1 .
.
.
.
1 Unit 1 Emergence of Information Systems and Technology (Information Triad) .
1 Unit 2 Speech and Speaker Recognition .. 17 Unit 3 Electronic Video Conferencing .
30 Unit 4 Web Conferencing and Webcasting .. 44 Unit 5 Electronic Payment Systems .
.
52 Module 2 .
.
72 Unit 1 Digital Signature and Electronic Signature .. 72 Unit 2 Multimedia 84 Unit 3 The World Wide Web ... 94 Unit 4 Personal Digital Assistant and Sub Notebooks .. 107 Unit 5 Bluetooth ...120 Module 3 .
... 132 Unit 1 The Ipod Technology ... 132 Unit 2 USB Flash Drive .
147 Unit 3 MP3 Technology .. ... 161 Unit 4 Internet Radio and Television .. 173 Unit 5 Blended Learning .. .. 183 Unit 6 Voice over Internet Protocol .. ..... 196 ix   MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY MODULE 1 Unit 1 Emergence of Information Systems and Technology (Information Triad) Unit 2 Speech and Speaker Recognition Unit 3 Electronic Video Conferencing Unit 4 Web Conferencing and Webcasting Unit 5 Electronic Payment Systems UNIT 1 EMERGENCE OF INFORMATIO N SYSTEMS AND TECHNOLOGY (INFORMATIO N TRIAD) CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 The Prehistory of Information Technology in the Firm 3.2 The First Wave: Transaction Processing 3.3 The Second Wave: Decision Support 3.4 The Third Wave: Business Communications 3.5 The Transition Points 3.6 Transcending the Triad: The Emergence of Enterprise Workflow 3.7 The State of the Firm Today 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N The primary problem of information technology today is our inability to speak about its workings, its motives and its benefits in a way that is intelligible and relevant to commerce.
The language of baud, byte and bandwidth is, as the philosopher Richard Rorty would put it, incommensurate with the language of the balance sheet and the board room.
This is due, for the most part, to the absence of: • a crisp, plain-speaking model of information technology in the abstract - what is IT and what can IT be used for?
1  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • an equally crisp history of the uses of information technology in the firm historically - where did IT come from and how have we used it?
• a model that allows business and technology to speculate together, in plain language, about the future uses of information technology within the firm, the problems and challenges associated with those future uses, and the decisions to be taken, today and in future.
Figure1 : The Basic Informatio n Triad Information Triad captures a simple, fundamental insight about the limits of information technology now and in the near future.
Put succinctly, all information technology does or can do for firms and individuals today is: capture and store data distribute data for consumption and analysis, to produce information connect people together into collaborative working environments where information is shared to produce knowledge.
These three vectors form the essential IT triad.
(Figure 3) 2.0 OBJECTIVES At the end of this unit, you should be able to: briefly trace the trend of information technology in firms know the technical characteristics of transaction processing, decision support s ystems and business communications differentiate the different waves in the development of information systems know the transition points of the information system triad 2  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY understand the emerging concept of enterprise workflow answer the question of what is the state of the firm today?
3.0 MAIN CONTENT 3.1 The P rehistory of Information Technology in the Firm For firms of more than 30 years standing, it is highly probable that information technology entered the firm through the accounting department, the director of finance and the firm s audit partner.
The first computers, after all, had been built to deal with particular complex problems of calculation, having to do with census data, and audit firms were quick to realise that the computer offered firms - particularly publicly-traded regulated firms - a way to: increase the accuracy of accounting and finance functions reduce the time required to complete formal reportage requirements and to close months, quarters and years reduce the number of people required to perform those tasks.
The use of IT was in short justified in both its target - the counting of money - and its use - the saving of money.
IT was fundamentally about cost reduction, and - far from being a strategic element - was a tactical convenience of a firm that was always doing something other than IT and which based its performance on something other than information.
3.2 The F irst Wave: Transaction Processing Once information technology had demonstrated its usefulness in accountancy, it began to be applied to money-counting and ultimately product-making and product-moving problems across the firm.
IT became transaction processing, first batch-oriented, and, later, as the pace of the economy increased, online transaction processing (OLTP), which is how we understand this leg of the Triad today.
This first wave of IT usage dominated the firm s model of how IT could be used from the 1960s until roughly 1980, and had a number of deep, salient characteristics worth enumerating.
3  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Table 1: Transa ctio n Pro cessing Technology chara cteristics These technical characteristics were in turn a reflection of some fundamental business assumptions that drove IT into its role in the firm, and focused IT at particular targets with particular objectives.
In the final analysis, the transaction processing leg of the triad:focused on the tasks of individual production workers ,regiments, controls and deskills that work in the interest of cost containment, variance elimination, and increased efficiency based on the premises that the risk to capital warrants both the deskilling and the accumulation of data, and that the internal efficiency of the firm is the primary guarantor of market success.
Table 2: Transa ctio n Pro cessing Business Chara cteristics 4  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.3 The Second Wave: Decision Support Beginning in about 1985, the average firm began to realise two things: Its quest for the automation of transaction processing was nearing completion, or, at the very least, that the Pareto effect had set in the accumulation of data did not in fact alleviate the risk to capital, which was in any case no longer the primary risk from a strategic perspective.
At this point, firms began to take the first tentative steps down the second leg of the Triad: the leg we refer to as decision support.
Now it is true that firms had, since the early 1970s, decision support capabilities, in the form of MIS and EIS systems, but these systems were flawed in conception in at least two regards: 1.
They assumed decision-making (or interesting decision-making at least) occurred in the central corporate regime, rather than in the peripheral business team environment.
2.
They assumed the kinds of analyses performed on operational data were uniform - that in effect the world was stable, and all firms instrumented themselves in the same basic fashion.
In fact, neither was true.
The important decisions, it turns out, were being made all the time everywhere, and the kinds of analyses being performed were highly idiosyncratic and in a more or less constant state of flux.
MIS and EIS were overturned, by 1990 or so, in favour of a new model of information distribution and analysis: client/server DSS targeted at the knowledge workers.
The fundamental shift we need to mark here is the shift from data - the sine qua non of the first leg of the Triad - to information, placed on the desktops of knowledge workers for analysis and action.
This shift exhibited itself as indicated in the chart below.
5  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Table 3: Decision Suppo rt Technica l characteristics And there were of course similar underlying shifts in the business model, shifts that both forced and supported the decline of transaction processing and the rise of decision support as the dominant focus within firms.
The Decision Su pport Leg of the Tria d Focused on the decisions of individual knowledge workers supports and enables that work in the interest of management-by-fact, and tactical revenue-enhancement opportunities based on the premises that the risk of incomplete information warrants the re-centering of the knowledge worker and the desktop in the enterprise, even though both are fundamentally destabilising.
6  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.4 The Third Wave: Business Communications Early adopter firms that made the transition from the first to the second leg of the Triad in the early 80s made, in the late 80s, a crucial discovery - knowledge workers are gregarious.
They are relentless chatterers and scribblers, and their capabilities are amplified by two kinds of networks: 1.
A more or less formal network of supplier-consumer relationships among knowledge and production workers embodied in the firm s (often informal) business processes.
2.
A very informal, cross-functional, cross-process network of contacts and associations among workers in and across firms.
Both of these affiliational structures, it turned out, needed to be supported explicitly by IT for the decision-making process enabled by the decision support vector to actually result in effective decision-making and execution on taken decisions.
Hence the third leg of the triad - business communications - which focuses on providing the infrastructure and agency to connect people, electronically, to other people across time, space and organisational regime.
Electronic messaging, use of collaborative work environments like Notes and the World Wide Web, even desktop video conferencing - - all these technologies focus on the relationships among people, a model in which data-become-information is transformed, through collaboration, into actionable stuff: into knowledge.
Like the vectors before it, business communication once again modified the core technical characteristics of the firm s IT focus.
7  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Ta ble 4 : Business Co mmuni cations Technical Characteristics And, once again, changes in these technical characteristics were driven by changes in the underlying strategic management regime.
This leg of the triad introduces more fundamental changes in the conceptual models of IT than either of the two previous vectors.
Business communications: focused on the collaborative behaviour of networks and business teams facilitates that collaboration and those networks in the interest of knowledge development and marketplace action based on knowledge developed by teams based on the premises that the increased process-orientation and value-chain orientation of the marketplace must be mirrored and supported by IT.
8  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.5 The Transition Points Each leg of the Triad, we can see, represents the dominance of a particular way of thinking about the design centre of IT projects.
The first leg, transaction processing, focused on routinising the tasks of the individual production worker in the name of deskilling and rendering more efficient that worker.
The second leg of the Triad, decision support, focuses on enabling the decision-making of the individual knowledge worker in the name of empowering and rendering more effective that worker.
The third leg, business communications, focuses on interconnecting all the employees of the firm, as well as employees in the firm s suppliers, downstream channel partners and customers, all in the name of facilitating their closer collaboration.
The shifts at the transition points are instructive, most importantly because they tell us something about the future: the next sets of transitions we will encounter.
Transition point 1 was essentially the decision to use IT to deskill and routinise production work.
This was a fairly conventional transition, and did not fundamentally disrupt the firm s culture or organisation, since in most cases this production work was already culturally devalued by the firm.
The essential drivers for this change were: (a) the availability of a solid-state-based technology that promised to do for the white collar factory what mechanical and electro- mechanical technology had done for the shop floor, and 9  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY (b) compelling existence proofs of the cost-reduction and cycle-time benefits of IT applied in this fashion (the accountancy function).
Fig ure 3 : Transition Po ints in the Triad Transition 2 was significantly more disruptive.
The fundamental decision was to open the data vaults and turn data over to the people who actually made the salient business decisions for the firm.
The change was heralded, if not catalysed, by two important drivers: the personal computer, and the design centre for the personal computer, the knowledge worker.
In the same way that a previous generation of white collar workers depended on calculators, typewriters and telephones to do their work, knowledge workers after 1988 or so were dependent on the availability of personal computers, and personal productivity tools (most particularly the spreadsheet and the word processor) to produce their intellectual property.
But Transition 2 did not disrupt some of the fundamental assumptions of Transition 1: specifically, the focus of IT was still the orchestration of the tasks of individuals.
The individual changed - from a production worker to a knowledge worker - and the type of task changed: from making and moving things, from data entry associated with making and moving things, to taking and making decisions.
And, if the guild of knowledge work had not been one in which the inspection and rationalisation of work processes violated the fundamental unspoken contract between the knowledge worker and the firm, I believe we might have tried to routinise and automate that class of task as well.
10  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY It is Transition 3 that finally breaks the orchestration model that focuses on individuals and tasks.
Driven in this case by (a) process-oriented management disciplines like TQM and BPR and (b) the attendant cultural emphasis on teaming, connectedness and asynchronous work, Transition 3 breaks open the whole concept of IT-as-orchestration by focusing IT on connecting collaborators, no t on managing the output (in the case of OLTP) or the input (in the case of DSS) of the tasks of individuals.
Transition 3 creates an infrastructure that is nearly purely facilitative.
Everyone is connected to everyone else, and what passes along those electronic connections is, by the time Transition 3 is complete, inscrutable to the firm: outside the pale of IT.
3.6 Transcending the Triad: The Emergence of Enterprise Workflow In other words, the model implies some cyclicality.
Once we have completed the business communications regime, we are right back where we started, at the fundamental question IT was brought into the firm to address: how to make rational, inspect-able and more efficient the work of employees, and thereby increase the production of economic value for the firm, its customers and its owners.
There is, it appears, a fourth transition point.
But what is it?
We refer to this transition as the transition to enterprise workflow: to the use of the infrastructure created by the triad to create a set of enterprise- wide applications that embody process logic, expose knowledge work to scrutiny and instrumentation, and explicitly orchestrate the tasks inside the firm s components of the value chain.
Enterprise workflow is the management of commitments: within the firm, between the firm and suppliers, between the firm and channel partners, between the firm and customers.
Enterprise workflow cannot come into existence until the Triad is complete and functional.
11  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Figure 4: Enterprise Wo rkflo w 12  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 13  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY We can speculate on the business drivers for these changes.
But in the final analysis, we know too little about the socio-cultural context in which this paradigm comes to dominate IT application within the firm to be sure exactly how it manifests itself.
Breaking open the guild of knowledge work is a problematic cultural and ethical issue, on which far too little theoretical work has been done.
3.7 The State of the Firm Today So, assuming we are more or less accurate in our historical assessment and projections, the question arises: where are we exactly?
In what state is the typical firm s Information Triad?
In our experience, the typical firm is:80% complete in transaction processing, 25% - 40% complete in decision support and about 15% complete in business communications.
14  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In the case of transaction processing, the following are generally the case: The firm is capturing almost al l the data it can capture as efficiently as it can capture it, the remaining data required does not belong to and cannot be generated by the firm; it is external data held by proto-data syndicates, regulatory bodies or the market at large.A few critical strategic transaction processing application opportunities exist in most firms: new applications that require new development technologies and methodologies; some firms, unaware of the Triad, are re-engineering their existing, functional OLTP applications instead of migrating to the second leg of the triad.
In the case of decision support, most firms are doing little more than dumping (in many cases dirty and incomplete) data sets on the desktops of knowledge workers for analysis.
What remains to be done in this leg of the triad?
1.
Completion and cleansing of the decisional data 2.
Explicit support for the decision making process - structuring, analysis, and option selection 3.
Explicit support for the propagation, monitoring and evaluation of taken decisions.
In the case of business communications, little exists in the typical firm beyond isolated, unconnected electronic mail domains.
External connectivity, message-enabled applications, collaborative work environment s, knowledge management systems - all remain to be constructed.
15  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 4.0 CO NCLUSIO N The search for a model and a language to bridge the gap between IT professionals and their business counterparts is nothing new.
Since at least the late sixties, we have seen, regularly, the release of methodologies, models and formulae that promised to produce a common language for busines s and IT, a common language that was usually neither precisely the language of IT nor precisely the language of business.
Such efforts strike one as analogous to Esperanto: a language no one speaks as a native is no language at all.
The common language of IT and the business is the language of the marketplace, the language of commerce, the language of the balance sheet and the board room.
IT professionals may feel allegiance to an IT guild practice first and the firms they work for second, but this is not the necessary state of affairs.
It is in fact the product of where and what we have been, together.
What we need to transcend the gap, rather than bridging it, is common cause.
5.0 SUMMARY The primary problem of information technology today is our inability to speak about its workings, its motives and its benefits in a way that is intelligible and relevant to commerce.
For firms of more than 30 years standing, it is highly probable that information technology entered the firm through the accounting department, the director of finance and the firm s audit partner.
Once information technology had demonstrated its usefulness in accountancy, it began to be applied to money-counting and ultimately product-making and product-moving problems across the firm.
The fundamental shift we need to make here is the shift from data - the sine qua non of the first leg of the Triad - to information, placed on the desktops of knowledge workers for analysis and action.
Early adopter firms that made the transition from the first to the second leg of the Triad in the early 80s made, in the late 80s, a crucial discovery - knowledge workers are gregarious.
Each leg of the Triad, we can see, represents the dominance of a particular way of thinking about the design centre of IT projects.
Once we have completed the business communications regime, we are right back where we started, at the fundamental question IT was brought into the firm to address: how to make rational, inspect-able and more 16  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY efficient the work of employees, and thereby increase the production of economic value for the firm, its customers and its owners.
So, assuming we are more or less accurate in our historical assessment and projections, the question arises: where are we exactly?
In what state is the typical firm s Information Triad?
6.0 TUTOR-MARKED ASSIGNMENT Identify the components of the transaction processing leg of the information triad.
Briefly discuss each of the transition points/leg of the information triad.
7.0 REF ERENCE/FURTHER READING Demarest, M., "The Information Triad: A Model of Past, Current and Future Information Technology Utilization in the Firm" DSSResources.COM, 06/29/2007.
17  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 2 SPEECH AND SPEAKER RECOGNITION CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History 3.2 Applications 3.3 Performance of Speech Recognition Systems 3.4 Speaker Recognition 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N Speech reco gnitio n (also known as a uto matic speech recognition or computer speech recognitio n) converts spoken words to machine- readable input (for example, to key presses, using the binary code for a string of character codes).
The term voice recognition may also be used to refer to speech recognition, but more precisely refers to speaker recog nitio n, which attempts to identify the person speaking, as opposed to what is being said.
Speech recognition applications include voice dialing (e.g., "Call home"), call routing (e.g., "I would like to make a collect call"), domotic appliance control and content-based spoken audio search (e.g., find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g., a radiology report), speech-to-text processing (e.g., word processors or emails), and in aircraft cockpits (usually termed Direct Voice Input).
2.0 OBJECTIVES At the end of this unit, you should be able to: define speech and speaker recognition trace the history and development of speech and speaker recognition identify the various applications of speech and speaker recognition differentiate speech and speaker recognition identify the features of speech and speaker recognition.
18  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.0 MAIN CONTENT 3.1 H istory One of the most notable domains for the commercial application of speech recognition in the United States has been health care and in particular the work of the medical transcriptionist (MT).
According to industry experts, at its inception, speech recognition (SR) was sold as a way to completely eliminate transcription rather than make the transcription process more efficient, hence it was not accepted.
It was also the case that SR at that time was often technically deficient.
Additionally, to be used effectively, it required changes to the ways physicians worked and documented clinical encounters, which many if not all were reluctant to do.
The biggest limitation to speech recognition automating transcription, however, is seen as the software.
The nature of narrative dictation is highly interpretive and often requires judgment that may be provided by a real human but not yet by an automated system.
Another limitation has been the extensive amount of time required by the user and/or system provider to train the software.
A distinction in ASR is often made between "artificial syntax systems" which are usually domain-specific and "natural language processing" which is usually language-specific.
Each of these types of application presents its own particular goals and challenges.
3.2 Applications Health Ca re In the health care domain, even in the wake of improving speech recognition technologies, medical transcriptionists (MTs) have not yet become obsolete.
Many experts in the field [who?]
anticipate that with increased use of speech recognition technology, the services provided may be redistributed rather than replaced.
Speech recognition can be implemented in front-end or back-end of the medical documentation process.
Front-End SR is where the provider dictates into a speech-recognition engine, the recognised words are displayed right after they are spoken, and the dictator is responsible for editing and signing off on the document.
It never goes through an MT/editor.
Back-End SR or Deferred SR is where the provider dictates into a digital dictation system, and the voice is routed through a speech-recognition machine and the recognised draft document is routed along with the 19  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY original voice file to the MT/editor, who edits the draft and finalises the report.
Deferred SR is being widely used in the industry currently.
Many Electronic Medical Records (EMR) applications can be more effective and may be performed more easily when deployed in conjunction with a speech-recognition engine.
Searches, queries, and form filling may all be faster to perform by voice than by using a keyboard.
Military High-Performance Fighter Aircraft Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft.
Of particular note are the US programme in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France on installing speech recognition systems on Mirage aircraft, and programmes in the UK dealing with a variety of aircraft platforms.
In these programs, speech recognisers have been operated successfully in fighter aircraft with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight displays.
Generally, only very limited, constrained vocabularies have been used successfully, and a major effort has been devoted to integration of the speech recogniser with the avionics system.
Some important conclusions from the work were as follows: Speech recognition has definite potential for reducing pilot workload, but this potential was not realised consistently.
Achievement of very high recognition accuracy (95% or more) was the most critical factor for making the speech recognition system useful with lower recognition rates, pilots would not use the s ystem.
More natural vocabulary and grammar, and shorter training times would be useful, but only if very high recognition rates could be maintained.
Laboratory research in robust speech recognition for military environments has produced promising results which, if extendable to the cockpit, should improve the utility of speech recognition in high- performance aircraft.
Working with Swedish pilots flying in the JAS-39 Gripen cockpit, England (2004) found recognition deteriorated with increasing G-loads.
It was also concluded that adaptation greatly improved the results in all 20  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY cases and introducing models for breathing was shown to improve recognition scores significantly.
Contrary to what might be expected, no effects of the broken English of the speakers were found.
It was evident that spontaneous speech caused problems for the recogniser, as could be expected.
A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.
The Euro-fighter Typhoon currently in service with the UK RAF employs a speaker-dependent system, i.e.
it requires each pilot to create a template.
The system is not used for any safety critical or weapon critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions.
Voice commands are confirmed by visual and/or aural feedback.
The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands.
Helico pters The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the fighter environment.
The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot generally does not wear a facemask, which would reduce acoustic noise in the microphone.
Substantial test and evaluation programmes have been carried out in the post decade in speech recognition systems applications in helicopters, notably by the US Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK.
Work in France has included speech recognition in the Puma helicopter.
There has also been much useful work in Canada.
Results have been encouraging, and voice applications have included: control of communication radios; setting of navigation systems; and control of an automated target handover system.
As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness.
Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment.
Much remains to be done both in speech recognition and in overall speech recognition technology, in order to consistently achieve performance improvements in operational settings.
21  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Battle Management Battle management command centres generally require rapid access to and control of large, rapidly changing information databases.
Commanders and system operators need to query these databases as conveniently as possible, in an eyes-bus y environment where much of the information is presented in a display format.
Human machine interaction by voice has the potential to be very useful in these environments.
A number of efforts have been undertaken to interface commercially available isolated-word recognisers into battle management environments.
In one feasibility study, speech recognition equipment was tested in conjunction with an integrated information display for naval battle management applications.
Users were very optimistic about the potential of the system, although capabilities were limited.
Speech understanding programs sponsored by the Defense Advanced Research Projects Agency (DARPA) in the US has focused on this problem of natural speech interface.
Speech recognition efforts have focused on a database of continuous speech recognition (CSR), large- vocabulary speech which is designed to be representative of the naval resource management task.
Significant advances in the state-of-the-art in CSR have been achieved, and current efforts are focused on integrating speech recognition and natural language processing to allow spoken language interaction with a naval resource management system.
Tra ining Air Tra ffic Controllers Training for military (or civilian) air traffic controllers (ATC) represents an excellent application for speech recognition systems.
Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog which the controller would have to conduct with pilots in a real ATC situation.
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel.
Air controller tasks are also characterised by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task.
The U.S.
Naval Training Equipment Center has sponsored a number of developments of prototype ATC trainers using speech recognition.
Generally, the recognition accuracy falls short of providing graceful interaction between the trainee and the system.
However, the prototype training systems have demonstrated a significant potential for voice interaction in these s ystems, and in other training applications.
The U.S. 22  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Navy has sponsored a large-scale effort in ATC training systems, where a commercial speech recognition unit was integrated with a complex training system including displays and scenario creation.
Although the recogniser was constrained in vocabulary, one of the goals of the training programs was to teach the controllers to speak in a constrained language, using specific vocabulary specifically designed for the ATC task.
Research in France has focused on the application of speech recognition in ATC training systems, directed at issues both in speech recognition and in application of task-domain grammar constraints.
The USAF, USMC, US Army, and FAA are currently using ATC simulators with speech recognition from a number of different vendors, including UFA, Inc. and Adacel Systems Inc (ASI).
This software uses speech recognition and synthetic speech to enable the trainee to control aircraft and ground vehicles in the simulation without the need for pseudo pilots.
Another approach to ATC simulation with speech recognition has been created by Supremis.
The Supremis system is not constrained by rigid grammars imposed by the underlying limitations of other recognition strategies.
Telephony a nd Other Doma ins ASR in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread.
Despite the high level of integration with word processing in general personal computing, however, ASR in the field of document production has not seen the expected increases in use.
Current speech-to-text programs are too large and require too much CPU power to be practical for the Pocket P C. Speech is used mostly as a part of User Interface, for creating pre-defined or custom speech commands.
Leading software vendors in this field are: Microsoft Corporation (Microsoft Voice Command); Nuance Communications (Nuance Voice Control); Vito Technology (VITO Voice2Go); Speereo Software (Speereo Voice Translator).
Peo ple with Disabilities People with disabilities are another part of the population that benefit from using speech recognition programmes.
It is especially useful for people who have difficulty with or are unable to use their hands, from mild repetitive stress injuries to involved disabilities that require alternative input for support with accessing the computer.
In fact, people who used the keyboard a lot and developed RSI became an urgent early 23  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY market for speech recognition.
Speech recognition is used in deaf telephony, such as spinvox voice-to-text voicemail, relay services, and captioned telephone.
Further applications Automatic translation Automotive speech recognition (e.g., Ford Sync) Telematics (e.g.
vehicle Navigation Systems) Court reporting (Realtime Voice Writing) Hands-free computing: voice command recognition computer user interface Home automation Interactive voice response Mobile telephony, including mobile email Multimodal interaction Pronunciation evaluation in computer-aided language learning applications Robotics Transcription (digital speech-to-text).
Speech-to-Text (Transcription of speech into mobile text messages) Air Traffic Control Speech Recognition 3.2 Performance of Speech Recognition Systems The performance of speech recognition systems is usually specified in terms of accuracy and speed.
Accuracy may be measured in terms of performance accuracy which is usually rated with word error rate (WER), whereas speed is measured with the real time factor.
Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
Most speech recognition users would tend to agree that dictation machines can achieve very high performance in controlled conditions.
There is some confusion, however, over the interchangeability of the terms "speech recognition" and "dictation" .
Commercially available speaker-dependent dictation systems usually require only a short period of training (sometimes also called `enrollment') and may successfully capture continuous speech with a large vocabulary at normal pace with a very high accuracy.
Most commercial companies claim that recognition software can achieve between 98% to 99% accuracy if operated under optimal conditions.
Optimal conditions' usually assume that users: 24  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY have speech characteristics which match the training data, can achieve proper speaker adaptation, and work in a clean noise environment (e.g.
quiet office or laboratory space).
This explains why some users, especially those whose speech is heavily accented, might achieve recognition rates much lower than expected.
Speech recognition in video has become a popular search technology used by several video search companies.
Limited vocabulary systems, requiring no training, can recognize a small number of words (for instance, the ten digits) as spoken by most speakers.
Such systems are popular for routing incoming phone calls to their destinations in large organizations.
Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms.
Hidden Markov models (HMMs) are widely used in many systems.
Language modeling has many other applications such as smart keyboard and document classification.
Hidden Ma rkov Mo del (Hmm)-B ased Speech Reco gnitio n Modern general-purpose speech recognition systems are generally based on HMMs.
These are statistical models which output a sequence of symbols or quantities.
One possible reason why HMMs are used in speech recognition is that a speech signal could be viewed as a piecewise stationary signal or a short-time stationary signal.
That is, one could assume in a short-time in the range of 10 milliseconds, speech could be approximated as a stationary process.
Speech could thus be thought of as a Markov model for many stochastic processes.
Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use.
In speech recognition, the hidden Markov model would output a sequence of n-dimensional real-valued vectors (with n being a small integer, such as 10), outputting one of these every 10 milliseconds.
The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients.
The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians which will give likelihood for each observed vector.
Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the 25  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY separate words and phonemes.
Described above are the core elements of the most common, HMM- based approach to speech recognition.
Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above.
A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realisations as HMM states); it would use cepstral normalisation to normalise for different speaker and recording conditions; for further speaker normalisation it might use vocal tract length normalisation (VTLN) for male-female normalisation and maximum likelihood linear regression (MLLR) for more general speaker adaptation.
The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semitied covariance transform (also known as maximum likelihood linear transform, or MLLT).
Many systems use so- called discriminative training techniques which dispense with a purely statistical approach to HMM parameter estimation and instead optimise some classification-related measure of the training data.
Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).
Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model which includes both the acoustic and language model information, or combining it statically beforehand (the finite state transducer, or FST, approach).
Dynamic Time Warping (DTW) -Ba sed Speech Recognition Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
Dynamic time warping is an algorithm for measuring similarity between two sequences which may vary in time or speed.
For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another they were walking more quickly, or even if there were accelerations and decelerations during the course of one observation.
DTW has been applied to video, audio, and graphics indeed, any data which can be turned into a linear representation can be analysed with 26  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY DTW.
A well known application has been automatic speech recognition, to cope with different speaking speeds.
In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g.
time series) with certain restrictions, i.e.
the sequences are "warped" non-linearly to match each other.
This sequence alignment method is often used in the context of hidden Markov models.
3.3 Speaker Recognition Spea ker Recognition is the computing task of validating a user's claimed identity using characteristics extracted from their voices.
There is a difference between speaker recognition (recognising who is speaking) and speech recognition (recognising what is being said).
These two terms are frequently confused, as is voice recognition.
Voice recognition is a synonym for speaker, and thus not speech, recognition.
In addition, there is a difference between the act of authentication (commonly referred to as spea ker v erification or spea ker a uthentica tion) and identification.
Speaker recognition has a history dating back some four decades and uses the acoustic features of speech that have been found to differ between individuals.
These acoustic patterns reflect both anatomy (e.g., size and shape of the throat and mouth) and learned behavioural patterns (e.g., voice pitch, speaking style).
Because speaker verification has earned speaker recognition its classification as a "behavioural biometric."
Verification versus Identifica tion There are two major applications of speaker recognition technologies and methodologies.
If the speaker claims to be of a certain identity and the voice is used to verify this claim this is called verification or authentication.
On the other hand, identification is the task of determining an unknown speaker's identity.
In a sense speaker verification is a 1:1 match where one speaker's voice is matched to one template (also called a "voice print" or "voice model") whereas speaker identification is a 1: N match where the voice is compared against N templates.
From a security perspective, identification is different from verification.
For example, presenting your passport at border control is a verification process - the agent compares your face to the picture in the document.
Conversely, a police officer comparing a sketch of an assailant against a 27  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY database of previously documented criminals to find the closest match (es) is an identification process.
Speaker verification is usually employed as a "gatekeeper" in order to provide access to a secure system (e.g.
: telephone banking).
These systems operate with the user s knowledge and typically require their cooperation.
Speaker identification systems can also be implemented covertly without the user's knowledge to identify talkers in a discussion, alert automated systems of speaker changes, check if a user is already enrolled in a system, etc.
In forensic applications, it is common to first perform a speaker identification process to create a list of "best matches" and then perform a series of verification processes to determine a conclusive match.
Variants of Speaker Recog nitio n Each speaker recognition system has two phases: Enrollment and verification.
During enrollment, the speaker's voice is recorded and typically a number of features are extracted to form a voice print, template, or model.
In the verification phase, a speech sample or "utterance" is compared against a previously created voice print.
For identification systems, the utterance is compared against multiple voice prints in order to determine the best match (es) while verification systems compare an utterance against a single voice print.
Because of the process involved, verification is faster than identification.
Speaker recognition systems fall into two categories: text-dependent and text-independent.
If the text must be the same for enrollment and verification this is called text-dependent recognition.
In a text-dependent system, prompts can either be common across all speakers (e.g: a common pass phrase) or unique.
In addition, the use of shared-secrets (e.g: passwords and PINs) or knowledge-based information) can be employed in order to create a multi-factor authentication scenario.
Text-independent systems are most often used for speaker identification as they require very little if any cooperation by the speaker.
In this case the text during enrollment and test is different.
In fact, the enrollment may happen without the user's knowledge, as in the case for many forensic applications.
As text-independent technologies do not compare what was said at enrollment and verification, verification applications tend to also employ speech recognition to determine what the user is saying at the point of authentication.
Techno log y The various technologies used to process and store voice prints include 28  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY frequency estimation, hidden Markov models, gaussian mixture models, pattern matching algorithms, neural networks, matrix representation and decision trees.
Some systems also use "anti-speaker" techniques, such as cohort models, and world models.
Ambient noise levels can impede both collections of the initial and subsequent voice samples.
Noise reduction algorithms can be employed to improve accuracy, but incorrect application can have the opposite effect.
Performance degradation can result from changes in behavioral attributes of the voice and from enrollment using one telephone and verification on another telephone ("cross channel").
Integration with two-factor authentication products is expected to increase.
Voice changes due to aging may impact s ystem performance over time.
Some systems adapt the speaker models after each successful verification to capture such long-term changes in the voice, though there is debate regarding the overall security impact imposed by automated adaptation.
Capture of the biometric is seen as non-invasive.
The technology traditionally uses existing microphones and voice transmission technology allowing recognition over long distances via ordinary telephones (wired or wireless).
4.0 CONCLUSION With the speech recognition and speaker recognition technologies it is now much easier to input data and get out the out put fast.
This used to be a technology limited to a few people especially the disabled, but is now made available to all.
The usage is still limited to a privileged few because of the cost.
5.0 SUMMARY • Speech recognition (also known as automatic speech recognition or computer speech recognition) converts spoken words to machine-readable input (for example, to key presses, using the binary code for a string of character codes).
• One of the most notable domains for the commercial application of speech recognition in the United States has been health care and in particular the work of the medical transcriptionist (MT).
• Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft.
• Training for military (or civilian) air traffic controllers (ATC) represents an excellent application for speech recognition systems 29  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • The performance of speech recognition systems is usually specified in terms of accuracy and speed • Speaker recognition is the computing task of validating a user's claimed identity using characteristics extracted from their voices.
• The various technologies used to process and store voice prints include frequency estimation, hidden Markov models, gaussian mixture models, pattern matching algorithms, neural networks, matrix representation and decision trees 6.0 TUTOR-MARK ED ASSIGNMENT 1.
Mention the applications of speech recognition in the military.
2.
Briefly discuss the variants of speaker recognition.
7.0 REFERENCES/F URTHER READ ING Karat, Clare-Marie; Vergo, John & Nahamoo, David (2007).
"Conversational Interface Technologies", in Sears, Andrew & Jacko, Julie A., The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications (Human Factors and Ergonomics), Lawrence Erlbaum Associates Inc, ISBN 978-0805858709 .
Cole, Ronald; Mariani, Joseph & Uszkoreit, Hans et al., eds.
(1997)Survey of the State of the Art in Human LanguageTtechnology.
Cambridge Studies In Natural Language Processing, XII XIII.
Cambridge University Press, ISBN 0-521- 59277-1.
Junqua, J.C. & Haton, J.-P. (1995).
Robustness in Automatic Speech Recognition: Fundamentals and Applications.
Kluwer Academic Publishers, ISBN 978-0792396468 .
Eurofighter Direct Voice Input Opportunities for Advanced Speech Processing in Military Computer- Based Systems* 30  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 3 ELECTRONIC VIDEO CONFERENCING CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History 3.2 Technology 3.3 Issues 3.4 Standards 3.5 Impact on the General Public 3.6 Impact on Education 3.7 Impact on Medicine and Health 3.8 Impact on Business 3.9 Impact on Law 3.10 Application in Teaching 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N A v ideoconference (also known as a video teleco nference) is set of interactive telecommunicatio n techno logies which allow two or more locations to interact via two-way video and audio transmissions simultaneously.
It has also been called visual co lla bo ratio n and is a type of groupware.
It differs from videopho ne in that it is designed to serve a conference rather than individuals.
Simple analog videoconferences could be established as early as the invention of the television.
Such videoconferencing systems usually consisted of two closed-circuit television systems connected via cable.
During the first manned space flights, NASA used two radiofrequency (UHF or VHF) links, one in each direction.
TV channels routinely use this kind of videoconferencing when reporting from distant locations, for instance.
Then mobile links to satellites using specially equipped trucks became rather common.
31  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 2.0 OBJECTIVES At the end of this unit, you should be able to: define video conferencing trace the history and the development of the concept of video conferencing have understanding of how the technology works understand the impacts of the technology to life, education and business explain the standards that guide the operation of video conferencing.
3.0 MAIN CONTENT 3.1 History Video conferencing uses telecommunications of audio and video to bring people at different sites together for a meeting.
This can be as simple as conversation between two people in the private offices (point- to-point) or involve several sites (multi-point) with more than one person in large rooms at different sites.
Besides the audio and the visual transmission of meeting activities, video conferencing can be used to share documents, computer and displayed information, and whiteboards.
Simple analog video conferencing could be established as early as the invention of the television.
Such video conferencing systems consisted of two televisions, closed circuit television systems connected via cable.
During the first manned space flights, NASA used two radio frequency UHF or VHF links, one in each direction.
TV channels routinely use this kind of video conferencing when reporting from different locations, for instance.
Then mobile links to satellites using specially equipped trucks became rather common video conferencing, first demonstrated in 1968.
This technique was very expensive, though, and could not be used for mundane applications, such as telemedicine, distance education, business meetings, and so on, particularly in long distance applications.
Attempts at using normal telephony networks to transmit slow scan video, such as the first systems developed by AT&T, failed mostly due to the poor picture quality and the lack of efficient compression techniques.
The greater bandwidth and 6 Mbit/s bit rate picturephone in the 1970s also did not cause the service to prosper.
It was in the 1980s that digital telephony transmission networks became possible, such as ISDN, assuring a minimum bit rate for compressed video and audio transmission.
The first dedicated systems, such as those manufactured by pioneering VTC firms like Picture Tel, started to appear in the market as ISDN networks were expanding throughout the 32  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY world.
Video teleconference systems throughout the 1990s rapidly evolved from highly expensive proprietary equipment, software and network requirement standards based technology that is readily available to the general public at a reasonable cost.
Finally, in the 1990s, IP (internet Protocol) based video conferencing became possible, and more efficient video compressing technologies were developed, permitting desktop, or personal computer (PC) based video conferencing.
In 1992, CU-SeeMe was developed by Tim Dorcey et al.
IVS was designed by INRIA, VTC arrived to the masses and free services, web plugins and software such as NetMeeting, MSN Messenger, Yahoo MessengerSightSpeed, Skype and others brought cheap, albeit low quality, VTC.
3.2 Technology Dual pla sma display video conferencing sy stem: The screen on the left is primarily used to show people during the conference or the user interface when setting up the call.
The one on the right shows data in this case but can display a 2nd 'far site' in a multipoint call.
The core technology used in a video teleconference (VTC) s ystem is digital compression of audio and video streams in real time.
The hardware or software that performs compression is called a codec (coder/decoder).
Compression rates of up to 1:500 can be achieved.
The resulting digital stream of 1s and 0s is subdivided into labeled packets, which are then transmitted through a digital network of some kind (usually ISDN or IP).
The use of audio modems in the transmission line allow for the use of POTS, or the Plain Old Telephone System, in some low-speed applications, such as video telephony, because they convert the digital pulses to/from analog waves in the audio spectrum range.
The other components required for a VTC s ystem include: Video input : video camera or webcam Video output : computer monitor , television or projector Audio input: microphones Audio o utput: usually loudspeakers associated with the display device or telephone Data transfer: analog or digital telephone network, LAN or Internet There are basically two kinds of VTC systems: Dedicated Sy stems (manufactured by companies such as Polycom, Sony, Tandberg, Radvision Ltd.,and LifeSize) have all required components packaged into a single piece of equipment, usually a console with a high quality remote controlled video camera.
These cameras can be controlled at a distance to pan left and right, tilt up and 33  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY down, and zoom.
They became known as PTZ cameras.
The console contains all electrical interfaces, the control computer, and the software or hardware-based codec.
Omni-directional microphones are connected to the console, as well as a TV monitor with loudspeakers and/or a video projector.
There are several types of dedicated VTC devices: Large group VTC are non-portable, large, more expensive devices used for large rooms and auditoriums.
Small group VTC are non-portable or portable, smaller, less expensive devices used for small meeting rooms.
Individual VTC are usually portable devices, meant for single users, have fixed cameras, microphones and loudspeakers integrated into the console.
Desktop Sy stems are add-ons (hardware boards, usually) to normal PCs, transforming them into VTC devices.
A range of different cameras and microphones can be used with the board, which contains the necessary codec and transmission interfaces.
Most of the desktop systems work with the H.323 standard.
Video conferences carried out via dispersed PCs are also known as e-meetings.
Echo Ca ncellatio n A fundamental feature of professional VTC systems is acoustic echo cancellation (AEC).
AEC is an algorithm which is able to detect when sounds or utterances re-enter the audio input of the VTC codec, which came from the audio output of the same system, after some time delay.
If unchecked, this can lead to several problems including: 1) the remote party hearing their own voice coming back at them (usually significantly delayed) 2) strong reverberation, rendering the voice channel useless as it becomes hard to understand and 3) howling created by feedback.
Echo cancellation is a processor- intensive task that usually works over a narrow range of sound delays.
Multipoint Video Conferencing Simultaneous video conferencing among three or more remote points is possible by means of a Multipoint Control Unit (MCU).
This is a bridge 34  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY that interconnects calls from several sources (in a similar way to the audio conference call).
All parties call the MCU unit, or the MCU unit can also call the parties which are going to participate, in sequence.
There are MCU bridges for IP and ISDN-based video conferencing.
There are MCUs which are pure software, and others which are a combination of hardware and software.
An MCU is characterised according to the number of simultaneous calls it can handle, its ability to conduct transposing of data rates and protocols, and features such as Continuous Presence, in which multiple parties can be seen onscreen at once.
MCUs can be stand-alone hardware devices, or they can be embedded into dedicated VTC units.
Some systems are capable of multipoint conferencing with no MCU, stand-alone, embedded or otherwise.
These use a standards-based H.323 technique known as "decentralized multipoint", where each station in a multipoint call exchanges video and audio directly with the other stations with no central "manager" or other bottleneck.
The advantages of this technique are that the video and audio will generally be of higher quality because they don't have to be relayed through a central point.
Also, users can make ad-hoc multipoint calls without any concern for the availability or control of an MCU.
This added convenience and quality comes at the expense of some increased network bandwidth, because every station must transmit to every other station directly.
3.3 Issues Some observers argue that two outstanding issues are preventing video conferencing from becoming a standard form of communication, despite the ubiquity of video conferencing-capable systems.
These issues are: Ey e Co ntact: It is known that eye contact plays a large role in conversational turn-taking, perceived attention and intent, and other aspects of group communication.
While traditional telephone conversations give no eye contact cues, video conferencing systems are arguably worse in that they provide an incorrect impression that the remote interlocutor is avoiding eye contact.
Tele-presence systems such as the Polycom RPX have cameras located in the screens that reduce the amount of parallax observed by the users.
This issue is also being addressed through research that generates a synthetic image with eye contact using stereo reconstruction.
Appearance Co nscio usness: A second problem with video conferencing is that one is on camera, with the video stream possibly even being recorded.
The burden of presenting an acceptable on-screen appearance is not present in audio-only communication.
Early studies by 35  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Alphonse Chapanis found that the addition of video actually impaired communication, possibly because of the consciousness of being on camera.
The issue of eye-contact may be solved with advancing technology, and presumably the issue of appearance consciousness will fade as people become accustomed to video conferencing.
3.4 Standards The International Telecommunications Union (ITU) (formerly: Consultative Committee on International Telegraphy and Telephony (CCITT)) has three umbrellas of standards for VTC.
ITU H.320 is known as the standard for public switched telephone networks (PSTN) or VTC over integrated services digital networks (ISDN) basic rate interface (BRI) or primary rate interface (PRI).
H.320 is also used on dedicated networks such as T1 and satellite-based networks; ITU H.323 is known as a standard for transporting multimedia applications over LANs.
This same standard also applies to older implementations of voice over IP VoIP.
In recent years, the IETF' Session Initiation Protocol (SIP) has gained considerable momentum in practice for these two services.
ITU H.324 is the standard for transmission over POTS, or audio telephony networks.
3G-324M is a 3GPP implementation for video call on 3G mobile phones.
In recent years, IP based video conferencing has emerged as a common communications interface and standard provided by VTC manufacturers in their traditional ISDN-based systems.
Business, government and military organisations still predominantly use H.320 and ISDN VTC.
Though, due to the price point and proliferation of the Internet, and broadband in particular, there has been a strong spurt of growth and use of H.323, IP VTC.
H.323 has the advantage that it is accessible to anyone with a high speed Internet connection, such as DSL.
In addition, an attractive factor for IP VTC is that it is easier to set-up for use with a live VTC call along with web conferencing for use in data collaboration.
These combined technologies enable users to have a much richer multimedia environment for live meetings, collaboration and presentations.
3.5 Impact on the G eneral Public High speed Internet connectivity has become more widely available at a reasonable cost and the cost of video capture and dis play technology has 36  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY decreased.
Consequently personal video teleconference systems based on a webcam, personal computer system, software compression and broadband Internet connectivity have become affordable for the general public.
Also, the hardware used for this technology has continued to improve in quality, and prices have dropped dramatically.
The availability of freeware (often as part of chat programmes) has made software based video conferencing accessible to many.
For many years, futurists have envisioned a future where telephone conversations will take place as actual face-to-face encounters with video as well as audio.
Sometimes it is simply not possible or practical to have a face-to-face meeting with two or more people.
Sometimes a telephone conversation or conference call is adequate.
Other times, an email exchange is adequate.
Video conferencing adds another possible alternative, and can be considered when: a live conversation is needed; visual information is an important component of the conversation; the parties of the conversation can't physically come to the same location; or the expense or time of travel is a consideration.
Deaf and hard of hearing individuals have a particular interest in the development of affordable high-quality video conferencing as a means of communicating with each other in sign language.
Unlike Video Relay Service, which is intended to support communication between a caller using sign language and another party using spoken language, video conferencing can be used between two signers.
Mass adoption and use of video conferencing is still relatively low, with the following often claimed as causes: Complexity of systems: Most users are not technical and want a simple interface.
In hardware systems an unplugged cord or a flat battery in a remote control is seen as failure, contributing to perceived unreliability which drives users back to traditional meetings.
Successful systems are backed by support teams who can pro- actively support and provide fast assistance when required.
Perceived lack of interoperability: Not all systems can readily interconnect, for example ISDN and IP systems require a bridge.
Popular software solutions cannot easily connect to hardware systems.
Some systems use different standards, features and qualities which can require additional configuration when connecting to dis-similar systems.
37  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Bandwidth and quality of service: In some countries it is difficult or expensive to get a high quality connection that is fast enough for good-quality video conferencing.
Technologies such as ADSL have limited upload speeds and cannot upload and download simultaneously at full speed.
As Internet speeds increase higher quality and high definition video conferencing will become more readily available.
Expense of commercial systems: A well designed system requires a specially designed room and can cost hundreds of thousands of dollars to fit out the room with codecs, integration equipment and furniture.
For these reasons many hardware systems are often used for internal corporate use only, as they are less likely to run into problems and lose a sale.
An alternative is companies that hire out video conferencing equipped meeting rooms in cities around the world.
Customers simply book the rooms and turn up for the meeting - everything else is arranged and support is readily available if anything should go wrong.
3.6 Impact on Education Video conferencing provides students with the opportunity to learn by participating in a 2-way communication platform.
Furthermore, teachers and lecturers from all over the world can be brought to classes in remote or otherwise isolated places.
Students from diverse communities and backgrounds can come together to learn about one another.
Students are able to explore, communicate, analyse and share information and ideas with one another.
Through video conferencing students can visit another part of the world to speak with others, visit a zoo, a museum and so on, to learn.
These "virtual field trips" can bring opportunities to children, especially those in geographically isolated locations, or the economically disadvantaged.
Small schools can use this technology to pool resources and teach courses (such as foreign languages) which could not otherwise be offered.
Here are a few examples of how video conferencing can benefit people around campuses: faculty member keeps in touch with class while away for a week at a conference guest lecturer brought into a class from another institution researcher collaborates with colleagues at other institutions on a regular basis without loss of time due to travel faculty member participates in a thesis defense at another institution 38  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY administrators on tight schedules collaborate on a budget preparation from different parts of campus faculty committee auditions a scholarship candidate researcher answers questions about a grant proposal from an agency or review committee student interviews with an employer in another city Tele seminar 3.7 Impact on Medicine and Hea lth Video conferencing is a very useful technology for telemedicine and telenursing applications, such as diagnosis, consulting, transmission of medical images, etc., in real time in countries where this is legal.
Using VTC, patients may contact nurses and physicians in emergency or routine situations, physicians and other paramedical professionals can discuss cases across large distances.
Rural areas can use this technology for diagnostic purposes, thus saving lives and making more efficient use of health care money.
Special peripherals such as microscopes fitted with digital cameras, video endoscopes, medical ultrasound imaging devices, otoscopes, etc., can be used in conjunction with VTC equipment to transmit data about a patient.
3.8 Impact on Business Video conferencing can enable individuals in faraway places to have meetings on short notice.
Time and money that used to be spent in traveling can be used to have short meetings.
Technology such as VOIP can be used in conjunction with desktop video conferencing to enable low-cost face-to-face business meetings without leaving the desk, especially for businesses with wide-spread offices.
The technology is also used for telecommuting, in which employees work from home.
Video conferencing is now being introduced to online networking websites, in order to help business es form profitable relationships quickly and efficiently without leaving their place of work.
Although it already has proven its potential value, research has shown that many employees do not use the videoconference equipment because they are afraid that they will appear to be wasting time or looking for the easiest way if they use video conferencing to enhance customer and supplier relationships.
This anxiety can be avoided if managers use the technology in front of their employees.
39  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.9 Impact on Law Video conferencing has allowed testimony to be used for individuals who are not able to attend the physical legal settings.
In a military investigation in North Carolina, Afghan witnesses have testified using video conferencing.
3.10 Application in Teaching Things to Do Plan student activities weeks before the semester starts.
Create an outline of different types of activities that challenge students to utilise the conference's potential as more of the semester progresses, e.g.
: personal introductions chapter summaries literature reviews group debates "fieldwork" assignments (e.g., case studies) short research projects proposal writing Require students to log in at least twice every week (early during the week to enter their comment to a topic, and later that week to read and respond to replies others have made to that topic).
Distinguish between Two Types o f Co nferences: (a) formal and (b) informal ones.
The former are for official class or group-project discussion; the latter are for social networking and peer support.
Social support activities are very important for a class.
They provide the emotional glue that motivates students to learn together and to learn from each other (rather than only from the teacher).
Informal conferences should be open to the whole course, whereas some formal conferences need to be reserved as group work-space.
Provide students with thorough handouts and training (STEP classes or demonstrations) on the use of the software and conference, but then make them responsible for familiarising themselves and each other with the technology.
Set aside enough time (create labs, if necessary) to allow students to collaborate in groups.
40  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY As early as week one or two, pair students up into "learning partnerships" that help them communicate about the unusual format of the course.
Enter a controversial topic related to the class content each week and request that students discuss it electronically.
Make students work with the transcripts of the whole electronic class discussion (best toward the end of the semester).
This can be done by saving or extracting overriding issues for the whole semester and assigning student teams to defend either a pro or a contra position searching through the whole conference.
Create multiple conferences/sub-conferences for your course.
This way you can keep each one focused on one purpose (e.g., one for socialising, one for assisting each other with technical advice, one for sharing literature references, one for each small-group discussion, one for the weekly discussion topics, etc.).
Use metaphors to create a sense of architecture for your students to orient themselves in the various conferences you create for them.
For instance, you can call a conference for socialising "The Coffee House", a conference for exchanging literature references "The Library", and conference for providing each other with technical assistance, "The Technical Assist Shop", etc.
Students should be aware that each conference environment has its own standards of (verbal) behavior.
It's OK to make typos and other mistakes when "walking" into the Coffee Shop; whereas a formal conference requires attention to correct language use.
If it seems appropriate, invite online experts to the electronic class discussion.
Faculty members from the same or other departments on campus may be available for a week to respond to student questions.
Authors whose books or articles are part of the course readings might be reached for electronic reactions to questions via e-mail, etc.
Make computer conferencing an integral part of your course.
If students are to make the effort to learn the technology and be enthusiastic about its learning potential, it has to be important to the course and the way they are evaluated.
Encourage students to do work for the conference off-line.
In some cases, they may need to learn how to transfer files, but in many cases, 41  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY they will be able to copy and paste text from word processors directly into conference messages.
Things to Avoid 1.
Don't make computer conferencing an add-on of low priority and infrequent use.
Students will not master the technology if they use it only occasionally, and they will not bother to explore the potential of electronic in-depth discussions when those make up only 10% of their course grade.
2.
Don't separate what's happening in the conference from what's happening in the face-to-face class m eetings.
Students need to see the function of the technology used for their regular class activities or they will not take the conference seriously.
3.
Especially with larger classes, don't expect students to be able to frequently meet in small groups outside of class time.
4.
Don't expect students to know how to collaborate.
Allow for discussion forums on group conflicts and group dynamics.
5.
Don't expect a structure for the conference to emerge if you have no framework planned ahead of time.
6.
Don't make students dependent on you as the technical expert for the system; otherwise they'll never learn to collaborate with their peers.
7.
Don't make hasty allowances for students' perceived incompetence of dealing with computers.
They'll learn once they find out it's important for the course.
8.
Don't squeeze everything into a single conference.
Any lengthy exchange of ideas becomes rather confusing.
Move things into new conferences or create new topics as they become necessary.
4.0 CO NCLUSIO N Video conferencing as a technology has brought much relief to various sectors of human endeavour especially in the education sector as well as corporate training.
It has gone a long way in reducing the costs of trainings, seminars and conferences.
Though travel cost has been cut drastically,this technology still needs to be perfected in terms of bandwidth especially to developing countries.
5.0 SUMMARY Video conferencing uses telecommunications of audio and video to bring people at different sites together for a meeting.
This can be as simple as conversation between two people in the private offices (point-to-point) or involve several sites (multi-point) with more than one person in large rooms at different sites.
42  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In dual plasma display video conferencing system, the screen on the left is primarily used to show people during the conference or the user interface when setting up the call.
Some observers argue that two outstanding issues are preventing video conferencing from becoming a standard form of communication, despite the ubiquity of video conferencing-capable s ystems.
The International Telecommunications Union (ITU) (formerly: Consultative Committee on International Telegraphy and Telephony (CCITT)) has three umbrellas of standards for VTC.
High speed Internet connectivity has become more widely available at a reasonable cost and the cost of video capture and display technology has decreased.
Video conferencing provides students with the opportunity to learn by participating in a 2-way communication platform.
Video conferencing is a very useful technology for telemedicine and telenursing applications, such as diagnosis, consulting, transmission of medical images, etc., in real time in countries where this is legal.
Video conferencing can enable individuals in faraway places to have meetings on short notice.
Time and money that used to be spent in traveling can be used to have short meetings.
Video conferencing has allowed testimony to be used for individuals who are not able to attend the physical legal settings.
6.0 TUTOR-MARKED ASSIGNMENT 1.
Mention five major components of Video conferencing.
2.
Identify five things to avoid in application of Video conferencing in teaching.
7.0 REF ERENCES/FURTHER READING Using "Skype" for Desktop Video Conferences 2008, Skype Video- Conference Guide.
Jim Van Meggelen (2005).
The Problem with Video Conferencing.
Vertegaal, "Explaining Effects of Eye Gaze on Mediated Group Conversations: Amount or Synchronization?"
ACM Conference on Computer Supported Cooperative Work, 2002.).
Computer Vision Approaches to Achieving Eye Contact Appeared in the 1990s, such as Teleconferencing Eye Contact Using a Virtual Camera, ACM CHI 1993.
More Recently Gaze Correction Systems using only a Single Camera have been shown, such as.
Microsoft's GazeMaster System.
43  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Wolfe, Mark (2007).
Broadband Videoconferencing as Knowledge Management Tool, Journal of Knowledge Management 11, no.
2.
UNIT 4 WEB CONFERENCING AND WEBCASTING CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Web Conferencing 44  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.1.1 Features 3.2.2 Standards 3.3.3 History 3.3.4 Software and Service P roviders 3.2 Webcast 3.2.1 Origins 3.2.2 Examples 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N Web conferencing is used to conduct live meetings, training, or presentations via the Internet.
In a web conference, each participant sits at his or her own computer and is connected to other participants via internet.
This can be either a downloaded application on each of the attendees computers or a web-based application where the attendees access the meeting by clicking on a link distributed by e-mail (meeting invitation) to enter the conference.
A webina r is a neolo gism to describe a specific type of web conference.
It is typically one-way from the speaker to the audience with limited audience interaction, such as in a webcast.
A webinar can be collaborative and include polling and question and answer sessions to allow full participation between the audience and the presenter.
In some cases, the presenter may speak over a standard telephone line, while pointing out information being presented onscreen, and the audience can respond over their own telephones, speaker phones allowing the greatest comfort and conveniences.
There are web conferencing technologies on the market that have incorporated the use of VoIP audio technology, to allow for a completely web-based communication.
Depending upon the provider, webinars may provide hidden or anonymous participants functionality, making participants unaware of other participants in the same meeting.
2.0 OBJECTIVES At the end of this unit, you should be able to: define web conferencing and web cast know the basic features of web conferencing differentiate web conferencing from web casting know some software and service providers for web conferencing.
45  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.0 MAIN CONTENT 3.1 Web Conferencing Web co nferencing is used to conduct live meetings or presentations over the Internet.
In a web conference, each participant sits at his or her own computer and is connected to other participants via the Internet.
This can be either a downloaded application on each of the attendees computers or a web-based application where the attendees will simply enter a URL (website address) to enter the conference.
A webinar is a neologism to describe a specific type of web conference.
It is typically one-way, from the speaker to the audience with limited audience interaction, such as in a webcast.
A webinar can be collaborative and include polling and question & answer sessions to allow full participation between the audience and the presenter.
In some cases, the presenter may speak over a standard telephone line, pointing out information being presented on screen and the audience can respond over their own telephones, preferably a speaker phone.
There are web conferencing technologies on the market that have incorporated the use of VoIP audio technology, to allow for a truly web-based communication.
Webinars may (depending upon the provider) provide hidden or anonymous participant functionality, enabling paticipants to be unaware of other participants in the same meeting.
In the early years of the Internet, the terms "web conferencing" was often used to describe a group discussion in a message board and therefore not live.
The term has evolved to refer specifically to live or "synchronous" meetings.
3.1.1 Features Other typical features of a web conference include: Slide presentations (often created through PowerPoint or Keynote on a Mac) Live video (via webcam or digital video camera) VoIP (Real time audio communication through the computer via use of headphones and speakers) Web tours - where URLs, data from forms, cookies, scripts and session data can be pushed to other participants enabling them to be pushed through web based logons, clicks, etc.
This type of feature works well when demonstrating websites where users themselves can also participate.
Recording (for viewing at a later time by anyone using a unique web 46  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY address) Whiteboard with annotation (allowing the presenter and/or attendees to highlight or mark items on the slide presentation.
Or, simply make notes on a blank whiteboard.)
Texts chat - For live question and answer sessions, limited to the people connected to the meeting.
Text chat may be public (echoed to all participants) or private (between 2 participants).
Polls and surveys (allows the presenter to conduct questions with multiple choice answers directed to the audience) Screen sharing/desktop sharing/application sharing (where participants can view anything the presenter currently has shown on their screen.
Some screen sharing applications allow for remote desktop control, allowing participants to manipulate the presenters screen, although this is not widely used.)
Web conferencing is often sold as a service, hosted on a web server controlled by the vendor, either on a usage basis (cost per user per minute) or for a fixed fee (cost per "seat").Some vendors make their conferencing software available as a licensed product, allowing organisations that make heavy use of conferencing to install the software on their own servers.
Some web conferencing software is distributed free for hosting on the MC's server.
There is also software available that is installed on the MC's computer and does not require server configuration software.
An important capability of web conferencing software is application sharing, the ability for one party in the conference to share an application (such as a web browser, spread sheet, etc.)
from their desk top with every one else in the meeting and pass the control of the application to someone else in the meeting.
3.1.2 Standards Web conferencing technologies are not standardised, which has been a significant factor in the lack of interoperability, platform dependence, security issues, cost and market segmentation.
In 2003, the IETF established a working group to establish a standard for web conferencing, called "Centralised Conferencing (xcon)".
The planned deliverables of xcon include: A basic floor control protocol.
Binary Floor Control Protocol (BFCP) published as RFC 4582 A mechanism for membership and authorisation control A mechanism to manipulate and describe media "mixing" or "topology" for multiple media types (audio, video, text) A mechanism for notification of conference related events/changes (for 47  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY example a floor change) Webinars are first and foremost best practices 3.1.3 History Real-time text chat facilities such as IRC appeared early in the internet's history.[when?]
Web-based chat and instant messaging software appeared in the mid-1990s.
In the late 1990s, the first true web conferencing capability became available and dozens of other web conferencing venues followed thereafter.
A trademark for the term "webinar" has been registered in 1998 by Eric R. Korb (Serial Number 75478683, USPTO) but was difficult to defend; it is currentl y assigned to InterCall.
3.1.4 Software and Service Providers Adobe Acrobat Connect Convenos Meeting Center Genesys Meeting Center Glance GoToMeeting IBM Lotus Sametime InterCall Microsoft Office Live Meeting WebEx WebTrain Webcast 3.2 Webcast A webca st is a media file distributed over the Internet using streaming media technology.
As a broadcast may either be live or recorded, similarly, a webcast may either be distributed live or recorded.
Essentially, webcasting is broadcasting over the Internet.
The generally accepted use of the term webcast is the "transmission of linear audio or video content over the Internet".
A webcast uses streaming media technology to take a single content source and distribute it to many simultaneous listeners/viewers.
The largest "webcasters" include existing radio and TV stations that "simulcast" their output, as well as a multitude of Internet only "stations".
The term webcasting is usually reserved for referring to non- interactive linear streams or events.
48  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Rights and licensing bodies offer specific "webcasting licenses" to those wishing to carry out Internet broadcasting using copyright material.
Webcasting is also used extensively in the commercial sector for investor relations presentations (such as Annual General Meetings), in E-learning (to transmit seminars), and for related communications activities.
However, webcasting does not bear much, if any, relationship to the idea of web conferencing which is designed for many-to-many interaction.
The ability to webcast using cheap/accessible technology has allowed independent media to flourish.
There are many notable independent shows that broadcast regularly online.
Often produced by average citizens in their homes they cover many interests and topics; from the mundane to the bizarre.
Webcasts relating to computers, technology, and news are particularly popular and many new shows are added regularly.
3.2.1 O rigins "Webcasting" was first publicly described and presented by Brian Raila of GTE Laboratories at InterTainment '89, 1989, held in New York City, USA.
Raila recognised that a viewer/listener need not download the entirety of a program to view/listen to a portion thereof, so long as the receiving device ("client computer") could, over time, receive and present data more rapidly than the user could digest same.
Raila used the term "buffered media" to describe this concept.
Raila was joined by James Paschetto of GTE Laboratories to further demonstrate the concept.
Paschetto was singularly responsible for the first workable prototype of streaming media, which Raila presented and demonstrated at the Voice Mail Association of Europe 1995 Fall Meeting of October, 1995, in Montreux, Switzerland.
Alan Saperstein (Visual Data, now known as Onstream Media (Nasdaq:ONSM), was the first company to feature streaming video in June of 1993 with HotelView, a travel library of 2 minute videos featuring thousands of hotel properties worldwide.
On November 7, 1994, WXYC, the college radio station of the University of North Carolina at Chapel Hill became the first radio station in the world to broadcast its signal over the internet.
The term webcasting was coined (in the early/mid 1990s) when webcast/streaming pioneers Mark Cuban (Audionet), Howard Gordon (Xing Technologies), William Mutual (ITV.net) and Peggy Miles (InterVox Communications) got together with a community of webcasters to pick a term to describe the technology of sending audio 49  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY and video on the Net that might make sense to people.
The term netcasting was a consideration, but one of the early webcast community members owned a company called NetCast, so that term was not used, seeking a name that would not be branded to one company.
Discussions were also conducted about the term with the National Association of Broadcasters for their books - Internet Age Broadcaster I and II, written by Peggy Miles and Dean Sakai.
The actual word "webcast" was coined by Daniel Keys Moran in his 1988 novel The Armageddon Blues: " DataWeb News had done an in- depth on it not two weeks ago, and tourists had been trekking up into the New York hills ever since the webcast."
- page 191 of the Bantam paperback.
3.1.2 Examples Virtually all the major broadcasters have a webcast of their output, from the BBC to CNN to Al Jazeera to UNTV Webcast in television to Radio China, Vatican Radio, United Nations Radio and the World Service in radio.
A notable webcast took place in September 1999 to launch NetAid, a project to promote Internet use in the world's poorest countries.
Three high profile concerts were to be broadcast simultaneously on the BBC, MTV and over the Internet; a London concert at Wembley Stadium featuring the likes of Robbie Williams, George Michael; a New York concert featuring Bono of U2 and Wyclef Jean; a Geneva concert.
More recently, Live8 (AOL) claimed around 170,000 concurrent viewers (up to 400 Kbit/s) and the BBC received about the same (10 Gbit/s) on the day of the 7 July 2005 bombings in London.
The growth of webcast traffic has roughly doubled, year on year, since 1995 and is directly linked to broadband penetration.
Connecting Media was one of the first companies to do live webcasting using a special IFP Van (Internet Field Production) dedicated to webcasting.
Today, webcasts are being used more frequently and by novice users.
Live webcasts are allowing viewing of presentations, business meetings, and seminars etc.
for those that telecommute rather than attend.
Such sites offer live broadcasting as an affordable solution to public speaking events that expands the viewing audience to anyone that has an internet connection.
Live sporting events, both local and national, have also quickly become frequent webcast subjects.
With regard to smaller events such as Little 50  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY League, amateur sports, small college sports, and high school sports, webcasting allows these events to have full audio or video coverage online when they may not be able to book standard radio or TV time.
Websites like Meridix Webcast Network,Texas Sports Radio Network, SportsJuice, and others allow local schools, teams, and broadcasters to produce their own webcasts, which also have the advantage of being accessible to anyone with an internet connection (i.e.
relatives several states away), unlike the range and market limitations of terrestrial radio and TV.
4.0 CONCLUSION Much like video conferencing, web conferencing and webcasting has brought a new dimension to all aspects of life especially in education and business.
Business time has been cut down drastically as an organisation can broadcast to several clienteles at the same time.
Again this technology is being improved upon to accommodate those with countries and organisations that cannot afford access to broader bandwidth.
5.0 SUMMARY Web conferencing is used to conduct live meetings or presentations over the Internet.
In a web conference, each participant sits at his or her own computer and is connected to other participants via the Internet.
In the early years of the Internet, the term "web conferencing" was often used to describe a group discussion in a message board and therefore not live.
Web conferencing is often sold as a service, hosted on a web server controlled by the vendor, either on a usage basis (cost per user per minute) or for a fixed fee.
Web conferencing technologies are not standardised, which has been a significant factor in the lack of interoperabilit y, platform dependence, security issues, cost and market segmentation.
Real-time text chat facilities such as IRC appeared early in the internet's history.[when?]
Web-based chat and instant messaging software appeared in the mid-1990s.
A webcast is a media file distributed over the Internet using streaming media technology.
As a broadcast may either be live or recorded, similarly, a webcast may either be distributed live or recorded.
Webcasting was first publicly described and presented by Brian Raila of GTE Laboratories at InterTainment '89, 1989, held in New York City, USA.
Connecting Media was one of the first companies to do live webcasting using a special IFP Van (Internet Field Production) dedicated to 51  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY webcasting.
6.0 TUTOR-MARK ED ASSIGNMENT List 10 typical applications of web conference.
7.0 REF ERENCES/FURTHER READING Webinar Definition".
PC Magazine Encyclopedia.
World Web Event Services Markets - N100-64, Frost and Sullivan, page 10, 2006, "The Main Features within the Web Event Services Market".
Centralised Conferencing (xcon) "Binary Floor Control Protocol.
Internet Society IETF (November 2006).
"Trademark Assignment for Webinar".
United States P atent and Trademark Office (February 6, 2003).
Grossman, Wendy (1995-01-26).
"Communications: Picture the Scene", Online, Manchester, United Kingdom: The Guardian, pp.
4 WXYC 89.3 FM (1994-11-07).
"WXYC Announces the first 24-hour Real-Time World-Wide Internet radio simulcast".
P ress release.
52  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 5 ELECTRONIC PAYMENT SYSTEMS CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Overview of Electronic Payment S ystems 3.2 Electronic Bill Payment 3.2.1 Limitations (United States) 3.3 Electronic Funds Transfer 3.3.1 What is Electronic Fund Transfer?
3.3.2 EFTPOS 3.3.3 Card-Based EFT 3.3.4 Transaction Types 3.3.5 Authorisation 3.3.6 Dual Message Authorisation/Clearing 3.3 Electronic Money 3.4 Off-Line Anonymous Electronic Money 3.5 Wire Transfer 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N As payment is an integral part of mercantile process, electronic payment system is an integral part of e-commerce.
The emergence of e- commerce has created new financial needs that in many cases cannot be effectively fulfilled by tradition payment systems.
For instance, new types of purchasing relationships-such as auction between individuals online-have resulted in the need for peer-to peer 3 payment methods hat allows individuals to e-mail payments to the other individual.
Recognizing this, virtually all interested parties (i.e.
academicians, government, business community and financial service providers) are exploring various types of electronic payment system and issues surrounding electronic payment s ystem and issues surrounding electronic payment system and digital currency.
Some proposed electronic payment systems are simply electronic version of existing payment systems such as cheques and credit cards, while, others are based on the digital currency technology and have the potential for definitive impact on today s financial and monetary system.
While popular developers of electronic payment system predict fundamentals changes in the financial sector because of the innovations in electronic payment system (Kalakota & Ravi, 1996).
Therefore, electronic 53  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY payments systems and in particular, methods of payment being developed to support electronic commerce cannot be studied in an isolation.
A failure to take place these developments into the proper context is likely to result in undue focus on the various experimental initiatives to develop electronic forms of payment without a proper reflection on the broader implications for the existing payment system.
2.0 OBJECTIVES At the end of this unit, you should be able to: • identify the different types of electronic systems • differentiate the forms of electronic systems • appreciate their strengths and weaknesses • understand the specific applications of each type.
3.0 MAIN CONTENT 3.1 Overview of Electronic payment Systems There are over a dozen proposals for electronic payment systems on the Internet.
To briefly understand these systems, let us examine a few issues by trying to pay a bill via the Internet with a credit card.
In comparison to using cash in the real world, transmitting a credit card number over the Internet might lead to the following difficulties.
First, there is the entire question of security.
Credit card numbers may be viewed by unauthorised individuals because the Internet is an open system.
In the real world, there are a number of means to minimise fraud.
A customer using a credit card will usually opt to carry out transactions at trustworthy or familiar facilities, stores, and markets.
Second, credit cards can be used only at authorised stores.
Unauthorised small businesses or individuals generally cannot carry out transactions with credit cards.
In other words, credit cards cannot be used for peer-to- peer payment.
Cash encourages peer-to-peer payments.
Third, credit card payments usually charge a small fee.
Although this cost is low, it can be a significant cost when the payment itself is very small, such as less than 20 cents.
As a result, credit cards can not be used for micro-payments.
Cash payments are used for even the smallest financial transactions.
Finally, receipts from credit card payments leave residual records of expenditures.
Those who issue credit cards know exactly what kinds of goods and services have been purchased, as well as where and when 54  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY they were acquired.
In other words, user's expenditures by credit card can be traced while cas h payments are untraceable.
Electronic payment systems, more or less, try to cope with the above issues.
According to the extent to which these systems cope with these problems, I classify digital cash programmes into three categories.
1 .
Credit Card B ase Type To minimise security risks and the loss of credit card numbers in transit, First Virtual Holding began a payment system in which users transmit passwords instead of credit card numbers when purchasing an item (See Figure 1-i).
Customer sends his ID or encrypted credit card number to the shop.
Shop asks for payment to the Credit card company, which confirms customer b y e-mail.
After the confirmation, payment is done.
Card number itself never goes through the Net.
Security X Peer-to-peer - Low fees - Untraceability - Person "A" issues his electronic check.
He sends it to person "B" and informs the bank of his check.
Person "B" asks for payment to the B ank.
After the confirmation, the bank transfers money from person 55  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY A account to person B.
Security X Peer-to-peer X Low fees X Untraceability - Person "A" asks the bank to issue digital cash.
The bank issues digital cash and reduces his account by that amount.
He sends it to person "B".
Person "B" asks the bank for payment.
After confirming that the digital cash is not double-spent, the bank increases person Bs account by that amount.
Note that the bank cannot know who sent that digital cash to person B.
(Untraceability) Security X Peer-to-peer X Low fees X Untraceability X In this system, a user registers in advance with First Virtual to secure a password corresponding to a credit card number.
With the purchase of goods or services on the Internet, only a password is transmitted to complete the transactions.
After the actual purchase, a confirmation electronic mail message confirms the validity of the transaction.
This system is simple and is already in use to some extent.
Visa and MasterCard are planning a similar payment programme using encryption instead of passwords.
These credit-card based solutions solve only the security question.
As Figure 1-(i) illustrates, the actual communication between the consumer 56  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY and electronic storefront are addressed by this strategy.
The transaction of real money remains to be done by conventional credit card transactions.
These transactions require a fee.
A peer-to-peer transaction is impossible.
Certainly, the entire transaction is also traceable.
2 .
Check Type Checks are closer transactionally to cash than to credit cards, because peer-to-peer transfers are possible.
Micro-payments are possible as well though banks are reluctant to accept process micro-payments by checks thanks to the high operational cost of check clearance.
As a result, several proposals (CyberCash, NetCheck, and others) have emerged to invent checks on the Internet, which would be transferable between individuals.
As Figure 1-(ii) shows, a customer opens an account in a bank on the Internet, and issues an electronic check to pay a bill.
The recipient of this digital check sends it to the Internet bank to confirm and cash it.
Security is guaranteed by both encryption and the bank's confirmation process with the issuer of the check.
This system permits peer-to-peer payments and reduces fees to some extent.
But transactions are still traceable since a bank can track the actual use of the electronic check.
3 .
Cash Type Cash transactions are untraceable and anonymous.To achieve untraceability on the Internet, encryption has to be fully employed to prevent untraceable money from being easily copied and spent twice, a phenomenon known as double-spending.
David Chaum as well as Tatsuaki Okamoto and Kazuo Ohta have proposed untraceable electronic payment s ystems using advanced encryption technology.
The mechanism in this system is similar to an electronic check, but it prevents banking institutions from linking purchasers to specific goods and services (see Figure 1-(iii)).
How does this work?
First, an Internet user opens an account with real money at an Internet-based bank.
The customer asks the bank to issue a certain amount of digital cash for use on the Internet.
The bank issues this digital cash using encryption and deducts the funds from the established account.
An example of a bank that performs these sorts of transactions is Mark Twain Banks, operating since late in 1995.
This digital cash is a combination of two huge integers which have special mathematical relation.
No other person or institution, but the bank, can imitate this relation.
Any calculation that would attempt to duplicate this relation would take an almost infinite amount of time in the absence of the bank's secret key.
57  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY When an individual uses digital cash, this unique data that defines the actual electronic currency is given to the merchant.
The merchant in turn sends this data to the bank to confirm it.
If the bank confirms it, the bank credits the merchant's bank account by that amount, or alternatively issues the merchant a sum of digital cash in the same amount.
Only the bank can confirm that this data - or, digital cash - is legitimate and actually issued by the bank.
Only the bank can verify that this data has not been used elsewhere, or double-spent.
The bank cannot know who used the digital cash, as long as customers of the bank do not use it twice.
This payment system deserves the name of "cash on the Internet" because it is almost equal to a cash payment in terms of security, fee, peer-to-peer payment, and untraceability.
I will now focus on this cash- type "digital cash."
3.2 Electronic Bill Payment Electronic Bill Pay ment is a feature of online banking, similar in its effect to a giro, allowing a depositor to send money from his demand account to a creditor or vendor such as a public utility or a department store to be credited against a specific account.
The pa yment is optimally executed electronically in real time, though some financial institutions or payment services will wait until the next business day to send out the payment.
The bank can usually also generate and mail a paper cheque or banker's draft to a creditor who is not set up to receive electronic payments.
Electronic billing can also feature invoices sent by e-mail or viewed on a secure web site (with notices of new invoices being sent by e-mail).
Most large banks also offer various convenience features with their electronic bill payment systems, such as the ability to schedule payments in advance to be made on a specified date, the ability to manage payments from any computer with a web browser, and various options for searching one's recent payment history: when did I last pay Company X?
To whom did I make my most recent payment?
In many cases one can also integrate the electronic payment data with accounting or personal finance software.
Peer-to-peer payment s ystems are extremely popular.
The best and most widely known example is PayPal.
PayPal allows you to pay for just about anything online as long as the seller also has a PayPal account.
Many online sellers use PayPal such as 75% of eBay sellers, overstock.com, ritzcamera.com, and Walgreens.com (Traver, 2004).
58  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY PayPal is also sometimes used to pay for personal debts in situations where both parties have an account.
Electronic bill payment and presentment (EBPP) includes an electronic bill payment system (EBPS).
Electronic bill payment and presentment is the electronic bill presentment to the consumer and the electronic initiation of payment by the consumer (Alexandria Andreeff).
This was done completely by postal mail before the internet.
Sending bills electronically via the internet is much faster and cheaper though.
Although this technology was available before December in 1998, only 26.2% of U.S. households had internet access at that time according to the U.S. Department of Commerce in 2000 (Alexandria Andreeff).
By August of 2000, electronic bill payment and presentment systems started to dramatically increase in popularity because 41.5% of U.S. households had internet access by then according to the U.S. Department of Commerce in 2000 (Alexandria Andreeff).
In this model, the one who is charging the consumer, notifies the customer (usually) through e-mail (Alexandria Andreeff).
The customer is then responsible to log on to the biller s website to pay the bill (Alexandria Andreeff).]
3.2.1 Limitations (United States) Typically, US financial institutions formally prohibit the use of their consumer electronic bill payment systems for payments to any tax authorities, collection agencies, or recipients of court-ordered payments like child support or alimony.
Any organisations or individuals outside of the United States are also usually excluded.
Payments to government agencies for utilities such as water are usually permitted.
3.3 Electronic F unds Transfer 3.3.1 What is Electronic Fund Transfer?
Elect ronic funds t ransfer or EFT refers to the computer-based systems used to perform financial transactions electronically.
The term is used for a number of different concepts: Cardholder-initiated transactions, where a cardholder makes use of a payment card.
Direct deposit payroll payments for a business to its employees, possibly via a payroll services company.
Direct debit payments from customer to business, where the transaction is initiated by the business with customer permission.
59  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Electronic bill payment in online banking, which may be delivered by EFT or paper check.
Transactions involving stored value of electronic money, possibly in a private currency.
Wire transfer via an international banking network (generally carries a higher fee).
Electronic Benefit Transfer 3.3.2 EFTPOS EFTPOS (short for Electronic Funds Transfer at Point of Sale) is an Australian and New Zealand electronic processing system for credit cards, debit cards and charge cards.
EFTPOS also allows users of the system to withdraw cash at the time of purchasing a product or service through the merchant's EFTPOS terminal.
This functionality is called debit card cashback in other countries.
The name and logo for EFTPOS in Australia were originally owned by the National Australia Bank and were trade marks from 1986 until 1991.
There are over 60,000 participating EFTPOS outlets in Australia.
European banks and card companies also sometimes reference "EFTPOS" as the system used for processing card transactions through terminals on points of sale, though the system is not the trademarked Australian/New Zealand variant.
3.3.3 Card-based EFT EFT may be initiated by a cardholder when a payment card such as a credit card or debit card is used.
This may take place at an automated teller machine (ATM) or point of sale (POS), or when the card is not present, which covers cards used for mail order, telephone order and internet purchases.
Card-based EFT transactions are often covered by the ISO 8583 standard.
60  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.3.4 Transaction types A number of transaction types may be performed, including the following: Sa le: where the cardholder pays for goods or service Refund: where a merchant refunds an earlier payment made by a cardholder Withd rawal: the cardholder withdraws funds from their account, e.g.
from an ATM.
The term Cash Advance may also be used, typically when the funds are advanced by a merchant rather than at an ATM Depo sit: where a cardholder deposits funds to their own account (typically at an ATM) Cashback: where a cardholder withdraws funds from their own account at the same time as making a purchase Inter-Account Tra nsfer: transferring funds between linked accounts belonging to the same cardholder Payment: transferring funds to a third party account Enqui ry: a transaction without financial impact, for instance balance enquiry, available funds enquiry, linked accounts enquiry, or request for a statement of recent transactions on the account E To p-Up: where a cardholder can use a device (typicall y POS or ATM) to add funds (top-up) to their pre-pay mobile phone Mini-Sta tement: where a cardholder uses a device (typically an ATM) to obtain details of recent transactions on their account Administ rativ e: this covers a variety of non-financial transactions including PIN change The transaction types offered depend on the terminal.
An ATM would offer different transactions from a POS terminal, for instance.
3.3.5 Authorisation EFT transactions require communication between a number of parties.
When a card is used at a merchant or ATM, the transaction is first routed to an acquirer, then through a number of networks to the issuer where the cardholder's account is held.
A transaction may be authorised offline by any of these entities through a stand-in agreement.
Stand-in authorisation may be used when a communication link is not available, or simply to save communication cost or time.
Stand-in is subject to the transaction amount being below agreed limits, known as floor limits.
These limits are calculated based on the risk of authorising a transaction offline, and thus vary between merchants and card types.
Offline transactions may be subject to other 61  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY security checks such as checking the card number against a 'hotcard' (stolen card) list, velocity checks (limiting the number of offline transactions allowed by a cardholder) and random online authorisation.
Before online authorisation was standard practice and credit cards were processed using manual vouchers, each merchant would agree a limit ("floor limit) with his bank above which he must telephone for an authorisation code.
If this was not carried out and the transaction subsequently was refused by the issuer ("bounced"), the merchant would not be entitled to a refund.
3.3.6 Dual Message Authorisation/Clearing Depending on the business rules of the issuer, a "hold" may be placed on the funds authorised.
This hold reserves that amount of money for a defined period.
If a transaction is not cleared within the defined period then the "hold" will be removed and the funds made available again.
Exa mple - Purchase fo r £1 0 on Day 2 never completes so ho ld removed on Day 4: Clea red B ala nce Ava ila ble Ba lance Day 1 £100 £100 Day 2 £100 £90 (Hold for a purchase of £10) Day 3 £100 £90 Day 4 £100 £100 (Hold for £10 purchase removed) Exa mple - Purcha se for £ 10 on Da y 2 co mpletes on Day 4: Cleared Ba lance Ava ila ble Balance Day 1 £100 £100 Day 2 £100 £90 (Hold for a purchase of £10) Day 3 £100 £90 Day 4 £90 £90 (Transaction completes.
Hold removed.
Both balances updated with purchase amount) An offline process, driven by the networks' clearing systems, generates clearing files which are sent to the card issuers on a daily basis.
These files contain the completions messages to the on-line authorisations.
In addition, not all transactions in a dual-message environment require authorisation.
Depending on the type of card used, and the floor-limit of 62  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY the merchant, it may be that there are transactions in the clearing files which have not been authorised on-line.
This is a financial exposure for banks as they have to honour the clearing records regardless of the balance on the cardholder's account.
Exa mple - Purchase for £30 on Day 2 fo r a transa ctio n no t requiring a uthorisatio n: Cleared Bala nce Available Ba lance Day 1 £10 £10 Day 2 -£20 -£20 (Offline purchase of £30) This transaction has to be applied even if the cardholder does not have sufficient funds or an overdraft.
3.3.7 Single Message Authorisation/Clearing Some financial networks operate a single message solution, in which a transaction is authorised and cleared via the same message.
A transaction will be authorised via a pre-authorisation step, where the merchant requests the issuer to reserve an amount on the cardholder's account for a specific time, followed by completion, where the merchant requests an amount blocked earlier with a pre-authorisation.
This transaction flow in two steps is often used in businesses such as hotels and car rental where the final amount is not known, and the pre- authorisation is made based on an estimated amount.
Completion may form part of a settlement process, typically performed at the end of the day when the day's completed transactions are submitted.
All these messages will be sent "on-line" from the merchant acquirer to the issuing bank.
3.3.8 Authentication EFT transactions may be accompanied by methods to authenticate the card and the card holder.
The merchant may manually verify the card holder's signature, or the card holder's personal identification number (PIN) may be sent online in an encrypted form for validation by the card issuer.
Other information may be included in the transaction, some of which is not visible to the card holder (for instance magnetic stripe data), and some of which may be requested from the card holder (for instance the card holder's address or the CVV2 value printed on the card).
EMV cards are smartcard-based payment cards, where the smartcard technology allows for a number of enhanced authentication measures.
63  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.4 Electronic Money 3.4.1 What is Electronic Money?
Electronic money (also known as e-mo ney, electro nic cash, elect ronic currency , digital money , digita l cash or dig ital currency ) refers to money which is exchanged only electronically.
Typically, this involves use of computer networks, the internet and digital stored value systems.
Electronic Funds Transfer (EFT) and direct deposit are examples of electronic money.
Also, it is a collective term for financial cryptography and technologies enabling it.
While electronic money has been an interesting problem for cryptography (see for example the work of David Chaum and Markus Jakobsson), to date, use of digital cash has been relatively low-scale.
One rare success has been Hong Kong, Octopus card system, which started as a transit payment system and has grown into a widely used electronic cash system.
Singapore also has an electronic money implementation for its public transportation system (commuter trains, bus, etc), which is very similar to Hong Kong's Octopus card and based on the same type of card (FeliCa).
A very successful implementation is in the Netherlands, known as Chipknip.
3.4.2 Alternative systems Technically electronic or digital money is a representation, or a system of debits and credits, used (but not limited to this) to exchange value, within another system, or itself as a stand alone system, online or offline.
Also sometimes the term electronic money is used to refer to the provider itself.
A private currency may use gold to provide extra security, such as digital gold currency.
An e-currency system may be fully backed by gold (like e-gold and c-gold), non-gold backed, or both gold and non-gold backed (like e-Bullion and Liberty Reserve).
Also, some private organisations, such as the US military use private currencies such as Eagle Cash.
Many systems will sell their electronic currency directly to the end user, such as Paypal and WebMoney, but other systems, such as e-gold, sell only through third party digital currency exchangers.
In the case of Octopus Card in Hong Kong, deposits work similarly to banks'.
After Octopus Card Limited receives money for deposit from users, the money is deposited into banks, which is similar to debit-card- issuing banks re-depositing money at central banks.
64  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Some community currencies, like some LETS systems, work with electronic transactions.
Cyclos Software allows creation of electronic community currencies.
Ripple monetary s ystem is a project to develop a distributed system of electronic money independent of local currency.
3.5 O ff-Line Anonymous Electronic Money In off-line electronic money the merchant does not need to interact with the bank before accepting a coin from the user.
Instead he can collect multiple coins Spent by users and Deposit them later with the bank.
In principle this could be done off-line, i.e.
the merchant could go to the bank with his storage media to exchange e-cash for cash.
Nevertheless the merchant is guaranteed that the user's e-coin will either be accepted by the bank, or the bank will be able to identify and punish the cheating user.
In this way a user is prevented from spending the same coin twice (double-spending).
Off-line e-cash schemes also need to protect against cheating merchants, i.e.
merchants that want to deposit a coin twice (and then blame the user).
Using cryptography, anonymous e-cash was introduced by David Chaum.
He used blind signatures to achieve unlinkability between withdrawals and spend transactions.
In cryptography, e-cash usually refers to anonymous e-cash.
Depending on the properties of the payment transactions, one distinguishes between on-line and off-line e-cash.
The first off-line e-cash system was proposed by Chaum and Naor.
Like the first on-line scheme, it is based on RSA blind signatures.
3.5.1 F uture Evolution The main focuses of digital cash development are 1) being able to use it through a wider range of hardware such as secured credit cards; and 2) linked bank accounts that would generally be used over an internet means, for exchange with a secure micropayment system such as in large corporations (PayPal).
Theoretical developments in the area of decentralised money are underway that may rival traditional, centralised money.
Systems of accounting such as Altruistic Economics are emerging that are entirely electronic, and can be more efficient and more realistic because they do not assume a zero-sum transaction model.
3.5.2 Issues 65  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Although digital cash can provide many benefits such as convenience and privacy, increased efficiency of transactions, lower transaction fees, and new business opportunities with the expansion of economic activities on the Internet, there are many potential issues with the use of digital cash.
The transfer of digital currencies raises local issues such as how to levy taxes or the possible ease of money laundering.
There are also potential macroeconomic effects such as exchange rate instabilities and shortage of money supplies (total amount of digital cash versus total amount of real cash available, the possibility that digital cash could exceed the real cash available).
These issues may only be addressable by some type of cyberspace regulations or laws that regulate the transactions and watch for signs of trouble.
3.6 Wire Transfer 3.6.1 What is Wire Transfer?
Wire transfer is a method of transferring money from one entity to another.
A wire transfer can be made from one entity's bank account to the other entity's bank account, and by a transfer of cash at a cash office.
3.6.2 History Although the genesis of the idea dates as far back as the giro, the modern wire transfer was a product of the telegraph companies, which made it possible to wire a money order from one office to another.
Later, it became possible to wire money between banks, which is essentially the same process as the giro.
Therefore, the term giro is still used for it in many other European countries.
3.6.3 Process Bank wire transfers are often the most expedient method for transferring funds between bank accounts.
A bank wire transfer is effected as follows: i.
The person wishing to do a transfer (or someone who he has appointed and empowered financially to act on hi s behalf) goes to the bank and gives the bank the order to transfer a certain amount of money.
IBAN and BIC code are given as well so the bank knows where the money needs to be sent to.
ii.
The sending bank transmits a message, via a secure system (such as SWIFT or Fedwire), to the receiving bank, requesting that it effect payment according to the instructions given.
iii.
The message also includes settlement instructions.
The actual transfer is not instantaneous: funds may take several hours or 66  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY even days to move from the sender's account to the receiver's account.
iv.
Either the banks involved must hold a reciprocal account with each other, or the payment must be sent to a bank with such an account, a correspondent bank, for further benefit to the ultimate recipient.
3.6.4 Regulation Bank transfer is the most common payment method in Europe, with several million transactions processed each day.
Debit cards are used extensively to pay in stores, while monthly bills are usually paid with a direct transfer (by cellular phone or Internet, or at the bank or an ATM).
In 2002, the European Commission relegated the regulation of the fees that a bank may charge for payments in euros between European Union member countries down to the domestic level, resulting in very low or no fees for transfers within the Euro zone; wire transfers between this zone and external areas can be expensive.
In the United States, domestic wire transfers are governed by Federal Regulation J and by Article 4A of the Uniform Commercial Code.
3.6.5 Security Bank-to-bank wire transfer is considered the safest international payment method.
Each account holder must have a proven identity.
Chargeback is unlikely, although wires can be recalled.
Information contained in wires is transmitted securely through encrypted communications methods.
The price of bank wire transfers varies greatly, depending on the bank and its location; in some countries, the fee associated with the service can be costly.
Wire transfers done through cash offices are essentially anonymous and are designed for transfer between persons who trust each other.
It is unsafe to send money by wire to an unknown person to collect at a cash office: the receiver of the money may, after collecting it, simply disappear.
This scam has been used often, especially in so-called Nigerian letters, also called advance fee fraud or 419 scams.
International transfers involving the United States are subject to monitoring by the Office of Foreign Assets Control (OFAC), which monitors information provided in the text of the wire to ascertain whether money is being transferred to terrorist organisations or countries or entities under sanction by the United States government.
If a financial 67  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY institution suspects that funds are being sent from or to one of these entities, it must block the transfer and freeze the funds.
3.6.6 Methods Western Unio n One of the largest companies that offer wire transfer is Western Union, which allows individuals to transfer or receive money without an account with Western Union or any financial institution.
Concern and controversy about Western Union transfers have increased in recent years, because of the increased monitoring of money-laundering transactions, as well as concern about terrorist groups using the service, particularly in the wake of the September 11, 2001 attacks.
Although Western Union keeps information about senders and receivers, some transactions can be done essentially anonymously, for the receiver is not always required to show identification International Most international transfers are executed through SWIFT, a co-operative society, founded in 1974 by seven international banks, which operates a global network to facilitate the transfer of financial messages.
Using these messages, banks can exchange data for funds transfer between financial institutions.
SWIFT's headquarters are in La Hulpe, on the outskirts of Brussels, Belgium.
The society also acts as a United Nations sanctioned international-standards body, for the creation and maintenance of financial-messaging standards.
See SWIFT Standards.
Each financial institution is provided an ISO 9362 code, also called a Bank Identifier Code (BIC) or SWIFT Code.
These codes generally are eight characters long.
For example: Deutsche Bank is an international bank, with its head office in Frankfurt, Germany, the SWIFT Code for which is DEUTDEFF: DEUT identifies Deutsche Bank.
DE is the country code for Germany.
FF is the code for Frankfurt.
Using an extended code of 11 digits (if the receiving bank has assigned extended codes to branches or to processing areas) allows the payment to be directed to a specific office.
For example: DEUTDEFF500 would direct the payment to an office of Deutsche Bank in Bad Homburg.
United States 68  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Banks in the United States use SWIFT to make payments to banks in other countries.
Domestic bank-to-bank transfers are conducted through the Fedwire system, which uses the Federal Reserve System and its assignment of routing transit number, which uniquely identify each bank.
4.0 CONCLUSION Advances in information technology have presented financial institutions with several options of electronic transaction methods to facilitate business processes.
The coming of the Internet and the World Wide Web has made the operation of these electronic payment options to be worthwhile.
Despite some lapses especially in the area of security, electronic payment systems have come to stay in modern-day business financial transactions.
5.0 SUMMARY • Elect ronic Bill Payment is a feature of online banking, similar in its effect to a giro, allowing a depositor to send money from his demand account to a creditor or vendor such as a public utility or a department store to be credited against a specific account.
• EFTPOS (short for Electronic Funds Transfer at Point of Sale) is an Australian and New Zealand electronic processing system for credit cards, debit cards and charge cards.
• EFT may be initiated by a cardholder when a payment card such as a credit card or debit card is used.
This may take place at an automated teller machine (ATM) or point of sale (POS), or when the card is not present, which covers cards used for mail order, telephone order and internet purchases • Elect ronic money (also known as e-money , electronic cash, electronic currency, digita l mo ney, dig ital cash or digita l currency ) refers to money which is exchanged only electronically.
• In off-line electronic money the merchant does not need to interact with the bank before accepting a coin from the user.
Instead he can collect multiple coins Spent by users and Deposit them later with the bank.
• Wire Transfer is a method of transferring money from one entity to another.
A wire transfer can be made from one entity's bank account to the other entity's bank account, and by a transfer of cash at a cash office.
• Although the genesis of the idea dates as far back as the giro, the 69  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY modern wire transfer was a product of the telegraph companies, which made it possible to wire a money order from one office to another.
• Bank wire transfers are often the most expedient method for transferring funds between bank accounts.
• Bank transfer is the most common payment method in Europe, with several million transactions processed each day.
• Bank-to-bank wire transfer is considered the safest international payment method.
Each account holder must have a proven identity.
• One of the largest companies that offer wire transfer is Western Union, which allows individuals to transfer or receive money without an account with Western Union or any financial institution.
6.0 TUTOR-MARK ED ASSIGNMENT 1.
Identify 5 concepts associated with the definition of electronic fund transfer.
2.
Discuss briefly the processes of bank wire transfer.
7.0 REF ERENCES/FURTHER READING EFTPOS.
Merchant banking services.
EFTPOS.
Bank of Queensland Australia.
Nab - Eftpos ATMOSS - Australian Trade Mark Online Search System.
Nab - Eftpos David Chaum, Blind signatures for untraceable payments, Advances in Cryptology - Crypto '82, Springer-Verlag (1983), 199-203.
(P DF) Chaum, D., Fiat, A., and Naor, M. 1990.
Untraceable electronic cash.
In Proceedings on Advances in Cryptology (Santa Barbara, California, United States).
S. Goldwasser, Ed.
Springer-Verlag New York, New York, NY, 319-327.
(PDF) registered.
Regulation (EC) No 2560/2001.
European Parliament and the Council of the European Union Regulation J- Check Collection and Funds Transfer.
BankersOnline.com Section 4A of Universal Commercial Code.
Legal Information Institute.
70  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY OFAC Facts Western Union Mone y Transfer Options Can Western Union Keep On Delivering?
BusinessWeek About BIC.
Swift - BIC Portal 71  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY MODULE 2 Unit 1 Digital Signature and Electronic Signature Unit 2 Multimedia Unit 3 The World Wide Web Unit 4 Personal Digital Assistant and Sub Notebooks Unit 5 Bluetooth UNIT 1 DIGITAL SIGNATURE AND ELECTRO NIC SIGNATURE CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Definition 3.2 History 3.3 Digital Signature Vs Electronic Signature 3.4 Notions of Security 3.5 Benefits of Digital Signatures 3.6 Drawbacks of Digital Signatures 3.7 Additional Security Precautions 3.8 Some Digital Signature Algorithms 3.9 The Current State of Use-Legal and Practical 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N A digita l signa ture or digital signature scheme is a type of asymmetric cryptography used to simulate the security properties of a handwritten signature on paper.
Digital signature schemes consist of at least three algorithms: a ke y generation algorithm, a signature algorithm, and a verification algorithm.
A digital signature mainly provides authentication of a "message".
In theory it can also provide non- repudiation, meaning that the authenticity of signed messages can be publicly verified, not only by the intended recipient.
Messages may be anything, from electronic mail to a contract, or even a message sent in a more complicated cryptographic protocol.
Digital signatures are often used to implement electronic signatures, a broader term that refers to any electronic data that carries the intent of a 72  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY signature, but not all electronic signatures use digital signatures.
In some countries, including the United States, and in the European Union, electronic signatures have legal significance.
However, laws concerning electronic signatures do not always make clear their applicability towards cryptographic digital signatures, leaving their legal importance somewhat unspecified.
Digital signature is a subset of electronic signature.
It is an emerging form of electronic signature.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define digital and electronic signatures • compare and contrast digital and electronic signatures • identify the basic features of digital and electronic signatures • understand the applications of digital and electronic signatures • trace the history and development of digital and electronic signatures • identify the drawbacks associated with digital and electronic signatures.
3.0 MAIN CONTENT 3.1 Definition A digital signature scheme typically consists of three algorithms: A key generation algorithm that selects a private key uniformly at random from a set of possible private keys.
The algorithm outputs the private key and a corresponding public key.
A signing algorithm which, given a mess age and a privat e key, produces a signature.
A signature verifying algorithm which given a message, public key and a signature, either accepts or rejects.
Two main properties are required.
First, a signature generated from a fixed message and fixed private key should verify on that message and the corresponding public key.
Secondly, it should be computationally infeasible to generate a valid signature for a party who does not possess the private key.
73  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.2 History In the famous paper "New Directions in Cryptography", Whitfield Diffie and Martin Hellman first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.
Soon afterwards, Ronald Rivest, Adi Shamir, and Len Adleman invented the RSA algorithm that could be used for primitive digital signatures (Note that this just serves as a proof-of-concept, and "plain" RSA signatures are not secure.).
The first widely marketed software package to offer digital signature was Lotus Notes 1.0, released in 1989, which used the RSA algorithm.
Basic RSA signatures are computed as follows.
To generate RSA signature keys, one simply generates an RSA key pair containing a modulus N that is the product of two large primes, along with integers e and d such that e d = 1 mod (N), where is the Euler phi-function.
The signer's public key consists of N and e, and the signer's secret key contains d. To sign a message m, the signer computes =md mod N. To verify, the receiver checks that e = m mod N. As noted earlier, this basic scheme is not very secure.
To prevent attacks, one can first apply a cryptographic hash function to the message m and then apply the RSA algorithm described above to the result.
This approach can be proven secure in the so-called random oracle model.
Other digital signature schemes were soon developed after RSA, the earliest being Lamport signatures, Merkle signatures (also known as "Merkle trees" or simply "Hash trees"), and Rabin signatures.
In 1984, Shafi Goldwasser, Silvio Micali, and Ronald Rives t became the first to rigorously define the security requirements of digital signature schemes.
They described a hierarchy of attack models for signature schemes, and also presented the GMR signature scheme, the first that can be proven to prevent even an existential forgery against a chosen message attack.
Most early signature schemes were of a similar type: they involve the use of a trapdoor permutation, such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite n. A trapdoor permutation family is a family of permutations, specified by a parameter, which is easy to compute in the forward direction, but is difficult to compute in the reverse direction.
However, for every parameter there is a "trapdoor" that enables easy computation of the reverse direction.
Trapdoor permutations can be viewed as public-key 74  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY encryption systems, where the parameter is the public key and the trapdoor is the secret key, and where encrypting corresponds to computing the forward direction of the permutation, while decrypting corresponds to the reverse direction.
Trapdoor permutations can also be viewed as digital signature schemes, where computing the reverse direction with the secret key is thought of as signing, and computing the forward direction is done to verify signatures.
Because of this correspondence, digital signatures are often described as based on public-key cryptosystems, where signing is equivalent to decryption and verification is equivalent to encryption, but this is not the only way digital signatures are computed.
Used directly, this type of signature scheme is vulnerable to a key-only existential forgery attack.
To create a forgery, the attacker picks a random signature and uses the verification procedure to determine the message m corresponding to that signature.
In practice, however, this type of signature is not used directly, but rather, the message to be signed is first hashed to produce a short digest that is then signed.
This forgery attack, then, only produces the hash function output that corresponds to , but not a message that leads to that value, which does not lead to an attack.
In the random oracle model, this hash-and-decrypt form of signature is existentially unforgeable, even against a chosen- message attack.
There are several reasons to sign such a hash (or message digest) instead of the whole document.
For Efficiency : The signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.
For Co mpa tibility: Messages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number N).
A hash function can be used to convert an arbitrary input into the proper format.
For Integrity: Without the hash function, the text "to be signed" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly.
However, the receiver of the signed blocks is not able to recognise if all the blocks are present and in the appropriate order.
3.3 Digital Signature vs Electronic Signature The term electronic sig na ture has several meanings.
Among the more expansive is that given by US law, influenced by ABA committee white papers and the uniform law promulgated by the National Conference of 75  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Commissioners on Uniform State Laws (NCCUSL).
Under the Uniform Electronic Transactions Act or "UETA" released by NCCUSL in 1999, the term means "an electronic sound, symbol, or process, attached to or logically associated with a record and executed or adopted by a person with the intent to sign the record."
This definition and many other core concepts of UETA are echoed in the U.S. E Sign Act of 2000 46 US states, the District of Columbia, and the US Virgin Islands have enacted UETA.
The concept itself is not new.
US and other common law contain references to t elegraph signatures and faxed signatures, some as far back as the mid-19th century.
For that matter, the text of, and comments to, US Federal Rules of Evidence 1001, 1002, and 1003, among others, give good support for the proposition that electronic records and signatures would be admissible in court.
There is confusion between the terms electronic signature and digital signature.
Most, especially those with an information theory or cryptography background, use "digital signature" to refer to a digital signature protocol using cryptographic techniques, as is sometimes applied to an 'electronic document'.
Many, however, use the terms interchangeably, leading to considerable confusion as cryptographic signature techniques are very different, whatever the term used, than other electronic signatures and have extremely different security properties.
Since it is the security properties which are of interest in signatures of all kinds, this is a very significant distinction.
Digital signature is properly a subset of electronic signature.
In the European Union, the EU Directive on Electronic Signatures or the EU Electronic Signatures Directive was published in the EC Official Journal, as Directive 1999/93/EC of the European P arliament and of the Council of 13 December 1999 on a Community framework for electronic signatures (OJ No L 13 p.12 19/1/2000).
3.4 Notions of Security In their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures: In a key-only attack, the attacker is only given the public verification key.
In a known message attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.
76  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In an adaptive chosen message attack, the attacker first learns signatures on arbitrary messages of the attacker's choice.
They also describe a hierarchy of attack results: • A total break results in the recovery of the signing key.
• A universal forgery attack results in the ability to forge signatures for any message.
• A selective forgery attack results in a signature on a message of the adversary's choice.
• An existential forgery merely results in some valid message/signature pair not already known to the adversary.
The strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.
3.5 Benefits of Digital Signature s Below are some common reasons for applying a digital signature to communications: Authentica tio n Although messages may often include information about the entity sending a message, that information may not be accurate.
Digital signatures can be used to authenticate the source of messages.
When ownership of a digital signature secret key is bound to a specific user, a valid signature shows that the message was sent by that user.
The importance of high confidence in sender authenticity is especially obvious in a financial context.
For example, suppose a bank's branch office sends instructions to the central office requesting a change in the balance of an account.
If the central office is not convinced that such a message is trul y sent from an authorised source, acting on such a request could be a grave mistake.
Integ rity In many scenarios, the sender and receiver of a message may have a need for confidence that the message has not been altered during transmission.
Although encryption hides the contents of a message, it may be possible to change an encrypted message without understanding it.
(Some encryption algorithms, known as nonmalleable ones, prevent this, but others do not.)
However, if a message is digitally signed, any change in the message will invalidate the signature.
Furthermore, there is no efficient way to modify a message and its signature to produce a 77  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY new message with a valid signature, because this is still considered to be computationally infeasible by most cryptographic hash functions 3.6 Drawbacks of Digital Signatures Despite their usefulness, digital signatures alone do not solve the following problems: Asso cia tion of Dig ital Sig na tures and Trusted Time Sta mping Digital signature algorithms and protocols do not inherently provide certainty about the date and time at which the underlying document was signed.
The signer might have included a time stamp with the signature, or the document itself might have a date mentioned on it.
Regardless of the document's contents, a reader cannot be certain the signer did not, for example, backdate the date or time of the signature.
Such misuse can be made impracticable by using trusted time stamping in addition to digital signatures.
Non-Repudiatio n In a cryptographic context, the word repudiation refers to any act of disclaiming responsibility for a message.
A message's recipient may insist the sender attach a signature in order to make later repudiation more difficult, since the recipient can show the signed message to a third party (e.g., a court) to reinforce a claim as to its signatories and integrity.
However, loss of control over a user's private key will mean that all digital signatures using that key, and so ostensibly 'from' that user, are suspect.
Nonetheless, a user cannot repudiate a signed message without repudiating their signature key.
This is aggravated by the fact that there is no trusted time stamp, so new documents (after the key compromise) cannot be separated from old ones, further complicating signature key invalidation.
A non-repudiation service requires the existence of a public key infrastructure (PKI) which is complex to establish and operate.
The Certificate authorities in a PKI usually maintain a public repository of public keys so the associated private key is certified and signatures cannot be repudiated.
Expired certificates are normall y removed from the repository.
It is a matter for the security policy and the responsibility of the authority to keep old certificates for a period of time if non- repudiation of data service is provided.
WYSIWY S Technically speaking, a digital signature applies to a string of bits, whereas humans and applications "believe" that they sign the semantic interpretation of those bits.
In order to be semantically interpreted the bit 78  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system.
The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content.
It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed.
From a semantic perspective this creates uncertainty about what exactly has been signed.
WYSIWYS (What You See Is What You Sign) means that the semantic interpretation of a signed message can not be changed.
In particular this also means that a message can not contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied.
WYSIWYS is a desirable property of digital signatures that is difficult to guarantee because of the increasing complexity of modern computer systems.
3.7 Additional Security Precautions Putting the Private Key on a Smart Card All public key / private key cryptosystems depend entirely on keeping the private key secret.
A private key can be stored on a user's computer, and protected by a local password, but this has two disadvantages: • the user can only sign documents on that particular computer • the security of the private key depends entirely on the security of the computer A more secure alternative is to store the private key on a smart card.
Many smart cards are designed to be tamper-resistant (although some designs have been broken, notably by Ross Anderson and his students).
In a typical digital signature implementation, the hash calculated from the document is sent to the smart card, whose CPU encrypts the hash using the stored private key of the user, and then returns the encrypted hash.
Typically, a user must activate his smart card by entering a personal identification number or PIN code (thus providing two-factor authentication).
It can be arranged that the private key never leaves the smart card, although this is not always implemented.
If the smart card is stolen, the thief will still need the PIN code to generate a digital signature.
This reduces the security of the scheme to that of the PIN system, although it still requires an attacker to possess the card.
A mitigating factor is that private keys, if generated and stored on smart cards, are usually regarded as difficult to copy, and are assumed to exist in exactly one copy.
Thus, the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked.
79  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Private keys that are protected by software only may be easier to copy, and such compromises are far more difficult to detect.
Using Smart Ca rd Readers with a Separate Keyboard Entering a PIN code to activate the smart card commonly requires a numeric keypad.
Some card readers have their own numeric keypad.
This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard.
Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a keystroke logger, potentially compromising the PIN code.
Specialised card readers are also less vulnerable to tampering with their software or hardware and are often EAL3 certified.
Other Smart Card Desig ns Smart card design is an active field, and there are smart card schemes which are intended to avoid these particular problems, though so far with little security proofs.
Using Digital Sig natures Only with Trusted Applicatio ns One of the main differences between a digital signature and a written signature is that the user does not "see" what he signs.
The user application presents a hash code to be encrypted by the digital signing algorithm using the private key.
An attacker who gains control of the user's PC can possibly replace the user application with a foreign substitute, in effect replacing the user's own communications with those of the attacker.
This could allow a malicious application to trick a user into signing any document by displaying the user's original on-screen, but presenting the attacker's own documents to the signing application.
To protect against this scenario, an authentication system can be set up between the user's application (word processor, email client, etc.)
and the signing application.
The general idea is to provide some means for both the user application and signing application to verify each other's integrity.
For example, the signing application may require all requests to come from digitally-signed binaries.
3.8 Some Digital Signature Algorithms Full Domain Hash, RSA-PSS etc., based on RSA DSA ECDSA ElGamal signature scheme 80  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Undeniable signature SHA (typically SHA-1) with RSA Rabin signature algorithm Pointcheval-Stern signature algorithm Schnorr signature Aggregate signature - a signature scheme that supports aggregation: Given n signatures on n messages from n users, it is possible to aggregate all these signatures into a single signature whose size is constant in the number of users.
This single signature will convince the verifier that the n users did indeed sign the n original messages.
3.9 The Current State of Use-Legal and Practical Digital signature schemes all have several prior requirements without which no such signature can mean anything, what ever the cryptographic theory or legal provision.
First, quality algorithms ; some public-key algorithms are known to be insecure, practicable attacks against them having been discovered.
Second, quality implementations; an implementation of a good algorithm (or protocol) with mistake(s) will not work.
Third, the private key must remain actually secret; if it becomes known to any other party, that party can produce perfect digital signatures of anything whatsoever.
Fourth, distribution of public keys must be done in such a way that the public key claimed to belong to, say, Bob actually belongs to Bob, and vice versa.
This is commonly done using a public key infrastructure and the public key user association is attested by the operator of the PKI (called a certificate authority).
For 'open' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected identity certificate), the possibility of mistaken attestation is non trivial.
Commercial PKI operators have suffered several publicly known problems.
Such mistakes could lead to falsely signed, and thus wrongly attributed, documents.
'Closed' PKI systems are more expensive, but less easily subverted in this way.
Fifth, users (and their software) must carry out the signature protocol properly.
Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its 81  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY contents.
Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.
Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorising, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect.
California and other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time.
These enactments (or proposed enactments) vary from place to place, and have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying cryptographic engineering, and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable.
Adoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on interoperability, algorithm choice, key lengths, and so on what the engineering is attempting to provide.
Using Separa te Key Pairs for Sig ning and Encryptio n In several countries, a digital signature has a status somewhat like that of a traditional pen and paper signature.
Generally, these provisions mean that what is digitally signed legally binds the signer of the document to the terms therein.
For that reason, it is often thought best to use separate key pairs for encrypting and signing.
Using the encryption key pair, a person can engage in an encrypted conversation (e.g., regarding a real estate transaction), but the encryption does not legally sign every message he sends.
Only when both parties come to an agreement do they sign a contract with their signing keys, and only then are they legally bound by the terms of a specific document.
After signing, the document can be sent over the encrypted link.
4.0 CO NCLUSIO N Electronic signature and its offshoot, digital signature has helped tremendously in tracking and minimising illegal access to databases and accounts.
It has also brought about ease and speed in transactions.
Though there are attending challenges, but this is far below the advantages.
82  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 5.0 SUMMARY • A digita l signature or dig ital signature scheme is a type of asymmetric cryptography used to simulate the security properties of a handwritten signature on paper.
• In the famous paper "New Directions in Cryptography", Whitfield Diffie and Martin Hellman first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.
• The term electronic sig na ture has several meanings.
Among the more expansive is that given by US law, influenced by ABA committee white papers and the uniform law promulgated by the National Conference of Commissioners on Uniform State Laws (NCCUSL), under the Uniform Electronic Transactions Act or "UETA" released by NCCUSL in 1999.
• In their foundational paper, Goldwasser, Micali, and Rivest laid out a hierarchy of attack models against digital signatures.
• Although messages may often include information about the entity sending a message, that information may not be accurate.
Digital signatures can be used to authenticate the source of messages.
• Digital signature algorithms and protocols do not inherently provide certainty about the date and time at which the underlying document was signed.
• All public key / private key cryptosystems depend entirely on keeping the private key secret.
A private key can be stored on a user's computer, and protected by a local password, but this has two disadvantages.
• Digital signature schemes all have several prior requirements without which no such signature can mean anything, whatever the cryptographic theory or legal provision 6.0 TUTOR-MARKED ASSIGNMENT 1.
Briefly describe the hierarchy of attack result in digital signature security.
2.
List 5 digital signature algorithms 7.0 REF ERENCES/FURTHER READING US ESIGN Act of 2000 The University of Virginia State of WI National Archives of Australia 83  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY "New Directions in Cryptography", IEEE Transactions on Information Theory, IT-22(6):644-654, Nov. 1976.
"Signature Schemes and Applications to Cryptographic Protocol Design", Anna Lys yanskaya, PhD Thesis, MIT, 2002.
"A Method For Obtaining Digital Signatures and Public-Key Cryptosystems," Communications of the ACM, 21(2): 120-126, Feb. 1978.
"Constructing Digital Signatures from a One-Way Function.
", Leslie Lamport, Technical Report CSL-98, SRI International, Oct. 1979.
"A certified digital signature", Ralph Merkle, In Gilles Brassard, ed., Advances in Cryptology -- CRYPTO '89, vol.
435 of Lecture Notes in Computer Science, pp.
218-238, Spring Verlag, 1990.
"Digitalized Signatures as Intractable as Factorization."
Michael O. Rabin, Technical Report MIT/LCS/TR-212, MIT Laboratory for Computer Science, Jan. 1979 A Digital Signature Scheme Secure Against Adaptive Chosen-Message Attacks.
Shafi Goldwasser, Silvio Micali, and Ronald Rivest.
SIAM Journal on Computing, 17(2):281-308, Apr.
1988.
"Modern Cryptography: Theory & Practice", Wenbo Mao, Prenti ce Hall Professional Technical Reference, New Jersey, 2004, pg.
308.
ISBN 0-13-066943-1 A. Jøsang, D. Povey and A. Ho.
"What You See is Not Always What You Sign".
Proceedings of the Australian Unix User Group Symposium (AUUG2002), Melbourne, September, 2002.
PDF 84  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 2 MULTIMEDIA CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Categorisation of Multimedia 3.2 Major Characteristics of Multimedia 3.3 Terminology 3.4 Usage 3.5 Structuring Information in a Multimedia Form 3.6 Conferences 3.7 Multimedia Computer 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N Multim edia is media that utilises a combination of different content forms.
The term can be used as a noun (a medium with multiple content forms) or as an adjective describing a medium as having multiple content forms.
The term is used in contrast to media which only utilise traditional forms of printed or hand-produced text and still graphics.
In general, multimedia includes a combination of text, audio, still images, animation, video, and interactivity content forms.
Multimedia is usually recorded and played, displayed or accessed by information content processing devices, such as computerised and electronic devices, but can also be part of a live performance.
Multimedia (as an adjective) also describes electronic media devices used to store and experience multimedia content.
Multimedia is similar to traditional mixed media in fine art, but with a broader scope.
The term "rich media" is synonymous for interactive multimedia.
Hypermedia can be considered as one particular multimedia application.
2.0 OBJECTIVES At the end of this unit, you should be able to: define multimedia identify the categories of multimedia understand the basic characteristics of multimedia identify the uses of multimedia identify multimedia computer and it features.
85  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.0 MAIN CONTENT 3.1 Categorisa tion of Multimedia Multimedia may be broadly divided into linear and non-linea r categories.
Linear active content progresses without any navigation control for the viewer such as a cinema presentation.
Non-linear content offers user interactivity to control progress as used with a computer game or used in self-paced computer based training.
Non-linear content is also known as hypermedia content.
Multimedia presentations can be live or recorded.
A recorded presentation may allow interactivity via a navigation system.
A live multimedia presentation may allow interactivity via an interaction with the presenter or performer.
3.2 Major Characteristics of Multimedia Multimedia Presentatio ns may be viewed in person on stage, projected, transmitted, or played locally with a media player.
A broadcast may be a live or recorded multimedia presentation.
Broadcasts and recordings can be either analog or digital electronic media technology.
Digital online multimedia may be downloaded or streamed.
Streaming multimedia may be live or on-demand.
Multimedia Games and Simu latio ns may be used in a physical environment with special effects, with multiple users in an online network, or locally with an offline computer, game s ystem, or simulator.
The various formats of technological or digital multimedia may be intended to enhance the users' experience; for example to make it easier and faster to convey information, or in entertainment or art, to transcend everyday experience.
Enhanced levels of interactivity are made possible by combining multiple forms of media content.
Online multimedia is increasingly becoming object-oriented and data-driven, enabling applications with collaborative end-user innovation and personalisation on multiple forms of content over time.
Examples of these range from multiple forms of content on Web sites like photo galleries with both images (pictures) and title (text) user-updated, to simulations whose co-efficient, events, illustrations, animations or videos are modifiable, allowing the multimedia "experience" to be altered without reprogramming.
In addition to seeing and hearing, Haptic technology enables virtual objects to be felt.
Emerging technology involving illusions of taste and smell may also enhance the multimedia experience.
86  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.3 Terminology History of the Term In 1965 the term Multi-media was used to describe the Exploding Plastic Inevitable, a performance that combined live rock music, cinema, experimental lighting and performance art.
In the intervening forty years the word has taken on different meanings.
In the late 1970s the term was used to describe presentations consisting of multi-projector slide shows timed to an audio track.
In the 1990s it took on its current meaning.
In common usage the term multimedia refers to an electronically delivered combination of media including video, still images, audio, and text in such a way that can be accessed interactively.
Much of the content on the web today falls within this definition as understood by millions.
Some computers which were marketed in the 1990s were called "multimedia" computers because they incorporated a CD-ROM drive, which allowed for the delivery of several hundred megabytes of video, picture, and audio data.
Wo rd Usa ge a nd Context Since media is the plural of medium, the term "multimedia" is a pleonasm if "multi" is used to describe multiple occurrences of only one form of media such as a collection of audio CDs.
This is why it's important that the word "multimedia" is used exclusively to describe multiple forms of media.
The term "multimedia" is also ambiguous.
Static content (such as a paper book) may be considered multimedia if it contains both pictures and text or may be considered interactive if the user interacts by turning pages at will.
Books may also be considered non-linear if the pages are accessed non-sequentially.
The term "video" , if not used exclusively to describe motion photography, is ambiguous in multimedia terminology.
Video is often used to describe the file format, delivery format, or presentation format instead of "footage" which is used to distinguish motion photography from "animation", motion illustrations.
Multiple forms of information content are often not considered multimedia if they don't contain modern forms of presentation such as audio or video.
Likewise, single forms of information content with single methods of information processing (e.g.
non-interactive audio) are often called multimedia, perhaps to distinguish static media from active media.
87  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.4 Usage A presentation using powerpoint, corporate presentations may combine all forms of media.Virtual reality uses multimedia content.
Applications and delivery platforms of multimedia are virtually limitless.
Multimedia finds its application in various areas including, but not limited to, advertisements , art, education, entertainment, engineering, medicine, mathematics, business, scientific research and spatial temporal applications.
Several examples are as follows: Crea tive Industries Creative industries use multimedia for a variety of purposes ranging from fine arts, to entertainment, to commercial art, to journalism, to media and software services provided for any of the industries listed below.
An individual multimedia designer may cover the spectrum throughout their career.
Request for their skills range from technical, to analytical, to creative.
Commercial Much of the electronic old and new media utilised by commercial artists is multimedia.
Exciting presentations are used to grab and keep attention in advertising.
Industrial, business to business, and interoffice communications are often developed by creative services firms for advanced multimedia presentations beyond simple slide shows to sell ideas or liven-up training.
Commercial multimedia developers may be hired to design for governmental services and nonprofit services applications as well.
Entertainment and Fine Arts In addition, multimedia is heavily used in the entertainment industry, especially to develop special effects in movies and animations.
Multimedia games are a popular pastime and are software programmes available either as CD-ROMs or online.
Some video games also use multimedia features.
Multimedia applications that allow users to actively participate instead of just sitting by as passive recipients of information are called Interactive Multimedia.
In the Arts there are multimedia artists, whose minds are able to blend techniques using different media that in some way incorporates interaction with the viewer.
One of the most relevant could be Peter Greenaway who is melding Cinema with Opera and all sorts of digital media.
Another approach entails the creation of multimedia that can be displayed in a traditional fine arts arena, such as an art gallery.
Although multimedia display material may 88  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY be volatile, the survivability of the content is as strong as any traditional media.
Digital recording material may be just as durable and infinitely reproducible with perfect copies every time.
Educa tion In Education, multimedia is used to produce computer-based training courses (popularly called CBTs) and reference books like encyclopaedia and almanacs.
A CBT lets the user go through a series of presentations, text about a particular topic, and associated illustrations in various information formats.
Edutainment is an informal term used to describe combining education with entertainment, especially multimedia entertainment.
Learning theory in the past decade has expanded dramatically because of the introduction of multimedia.
Several lines of research have evolved (e.g.
Cognitive load, Multimedia learning, and the list goes on).
The possibilities for learning and instruction are nearly endless.
Engineering Software engineers may use multimedia in Computer Simulations for anything from entertainment to training such as military or industrial training.
Multimedia for software interfaces are often done as collaboration between creative professionals and software engineers.
Industry In the Industrial sector, multimedia is used as a way to help present information to shareholders, superiors and coworkers.
Multimedia is also helpful for providing employee training, advertising and selling products all over the world via virtually unlimited web-based technologies.
Mathema tica l and Scientific Research In Mathematical and Scientific Research, multimedia is mainly used for modelling and simulation.
For example, a scientist can look at a molecular model of a particular substance and manipulate it to arrive at a new substance.
Representative research can be found in journals such as the Journal of Multimedia.
89  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Medicine In Medicine, doctors can get trained by looking at a virtual surgery or they can simulate how the human body is affected by diseases spread by viruses and bacteria and then develop techniques to prevent it.
Miscellaneo us In Europe, the reference organisation for Multimedia industry is the European Multimedia Associations Convention (EMMAC).
An observatory for jobs in the multimedia industry provides surveys and analysis about multimedia and ITC jobs.
3.5 Structuring Information in a Multimedia F orm Multimedia represents the convergence of text, pictures, video and sound into a single form.
The power of multimedia and the Internet lies in the way in which information is linked.
Multimedia and the Internet require a completely new approach to writing.
The style of writing that is appropriate for the 'on-line world' is highly optimised and designed to be able to be quickly scanned by readers.
A good site must be made with a specific purpose in mind and a site with good interactivity and new technology can also be useful for attracting visitors.
The site must be attractive and innovative in its design, function in terms of its purpose, easy to navigate, frequently updated and fast to download.
When users view a page, they can only view one page at a time.
As a result, multimedia users must create a mental model of information structure .
Patrick Lynch, author of the Yale University Web Style Manual, states that users need predictability and structure, with clear functional and graphical continuity between the various components and subsections of the multimedia production.
In this way, the home page of any multimedia production should always be a landmark, able to be accessed from anywhere within a multimedia piece.
90  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.6 Conferences There are a large number of multimedia conferences, the two main scholarly scientific conferences being: ACM Multimedia; ICME, International Conference on Multimedia & Expo.
3.7 Multimedia Computer A multimedia computer is a computer that is optimised for high multimedia performance, enabling rich multimedia experience.
Early home computers simply lacked the power and storage necessary for true multimedia.
The games for these systems, along with the demo scene were able to achieve high sophistication and technical polish using only simple, blocky graphics and digitally-generated sound.
The Amiga 1000 from Commodore has been called the first multimedia computer.
Its groundbreaking animation, graphics and sound technologies enabled multimedia content to flourish.
Famous demos such as the Boing Ball and Juggler showed off the Amiga's abilities.
Later the Atari ST series and Apple Macintosh II extended the concept; the Atari integrated a MIDI port and was the first computer under $1000USD to have 1 megabyte of RAM which is a realistic minimum for multimedia content and the Macintosh was the first com puter able to display true photorealistic graphics as well as integrating a CD-ROM drive, whose high capacity was essential for delivering multimedia content in the pre- Internet era.
Multimedia capabilities weren't common on IBM PC compatibles until the advent of Windows 3.0 and the MPC standards in the early 1990s.
The original PCs were devised as "serious" business machines and colorful graphics and powerful sound abilities weren't a priority.
The few games available suffered from slow video hardware, P C speaker sound and limited color palette when compared to its contemporaries.
But as PCs penetrated the home market in the late 1980s, a thriving industry arose to equip PCs to take advantage of the latest sound, graphics and animation technologies.
Creative's SoundBlaster series of sound cards, as well as video cards from ATI, NVidia and Matrox soon became standard equipment for most PCs sold.
Most PCs today have good multimedia features.
They have dual- or single-core CP Us clocked at 3.0 GHz or faster, at least 1GB of RAM, a 128 MB or higher video card and TV Tuner card.
Popular graphics cards include Nvidia Gforce or ATI Radeon.
The Intel Viiv platform and 91  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Microsoft Windows XP Media Center Edition are some of today's products aimed at multimedia computing.
More recently, high-performance devices have become more compact, and multimedia computer capabilities are found in mobile devices such as the Apple iPhone and Nokia Nseries, featuring DVD-like video quality, megapixel class cameras, fully capable browser, music and video players, podcasting, blogging, as well as e-mail, instant messaging, presence and internet call (VoIP) functionality.
Multiradios help to offer broadband wireless connectivity, including for instance WCDM A/HSDPA and WLAN/Wifi.
Devices are also increasingly equipped with GPS receivers and maps applications, providing new capabilities for location-aware services.
The Nseries devices are also expandable, allowing for the addition of multiple applications and multimedia content.
4.0 CO NCLUSIO N Multimedia remains a technology that brought a tremendous change to the use of computer and its dimension of applications.
It made computer to be user friendly through the availability of audio and lively visuals.
It is a technology that has not been fully utilised especially in developing countries.
As a form of emerging technology it continues to experience changes to reflect the need of the business and computer world.
5.0 SUMMARY • Multimedia is media that utilises a combination of different content forms.
The term can be used as a noun (a medium with multiple content forms) or as an adjective describing a medium as having multiple content forms.
• Multimedia may be broadly divided into linear and non-linear categories.
Linear active content progresses without any navigation control for the viewer such as a cinema presentation.
• Multimedia presenta tio ns may be viewed in person on stage, projected, transmitted, or played locally with a media player.
A broadcast may be a live or recorded multimedia presentation.
• In 1965 the term Multi-media was used to describe the Exploding Plastic Inevitable, a performance that combined live rock music, cinema, experimental lighting and performance art.
• Much of the electronic old and new media utilised by commercial artists is multimedia.
Exciting presentations are used to grab and keep attention in advertising.
• Multimedia represents the convergence of text, pictures, video and sound into a single form.
The power of multimedia and the Internet lies in the way in which information is linked.
92  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • There are a large number of multimedia conferences.
• A multimedia computer is a com puter that is optimised for high multimedia performance, enabling rich multimedia experience.
6.0 TUTOR-MARKED ASSIGNMENT 1.
Mention 5 areas/sectors where multimedia is put to use.
2.
Briefly describe the categories of multimedia.
7.0 REF ERENCES/FURTHER READING Tay Vaughan Stewart, C and Kowaltzke, A.
1997, Media: New Ways and Meanings (second edition), JACARANDa, Milton, S ydney.
pp.102.
Jennifer Story, from Next Online,2002.
Lynch P., Yale University Web Style Manual, Http://info.med.yale.edu/caim/manual/sites/site_structure.heml.
Multimedia Making it Work, by Tay Vaughan.
Osborne McGraw Hill, 1993.
ISBN 0-07-881869.
93  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 3 THE WORLD W IDE WEB CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History of the World Wide Web 3.2 1980-91: Development of the World Wide Web 3.3 Growth of the WWW 3.4 1996-1998: Commercialisation of the WWW 3.5 Browser Wars 3.6 1999-2001: "Dot-com" Boom and Bust 3.7 2002-Present: The Web becomes Ubiquitous 3.8 How the Web Works 3.9 Standards 3.10 Publishing Web Pages 3.11 Statistics 3.12 Speed Issues 3.13 Security 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N The Wo rld Wide Web (commonly shortened to the Web) is a system of interlinked hypertext documents accessed via the Internet.
With a Web browser, a user views Web pages that may contain text, images, videos, and other multimedia and navigates between them using hyperlinks.
The World Wide Web was created in 1989 by Sir Tim Berners-Lee, working at the European Organisation for Nuclear Research (CERN) in Geneva, Switzerland and released in 1992.
Since then, Berners-Lee has played an active role in guiding the development of Web standards (such as the markup languages in which Web pages are composed), and in recent years has advocated his vision of a Semantic Web.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define the world wide web • trace the origin and development of world wide web • describe how world wide web grew to what it is today 94  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • understand how the world wide web works • appreciate the security issues of the world wide web • identify the applications of the world wide web.
3.0 MAIN CONTENT 3.1 H istory of the World Wide Web Today, the Web and the Internet allow connectivity from literally everywhere on earth-even ships at sea and in outer space.
The Wo rld Wide Web ("WWW " or simply the "Web") is a global information medium which users can read and write via computers connected to the Internet.
The term is often mistakenly used as a synonym for the Internet itself, but the Web is a service that operates over the Internet, as e-mail does.
The history of the Internet dates back significantly further than that of the World Wide Web.
The hypertext portion of the Web in particular has an intricate intellectual history; notable influences and precursors include Vannevar Bush's Memex, IBM's Generalised Markup Language, and Ted Nelson's Project Xanadu.
The concept of a home-based global information system goes at least as far back as Isaac Asimov's short story "Anniversary" (Amazing Stories, March 1959), in which the characters look up information on a home computer called a "Multivac outlet" which was connected by a "planet wide network of circuits" to a mile-long "super-computer" somewhere in the bowels of the Earth.
One character is thinking of installing a Mulitvac, Jr. model for his children.
Interestingly, although the story was set in the far distant future when commercial space travel is a commonplace, the machine "prints the answer on a slip of tape" that comes out a slot there is no video display and the owner of the home computer says that he doesn't spend the kind of money to get a Multivac outlet that talks.
3.2 1980-91: Development of the World Wide Web The NeXTcube used by Tim Berners-Lee at CERN became the first Web server.
In 1980, the Englishman Tim Berners-Lee, an independent contractor at CERN, Switzerland, built ENQUIRE, as a personal database of people and software models, but also as a way to play with hypertext; each new page of information in ENQUIRE had to be linked to an existing page.
95  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In 1984 Berners-Lee returned to CERN, and considered its problems of information presentation: physicists from around the world needed to share data, with no common machines and no common presentation software.
He wrote a proposal in March 1989 for "a large hypertext database with typed links", but it generated little interest.
His boss, Mike Sendall, encouraged Berners-Lee to begin implementing his system on a newly acquired NeXT workstation.
He considered several names, including Information Mesh, The Information Mine (turned down as it abbreviates to TIM, the WWW's creator's name) or Mine of Information (turned down because it abbreviates to MOI which is "Me" in French), but settled on World Wide Web.
Robert Ca illiau, J ean-Franço is Abra matic a nd Tim Berners-Lee a t the 10 th anniversa ry of the WWW Co nsortium.
He found an enthusiastic collaborator in Robert Cailliau, who rewrote the proposal (published on November 12, 1990) and sought resources within CERN.
Berners-Lee and Cailliau pitched their ideas to the European Conference on Hypertext Technology in September 1990, but found no vendors who could appreciate their vision of marrying hypertext with the Internet.
By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the first Web browser, Worldwide Web (which was also a Web editor), the first Web server (info.cern.ch), and the first Web pages that described the project itself.
The browser could access Usenet newsgroups and FTP files as well.
However, it could run only on the NeXT; Nicola Pellow therefore created a simple text browser that could run on almost any computer.
To encourage use within CERN, they put the CERN telephone directory on the web previously users had had to log onto the mainframe in order to look up phone numbers.
Paul Kunz from the Stanford Linear Accelerator Center visited CERN in May 1991, and was captivated by the Web.
He brought the NeXT software back to SLAC, where librarian Louise Addis adapted it for the VM/CMS operating system on the IBM mainframe as a way to display SLAC s catalog of online documents; this was the first web server outside CERN and the first in North America.
On August 6, 1991, Berners-Lee posted a short summary of the World Wide Web project on the alt.hypertext newsgroup.
This date also marked the debut of the Web as a publicly available service on the Internet.
The Worldwide Web (WWW) project aims to allow links to be made to any information anywhere.
[...] The WWW project was 96  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY started to allow high energy physicists to share data, news, and documentation.
We are very interested in spreading the web to other areas, and having gateway servers for other data.
Collaborators welcome!"
from Tim Berners-Lee's first message.
An early CERN-related contribution to the Web was the parody band Les Horribles Cernettes, whose promotional image is believed to be among the Web's first five pictures.
3.3 Growth of the WWW In keeping with its birth at CERN, early adopters of the World Wide Web were primarily university-based scientific departments or physics laboratories such as Fermilab and SLAC.
Early websites intermingled links for both the HTTP web protocol and the then-popular Gopher protocol, which provided access to content through hypertext menus presented as a file system rather than through HTML files.
Early Web users would navigate either by bookmarking popular directory pages, such as Berners-Lee's first site at http://info.cern.ch/, or by consulting updated lists such as the NCSA "What's New" page.
Some sites were also indexed by WAIS, enabling users to submit full-text searches similar to the capability later provided by search engines.
There was still no graphical browser available for computers besides the NeXT.
This gap was filled in April 1992 with the release of Erwise, an application developed at Helsinki University of Technology, and in May by ViolaWWW, created by Pei-Yuan Wei, which included advanced features such as embedded graphics, scripting, and animation.
Both programs ran on the X Window System for UNIX.
Students at the University of Kansas adapted an existing text-only hypertext browser, Lynx, to access the web.
Lynx was available on UNIX and DOS, and some web designers, unimpressed with glossy graphical websites, held that a website not accessible through Lynx was notworth visiting.
Ea rly Browsers The turning point for the World Wide Web was the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign (NCSA-UIUC), led by Marc Andreessen.
Funding for Mosaic came from the High- 97  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Performance Computing and Communications Initiative, a funding program initiated by thenSenator Al Gore's High Performance Computing and Communication Act of 1991 also known as the Gore Bill The origins of Mosaic begin in 1992.
In November 1992, the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign (UIUC) established a website.
In December 1992, Andreessen and Eric Bina, students attending UIUC and working at the NCSA, began work on Mosaic.
They released an X Window browser in February 1993.
It gained popularity due to its strong support of integrated multimedia, and the authors rapid response to user bug reports and recommendations for new features.
After graduation, Andreessen and James H. Clark, former CEO of Silicon Graphics, met and formed Mosaic Communications Corporation to develop the Mosaic browser commercially.
The company changed its name to Netscape in April 1994, and the browser was developed further as Netscape Navigator.
The first Microsoft Windows browser was Cello, written by Thomas R. Bruce for the Legal Information Institute at Cornell Law School to provide legal information, since most law yers had access to Windows but not to UNIX.
Cello was released in June 1993.
Web Orga nisatio n In May 1994 the first International WWW Conference, organised by Robert Cailliau, was held at CERN; the conference has been held every year since.
In April of 1993 CERN had agreed that anyone could use the Web protocol and code royalty-free; this was in part a reaction to the perturbation caused by the University of Minnesota announcing that it would begin charging license fees for its implementation of the Gopher protocol.
In September 1994, the World Wide Web Consortium was founded at the Massachusetts Institute of Technology as an industry organisation, with Tim Berners-Lee as director.
The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee after he left the European Organisation for Nuclear Research (CERN) in October, 1994.
It was founded at the Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) with support from the Defense Advanced Research Projects Agency (DARPA) - which had pioneered the Internet - and the European Commission.
98  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.4 1996-1998: Commercialisation of the WWW By 1996 it became obvious to most publicly traded companies that a public Web presence was no longer optional.
Though at first people saw mainly the possibilities of free publishing and instant worldwide information, increasing familiarity with two-way communication over the "Web" led to the possibility of direct Web-based commerce (e- commerce) and instantaneous group communications worldwide.
These concepts in turn intrigued many bright, young, often underemployed people (many of Generation X), who realised that new business models would soon arise based on these possibilities, and wanted to be among the first to profit from these new models.
3.5 Browser Wars Given its early start, Netscape was the web browser of choice for approximately 80% of users in 1996.
Netscape's failure to use the massive funding it received from its public stock offering to create a better browser and stay ahead of Microsoft's Internet Explorer version 4 (released in 1997), coupled with Microsoft bundling IE with its Windows desktop operating system, caused a gradual shift of users from Netscape to Internet Explorer and by 2001 IE had about 90% market share (when IE 6 was released).
In 1998, Netscape released the source code of its flagship product as the open source browser Mozilla.
It was soon decided that further development of the Netscape code base would be too complicated, and the browser was re-written from scratch.
By 2006, Mozilla-based browsers including Firefox and other competition had reduced Internet Explorer's market share from its peak of about 95% down to around 85%.
3.6 1999-2001: "Do t-com" Boom and Bust The low interest rates in 1998 99 helped increase the start-up capital amounts.
Although a number of these new entrepreneurs had realistic plans and administrative ability, most of them lacked these characteristics but were able to sell their ideas to investors because of the novelty of the dot-com concept.
Historically, the dot-com boom can be seen as similar to a number of other technology-inspired booms of the past including railroads in the 1840s, radio in the 1920s, transistor electronics in the 1950s, computer time-sharing in the 1960s, and home computers and biotechnology in the early 1980s.
99  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In 2001 the bubble burst, and many dot-com startups went out of business after burning through their venture capital and failing to become profitable.
3.7 2002-Present: The Web Becomes Ubiquitous In the aftermath of the dot-com bubble, telecommunications companies had a great deal of overcapacity as many Internet business clients went bust.
That, plus ongoing investment in local cell infrastructure kept connectivity charges low, and helping to make high-speed Internet connectivity more affordable.
During this time, a handful of companies found success developing business models that helped make the World Wide Web a mo re compelling experience.
These include airline booking sites, Google's search engine and its profitable approach to simplified, keyword-based advertising, as well as EBay s do-it-yourself auction site and Amazon.com's big selection of books.
This new era also begot social networking websites, such as MySpace, Xanga, Friendster, and Facebook, which, though unpopular at first, very slowly gained acceptance to become a popular part of youth culture.
Then, starting in 2002, a plethora of new ideas for sharing and exchanging certain types of content ad hoc, such as RSS and Weblogs, rapidly gained acceptance by Web developers eager to 'do more with less effort' by syndicating third-party content and soliciting new content from their users.
And so, with this new model for DIY, user-editable websites, simple content syndication RSS, and ad hoc broadcasting, and a new dot-com boom was afoot.
The Web 2.0 boom saw many new service-oriented startups catering to this new democratised web take flight, whilst Google's improvements in search engine technology 'cleaned up' an Internet which seem ed doomed by a rapidly expanding universe of content.
The Web 3.0 epoch appears to be in the last 2000's.
Predictably, as the World Wide Web became easier to query, attained higher degree usability, and shed its esoteric reputation, it gained a sense of organisation and unsophistication which opened the floodgates and ushered in a rapid period of popularisation.
New sites such as Wikipedia and its sister projects proved revolutionary in executing the User edited content concept.
In 2005, 3 ex-PayPal employees formed a video viewing website called YouTube.
Only a year later, YouTube was proven the most quickly popularised website in history, and even started a new concept of user-submitted content in major events, as in the CNN- YouTube Presidential Debates.
100  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Continued extension of the World Wide Web has focused on connecting devices to the Internet, coined Intelligent Device Management.
As Internet connectivity becomes ubiquitous, manufacturers have started to leverage the expanded computing power of their devices to enhance their usability and capability.
Through Internet connectivity, manufacturers are now able to interact with the devices they have sold and shipped to their customers, and customers are able to interact with the manufacturer (and other providers) to access new content.
3.8 H ow the Web works Viewing a Web page on the World Wide Web normally begins either by typing the URL of the page into a Web browser, or by following a hyperlink to that page or resource.
The Web browser then initiates a series of communication messages, behind the scenes, in order to fetch and display it.
First, the server-name portion of the URL is resolved into an IP address using the global, distributed Internet database known as the domain name system, or DNS.
This IP address is necessary to contact and send data packets to the Web server.
The browser then requests the resource by sending an HTTP request to the Web server at that particular address.
In the case of a typical Web page, the HTML text of the page is requested first and parsed immediately by the Web browser, which will then make additional requests for images and any other files that form a part of the page.
Statistics measuring a website's popularity are usually based on the number of 'page views' or associated server 'hits', or file requests, which take place.
Having received the required files from the Web server, the browser then renders the page onto the screen as specified by its HTML, CSS, and other Web languages.
Any images and other resources are incorporated to produce the on-screen Web page that the user sees.
Most Web pages will themselves contain hyperlinks to other related pages and perhaps to downloads, source documents, definitions and other Web resources.
Such a collection of useful, related resources, interconnected via hypertext links, is what was dubbed a "web" of information.
Making it available on the Internet created what Tim Berners-Lee first called the WorldWideWeb (a term written in CamelCase, subsequently discarded) in 1990.
101  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.9 Standards Many formal standards and other technical specifications define the operation of different aspects of the World Wide Web, the Internet, and computer information exchange.
Many of the documents are the work of the World Wide Web Consortium (W3C), headed by Berners-Lee, but some are produced by the Internet Engineering Task Force (IETF) and other organisations.
Usually, when Web standards are discussed, the following publications are seen as foundational: Recommendations fo r markup languages, especially HTML and XHTML, from the W3 C. These define the st ructure and interpretatio n o f hy pertext do cuments.
Recommendations fo r stylesheets, especia lly CSS, from the W3 C. Standards for ECMAScript (usually in the fo rm of Ja va Script), fro m Ecma Internatio nal.
Recommendations fo r the Document Object Model, fro m W3C.
Additional publications provide definitions of other essential technologies for the World Wide Web, including, but not limited to, the following: Uniform Resource Identifier (URI), which is a universal system for referencing resources on the Internet, such as hypertext documents and images.
URIs, often called URLs, are defined by the IETF's RFC 3986 / STD 66: Uniform Resource Identifier (URI): Generic Syntax, as well as its predecessors and numerous URI scheme-defining RFCs; HyperText Transfer Protocol (HTTP), especially as defined by RFC 2616: HTTP/1.1 and RFC 2617: HTTP Authentication, which specify how the browser and server authenticate each other.
3.10 Publishing W eb Pages Web page production is available to individuals outside the mass media.
In order to publish a Web page, one does not have to go through a publisher or other media institution, and potential readers could be found in all corners of the globe.
Many different kinds of information are available on the Web, and for those who wish to know other societies, cultures, and peoples, it has become easier.
102  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY The increased opportunity to publish materials is observable in the countless personal and social networking pages, as well as sites by families, small shops, etc., facilitated by the emergence of free Web hosting services.
3.11 Statistic s According to a 2001 study, there were massively more than 550 billion documents on the Web, mostly in the "invisible Web", or deep Web.
A 2002 survey of 2,024 million Web pages determined that by far the most Web content was in English: 56.4%; next were pages in German (7.7%), French (5.6%), and Japanese (4.9%).
A more recent study, which used Web searches in 75 different languages to sample the Web, determined that there were over 11.5 billion Web pages in the publicly indexable Web as of the end of January 2005.
As of June 2008, the indexed web contains at least 63 billion pages.
Over 100.1 million websites operated as of March 2008.
Of these 74% were commercial or other sites operating in the .com generic top-level domain.
Among services paid for by advertising, Yahoo!
could collect the most data about commercial Web users, about 2,500 bits of information per month about each typical user of its site and its affiliated advertising netw ork sites.
Yahoo!
was followed by MySpace with about half that potential and then by AOL-TimeWarner, Google, Facebook, Microsoft, and eBay.
About 26% of websites operated outside .com addresses.
3.12 Speed Issues Frustration over congestion issues in the Internet infrastructure and the high latency that results in slow browsing has led to an alternative, pejorative name for the World Wide Web: the World Wide Wait.
Speeding up the Internet is an ongoing discussion over the use of peering and QoS technologies.
Other solutions to reduce the World Wide Wait can be found on W3C.
Standard guidelines for ideal Web response times are (Nielsen 1999, `Designing Web Usability', page 42): 0.1 second (one tenth of a second).
Ideal response tim e. The user doesn't sense any interruption.
1 second.
Highest acceptable response time.
Download times above 1 second interrupt the user experience.
103  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 10 seconds.
Unacceptable response time.
The user experience is interrupted and the user is likely to leave the site or system.
These numbers are useful for planning server capacity.
3.13 Security The Web has become criminals' preferred pathway for spreading malware.
Through HTML and URIs the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript and were exacerbated to some degree by Web 2.0 and Ajax web design that favors the use of scripts.
Web-based vulnerabilities now outnumber traditional computer security concerns.
Today by one estimate, 70% of all websites are open to XSS attacks on their users.
As measured by Google, about one in ten Web pages may contain malicious code.
Cybercrime carried out on the Web can include identity theft, fraud, espionage and intelligence gathering.
Most Web- based attacks take place on legitimate websites, and most, as measured by Sophos, are hosted in the United States, China and Russia.
Proposed solutions vary to extremes.
Large security vendors like McAfee already design governance and compliance suites to meet post- 9/11 regulations, and some, like Finjan have recommended active real- time inspection of code and all content regardless of its source.
Some have argued that for enterprise to see security as a business opportunity rather than a cost center "ubiquitous, always-on digital rights management" enforced in the infrastructure by a handful of organisations must replace the hundreds of companies that today secure data and networks.
Jonathan Zittrain has said users sharing responsibility for computing safety is far preferable to locking down the Internet.
4.0 CO NCLUSIO N The World Wide Web continues to provide avenues for leveling the business platforms.
Time is drastically reduced and smart small to medium scale business can compete effectively with big time players.
Among its several advantages, it has broadened the horizon of the business world.
Indeed it is the World Wide Web that really made the world a global village.
The technology itself continues to improve over time to accommodate the noticeable challenges.
104  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 5.0 SUMMARY • The World Wid e Web (commonly shortened to the Web) is a system of interlinked hypertext documents accessed via the Internet.
• Today, the Web and the Internet allow connectivity from literally everywhere on earth-even ships at sea and in outer space.
• The NeXTcube used by Tim Berners-Lee at CERN became the first Web server.
In 1980, the Englishman Tim Berners-Lee, an independent contractor at CERN, Switzerland, built ENQUIRE, as a personal database of people and software models, but also as a way to play with hypertext; each new page of information in ENQUIRE had to be linked to an existing page • In keeping with its birth at CERN, early adopters of the World Wide Web were primarily university-based scientific departments or physics laboratories such as Fermilab and SLAC.
• By 1996 it became obvious to most publicly traded companies that a public Web presence was no longer optional.
• Given its early start, Netscape was the web browser of choice for approximately 80% of users in 1996.
• The low interest rates in 1998 99 helped increase the start-up capital amounts.
• In the aftermath of the dot-com bubble, telecommunications companies had a great deal of overcapacity as many Internet business clients went bust.
• Viewing a Web page on the World Wide Web normally begins either by typing the URL of the page into a Web browser, or by following a hyperlink to that page or resource.
• Many formal standards and other technical specifications define the operation of different aspects of the World Wide Web, the Internet, and computer information exchange.
• Web page production is available to individuals outside the mass media.
In order to publish a Web page, one does not have to go through a publisher or other media institution, and potential readers could be found in all corners of the globe.
• According to a 2001 study, there were massively more than 550 billion documents on the Web, mostly in the "invisible Web", or deep web.
• Frustration over congestion issues in the Internet infrastructure and the high latency that results in slow browsing has led to an alternative, pejorative name for the World Wide Web: the World Wide Wait.
• The Web has become criminals' preferred pathway for spreading malware.
105  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 6.0 TUTOR-MARK ED ASSIGNMENT 1.
Discuss briefly the commercialisation of the World Wide Web 2.
Mention 5 forms of publications associated with web standard 7.0 REF ERENCES/FURTHER READING Robert Cailliau, James Gillies, How the Web Was Born: The Story of the World Wide Web, ISBN 978-0-19-286207-5, Oxford University Press (Jan 1, 2000).
Tim Berners-Lee with Mark Fischetti, Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor, ISBN 978-0-06-251586-5, HarperSanFrancisco, 1999.
Andrew Herman, The World Wide Web and Contemporary Cultural Theory : Magic, Metaphor, Power, ISBN 978-0-415-92502-0, Routledge, 1st Edition (June 2000).
Fielding, R.; Gettys, J.; Mogul, J.; Frystyk, H.; Masinter, L.; Leach, P.; Berners-Lee, T. (June 1999).
"Hypertext Transfer Protocol HTTP/1.1".
Request for Comments 2616.
Information Sciences Institute.
Berners-Lee, Tim; Bray, Tim; Connolly, Dan; Cotton, Paul; Fielding, Roy; Jeckle, Mario; Lilley, Chris; Mendelsohn, Noah; Orchard, David; Walsh, Norman; Williams, Stuart (December 15, 2004).
"Architecture of the World Wide Web, Volume One".
Version 20041215.
W3C.
Polo, Luciano (2003).
World Wide Web Technology Architecture: A Conceptual Analysis.
New Devices.
Retrieved on July 31, 2005.
106  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 4 PERSO NAL DIG ITAL ASSISTANT AND SUB NOTEBOOK S CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History 3.2 Typical Features 3.3 Uses 3.4 Technical Details 3.5 Decline of stand-alone PDAs vs phones 3.6 Subnotebook 3.7 History of Subnotebook 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N A perso na l dig ital assista nt (PDA) is a handheld computer, also known as small or palmto p computers.
Newer PDAs also have both color screens and audio capabilities, enabling them to be used as mobile phones (smartphones), web browsers, or portable media players.
Many PDAs can access the Internet, intranets or extranets via Wi-Fi, or Wireless Wide-Area Networks (WWANs).
Many PDAs employ touch screen technology.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define personal digital assistants and sub notebooks • trace the trend in the development of personal digital assistants and sub notebooks • identify the basic features of personal digital assistants and sub notebooks • know the uses of personal digital assistants and sub notebooks • differentiate personal digital assistants and sub notebooks.
107  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.0 MAIN CONTENT 3.1 History The first PDA is considered to be the CASIO PF-3000 released in May 1983.
GO Corp. was also pioneering in the field.
The term was first used on January 7, 1992 by Apple Computer CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton.
PDAs are sometimes referred to as "Palms", "Palm Pilot", or "Palm Tops".
3.2 Typical Features Currently, a typical PDA has a touch screen for entering data, a memory card slot for data storage and at least one of the following for connectivity: IrDA, Bluetooth and/or WiFi.
However, many PDAs (typically those used primarily as telephones) may not have a touch screen, using softkeys, a directional pad and either the numeric keypad or a thumb keyboard for input.
Software typically required for a PDA includes an appointment calendar, a to-do list, an address book for contacts and some sort of note program.
Connected PDAs also typically include E-mail and Web support.
Touch Screen Many original PDAs, such as the Apple Newton and the Palm Pilot, featured touch screens for user interaction, having only a few buttons usually reserved for shortcuts to often used programs.
Touch screen PDAs, including Windows Pocket PC devices, usually have a detachable stylus that can be used on the touch screen.
Interaction is then done by tapping the screen to activate buttons or menu choices, and dragging the stylus to, for example, highlight.
Text input is usually done in one of four wa ys: • Using a virtual keyboard, where a keyboard is shown on the touch screen.
Input is done by tapping letters on the screen.
• Using external ke yboard or chorded keyboard connected by USB, IR or Bluetooth.
• Using letter or word recognition, where letters or words are written on the touch screen, and then "translated" to letters in the currently activated text field.
Despite rigorous research and development projects, end-users experience mixed results with this input method, with some finding it frustrating and inaccurate, while others are satisfied with the quality Recognition and 108  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development.
• Stroke recognition (termed Graffiti by Palm).
In this system a predefined set of strokes represents the various characters needed.
The user learns to draw these strokes on the screen or in an input area.
The strokes are often simplified character shapes to make them easier to remember.
PDAs for business use, including the BlackBerry and Treo, have full keyboards and scroll wheels or thumb wheels to facilitate data entry and navigation, in addition to supporting touch-screen input.
There are also full-size foldable keyboards available that plug directly, or use wireless technology to interface with the PDA and allow for normal typing.
BlackBerry has additional functionality, such as push-based email and applications.
Newer PDAs, such as the Apple iPhone and iPod touch include new user interfaces using other means of input.
The iPhone and iPod touch uses a technology called Multi-touch.
Memo ry Cards Although many early PD As did not have memory card slots, now most have either an SD (Secure Digital) and/or a Compact Flash slot.
Although originally designed for memory, SDIO and Compact Flash cards are available for such things as Wi-Fi and Webcams.
Some PDAs also have a USB port, mainly for USB flash drives.
As more PDAs include telephone support, to keep the size down, many now offer miniSD or microSD slots instead of full-sized SD slots.
Wired Connectivity While many earlier PDAs connected via serial ports or other proprietary format, many today connect via USB cable.
This served primarily to connect to a computer, and few, if any PDAs were able to connect to each other out of the box using cables, as USB requires one machine to act as a host - functionality which was not often planned.
Some PDAs were able to connect to the internet, either by means of one of these cables, or by using an extension card with an ethernet port/RJ-45 adaptor.
Wireless Connectivity Most modern P DAs have Bluetooth wireless connectivity, an increasingly popular tool for mobile devices.
It can be used to connect keyboards, headsets, GPS and many other accessories, as well as 109  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY sending files between PDAs.
Many mid-range and superior PDAs have Wi-Fi/WLAN/802.11-connectivity, used for connecting to Wi-Fi hotspots or wireless networks.
Older PDAs predominantly have an IrD A (infrared) port; however fewer current models have the technology, as it is slowly being phased out due to support for Bluetooth and Wi-Fi.
IrDA allows communication between two PDAs: a PDA and any device with an IrDA port or adapter.
Most universal PDA keyboards use infrared technology because many older PDAs have it, and infrared technology is low-cost and has the advantage of being permitted aboard aircraft.
Synchronisa tion An important function of PDAs is synchronising data with a PC.
This allows up-to-date contact information stored on software such as Microsoft Outlook or ACT!
to update the database on the PDA.
The data synchronisation ensures that the PDA has an accurate list of contacts, appointments and e-mail, allowing users to access the same information on the PDA as the host computer.
The synchronising also prevents the loss of information stored on the device in case it is lost, stolen, or destroyed.
Another advantage is that data input is usually a lot quicker on a PC, since text input via a touch screen is still not quite optimal.
Transferring data to a PDA via the computer is therefore a lot quicker than having to manually input all data on the handheld device.
Most PDAs come with the ability to synchronise to a PC.
This is done through synchronisation software provided with the handheld, such as HotSync Manager, which comes with Palm OS handhelds, Microsoft ActiveS ync for older versions of Windows or Windows Mobile Device Center on Windows Vista, which comes with Windows Mobile handhelds.
These programmes allow the PDA to be synchronised with a P ersonal information manager.
This personal information manager may be an outside programme or a proprietary programme.
For example, the BlackBerry PDA comes with the Desktop Manager programme which can synchroni se to both Microsoft Outlook and ACT!.
Other PDAs come only with their own proprietary software.
For example, some early Palm OS P DAs came only with Palm Desktop while later Palms such as the Treo 650 has the built-in ability to s ync to Palm Desktop and/or Microsoft Outlook, while Microsoft's ActiveSync and Windows Mobile Device Center only synchronise with Microsoft Outlook or a Microsoft Exchange server.
110  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Third-party synchronisation software is also available for many PDAs from companies like Intellisync and CompanionLink.
This software synchronises these handhelds to other personal information managers which are not supported by the P DA manufacturers, such as GoldMine and Lotus Notes.
3.3 Uses PDAs are used to store information that can be accessed at any time and anywhere.
Automo bile Na vigation Many PDAs are used in car kits and are fitted with differential Global Positioning System (GPS) receivers to provide realtime automobile navigation.
PDAs are increasingly being fitted as standard on new cars.
Many systems can also display traffic conditions, dynamic routing and roadside mobile radar guns.
Popular software in Europe and in America for this functionality are TomTom, Garmin, iGO etc.
showing road conditions and 2D or 3D environments.
Ruggedised PDAs For many years businesses and government organisations have relied upon rugged PDAs also known as enterprise digital assistants (EDAs) for mobile data applications.
Typical applications include supply chain management in warehouses, package delivery, route accounting, medical treatment and record keeping in hospitals, facilities maintenance and management, parking enforcement, access control and security, capital asset maintenance, meter reading by utilities, and "wireless waitress" applications in restaurants and hospitality venues.
A common feature of EDAs are the integration of Data Capture devices like Bar Code, RFID and Smart Card Readers.
Medical a nd Scientific Uses In medicine, PDAs have been shown to aid diagnosis and drug selection and some studies have concluded that their use by patients to record symptoms improves the effectiveness of communication with hospitals during follow-up.
A range of resources have been developed to cater for the demand from the medical profession, including Epocrates and ABX guide, which supply drug databases, treatment information and relevant news in formats specific to mobile devices and services such as AvantGo which translate medical journals into readable formats and provide updates from journals.
WardWatch organises medical records to remind doctors making ward rounds of information such as the 111  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY treatment regimens of patients and programmes.
Finally, Pendragon and Syware provide tools for conducting research with mobile devices, and connecting to a central server allowing the user to enter data into a centralised database using their PDA.
Additionally, Microsoft Visual Studio and Sun Java provide programming tools for developing survey instruments on the handheld.
These development tools allow for integration with SQL databases that are stored on the handheld and can be synchronised with a desktop/server based database.
Recently the development of Sensor Web technology has led to discussion of using wearable bodily sensors to monitor ongoing conditions like diabetes and epilepsy and alerting medical staff or the patient themselves to the treatment required via communication between the web and PDAs.
Educational Uses As mobile technology has become very common, it is no surprise that personal computing has become a vital learning tool by this time.
Educational institutes have commenced a trend of integrating PDAs into their teaching practices (mobile learning).
With the capabilities of PDAs, teachers are now able to provide a collaborative learning experience for their students.
They are also preparing their students for possible practical uses of mobile computing upon their graduation.
PDAs and handheld devices have recently allowed for digital note taking.
This has increased student s productivity by allowing individuals to quickly spell-check, modify, and amend their class notes or e-notes.
Educators are currentl y able to distribute course material through the use of the internet connectivity or infrared file sharing functions of the PDA.
With concerns to class material, textbook publishers have begun to release e-books, or electronic textbooks, which can be uploaded directly to a PDA.
This then lessens the effort of carrying multiple textbooks at one time.
To meet the instructive needs sought by educational institutes, software companies have developed programmes with the learning aspects in mind.
Simple programmes such as dictionaries, thesauri, and word processing software are important to the digital note taking process.
In addition to these simple programmes, encyclopaedias and digital planning lessons have created added functionality for users.
With the increase in mobility of PDAs, school boards and educational institutes have now encountered issues with these devices.
School boards are now concerned with students utilising the internet connectivity to share test answers or to gossip during class time, which 112  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY creates disruptions.
Many school boards have modernised their computer policies to address these new concerns.
Software companies such as Scantron Corp. have now created a programme for distributing digital quizzes.
The quiz software disables the infrared function on PDAs, which eliminates the element of information sharing among individuals during the examination.
Many colleges encourage the use of PDAs.
Sporting Uses PDAs are used by glider pilots for pre-flight planning and to assist navigation in cross-country competitions.
They are linked to a GPS to produce moving-map displays showing the tracks to turn-points, airspace hazards and other tactical information.
PDA's may also be used by music enthusiasts.
They can be used to play a variety of file formats (unlike most MP3 Players) during physical exercise (e.g.
running), unlike certain larger devices such as laptops.
PDAs can be used by road rally enthusiasts.
PDA software can be used for calculating distance, speed, time, and GPS navigation as well as unassisted navigation.
PDA for People with Disabilities PDAs offer varying degrees of accessibility for people with differing abilities, based on the particular device and service.
People with vision, hearing, mobility, and speech impairments may be able to use PDAs on a limited basis, and this may be enhanced by the addition of accessibility software (e.g.
speech recognition for verbal input instead of manual input).
Universal design is relevant to PDAs as well as other technology, and are viable solution for many user-access issues, though it has yet to be consistently integrated into the design of popular consumer PDA devices.
PDAs have recently become quite useful in the Traumatic Brain Injury/Posttraumatic Stress Disorder population, especially seen in troops returning home from Operation Iraqi Freedom (OIF)/Operation Enduring Freedom (OEF).
PDAs address memory issues and help these men and women out with daily life organisation and reminders.
As of quite recently, the Department of Veterans' Affairs (VA) has begun issuing thousands of PDAs to troops who present the need for them.
Occupational therapists have taken on a crucial role within this population helping these veterans return to the normalcy of life they once had.
113  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.4 Tec hnical Details Architecture Many PDAs run using a variation of the ARM architecture (usually denoted by the Intel XScale trademark).
This encompasses a class of RISC microprocessors that are widely used in mobile devices and embedded systems, and its design was influenced strongly by a popular 1970s/1980s CPU, the MOS Technology 6502.
The currently major PDA operating systems are: Palm OS - owned by PalmSource Windows Mobile Professional and Classic for use on Pocket PCs, (based on the Windows CE kernel) - owned by Microsoft iPhone OS - owned by Apple Inc. BlackBerry OS - owned by Research In Motion Many operating systems based on the Linux kernel - free (not owned by any company) These include: Familiar (comes in three flavours: GPE, Opie and barebone) OpenZaurus (for Zaurus PDAs) Ångström, a descendent of OpenZaurus Intimate (for PDAs with an exceedingly large amount of memory) Symbian OS (formerly EPOC) owned by Motorola, Panasonic, Nokia, Samsung, Siemens and Sony Ericsson 3.5 Decline of stand-alone PDAs vs phones Stand-alone PDA sales fell 43.5% from 2006 to 2007.
Approximately 4 million PDAs are sold per year.
However, with Smartphone sales increasing from levels of approximately 60 million per year, more telephones are being used as PDAs with phone capability.
According to a Gartner market study, the overall market for PDAs grew by 20.7% in the third quarter (Q3) of 2005, compared to Q3 2004, with market share resolving as follows (by operating system): 1) Palm OS for Palm, Inc. PDAs and some other licensees- 14.9% (declining) 2) Windows Mobile for PDAs that comply with the Microsoft's Pocket PC specifications - 49.2% (increasing) 3) RIM BlackBerry (produced by Research In Motion) - 25.0% (increasing) 114  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 4) Symbian OS - 5.8% (increasing) 5) Various operating systems based on the Linux kernel for various special designed PDAs (many other supported) - 0.7% (stable) 6) Others - 4.4% (stable) 3.6 Subnote book A subno teboo k (also ultraporta ble laptop or minila pto p computer) is a small and lightweight portable computer, with most of the features of a standard laptop computer but smaller.
The term is often applied to systems that run full versions of desktop operating systems such as Windows or Linux, rather than specialised software such as Windows CE, Palm OS or Internet Tablet OS.
The term "ultra-mobile PC" (UMPC) is also frequently used to refer to such machines, although this also refers to a small form-factor tablet PC platform.
Subnotebooks are smaller than laptops but larger than handheld computers and UMPCs.
They often have smaller-sized screens, usually measuring from 7 inches (17.7 cm) to 13.3 inches (33.78 cm), and a weight from less than 1 kg (2.2 lbs) up to about 2 kg (4.4 lbs).
The savings in sise and weight are usually achieved partly by omitting ports or having removable media/optical drives; subnotebooks are often paired with docking stations to compensate.
Subnotebooks were seen as niche computing products and have rarely sold in large numbers until the 2007 introduction of the Asus Eee PC and the OLPC XO-1, known as ultra low-cost PC (ULPC or ULCPC), which are inexpensive in comparison to both existing machines in that form factor, and computers in general.
3.7 H istory of Subnotebook 19 90 The Compaq LTE, launched in 1989, was the first to be widely known as a "notebook computer" because its relatively small dimensions 4,8x22x28 cm = 1.9 × 8.5 × 11 inches were similar to an A4 paper notebook.
The Compaq was followed in October 1992 by the very popular IBM ThinkPad which was the first to include a 10.4 inch screen in a notebook measuring 2.2 × 8.3 × 11.7 inches.
Portables with smaller form factors thus became known as subnotebooks.
The term was also applied to the NEC UltraLite, unveiled in 1988, although its dimensions (1.4 × 8.3 × 11.75 inches) were very similar to the Compaq LT E. 115  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 1991 1995 One early subnotebook was the PowerBook 100 released in 1991 by Apple Inc., then the Gateway Handbook, originally released in 1992 and updated to use a 486 processor in late 1993.
Apple followed-up with PowerBook Duo series in October 1992, which further reduced their subnotebook line to 8.5" deep by 10.9" wide by 1.4" high and is an example of a portable supporting few on-board features, but which could be inserted into a docking station to achieve the full functionality of a desktop a feature soon emulated by other manufacturers.
Another early subnotebook was the Hewlett-Packard OmniBook 300, which was launched as a "superportable" in 1993, and was available with an optional Flash memory disk instead of a hard drive, to reduce the weight.
Toshiba, which had concentrated on portables in the 1980s, also entered the market that year with the Portege T3400, claiming that "It's the first subnotebook computer with all the functionality of a much larger computer".
Then Toshiba really put the subnotebook PC format on the map in 1995 with the Libretto 20.
This featured a 6.1 screen and 270 MB hard disks.
Compaq introduced its own short lived subnotebook line in 1994 called Contura Aero, which had two models: the greyscale display 4/25 and the color 4/33, notable for using a battery which was intended to be standard rather than only useful for Compaq products.
1996 2000 In 1997, Apple launched the relatively light-weight (4.4 lbs) but short- lived PowerBook 2400c.
This was co-designed by IBM and made for Apple by IBM Japan to replace the aging PowerBook Duo line.
IBM had sold "thin and light" models in its ThinkPad range, such as the ThinkPad 560 ultraportable (1996) and best-selling ThinkPad 600 (1998).
It finally entered the subnotebook market in 1999 targeted at business travellers.
Later, however, IBM replaced these with the X range, with 12.1 screens.
At 8.3 × 10.6in, the ThinkPad X40 is not much smaller than A4 (8.3 by 11.7in) and better described as an ultraportable, rather than a subnotebook.
Sony launched an ultraportable less than an inch thick in Japan in 1997, the PCG -505, which reached the US in 1997 as the VAIO 505GX.
This was followed by the even thinner Sony VAIO X505.
However, it was very expensive and had poor battery life, and was soon withdrawn.
Sony also launched the C1 range of subnotebooks, starting in Japan in January 2000.
116  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 20 01 present One of the most notable Sony models was the Transmeta-based Vaio PCG-C1VE or PictureBook (2001), it had a digital camera built into the lid, which could be used for video conferencing or swivelled round to photograph a scene.
This was followed in 2005 by the Transmeta-based Flybook convertible with a touch-sensitive 8.9 inch.
widescreen from Taiwan's Dialogue Technology.
(Later models used Intel ULV processors.)
The Flybook features a built-in phone connection for GPRS or 3G networking, and is available in a range of bright colors.
This attracted the attention of non- computer magazines including GQ, FHM, Elle and Rolling Stone.
In 2006, Microsoft stimulated a new round of subnotebook development with the UMPC or Ultra-Mobile PC format code-named Origami.
These are basically small versions of Tablet PC computers, which originally shipped with the Microsoft Windows XP Tablet PC Edition 2005.
An example is the Samsung Q1.
In 2007, Asus unveiled the Intel-based ASUS Eee PC range running Linux (or, after user modification, Microsoft Windows) on a 7 inch color screen.
Unlike most subnotebooks which sell in small numbers, the Eee PC has been a top seller on Amazon.com and is often sold out in retail stores.
The reason of success is largely contributed by the relatively low price (~USD $350).
Compare with similar products which easily priced above $1000 at that time.
In mid February 2008, Everex launched its VIA chipset based CloudBook, running gOS.
The CloudBook is based on the VIA nanobook reference design.
Unlike its closest competitor, the Eee PC, the CloudBook uses a hard-disk.
The design of the cloudbook is optimised so it can be held in one hand while typing, or in two hands when using the mouse-cursor control, with the left thumb controlling the two "mouse buttons", and the right thumb a small trackpad, both mousepad and keys are placed directly under the screen.
In April 3, 2008, Microsoft announced a programme to extend the availability of Windows XP in "ultra low-cost PCs", past its original deadline for ending the support of this operating system, as long as hardware developers deploy it on systems with limited hardware specifications.
Commentators have seen this announcement as a market movement both to prevent mobile PCs eating market share of full- featured desktop and laptop PCs, and to stop the advance of Linux installations on this format.
117  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY June 2008, MSI l aunches the MSI Wind PC notebook with features such as Bluetooth and a 10" led backlight 1024x600 screen.
This new laptop is the first built with Intel Atom low power technology and will closely compete with the HP 2133 Mini-Note PC which has an 8.9" screen and is capable of a higher resolution.
Both laptops are offered with SUSE Linux and Microsoft Windows preinstalled; but HP offers Windows Vista on their laptop while MSI ships only with XP Home edition.
4.0 CO NCLUSIO N Personal digital assistants have really brought about mobility in computing.
It is very relevant in the world of today where everything is tended towards mobility.
Different manufacturers flood the market almost annually with different versions of these mobile computers.
However they still remain a piece of hardware for individuals as they cannot handle large volume of data.
5.0 SUMMARY A personal digital assistant (PDA) is a handheld computer, also known as small or palmtop computers.
• The first PDA is considered to be the CASIO PF-3000 released in May 1983.
GO Corp. was also pioneering in the field.
• Currently, a typical PDA has a touch screen for entering data, a memory card slot for data storage and at least one of the following for connectivity: IrDA, Bluetooth and/or WiFi.
• PDAs for business use, including the BlackBerry and Treo, have full keyboards and scroll wheels or thumb wheels to facilitate data entry and navigation, in addition to supporting touch-screen input.
• An important function of PDAs is synchronising data with a PC.
This allows up-to-date contact information stored on software such as Microsoft Outlook or ACT!
to update the database on the PDA.
• PDAs are used to store information that can be accessed at any time and anywhere.
• As mobile technology has become very common, it is no surprise that personal computing has become a vital learning tool by this time.
• Many PDAs run using a variation of the ARM architecture (usually denoted by the Intel XScale trademark).
• Stand-alone PDA sales fell 43.5% from 2006 to 2007.
Approximately 4 million PDAs are sold per year.
• A subn otebook (also ultra po rtable laptop or minila ptop computer) is a small and lightweight portable computer, with 118  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY most of the features of a standard laptop computer but smaller.
• One early subnotebook was the PowerBook 100 released in 1991 by Apple Inc., then the Gateway Handbook, originally released in 1992 and updated to use a 486 processor in late 1993.
6.0 TUTOR-MARKED ASSIGNMENT 1.
Briefly discuss the basic features of PDAs.
2.
Mention 5 forms of PDA operating systems.
7.0 REF ERENCES/FURTHER READING ITworld.com - One million OLPC laptop orders confirmed Hardware T-series T3400CT Toshiba unwraps subnotebook - CNET News.com New Notebooks: IBM Targets Corporate Enterprises With Its First Mini-notebook for North America - ThinkP ad 240 - Product Announcement | Edge: Work-Group Computing Report | Find Articles at BNET.com Sony VAIO X505 series Laptop reviews - CNET Reviews http://frijoles.hungry.com/c1-info/index.html http://www.flybook.biz/en/?section=press ARN - Flybook and Libretto are miniature marvels Asus Eee News, Mods, and Hacks: Asus Eee PC Amazon Bestseller ^ Microsoft Announces Extended Availability of Windows XP Home for ULCPCs, April 3, 2008 Press release Microsoft to limit capabilities of cheap laptops, IT World May 12, 2008 News Computerworlduk - The latest, breaking IT news, reviews and analysis on Google, Yahoo, Facebook, AOL, Microsoft, Apple, Adobe Reader, IBM, Open Source http://global.msi.com.tw/index.php?func=prodpage2&maincat_no=135 &cat2_no=582 MSI HWR accuracy: 119  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 5 BLUETOOTH CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Uses 3.2 Computer Requirements 3.3 Mobile Phone Requirements 3.4 Specifications and Features 3.5 Future of Bluetooth 3.6 Communication and Connection 3.7 Security 3.8 Health Concerns 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N Bluetooth is a wireless protocol utilising short-range communications technology facilitating data transmission over short distances from fixed and/or mobile devices, creating wireless personal area networks (PANs).
The intent behind the development of Bluetooth was the creation of a single digital wireless protocol, capable of connecting multiple devices and overcoming problems arising from s ynchronisation of these devices.
Bluetooth uses a radio technology called frequency hopping spread spectrum.
It chops up the data being sent and transmits chunks of it on up to 75 different frequencies.
In its basic mode, the modulation is Gaussian frequency shift keying (GFSK).
It can achieve a gross data rate of 1 Mb/s.
Bluetooth provides a way to connect and exchange information between devices such as mobile phones, telephones, laptops, personal computers, printers, GPS receivers, digital cameras, and video game consoles over a secure, globally unlicensed Industrial, Scientific, and Medical (ISM) 2.4 GHz short-range radio frequency bandwidth.
The Bluetooth specifications are developed and licensed by the Bluetooth Special Interest Group (SIG).
The Bluetooth SIG consists of companies in the areas of telecommunication, computing, networking, and consumer electronics.
120  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 2.0 OBJECTIVES At the end of this unit, you should be able to: define and know the meaning of Bluetooth identify the various applications of Bluetooth know the basic requirements in using Bluetooth identify the major features of Bluetooth understand the security implications of using Bluetooth appreciate the health concerns in using Bluetooth.
3.0 MAIN CONTENT 3.1 Uses Bluetooth is a standard and communications protocol primarily designed for low power consumption, with a short range (power-class-dependent: 1 metre, 10 metres, 100 metres) based on low-cost transceiver microchips in each device.
Bluetooth enables these devices to communicate with each other when they are in range.
The devices use a radio communications system, so they do not have to be in line of sight of each other, and can even be in other rooms, as long as the received transmission is powerful enough.
Bluetooth device class indicates the type of device and the supported services of which the information is transmitted during the discovery process.
.
Range Cla ss Ma ximum Permitted Power mW(dB m) (a ppro xima te) Cla ss 1 100 mW (20 dBm) ~100 metres Cla ss 2 2.5 mW (4 dBm) ~10 metres Cla ss 3 1 mW (0 dBm) ~1 metre In most cases the effective range of class 2 devices is extended if they connect to a class 1 transceiver, compared to pure class 2 network.
This is accomplished by the higher sensitivity and transmission power of Class 1 devices.
Version Da ta Rate Version 1 .2 1 Mbit/s Version 2 .0 + EDR 3 Mbit/s W iMedia Allia nce (pro po sed) 53 480 Mbit/s 121  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Bluetooth Profiles In order to use Bluetooth, a device must be compatible with certain Bluetooth profiles.
Thes e define the possible applications and uses of the technology.
List of applicatio ns A typical Bluetooth mobile phone headset More prevalent applications of Bluetooth include: • Wireless control of and communication between a mobile phone and a hands-free headset.
This was one of the earliest applications to become popular.
• Wireless networking between PCs in a confined space and where little bandwidth is required.
• Wireless communications with PC input and output devices, the most common being the mouse, keyboard and printer.
• Transfer of files between devices with OBEX.
• Transfer of contact details, calendar appointments, and reminders between devices with OBEX.
• Replacement of traditional wired serial communications in test equipment, GPS receivers, medical equipment, bar code scanners, and traffic control devices.
• For controls where infrared was traditionally used.
• Sending small advertisements from Bluetooth enabled advertising hoardings to other, discoverable, Bluetooth devices.
• Two seventh-generation game consoles, Nintendo s Wii and Sony s PlayStation 3 use Bluetooth for their respective wireless controllers.
• Dial-up internet access on personal computer or PDA using a data-capable mobile phone as a modem.
122  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY B luetoo th vs. Wi-Fi in Netwo rking Bluetooth and Wi-Fi have different applications in today s offices, homes, and on the move: setting up networks, printing, or transferring presentations and files from PDAs to computers.
Both are versions of unlicensed wireless technology.
Wi-fi differs from Bluetooth in that it provides higher throughput and covers greater distances, but requires more expensive hardware and may present higher power consumption.
They use the same frequency range, but employ different modulation techniques.
While Bluetooth is a replacement for cabling in a variety of small-scale applications, Wi-Fi is a replacement for cabling for general local area network access.
Bluetooth can be taken as replacement for USB or any other serial cable link, whereas Wi-Fi is wireless Ethernet communications according to the protocol architectures of IEEE 802.3 with TCP/IP.
Both standards are operating at a specified bandwidth not identical with that of other networking standards; the mechanical plug compatibility problem known with cables is replaced by the compatibility requirement for an air interface and a protocol stack.
B luetoo th Devices Bluetooth exists in many products, such as telephones, modems and headsets.
The technology is useful when transferring information between two or more devices that are near each other in low-bandwidth situations.
Bluetooth is commonly used to transfer sound data with telephones (i.e.
with a Bluetooth headset) or byte data with hand-held computers (transferring files).
Bluetooth protocols simplify the discovery and setup of services between devices.
Bluetooth devices can advertise all of the services the y provide.
This makes using services easier because more of the security, network address and permission configuration can be automated than with many other network types.
Wi-Fi Wi-Fi is more like a traditional Ethernet network, and requires configuration to set up shared resources, transmit files, and to set up audio links (for example, headsets and hands-free devices).
Technologies such as Zeroconf (e.g.
Bonjour) and DHCP can automate some of this configuration, but not as much as Bluetooth.
Wi-Fi uses the same radio frequencies as Bluetooth, but with higher power resulting in a stronger connection.
Wi-Fi is sometimes called wireless Ethernet.
This description is accurate as it also provides an indication of its relative strengths and weaknesses.
Wi-Fi requires more setup, but is better suited for operating full-scale networks.
It also enables a faster connection, better range from the base station, and better security than Bluetooth.
123  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.2 Computer Requirements A personal computer must have a Bluetooth adapter in order to communicate with other Bluetooth devices (such as mobile phones, mice and keyboards).
While some desktop computers and most recent laptops come with a built-in Bluetooth adapter, others will require an external one in the form of a dongle.
Unlike its predecessor, IrDA, which requires a separate adapter for each device, Bluetooth allows multiple devices to communicate with a computer over a single adapter.
3.3 Mobile Phone Requirements A mobile phone that is Bluetooth enabled is able to pair with many devices.
To ensure the broadest support of feature functionality together with legacy device support, the OMTP forum has recently published a recommendations paper, entitled Bluetooth Local Connectivity .
This publication recommends two classes, Basic and Advanced, with requirements that cover imaging, printing, stereo audio and in car usage.
3.4 Specifications and Features The Bluetooth specification was developed in 1994 by Jaap Haartsen and Sven Mattisson, who were working for Ericsson Mobile Platforms in Lund, Sweden.
The specification is based on frequency-hopping spread spectrum technology.
The specifications were formalised by the Bluetooth Special Interest Group (SIG.
The SIG was formally announced on May 20, 1998).
Today it has a membership of over 11,000 companies worldwide.
It was established by Ericsson, IBM, Intel, Toshiba, and Nokia, and later joined by many other companies.
3.5 Future of Bluetooth Bro adcast Cha nnel: enables Bluetooth information points.
This will drive the adoption of Bluetooth into mobile phones, and enable advertising models based around users pulling information from the information points, and not based around the object push model that is used in a limited way today.
Topology Ma nagement: enables the automatic configuration of the piconet topologies especially in scatternet situations that are becoming 124  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY more common today.
This should all be invisible to the users of the technology, while also making the technology just work.
Alternate MAC PHY: enables the use of alternative MAC and PHY s for transporting Bluetooth profile data.
The Bluetooth Radio will still be used for device discovery, initial connection and profile configuration, however when lots of data needs to be sent, the high speed alternate MAC PHY s will be used to transport the data.
This means that the proven low power connection models of Bluetooth are used when the system is idle, and the low power per bit radios are used when lots of data needs to be sent.
QoS improvements: enable audio and video data to be transmitted at a higher quality, especially when best effort traffic is being transmitted in the same piconet.
3.6 Communication and Connection A master Bluetooth device can communicate with up to seven devices.
This network group of up to eight devices is called a piconet.
A piconet is an ad-hoc computer network, using Bluetooth technology protocols to allow one master device to interconnect with up to seven active devices.
Up to 255 further devices can be inactive, or parked, which the master device can bring into active status at any time.
At any given time, data can be transferred between the master and one other device, however, the devices can switch roles and the slave can become the master at any time.
The master switches rapidly from one device to another in a round-robin fashion.
(Simultaneous transmission from the master to multiple other devices is possible, but not used much.)
Bluetooth specification allows connecting two or more piconets together to form a scatternet, with some devices acting as a bridge by simultaneously playing the master role and the slave role in one piconet.
Many USB Bluetooth adapters are available, some of which also include an IrDA adapter.
Older (pre-2003) Bluetooth adapters, however, have limited services, offering only the Bluetooth Enumerator and a less- powerful Bluetooth Radio incarnation.
Such devices can link computers with Bluetooth, but they do not offer much in the way of services that modern adapters do.
125  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.7 Security Overview Bluetooth implements confidentiality, authentication and key derivation with custom algorithms based on the SAFER+ block cipher.
In Bluetooth, key generation is generally based on a Bluetooth PIN, which must be entered into both devices.
This procedure might be modified if one of the devices has a fixed PIN, e.g.
for headsets or similar devices with a restricted user interface.
During pairing, an initialisation key or master key is generated, using the E22 algorithm.
The E0 stream cipher is used for encrypting packets, granting confidentiality and is based on a shared cryptographic secret, namely a previously generated link key or master key.
Those keys, used for subsequent encryption of data sent via the air interface, rely on the Bluetooth PIN, which has been entered into one or both devices.
An overview of Bluetooth vulnerabilities exploits has been published by Andreas Becker.
Bluejacking Bluejacking is the sending of either a picture or a message from one user to an unsuspecting user through Bluetooth wireless technology.
Common applications are short messages (e.g.
You ve just been bluejacked!
), advertisements (e.g.
Eat at Joe s ) and business information.
Bluejacking does not involve the removal or alteration of any data from the device.
History of Security Concerns 2003 In November 2003, Ben and Adam Laurie from A.L.
Digital Ltd. discovered that serious flaws in Bluetooth security may lead to disclosure of personal data.
It should be noted, however, that the reported security problems concerned some poor implementations of Bluetooth, rather than the protocol itself.
In a subsequent experiment, Martin Herfurt from the trifinite.group was able to do a field-trial at the CeBIT fairgrounds, showing the importance of the problem to the world.
A new attack called BlueBug was used for this experiment.
This is one of a number of concerns that have been raised over the security of Bluetooth communications.
126  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 20 04 In 2004 the first purported virus using Bluetooth to spread itself among mobile phones appeared on the Symbian OS.
The virus was first described by Kaspersky Lab and requires users to confirm the installation of unknown software before it can propagate.
The virus was written as a proof-of-concept by a group of virus writers known as 29A and sent to anti-virus groups.
Thus, it should be regarded as a potential (but not real) security threat to Bluetooth or Symbian OS since the virus has never spread in the wild.
In August 2004, a world-record-setting experiment (see also Bluetooth sniping) showed that the range of Class 2 Bluetooth radios could be extended to 1.78 km (1.08 mile) with directional antennas and signal amplifiers.
This poses a potential security threat because it enables attackers to access vulnerable Bluetooth-devices from a distance beyond expectation.
The attacker must also be able to receive information from the victim to set up a connection.
No attack can be made against a Bluetooth device unless the attacker knows its Bluetooth address and which channels to transmit on.
20 05 In January 2005, a mobile malware worm known as Lasco.A began targeting mobile phones using Symbian OS (Series 60 platform) using Bluetooth-enabled devices to replicate itself and spread to other devices.
The worm is self-installing and begins once the mobile user approves the transfer of the file (velasco.sis ) from another device.
Once installed, the worm begins looking for other Bluetooth-enabled devices to infect.
Additionally, the worm infects other .SIS files on the device, allowing replication to another device through use of removable media (Secure Digital, Compact Flash, etc.).
The worm can render the mobile device unstable.
In April 2005, Cambridge University security researchers published results of their actual implementation of passive attacks against the PIN- based pairing between commercial Bluetooth devices, confirming the attacks to be practicably fast and the Bluetooth symmetric key establishment method to be vulnerable.
To rectify this vulnerability, they carried out an implementation which showed that stronger, asymmetric key establishment is feasible for certain classes of devices, such as mobile phones.
In June 2005, Yaniv Shaked and Avishai Wool published a paper describing both passive and active methods for obtaining the PIN for a Bluetooth link.
The passive attack allows a suitably equipped attacker to 127  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY eavesdrop on communications and spoof, if the attacker was present at the time of initial pairing.
The active method makes use of a specially constructed message that must be inserted at a specific point in the protocol, to make the master and slave repeat the pairing process.
After that, the first method can be used to crack the PIN.
This attack s major weakness is that it requires the user of the devices under attack to re- enter the PIN during the attack when the device prompts them to.
Also, this active attack probably requires custom hardware, since most commercially available Bluetooth devices are not capable of the timing necessary.
In August 2005, police in Cambridgeshire, England, issued warnings about thieves using Bluetooth-enabled phones to track other devices left in cars.
Police are advising users to ensure that any mobile networking connections are de-activated if laptops and other devices are left in this way.
2006 In April 2006, researchers from Secure Network and F -Secure published a report that warns of the large number of devices left in a visible state, and issued statistics on the spread of various Bluetooth services and the ease of spread of an eventual Bluetooth worm.
In October 2007, at the Luxemburgish Hack.lu Security Conference, Kevin Finistere and Thierry Zoller demonstrated and released a remote root shell via Bluetooth on Mac OS X v10.3.9 and v10.4.
They also demonstrated the first Bluetooth PIN and Linkkeys cracker, which is based on the research of Wool and Shaked.
3.8 Health Concerns Bluetooth uses the microwave radio frequency spectrum in the 2.4 GHz to 2.4835 GHz range.
Maximum power output from a Bluetooth radio is 100 mW, 2.5 mW, and 1 mW for Class 1, Class 2, and Class 3 devices respectively, which puts Class 1 at roughly the same level as mobile phones, and the other two classes much lower.
Accordingly, Class 2 and Class 3 Bluetooth devices are considered less of a potential hazard than mobile phones, and Class 1 may be comparable to that of mobile phones.
4.0 CO NCLUSIO N The Bluetooth technology as an emerging technology has given the computing world especially network new dimensions.
This technology has made networking to be so easy, neat and relatively inexpensive.
It is 128  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY technology deployed to several areas of human endeavour.
With it you can easily be connected to a network even on a mobile hand set.
It is a technology that is yet to be fully utilised to its potentials.
5.0 SUMMARY • B luetoo th is a wireless protocol utilising short-range communications technology facilitating data transmission over short distances from fixed and/or mobile devices, creating wireless personal area networks (PANs).
• Bluetooth is a standard and communications protocol primarily designed for low power consumption, with a short range based on low-cost transceiver microchips in each device • A personal computer must have a Bluetooth adapter in order to communicate with other Bluetooth devices (such as mobile phones, mice and keyboards).
• A mobile phone that is Bluetooth enabled is able to pair with many devices.
• The Bluetooth specification was developed in 1994 by Jaap Haartsen and Sven Mattisson, who were working for Ericsson Mobile Platforms in Lund, Sweden.
• B roadca st Channel: enables Bluetooth information points.
• A master Bluetooth device can communicate with up to seven devices.
This network group of up to eight devices is called a piconet.
• Bluetooth implements confidentiality, authentication and key derivation with custom algorithms based on the SAFER+ block cipher.
In Bluetooth, key generation is generally based on a Bluetooth P IN, which must be entered into both devices.
• Bluejacking is the sending of either a picture or a message from one user to an unsuspecting user through Bluetooth wireless technology.
• Bluetooth devices are considered less of a potential hazard than mobile phones, and Class 1 may be comparable to that of mobile phones.
6.0 TUTOR-MARKED ASSIGNMENT Mention 10 prevalent applications of the Bluetooth.
129  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 7.0 REF ERENCES/FURTHER READING Newton, Harold.
(2007).
Newton s telecom dictionary.
New York: Flatiron Publishing.
How Bluetooth Technology Works .
Bluetooth SIG.
Wii Controller .
Bluetooth SIG.
Apple (2002-07-17).
Apple Introduces Jaguar, the Next Major Release of Mac OS X .
Press release.
Network Protection Technologies .
Changes to Functionality in Microsoft Windows XP Service Pack 2.
Microsoft Technet.
BlueZ Official Linux Bluetooth protocol stack The Bluetooth Blues , Information Age (2001-05-24)..
Guy Kewney (2004-11-16).
High speed Bluetooth comes a step closer: enhanced data rate approved .Newswireless.net.
Specification Documents .
Bluetooth SIG.
TC TyTN Specification (PDF).
HTC.
(2006-08-03).
Simple Pairing Whitepaper (PDF).
Version V10r00.
Bluetooth SIG.
Michael Oryl (2007-03-15).
Bluetooth 2.1 Offers Touch Based Pairing, Reduced Power Consumption , MobileBurn.
Taoufik Ghanname (2007-02-14.)
How NFC can to speed Bluetooth transactions-today , Wireless Net DesignLine.
Nokia (2007-06-12).
Wibree forum merges with Bluetooth SIG (PDF).
Press release Stallings, William.
(2005).
Wireless communications & networks.
Upper Saddle River, NJ: Pearson Prentice Hall.
Juha T. Vainio (2000-05-25).
Bluetooth Security .
Helsinki University of Technology.
Archived from the original on 2006-05-19.
Andreas Becker (2007-08-16).
Bluetooth Security & Hacks (PDF).
Ruhr-Universität Bochum.
130  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY What is bluejacking?
.
Helsinki University of Technology.
Bluetooth .
The Bunker.
BlueBug .
Trifinite.org.
John Oates (2004-06-15).
Virus attacks mobiles via Bluetooth , The Register.
Long Distance Snarf .
Trifinite.org.
F-Secure Malware Information Pages: Lasco.A .
F-Secure.com.
Ford-Long Wong, Frank Stajano, Jolyon Clulow (2005-04).
Repairing the Bluetooth pairing protocol (PDF).
University of Cambridge Computer Laboratory.
Yaniv Shaked, Avishai Wool (2005-05-02).
Cracking the Bluetooth PIN.
School of Electrical Engineering Systems, Tel Aviv University.
Phone pirates in seek and steal mission , Cambridge Evening News.
(2006-05).
Going Around with Bluetooth in Full Safety (PDF).
F- Secure.
M. Hietanen, T. Alanko (2005-10).
Occupational Exposure Related to Radiofrequenc y Fields from Wireless Communication Systems (PDF).
XXVIIIth General Assembly of URSI Proceedings.
Union Radio-Scientifique Internationale.
Introduction to Bluetooth About the Bluetooth SIG.
Bluetooth SIG.
How Bluetooth got its name 131  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY MODULE 3 Unit 1 The iPod Technology Unit 2 USB Flash Drive Unit 3 MP3 Technology Unit 4 Internet Radio and Television Unit 5 Blended Learning Unit 6 Voice over Internet Protocol UNIT 1 THE iPOD TECHNOLOGY CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History and Design 3.2 Software 3.3 Hardware 3.4 Models 3.5 Sales 3.6 Industry Impact 3.7 Criticism 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N iPod is a popular brand of portable media players designed and marketed by Apple Inc. and launched on October 23, 2001.
As at 2008, the current product line-up includes the hard drive-based iPod Classic, the touchscreen iPod Touch, the video-capable iPod Nano, the screenless iP od Shuffle and the iPhone.
Former products include the compact iPod Mini and the spin-off iPod Photo (since re-integrated into the main iPod Classic line).
IPod Classic models store media on an internal hard drive, while all other models use flash memory to enable their smaller size (the discontinued Mini used a Microdrive miniature hard drive).
As with many other digital music players, iPods, excluding the iPod Touch, can also serve as external data storage devices.
Storage capacity varies by model.
Apple s iTunes software can be used to transfer music to the devices from computers using certain versions of Apple Macintosh and 132  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Microsoft Windows operating systems.
For users who choose not to use Apple s software or whose computers cannot run iTunes software, several open source alternatives to iTunes are also available.
iTunes and its alternatives may also transfer photos, videos, games, contact information, e-mail settings, Web bookmarks, and calendars to iPod models supporting those features.
Apple focused its development on the iPod line s unique user interface and its ease of use, rather than on technical capability.
As at September 2007, more than 150 million iPods had been sold worldwide, making it the best-selling digital audio player series in history.
2.0 OBJECTIVES At the end of this unit, you should be able to: define iPod trace the history and know the various designs available know the software and hardware support for iPod identify the different models of iPod available understand the sales pattern of iPod to date appreciate the impact of iPod on the industry.
3.0 MAIN CONTENT 3.1 H istory and Design iPod came from Apple's "digital hub" category, when the company began creating software for the growing market of personal digital devices.
Digital cameras, camcorders and organisers had well- established mainstream markets, but the company found existing digital music players "big and clunky or small and useless" with user interfaces that were " unbelievably awful," so Apple decided to develop its own.
As ordered by CEO Steve Jobs, Apple's hardware engineering chief Jon Rubinstein assembled a team of engineers to design the iPod line, including hardware engineers Tony Fadell and Michael Dhuey and design engineer Jonathan Ive.
The product was developed in less than one year and unveiled on 23 October 2001.
Jobs announced it as a Mac- compatible product with a 5 GB hard drive that p ut "1,000 songs in your pocket."
Uncharacteristically, Apple did not develop the iPod software entirely in-house, instead using PortalPlayer s reference platform based on 2 ARM cores.
The platform had rudimentary software running on a commercial microkernel embedded operating system.
PortalPlayer had previously been working on an IBM-branded MP3 player with Bluetooth headphones.
Apple contracted another company, Pixo, to help 133  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY design and implement the user interface under the direct supervision of Steve Jobs.
As development progressed, Apple continued to refine the software's look and feel.
Starting with the iPod Mini, the Chicago font was replaced with Espy Sans.
Later iPods switched fonts again to Podium Sans - a font similar to Apple's corporate font, Myriad.
iPods with color displays then adopted some Mac OS X themes like Aqua progress bars, and brushed metal in the lock interface.
In 2007, Apple modified the iPod interface again with the introduction of the sixth- generation iPod Classic and third-generation iPod Nano by changing the font to Helvetica and, in most cases, splitting the screen in half by displaying the menus on the left and album artwork, photos, or videos on the right (whichever was appropriate for the selected item).
In order to defeat a lawsuit from patent holding company Burst.com, Apple finally admitted in September 2008 that the true inventor of the device was not in fact employed by the company; it was Kane Kramer who patented the idea of a "plastic music box" in 1979, which he called the IXI.
He was unable to secure funding to renew the $120,000 worldwide patent, so it lapsed and Kramer never profited from his idea.
After saving Apple from Burst.com's expensive litigation, Kramer is now in talks with the company to discuss how he will be reimbursed.
3.2 Software The iPod line can play several audio file formats including MP3, AAC/M4A, Protected AAC, AIFF, WAV, Audible audiobook, and Apple Lossless.
The iPod Photo introduced the ability to display JPEG, BMP, GIF, TIFF, and PNG image file formats.
Fifth and sixth generation iPod Classics, as well as third generation iPod Nanos, can additionally play MPEG-4 (H.264/MPEG-4 AVC) and QuickTime video formats, with restrictions on video dimensions, encoding techniques and data-rates.
Originally, iPod software only worked with Mac OS; iPod software for Microsoft Windows was launched with the second generation model.
Unlike most other media players, Apple does not support Microsoft's WMA audio format - but a converter for WMA files without Digital Rights Management (DRM) is provided with the Windows version of iTunes.
MIDI files also cannot be played, but can be converted to audio files using the "Advanced" menu in iTunes.
Alternative open-source audio formats, such as Ogg Vorbis and FLAC, are not supported without installing custom firmware onto an iPod (e.g.
Rockbox).
During installation, an iPod is associated with one host computer.
Each time an iPod connects to its host computer, iTunes can synchronise entire music libraries or music playlists either automatically or manually.
Song ratings can be set on an iPod and synchronised later to 134  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY the iTunes library, and vice versa.
A user can access, play, and add music on a second computer if an iPod is set to manual and not automatic sync, but anything added or edited will be reversed upon connecting and syncing with the main computer and its library.
If a user wishes to automatically sync music with another computer, an iPod's library will be entirely wiped and replaced with the other computer's library.
3.3 H ardware Chipsets and electronics Chipset or electronic Product(s) Component(s) iPod (Classic) first to Two ARM 7TDMI-derived third generations CPUs running at 90 MHz iPod (Classic) fourth and Variable-speed ARM 7TDMI fifth generations, iPod CPUs, running at a peak of Mini, iPod Nano first 80 MHz to save battery life generation Microcontroller Samsung System-On-Chip, iPod Nano second based around an ARM generation processor.
SigmaTel STMP3550 chip that iPod Shuffle first handles both the music decoding generation and the audio circuitry.
All iPods (except the audio codecs developed by Audio chip shuffle a nd 6G) Wolfson Microelectronics Sixth generation iPods Cirrus Logic aud io codec chip 45.7 mm (1.8 in) hard drives iPod (Classic) first to ( ATA-6, 4200 rpm with sixth generation proprietary connectors) made by Toshiba Storage medium iPod Mini 25.4 mm (1 in) Microdrive b y Hitachi and Seagate iPod Nano Flash memory from Samsung, Toshiba, and others iPod Shuffle and Touch Flash memory iPod (Classic) first and Internal lithium polymer second generation, Nano, batteries Batteries Shuffle iPod (Classic) third to sixth generation Internal lithium-ion batteries 135  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Connectivity Two iPod wall chargers, with FireWire (left) and USB (right) connectors, which allow iPods to charge without a computer.
Originally, a FireWire connection to the host computer was used to update songs or recharge the battery.
The battery could also be charged with a power adapter that was included with the first four generations.
The third generation began including a dock connector, allowing for FireWire or USB connectivity.
This provided better compatibility with PCs, as most of them did not have FireWire ports at the time.
The dock connector also brought opportunities to exchange data, sound and power with an iPod, which ultimately created a large market of accessories, manufactured by third parties such as Belkin and Griffin.
The second generation iPod Shuffle uses a single 3.5 mm jack which acts as both a headphone jack and a data port for the dock.
Eventually Apple began shipping iPods with USB cables instead of FireWire, although the latter was available separately.
As for the first generation iPod Nano and the fifth generation iPod Classic, Apple discontinued using FireWire for data transfer and made a full transition to USB 2.0 in an attempt to reduce cost and form factor.
With these changes, FireWire could only be used for recharging.
Accessories Many accessories have been made for the iPod line.
A large number are made by third party companies, although many, such as the late iPod Hi- Fi, are made by Apple.
This market is sometimes described as the iPod ecosystem.
Some accessories add extra features that other music players have, such as sound recorders, FM radio tuners, wired remote controls, and audio/visual cables for TV connections.
Other accessories offer unique features like the Nike+iPod pedometer and the iPod Camera Connector.
Other notable accessories include external speakers, wireless remote controls, protective cases/films and wireless earphones.
Among the first accessory manufacturers were Griffin Technology, Belkin, JBL, Bose, Monster Cable, and SendStation.
The white earphones (or "earbuds") that ship with all iPods have become symbolic of the brand.
Advertisements feature them prominently, often contrasting the white earphones (and cords) with people shown as dark silhouettes.
The original earphones came with the first generation iPod.
They were revised to be smaller after Apple received complaints of the earbuds being too large.
The revised earphones were shipped with second through early fifth generation iPods, the iPod Mini, and the first generation Nanos.
The earbuds were revised again in 2006, featuring an 136  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY even smaller and more streamlined design.
This third design was shipped with late fifth generation iPods and the second-generation nanos.
All first generation iPod Shuffles and the second generation up until 30 January 2007 (when color models were introduced) were shipped with the second design; those that shipped after that date were distributed with the third design of the earbuds.
In 2005, New York's Metropolitan Transportation Authority placed advertisements on the subways warning passengers that "Earphones are a giveaway.
Protect your device", after iPod thefts on the subway rose from zero in 2004 to 50 in the first three months of 2005.
BMW released the first iPod automobile interface, allowing drivers of newer BMW vehicles to control an iPod using either the built-in steering wheel controls or the radio head-unit buttons.
Apple announced in 2005 that similar systems would be available for other vehicle brands, including Mercedes-Benz, Volvo, Nissan, Toyota, Alfa Romeo, Ferrari, Acura, Audi, Honda, Renault, Infiniti and Volkswagen Scion offers standard iPod connectivity on all their cars.
Some independent stereo manufacturers including JVC, Pioneer, Kenwood, Alpine, Sony, and Harman Kardon also have iPod-specific integration solutions.
Alternative connection methods include adaptor kits (that use the cassette deck or the CD changer port), audio input jacks, and FM transmitters such as the iTrip - although personal FM transmitters are illegal in some countries.
Many car manufacturers have added audio input jacks as standard.
Beginning in mid-2007, four major airlines, United, Continental, Delta, and Emirates reached agreements to install iPod seat connections.
The free service will allow passengers to power and charge an iPod, and view video and music libraries on individual seat-back displays.
Originally KLM and Air France were reported to be part of the deal with Apple, but they later released statements explaining that they were only contemplating the possibility of incorporating such systems.
3.4 Models The iPod line has been upgraded many times, and each significant revision is called a "generation".
Only the most recent (highest numbered) generation and refurbished units of previous generations of the iPod line are available from Apple for each model (Classic, Nano, Shuffle, Touch).
Each new generation usually has more features and refinements while typically being physically smaller and lighter than its predecessor, while usually (but not always) retaining the older model's price tag.
Notable changes include the touch-sensitive click wheel 137  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY replacing the mechanical scroll wheel, use of color displays, and flash memory replacing hard disks.
There are current ly a bo ut five mo dels of iPod Classic: This model has gone through about six generations.
1st Genera tion: This model was released on 23 October, 2001.It is the first model, with mechanical scroll wheel.
2nd Generation: Was released in July, 2002.
Has Touch-sensitive wheel.
FireWire port had a cover.
Hold switch revised.
Windows compatibility through Musicmatch.
3rd Generatio n: Released in April, 2003.
First complete redesign with all-touch interface, dock connector, and slimmer case.
Musicmatch support dropped with later release of iTunes 4.1 for Windows.
4th Genera tion: Released in October, 2004.
First to have color production.
Adopted Click Wheel from iPod Mini.
Premium spin-off of 4G iPod with color screen and picture viewing.
Later re-integrated into main iPod line.
5th Generation: Released in October, 2005.
Second full redesign with a slimmer case, and larger screen with video playback.
O ffered in black or white.
On 12 September, 2006, 60GB was upgraded to 80GB, 60% brighter screens added, gapless playback added, 3-D photo transitions added, a search feature and better batteries.
This updated 5G iPod was known as the 5.5G.
6th Generatio n: Released in September, 2007.
Introduced the "classic" suffix.
New interface and anodised aluminum front plate.
Silver replaces white.
On 9 September, 2008, the original 80 and 160 GB versions were replaced with a 'one size fits all' 120 GB version.
Mini: This model has two generations so far.
1st Generatio n: First released in January, 2004.
New smaller model, available in 5 colors.
Introduced the "Click Wheel.
2nd Generation: Released in February, 2005.
Brighter color variants with longer battery life.
Click Wheel lettering matched body color.
Gold color discontinued.
Later replaced by iPod Nano.
Nano: This model has four generations.
138  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 1 st Genera tion: First released in September, 2005.
Replaced Mini.
Available in black or white and used flash memory.
Color screen for picture viewing.
1 GB version released later.
2 nd Genera tion; Released in September, 2006.
Anodised aluminum casing and 6 colors available.
3 rd Genera tio n: Released in September, 2007.
Has 2" QVGA screen, colors refreshed with chrome back, new interface, video capability, smaller Click Wheel.
4 th Generation: Released in September, 2008.
Revert to tall form and all-aluminum enclosure, accelerometer for shake and horizontal viewing.
9 colors, now unable to charge over FireWire.
Shuffle: This model consists of two generations.
1 st Generation: Released in January, 2005.
New entry-level model.
Uses flash memory and has no screen.
2 nd Generatio n: Released in September, 2007.
First iPod with Wi-Fi and a Multi-Touch interface.
Features Safari browser and wireless access to the iTunes Store and YouTube.
32 GB model later added.
To uch: This model consists of two generations.
1 st Generatio n: Released in September, 2007.
First iPod with Wi-Fi and a Multi-Touch interface.
Features Safari browser and wireless access to the iTunes Store and YouTube.
32 GB model later added.
2 nd Generatio n: Released in September, 2008.
Nike+ functionality added, new tapered chrome back, dedicated volume buttons on the side, proper in-built speaker capable of playing music, now unable to charge over FireWire.
Sources: Apple Inc. model database Mactracker 3.5 Sales Since October, 2004, the iPod line has dominated digital music player sales in the United States, with over 90% of the market for hard drive- based players and over 70% of the market for all types of players.
During the year from January, 2004 to January, 2005, the high rate of sales caused its U.S. market share to increase from 31% to 65% and in July, 2005, this market share was measured at 74%.
In January, 2007 the iPod market share reached 72.7% according to Bloomberg Online.
139  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY The release of the iPod Mini helped to ensure this success at a time when competing flash-based music players were once dominant.
On 8 January, 2004, Hewlett-Packard (HP) announced that they would sell HP-branded iPods under a license agreement from Apple.
Several new retail channels were used-includingWal-Mart-and these iPods eventually made up 5% of all iPod sales.
In July, 2005, HP stopped selling iPods due to unfavorable terms and conditions imposed by Apple.
In January, 2007, Apple reported record quarterly revenue of US$7.1 billion, of which 48% was made from iPod sales.
On 9 April, 2007, it was announced that Apple had sold its one-hundred millionth iPod, making it the biggest selling digital music player of all time.
In April, 2007, Apple reported second quarter revenue of US$5.2 billion, of which 32% was made from iPod sales.
Apple and several industry analysts suggest that iPod users are likely to purchase other Apple products such as Mac computers.
On 5 September, 2007, during their "The Beat Goes On" event, Apple announced that the iPod line had surpassed 110 million units sold.
On 22 October 2007, Apple reported quarterly revenue of US$6.22 billion, of which 30.69% came from Apple notebook sales, 19.22% from desktop sales and 26% from iPod sales.
Apple's 2007 year revenue increased to US$24.01 billion with US$3.5 billion in profits.
Apple ended the fiscal year 2007 with US$15.4 billion in cash and no debt.
On 22 January, 2008, Apple reported the best quarter revenue and earnings in Apple's history so far.
Apple posted record revenue of $9.6 billion and record net quarterly profit of $1.58 billion.
42% of Apple's revenue for the First fiscal quarter of 2008 came from iPod sales, followed by 21% from notebook sales and 16% from desktop sales.
Apple has sold over 163M iPods to date .
It also posted record Mac and iPod sales to date.
3.6 Industry Impact iPods have won several awards ranging from engineering excellence, to most innovative audio product, to fourth best computer product of 2006. iPods often receive favorable reviews; scoring on looks, clean design, and ease of use.
PC World says that iPod line has "altered the landscape for portable audio players".
Several industries are modifying their products to work better with both the iPod line and the AAC audio format.
Examples include CD copy-protection schemes, and mobile phones, such as phones from Sony Ericsson and Nokia, which play AAC files rather than WMA.
140  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY In addition to its reputation as a respected entertainment device, iPods have also become accepted as business devices.
Government departments, major institutions and international organisations have turned to the iPod line as a delivery mechanism for business communication and training, such as the Royal and Western Infirmaries in Glasgow, Scotland where iPods are used to train new staff.
iPods have also gained popularity for use in education.
Apple offers more information on educational uses for iPods on their website, including a collection of lesson plans.
There has also been academic research done in this area in nursing education and more general K-16 education.
Duke University provided iPods to all incoming freshmen in the fall of 2004, and the iPod program continues today with modifications.
3.7 Criticism Ba ttery Issues The advertised battery life on most models is different from the real- world achievable life.
For example, the fifth generation 30 GB iPod is advertised as having up to 14 hours of music playback.
An MP3.com report stated that this was virtually unachievable under real-life usage conditions, with a writer for MP3.com getting on average less than 8 hours from an iPod.
In 2003, class action lawsuits were brought against Apple complaining that the battery charges lasted for shorter lengths of time than stated and that the battery degraded over time.
The lawsuits were settled by offering individuals either US$50 store credit or a free battery replacement.
iPod batteries are not designed to be removed or replaced by the user, although some users have been able to open the case themselves, usually following instructions from third-party vendors of iPod replacement batteries.
Compounding the problem, Apple initially would not replace worn-out batteries.
The official policy was that the customer should buy a refurbished replacement iPod, at a cost almost equivalent to a brand new one.
All lithium-ion batteries eventually lose capacity during their lifetime (guidelines are available for prolonging life-span) and this situation led to a small market for third-party battery replacement kits.
Apple announced a battery replacement program on 14 November, 2003, a week before a high publicity stunt and website by the Neistat Brothers.
The initial cost was US$99, and it was lowered to US$59 in 2005.
One week later, Apple offered an extended iPod warranty for US$59.
For the iPod Nano, soldering tools are needed because the battery is soldered onto the main board.
Fifth generation iPods have their battery attached to the backplate with adhesive.
141  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Reliability a nd Durability iPods have been criticised for their short life-span and fragile hard drives.
A 2005 survey conducted on the MacInTouch website found that the iPod line had an average failure rate of 13.7% (although they note that comments from respondents indicate that "the true iPod failure rate may be lower than it appears").
It concluded that some models were more durable than others.
In particular, failure rates for iPods employing hard drives was usually above 20% while those with flash memory had a failure rate below 10%, indicating poor hard drive durability.
In late 2005, many users complained that the surface of the first generation iPod Nano can become scratched easily, rendering the screen unusable.
A class action lawsuit was also filed.
Apple initially considered the issue a minor defect, but later began shipping these iPods with protective sleeves.
Allegations o f Worker Explo itation On 11 June, 2006, the British newspaper Mail on Sunday reported that iPods are mainly manufactured by workers who earn no more than US$50 per month and work 15-hour shifts.
Apple investigated the case with independent auditors and found that, while some of the plant's labour practices met Apple's Code of Conduct, others did not: Employees worked over 60 hours a week for 35% of the time, and worked more than six consecutive days for 25% of the time.
Foxconn, Apple's manufacturer, initially denied the abuses, but when an auditing team from Apple found that workers had been working longer hours than were allowed under Chinese law, they promised to prevent workers working more hours than the code allowed.
Apple hired a workplace standards auditing company, Verité, and joined the Electronic Industry Code of Conduct Implementation Group to oversee the measures.
On 31 December, 2006, workers at the Longhua, Shenzhen factory (owned by Foxconn) formed a union.
The union is affiliated with the world's largest and most powerful federation of trade unions, the All- China Federation of Trade Unions.
4.0 CO NCLUSIO N iPod came from Apple's digital hub strategy, when the company began creating software for the growing market of digital devices being purchased by consumers.
iPod and iPod photo are now one and the same, with every white iPod boasting a full-color display.
iPod makes your music look as good as it sounds, thanks to its big, bright color display.
142  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 5.0 SUMMARY • iPod is a popular brand of portable media players designed and marketed by Apple Inc. and launched on October 23, 2001.
• iPod came from Apple's "digital hub" category, when the company began creating software for the growing market of personal digital devices.
• The iPod line can play several audio file formats including MP3, AAC/M4A, Protected AAC, AIFF, WAV, Audible audiobook, and Apple Lossless.
• Two iPod wall chargers, with FireWire (left) and USB (right) connectors, allow iPods to charge without a computer.
• Many accessories have been made for the iPod line.
A large number are made by third party com panies, although many, such as the iPod Hi-Fi, are made by Apple.
• The iPod line has been upgraded many times, and each significant revision is called a "generation".
Only the most recent (highest numbered) generation and refurbished units of previous generations of the iPod line are available from Apple for each model.
• Since October, 2004, the iPod line has dominated digital music player sales in the United States, with over 90% of the market for hard drive-based players and over 70% of the market for all types of players.
• iPods have won several awards ranging from engineering excellence, to most innovative audio product, to fourth best computer product of 2006. iPods often receive favorable reviews; scoring on looks, clean design, and ease of use.
• iPods have been criticised for their short life-span and fragile hard drives.
• The advertised battery life on most models is different from the real-world achievable life.
6.0 TUTOR-MARKED ASSIGNMENT Discuss the generations of the classic model of iPod.
7.0 REF ERENCES/FURTHER READING Charles Gaba. "
iPod Sales: Quarterly & Total".. Apple Inc.. "iTunes system requirements.
Apple iTunes software currently runs on Macintosh OS X 10.3.9 or OS X 10.4.9 or later and on Microsoft Windows XP (Service Pack 2) or Vista".
143  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Ross McKillop, simplehelp.net.
"Alternatives to iTunes for managing your iPod" .
"2007 Engineer of the Year Finalist Michael Dhuey s Hardware Knowledge Helps Breathe Life Into iPod, TelePresence", Design News, 24 September 2007.
Matyszczyk, Chris Apple admits it didn't invent the iPod CNet News, 2008-09-07.
Retrieved on 2008-09-07. iPod Classic Technical Specs Scott-Joynt, Jeremy.
Apple targets TV and film market, BBC News, 2006-09-12.
Kanellos, Michael.
Real's Glaser exhorts Apple to open iPod, CNet News, 2004-03-23.
Orlowski, Andrew.
Your 99c belong to the RIAA Steve Jobs, The Register, 2003-11-07.
Evans, Jonny.
Universal confirms iTunes contract change, Macworld UK, 2007-07-04.
Kuzmanoski, Brian.
Analysis of the iPod's equalizer, DAP review.
MacInTouch reader report of iPod sound distortion, MacInTouch, July 2002.
Cassell, Jonathan.
Apple Delivers More for Less with New iPod Nano, iSuppli Corporation, 2006-09-20 Williams, Martyn.
How Much Should an IPod Shuffle Cost?, PC World, 2005-02-24.
Johnson, Joel (2008-07-10).
"How the "Apple Tax" Boosts Prices on iPod & iPhone Accessories", Popular Mechanics.
Darlin, Damon.
The iPod Ecosystem.
New York Times, 2006-02-03.
In-The-Ear Bluetooth Earphones.
Retrieved on 2007-02-17.
Dianner.
Earphones are a giveaway, Flickr, 2005-07-21.
MacMillan, Robert.
Somebody Out There wants your iPod, Washington Post, 2005-04-28.
144  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY iPod Your BMW.
Apple & Mercedes-Benz Unveil iPod Integration Kit, Apple Inc., 2005- 01-11.
Apple & Volvo Announce iPod Connectivity For Entire 2005 US Model Line, Apple Inc. 2005-01-11. http://www.gizmag.com/go/7945/ Apple & Leading Car Companies Team Up to Deliver iPod Integration in 2005, Apple Inc. 2005-01-11.
Honda Music Link for iPods, Honda.
Apple Car Integration page Apple Teams Up With Acura, Audi, Honda & Volkswagen to Deliver Seamless iPod Experience, Apple Inc. 2005-09-07.
Car Integration: iPod your car, Apple Inc. Apple Teams up with Continental, Delta, Emirates, & United to deliver iPod Integration, Apple Inc. 2006-11-14.
Marsal, Katie.
Two of six airlines say there's no ink on iPod deal, AppleInsider, 2006-11-15 Machrone, Bill.
iPod audio measurements, PC Magazine, 2005 Heijligers, Marc.
iPod audio measurements.
Heijligers, Marc.
iPod circuit design engineering, May 2006.
Identifying iPod models, retrieved 31 October 2007.
Mactracker (mactracker.ca), Apple Inc. model database, version as of 26 July 2007.
Apple Unveils New iPods, Apple Inc., 2002-07-17.
Apple Launches iTunes for Windows, Apple Inc.2003-10-16.
Dalrymple, Jim.
Limited Edition Madonna, Tony Hawk, Beck iPods.
Macworld, 2002-09-10.
Apple trgovina 145  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Apple press release library.
Apple faces patent lawsuits over its iPod, ChannelRegister, 2005-03-10.
U.S. Patent 6,665,797 "Protection of software again against unauthorized use" (corrected to "Computer Apparatus/Software Access Control").
Apple, Sony among those named in new DRM lawsuit, AppleInsider, 2005-08-16.
U.S. patent application 20030095096 Apple's application on "rotational user inputs".
U.S. Patent 6,928,433 Creative Technology's "Zen" patent.
Creative wins MP3 player patent, BBC News, 2005-08-30.
Creative sues Apple over patent, Macworld UK, 2006-05-16.
Apple & Creative Announce Broad Settlement... Apple Inc. Marsal, Katie.
iPod: how big can it get?, AppleInsider, 2006-05-24.
Apple Reports Fourth Quarter 2007 Results, Apple Inc. 2007-10-22.
Retrieved on 2007-10-22.
Apple Inc. (22 January 2008).
Apple Reports First Quarter Result.
Press release.
Retrieved on 2008-1-23 Adam White, With You in Technology, Technology Articles on ArticlesTree.com.
2007.
146  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 2 USB FLASH DRIVE CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Technology 3.2 History 3.3 Design and Implementation 3.4 Uses 3.5 Advantages and Disadvantages 3.6 Security 3.7 Social Relationships 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignments 7.0 References/Further Reading 1.0 INTRODUCTIO N A USB fla sh drive is a N AND-type flash memory data storage device integrated with a USB (universal serial bus) interface.
USB flash drives are typically removable and rewritable, much shorter than a floppy disk (1 to 4 inches or 2.5 to 10 cm), and weigh less than 2 ounces (60 g).
Storage capacities typically range from 64 MB to 64 GB with steady improvements in size and price per gigabyte.
Some allow 1 million write or erase cycles and have 10-year data retention, connected by USB 1.1 or USB 2.0.
USB Memory card readers are also available, whereby rather than being built-in, the memory is a removable flash memory card housed in what is otherwise a regular USB flash drive, as described below.
USB flash drives offer potential advantages over other portable storage devices, particularly the floppy disk.
They are more compact, faster, hold much more data, have a more durable design, and are more reliable for lack of moving parts.
Additionally, it has become increasingly common for computers to ship without floppy disk drives.
USB ports, on the other hand, appear on almost every current mainstream PC and laptop.
These types of drives use the USB mass storage standard, supported natively by modern operating systems such as Windows, Mac OS X, Linux, and other Unix-like systems.
USB drives with USB 2.0 support can also be faster than an optical disc drive, while storing a larger amount of data in a much smaller space.
147  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Nothing actually moves in a flash drive: it is called a drive because it is designed to read and write data using the same system commands as a mechanical disk drive, appearing to the computer operating system and user interface as just another drive.
A flash drive consists of a small printed circuit board protected inside a plastic, metal, or rubberised case, robust enough to be carried with no additional protection, in a pocket or on a key chain for example.
The USB connector is protected by a removable cap or by retracting into the body of the drive, although it is not liable to be damaged if exposed.
Most flash drives use a standard type-A USB connection allowing them to be plugged into a port on a personal computer.
To access the drive it must be connected to a USB port, which powers the drive and allows it to send and receive data.
Some flash drives, especially high-speed drives, may require more power than the limited amount provided by a bus-powered USB hub, such as those built into some computer keyboards or monitors.
These drives will not work properly unless plugged directly into a host controller (i.e., the ports found on the computer itself) or a self-powered hub.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define flash drive • trace the trend and history in the development of flash drive • answer the questions of design and implementation of flash drive • identify the advantages and disadvantages associated with the use of flash drive • know the various uses of flash drive • know how secure it is to use flash drive.
3.0 MAIN CONTENT 3.1 Tec hnology Flash memory is actually a combination of a number of older technologies, with the low cost, low power consumption and small size being made possible by recent advances in microprocessor technology.
The memory storage is based on earlier EPROM and EEPROM technologies.
These had very limited capacity, were very slow for both reading and writing, required complex high-voltage drive circuitry, and could only be re-written after erasing the entire contents of the chip.
148  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Later EEPROMS were developed where the erasure region was broken up into smaller "fields" that could be erased individually without affecting the others.
Altering the contents of a particular memory location involved first copying the entire field into an off-chip buffer memory, erasing the field, and then re-writing the data back into the same field, making the necessary alteration to the relevant memory location while doing so.
This required considerable computer support, and PC-Based EEPROM flash memory systems often carried their own dedicated microprocessor system.
Flash drives are more or less a miniaturised version of this.
The development of high-speed serial data interfaces such as USB for the first time made serially accessed storage memory systems viable, and the simultaneous development of small, high-speed, low-power microprocessor systems allowed this to be incorporated into extremely compact systems.
Serial access also greatly reduced the number of electrical connections required for the memory chips, which has allowed the successful manufacture of multi-gigabyte capacities (Every external electrical connection is a potential source of manufacturing failure, and with traditional manufacturing, a point is rapidly reached where the successful yield approaches zero).
Modern flash memory systems are accessed very much like hard disk drives, where the controller system has full control over where information is actually stored.
The actual EEPROM writing and erasure processes are, however, still very similar to the earlier systems described above.
Many low-cost MP3 Players simply add extra software to a standard flash memory control microprocessor so it can also serve as a music playback decoder.
Most of these players can also be used as a conventional flash drive.
3.2 H istory First Co mmercia l Product Flash drive with retractable USB connector Trek Technology and IBM began selling the first USB flash drives commercially in 2000.
Singaporean company Trek Technology sold a model dubbed the "ThumbDrive," and IBM marketed the first such drives in North America, with its product the "DiskOnKey" (which was 149  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY manufactured by M-Systems).
IBM's USB flash drive became available December 15, 2000, and had a storage capacity of 8 MB, more than five times the capacity of the (at the time) commonly used floppy disks.
In 2000 Lexar introduced a Compact Flash (CF) card with a USB connection, and a companion card read/writer and USB cable that eliminated the need for a USB hub.
In 2004 Trek Technology brought several lawsuits against other USB flash drive manufacturers and distributors in an attempt to assert its patent rights to the USB flash drive.
A court in Singapore ordered competitors to cease selling similar products that would be covered by Trek's patent, but a court in the United Kingdom revoked one of Trek's patents in that country.
Second Generatio n Toshiba TransMemory Flash Drive with cover on comes pre-installed with U3, allowing users to take their applications, fully installed and operational, to any desktop.
Modern flash drives have USB 2.0 connectivity.
However, they do not currently use the full 480 Mbit/s (60MB/s) the USB 2.0 Hi-Speed specification supports due to technical limitations inherent in NAND flash.
The fastest drives currently available use a dual channel controller, although they still fall considerably short of the transfer rate possible from a current generation hard disk, or the maximum high speed USB throughput.
Typical overall file transfer speeds vary considerably, and should be checked before purchase; speeds may be given in megaby tes or megabits per second.
Typical fast drives claim to read at up to 30 megabytes/s (MB/s) and write at about half that.
Older " USB full speed" 12 megabit/s devices are limited to a maximum of about 1 MB/s.
3.3 Design and Implementation One end of the device is fitted with a single male type-A USB connector.
Inside the plastic casing is a small printed circuit board.
Mounted on this board are some simple power circuitry and a small number of surface-mounted integrated circuits (ICs).
Typically, one of these ICs provides an interface to the USB port, another drives the onboard memory, and the other is the flash memory.
Drives typically use the USB mass storage device class to communicate with the host.
150  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Essential Components There are typically four parts to a flash drive: Male type-A USB connector provides an interface to the host computer.
USB mass storage controller implements the USB host controller.
The controller contains a small microcontroller with a small amount of on-chip ROM and RAM.
NAND flash memory chip stores data.
NAND flash is typically also used in digital cameras .
Crystal oscillator produces the device's main 12 MHz clock signal and controls the device's data output through a phase-locked loop.
Size a nd Style of Pa ckag ing Flash drives come in various, sometimes bulky or novelty, shapes and sizes, in this case ikura sushi.
Some manufacturers differentiate their products by using elaborate housings, which are often bulky and make the drive difficult to connect to the USB port.
Because the USB port connectors on computer housing are often closely spaced, plugging a flash drive into a USB port may block an adjacent port.
Such devices may only carry the USB logo if sold with a separate extension cable.
USB flash drives have been integrated into other commonly-carried items such as watches, pens, and even the Swiss Army Knife; others have been fitted with novelty cases such as toy cars or LEGO bricks.
The small size, robustness and cheapness of USB flash drives make them an increasingly popular peripheral for case molding.
Heavy or bulky flash drive packaging can make for unreliable operation when plugged directly into a USB port; this can be relieved by a USB extension cable.
Such cables are USB-compatible, but do not conform to the USB 1.0 standard.
File System Most flash drives ship preformatted with the FAT or FAT 32 file system.
The ubiquity of this file system allows the drive to be accessed on virtually any host device with USB support.
Also, standard FAT maintenance utilities (e.g.
ScanDisk) can be used to repair or retrieve corrupted data.
However, because a flash drive appears as a USB- connected hard drive to the host system, the drive can be reformatted to any file system supported by the host operating system.
151  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Flash drives can be defragmented, but this brings no advantage as there is no mechanical head slowed down by having to move from fragment to fragment; if anything, it shortens the life of the drive by making many unnecessary writes.
Some file systems are designed to distribute usage over an entire memory device without concentrating usage on any part (e.g., for a directory); this prolongs life of simple flash memory devices.
USB flash drives, however, have this functionality built into the controller to maximise device life, and use of such a file s ystem brings no advantage.
3.4 Use s Persona l Da ta Tra nsport The most common use of flash drives is to transport and store personal files such as documents, pictures and videos.
Individuals also store medical alert information on MedicTag flash drives for use in emergencies and for disaster preparation.
Secure Storage o f Data, Applica tio n and Software Files With wide deployment(s) of flash drives being used in various environments (secured or otherwise), the issue of data and information security remains of the utmost importance.
The use of biometrics and encryption is becoming the norm with the need for increased security for data; OTFE systems such as FreeOTFE and TrueCrypt are particularly useful in this regard, as they can transparently encrypt large amounts of data.
System Administratio n Flash drives are particularly popular among system and network administrators, who load them with configuration information and software used for system maintenance, troubleshooting, and recovery.
Computer Repa ir Flash drives enjoy notable success in the PC repair field as a means to transfer recovery and antivirus software to infected PCs, while allowing a portion of the host machine's data to be archived in case of emergency.
As the drives have increased in storage space, they have also replaced the need to carry a number of CD RO Ms and installers which were needed when reinstalling or updating a system.
152  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Applicatio n Carriers Flash drives are used to carry applications that run on the host computer without requiring installation.
While any standalone application can in principle be used this way, many programmes store data, configuration information, etc.
on the hard drive and registry of the host computer.
The U3 company works with drive makers (parent company SanDisk as well as others) to deliver custom versions of applications designed for Microsoft Windows from a special flash drive; U3-compatible devices are designed to autoload a menu when plugged into a computer running Windows.
Applications must be modified for the U3 platform and not to leave any data on the host machine.
U3 also provides a software framework for ISVs interested in their platform.
Ceedo is an alternative product with the key difference that it does not require Windows applications to be modified in order for them to be carried and run on the drive.
A range of portable applications which are all free of charge and able to run off a computer running Windows without storing anything on the host computer's drives or registry is available from portableapps.com; unlike U3 programs which run from a special U3-compatible USB stick, the PortableApps menu will run from a standard device, but will not autoload.
Computer Fo rensics and Law Enfo rcement A recent development for the use of a USB Flash Drive as an application carrier is to carry the Computer Online Forensic Evidence Extractor (COFEE) application developed by Microsoft.
COFEE is a set of applications designed to search for and extract digital evidence on computers confiscated from suspects.
Forensic software should not alter the information stored on the computer being examined in any way; other forensic suites run from CD-ROM or DVD-ROM, but cannot store data on the media they are run from (although they can write to other attached devices such as external drives or memory sticks).
Boo ting Opera ting Systems Most current PC firmware permits booting from a USB drive, allowing the launch of an operating system from a bootable flash drive.
Such a configuration is known as a Live USB.
153  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY While a Live USB could be used for general-purpose applications, size and memory wear make them poor choices compared to alternatives.
They are more suited to special-purpose or temporary tasks, such as: • Loading a minimal, hardened kernel for embedded applications (e.g.
network router, firewall).
• Bootstrapping an operating system install or disk cloning operation, often across a network.
• Maintenance tasks, such as virus scanning or low-level data repair, without the primary host operating s ystem loaded.
Windo ws Vista Rea dyBo ost In Windows Vista, the ReadyBoost feature allows use of some flash drives to augment operating system memory.
Audio Players Many companies make small solid-state digital audio players, essentially producing flash drives with sound output and a simple user interface.
Examples include the Creative MuVo and the iPod shuffle.
Some of these players are true USB flash drives as well as music players; others do not support general-purpose data storage.
Many of the smallest players are powered by a permanentl y fitted rechargeable battery, charged from the USB interface.
Music Storag e a nd Ma rketing Digital audio files can be transported from one computer to another like any other file, and played on a compatible media player (with caveats for DRM-locked files).
In addition, many home Hi-Fi and car stereo head units are now equipped with a USB port.
This allows a USB flash drive containing media files in a variety of formats to be played directly on devices which support the format.
The files may be ripped from CD or purchased or downloaded online, and there have been some cases of pre-encoded music sold or given away for promotion on USB flash drives.
In 2004, the German band WIZO made a recording industry first of releasing the "Stick EP ", an album available only as a USB drive.
In addition to five high-bitrate MP3s, it also included a video, pictures, lyrics, and guitar tablature.
Third album by Portishead in the limited edition box set.
Not Here to Please You EP by Hadouken!
154  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Icky Thump single by White Stripes Good Life single by Kanye West X album by Kylie Minogue Nine Inch Nails promoted their album Year Zero by hiding USB drives at concerts, usually with content like select songs contained on them.
Nothing In My Way single by Keane -The 512MB drive was loaded with the audio track, music video, and several extras.
The Bedlam in Goliath album by The Mars Volta was released on a USB drive shaped like an ouija planchette to coincide with the album's concept.
In addition to the music files, the drive can be used to unlock exclusive material on the band's website.
In 2007, a product known as the MU-STIK appeared in the market, offering a means of packaging music albums by containing all relevant digital audio/video data and a customisable player interface within a USB key.
The I Heart Revolution: With Hearts as One album by Hillsong United was released in 2008 on a flash drive embedded in a black rubber wristband.
In addition to the music files, the drive included the lyrical overhead master sheets, a PDF copy of the liner sleeve, and the two versions of the album cover.
In Arca des In the Arcade game In the Groove and more commonly In the Groove 2, flash drives are used to transfer high scores, screenshots, dance edits, and combos throughout sessions.
As of software revision 21 (R21), players can also store custom songs and play them on any machine on which this feature is enabled.
While use of flash drives is common, the drive must be Linux compatible, causing problems for some players.
B rand and product pro mo tio n The availability of inexpensive flash drives has enabled them to be used for promotional and marketing purposes, particularly within technical and computer-industry circles (e.g.
technology trade shows).
They may be given away for free, sold at less than wholesale price, or included as a bonus with another purchased product.
Usually, such drives will be custom-stamped with a company's logo, as a form of advertising to increase mind share and brand awareness.
The drive may be blank drive, or preloaded with graphics, documentation, web links, flash animation or other multimedia, and free or 155  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY demonstration software.
Some preloaded drives are read-only; others are configured with a read-only and a writeable partition.
Dual-partition drives are more expensive.
Flash drives can be set up to autorun stored presentations, websites and articles immediately on insertion of the drive by saving a file called autorun.inf with an appropriate shell script in the root directory of the drive.
Autoloading this way does not work on all computers; the U3 drives described above load more reliably.
Backup Some value-added resellers are now using a flash drive as part of small- business turnkey solutions (e.g.
point-of-sale systems).
The drive is u sed as a backup medium: at the close of business each night, the drive is inserted, and a database backup is saved to the drive.
Alternatively, the drive can be left inserted through the business day, and data regularly updated.
In either case, the drive is removed at night and taken offsite.
This is simple for the end-user, and done for the following reasons: • the drive is small and convenient, and more likely to be carried off-site for safety; • the drives are less fragile mechanically and magnetically than tapes; • the capacity is often large enough for several backup images of critical data; • and flash drives are cheaper than many other backup systems.
It is also easy to lose these small devices, and easy for people without a right to data to take illicit backups.
3.5 Advantages and Disadvantages Advantages Flash drives are impervious to scratches and dust, and mechanically very robust making them suitable for transporting data from place to place and keeping it readily at hand.
Flash drives also store data relatively densely compared to many removable media.
In mid-2008, 64 GB drives became available, with the ability to hold many times more data than a DVD.
Compared to hard drives, flash drives use little power, have no fragile moving parts, and for low capacities are small and light.
156  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Flash drives implement the USB mass storage device class so that most modern operating systems can read and write to them without installing device drivers.
The flash drives present a simple block-structured logical unit to the host operating system, hiding the individual complex implementation details of the various underlying flash memory devices.
The operating system can use any file s ystem or block addressing scheme.
Some computers can boot up from flash drives.
Some flash drives retain their memory after being submerged in water, even through a machine wash, although this is not a design feature and not to be relied upon.
Leaving the flash drive out to dry completely before allowing current to run through it has been known to result in a working drive with no future problems.
Channel Five's Gadget Show cooked a flash drive with propane, froze it with dry ice, submerged it in various acidic liquids, ran over it with a jeep and fired it against a wall with a mortar.
A company specialising in recovering lost data from computer drives managed to recover all the data on the drive.
All data on the other removal storage devices tested, using optical or magnetic technologies, were destroyed.
Disa dva ntag es Like all flash memory devices, flash drives can sustain only a limited number of write and erase cycles before failure.
This should be a consideration when using a flash drive to run application software or an operating system.
To address this, as well as space limitations, some developers have produced special versions of operating systems (such as Linux in Live USB) or commonplace applications (such as Mozilla Firefox) designed to run from flash drives.
These are t ypically optimised for size and configured to place temporary or intermediate files in the computer's main RAM rather than store them temporarily on the flash drive.
Most USB flash drives do not include a write-protect mechanism, although some have a switch on the housing of the drive itself to keep the host computer from writing or modifying data on the drive.
Write- protection makes a device suitable for repairing virus-contaminated host computers without risk of infecting the USB flash drive itself.
A drawback to the small size is that they are easily misplaced, left behind, or otherwise lost.
This is a particular problem if the data they contain are sensitive.
As a consequence, some manufacturers have added encryption hardware to their drives - although software encryption systems achieve the same thing, and are universally available for all USB flash drives.
Others just have the possibility of being attached to keychains, necklaces and lanyards.
157  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Compared to other portable storage, particularly external hard drives, USB flash drives have a high price per unit of storage and are only available in comparatively small capacities; but in the smaller capacities (4 GB and less), USB flash drives are much less expensive per unit of storage than the hard drives they have replaced.
3.6 Security Some flash drives feature encryption of the data stored on them, generally using full disk encryption below the file system.
This prevents an unauthorised person from accessing the data stored on it.
The disadvantage is that the drive is accessible only in the minority of computers which have compatible encryption software, for which no portable standard is widely deployed.
Some encryption applications allow running without installation.
The executable files can be stored on the USB drive, together with the encrypted file image.
The encrypted partition can be accessed on any computer running the correct operating system.
Other flash drives allow the user to configure secure and public partitions of different sizes.
Executable files for Windows, Macintosh, and Linux may be on the drive, depending on manufacturer support.
Some security software may require administrative rights on the host PC to access data.
Newer flash drives support biometric fingerprinting to confirm the user's identity.
As of mid-2005, this was a relatively costly alternative to standard password protection offered on many new USB flash storage devices.
Most fingerprint scanning drives rely upon the host operating system to validate the fingerprint via a software driver, often restricting the drive to Microsoft Windows computers.
However, there are USB drives with fingerprint scanners which use controllers that allow access to protected data without any authentication.
4.0 CO NCLUSIO N Flash drives have virtually ended the era of using floppy disks that had challenges of corruption by virus.
The major hallmark of the flash drive is that it is not only highly portable but could hold large amount of data.
This has taken away the problems hitherto facing the input devices world.
They come in different aerodynamic shapes and sizes.
158  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 5.0 SUMMARY A USB fla sh drive is a N AND-type flash memory data storage device integrated with a USB (universal serial bus) interface.
Flash memory is actually a combination of a number of older technologies, with the low cost, low power consumption and small size being made possible by recent advances in microprocessor technology.
One end of the device is fitted with a single male type-A USB connector.
Inside the plastic casing is a small printed circuit board.
The most common use of flash drives is to transport and store personal files such as documents, pictures and videos.
Individuals also store medical alert information on MedicTag flash drives for use in emergencies and for disaster preparation.
Flash drives are impervious to scratches and dust, and mechanically very robust making them suitable for transporting data from place to place and keeping it readily at hand.
Like all flash memory devices, flash drives can sustain only a limited number of write and erase cycles before failure.
Some flash drives feature encryption of the data stored on them, generally using full disk encryption below the file s ystem.
This prevents an unauthorised person from accessing the data stored on it 6.0 TUTOR-MARKED ASSIGNMENT 1.
Discuss briefly the essential parts of USB Flash Drive.
2.
Mention 5 ways the USB Flash Drive is of benefit to users.
7.0 REF ERENCES/FURTHER READING Engadget: BUSlink's 64GB USB 2.0 Flash Drive PRO 2 Series "Imation Swivel Pro Flash Drive", About.com, 2008, webpage: AboutCom-Swivel-P ro-Flash USB flash drives allow reading, writing, and erasing of data, with some allowing 1 million write/erase cycles in each cell of memory: if 100 uses per da y, 1 million cycles could span 10,000 days or over 27 years.
Some devices level the usage by auto-shifting activity to underused sections of memory.
"USB flash drive weighs 32 g", Trade Media Ltd, June 2006, webpage: GSources-flash-6102.
159  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY "8MB USB Memory Key - Overview".
www.ibm.com.
"8 MB USB Memory Key - User Guide - First Published December 2000".
www.ibm.com.
"Singapore firm wins patent on thumb drive", The Straits Times.
"Patent decision".
USB 1.0 Spec Position on Extension Cables and P ass-Through Monitors USB 2.0 Specification Engineering Change Notice (ECN) #1: Mini-B connector F958 KiB) "Microsoft device helps police pluck evidence from cyberscene of crime".
The Seattle Times.. CHR Available for Airplay.
FMQB.
Accessed September 23, 2007.
Keane's flash-y new single.
Crave.
"CC Sound Factory Presents S hiok Wave Album on a Memory Stick".
USB flash drive auto run setup article from Flashbay.com "Kingmax Super Stick".
testing removal media on the Gadget Show.
http://www.bress.net/blog/archives/114-How-Long-Does-a-Flash-Drive- Last.html http://www.corsairmemory.com/_faq/FAQ_flash_drive_wear_leveling.p df http://linux-usb.sourceforge.net/USB-guide/x498.html Flash Memory vs. HDD - Who Will Win?
- article on STORAGE search .com BUSlink's 64GB USB 2.0 Flash Drive PRO 2 Series Lexar Unveils New USB Card Form Factor and Introduces New USB FlashCard UNIT 3 MP3 TECHNOLOGY 160  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History 3.2 Encoding Audio 3.3 Decoding Audio 3.4 Audio Quality 3.5 Design Limitations 3.6 Volume Normalisation 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N MPEG-1 Audio Lay er 3, more commonly referred to as MP3, is a digital audio encoding format using a form of lossy data compression.
It is a common audio format for consumer audio storage, as well as a de facto standard encoding for the transfer and playback of music on digital audio players.
MP3 is an audio-specific format that was co-designed by several teams of engineers at Fraunhofer IIS in Erlangen, Germany, AT&T-Bell Labs in Murray Hill, NJ, USA, Thomson-Brandt, and CCETT.
It was approved as an ISO/IEC standard in 1991.
The use in MP3 of a loss y compression algorithm is designed to greatly reduce the amount of data required to represent the audio recording and still sound like a faithful reproduction of the original uncompressed audio for most listeners, but is not considered high fidelity audio by audiophiles.
An MP3 file that is created using the mid-range bit rate setting of 128 kbit/s will result in a file that is typically about 1/10th the size of the CD file created from the original audio source.
An MP3 file can also be constructed at higher or lower bit rates, with higher or lower resulting quality.
The compression works by reducing accuracy of certain parts of sound that are deemed beyond the auditory resolution ability of most people.
This method is commonly referred to as perceptual coding.
It internally provides a representation of sound within a short term time/frequency analysis window, by using ps ychoacoustic models to discard or reduce precision of components less audible to human hearing, and recording the remaining information in an efficient 161  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY manner.
This is relatively similar to the principles used by JPEG, an image compression format.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define MP3 • trace the trend and history in the development of MP3 • answer questions about the audio quality of MP3 • appreciate the various uses of MP3 • capture the design limitations of MP3.
3.0 MAIN CONTENT 3.1 History Development The MP3 audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking.In 1894, Mayer reported that a tone could be rendered inaudible by another tone of lower frequency.
In 1959, Richard Ehmer described a complete set of auditory curves regarding this phenomenon.
Ernst Terhardt et al.
created an algorithm describing auditory masking with high accuracy.
In 1983, at the University of Buenos Aires, Oscar Bonello started developing a PC audio card based on bit compression technology.
In 1989 he introduced the first working device based on a PC audio card using auditory masking: Audicom.
The ps ychoacoustic masking codec was first proposed in 1979, apparently independently, by Manfred Schroeder, et al.
from AT&T- Bell Labs in Murray Hill, NJ, and M. A. Krasner both in the United States.
Krasner was the first to publish and to produce hardware for speech, not usable as music bit compression, but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report did not immediately influence the mainstream of psychoacoustic codec development.
Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, and his paper had influence in acoustic and source-coding (audio data compression) research.
Both Krasner and Schroeder built upon the work performed by Eberhard F. Zwicker in the areas of tuning and masking of critical bands, which in turn built on the fundamental research in the area from Bell Labs of Harvey Fletcher and his 162  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY collaborators.
A wide variety of (mostly perceptual) audio compression algorithms were reported in IEEE's refereed Journal on Selected Areas in Communications.
That journal reported in February, 1988 on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations aimed at laboratory experiences.
This hardware was never used in PC audio cards.
The immediate predecessors of MP3 were "Optimum Coding in the Frequency Domain" (OCF), and Perceptual Transform Coding (PXFM).
These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement.
The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.
MP3 is directly descended from OCF and PXFM.
MP3 represents the outcome of the collaboration of Dr. Karlheinz Brandenburg, working as a postdoc at AT&T-Bell Labs with Mr. James D. Johnston of AT&T-Bell Labs, collaborating with the Fraunhofer Society for Integrated Circuits, Erlangen, with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders.
MPEG-1 Audio Layer 2 encoding began as the Digital Audio Broadcast (DAB) project managed by Egon Meier-Engelen of the Deutsche Forschungs- und Versuchsanstalt für Luft- und Raumfahrt (later on called Deutsches Zentrum für Luft- und Raumfahrt, German Aerospace Center) in Germany.
The European Community financed this project, commonly known as EU-147, from 1987 to 1994 as a part of the EUREKA research program.
As a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music.
He completed his doctoral work in 1989 and became an assistant professor at Erlangen-Nuremberg.
While there, he continued to work on music compression with scientists at the Fraunhofer Society (in 1993 he joined the staff of the Fraunhofer Institute).
In 1991 there were two proposals available: Musicam and ASPEC - (Short excerpt on German Wikipedia) (Adaptive Spectral Perceptual Entropy Coding).
The Musicam technique, as proposed by Philips (The Netherlands), CCETT (France) and Institut für Rundfunktechnik 163  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY (Germany) was chosen due to its simplicit y and error robustness, as well as its low computational power associated with the encoding of high quality compres sed audio.
The Musicam format, based on sub-band coding, was the basis of the MP EG Audio compression format (sampling rates, structure of frames, headers, number of samples per frame).
Much of its technology and ideas were incorporated into the definition of ISO MPEG Audio Layer I and Layer II and the filter bank alone into Layer III (MP3) format as part of the computationally inefficient hybrid filter bank.
Under the chairmanship of P rofessor Musmann (University of Hannover) the editing of the standard was made under the responsibilities of Leon van de Kerkhof (Layer I) and Gerhard Stoll (Layer II).
A working group consisting of Leon van de Kerkhof (The Netherlands), Gerhard Stoll (Germany), Leonardo Chiariglione (Italy), Yves-François Dehery (France), Karlheinz Brandenburg (Germany) and James D. Johnston (USA) took ideas from ASPEC, integrated the filter bank from Layer 2, added some of their own ideas and created MP3, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s.
All algorithms were approved in 1991 and finalised in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3, published in 1993.
Further work on MPEG audio was finalised in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3, originally published in 1995.
Compression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampl ing rate of the input signal.
Nevertheless, compression ratios are often published.
They may use the CD parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit).
Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term compression ratio for lossy encoders.
Karlheinz Brandenburg used a CD recording of Suzanne Vega s song "Tom's Diner" to assess and refine the MP3 compression algorithm.
This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks.
Some jokingly refer to Suzanne Vega as "The mother of MP3".
Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.)
were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats.
It is important to understand that Suzanne Vega is recorded in an interesting fashion that results in substantial difficulties that arise due 164  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY to Binaural Masking Level Depression (BMLD) as discussed in Brian C. J. Moore's book on the Psychology of Human Hearing, for instance.
Going Public A reference simulation software implementation, written in the C language and known as ISO 11172-5, was developed by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, and Layer 3).
Working in non-real time on a number of operating systems, it was able to demonstrate the first real time hardware decoding (DSP based) of compressed audio.
Some other real time implementation of MPEG Audio encoders were available for the purpose of digital broadcasting (radio DAB, television DVB) towards consumer receivers and set top boxes.
Later, on July 7, 1994 the Fraunhofer Society released the first software MP3 encoder called l3enc.
The filename extension .mp3 was chosen by the Fraunhofer team on July 14, 1995 (previously, the files had been named .bit).
With the first real-time software MP3 player Winplay3 (released September 9, 1995) many people were able to encode and play back MP3 files on their PCs.
Because of the relatively small hard drives back in that time (~ 500 MB) lossy compression was essential to store non-instrument based (see tracker and MIDI) music for playback on computer.
Internet From the first half of 1995 through the late 1990s, MP3 files began to spread on the Internet.
MP3's popularity began to rise rapidly with the advent of Nullsoft's audio player Winamp (released in 1997), and the Unix audio player mpg123.
The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from compact discs, which would previously have been nearly impossible.
The first large peer-to-peer file sharing network, Napster, was launched in 1999.
The ease of creating and sharing MP3s resulted in widespread copyright infringement.
Major record companies argue that this free sharing of music reduces sales, and call it "music piracy".
They reacted by pursuing lawsuits against Napster (which was eventually shut down) and eventually against individual users who engaged in file sharing.
Despite the popularity of MP3, online music retailers often use other proprietary formats that are encrypted or obfuscated in ways that make it difficult to use purchased music files in ways not specifically authorised by the record companies.
Attempting to control the use of files in this way is known as Digital Rights Management.
The record companies argue that this is necessary to prevent the files from being made 165  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY available on peer-to-peer file sharing networks.
However, this has other side effects such as preventing users from playing back their purchased music on d ifferent types of devices.
However, the audio content of these files can usually be converted into an unencrypted format.
For instance, users are often allowed to burn files to audio CD, which requires conversion to an unencrypted audio format.
Even when that option is not available, there are software and hardware solutions which allow the user to record anything they can play.
Unauthorised MP3 file sharing continues on next-generation peer-to- peer networks, and some authorised services, such as eMusic, Rhapsody and Amazon.com have begun selling unrestricted music in the MP3 format.
3.2 Encoding Audio The MPEG-1 standard does not include a precise specification for an MP3 encoder.
Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information in the raw audio (or rather its MDCT representation in the frequency domain).
During encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples.
If there is a transient, 192 samples are taken instead of 576.
This is done to limit the temporal spread of quantisation noise accompanying the transient.
As a result, there are many different MP3 encoders available, each producing files of differing quality.
Comparisons are widely available, so it is easy for a prospective user of an encoder to research the best choice.
It must be kept in mind that an encoder that is proficient at encoding at higher bit rates (such as LAME) is not necessarily as good at lower bit rates.
3.3 Decoding Audio Decoding, on the other hand, is carefully defined in the standard.
Most decoders are "bitstream compliant", which m eans that the decompressed output - that they produce from a given MP3 file - will be the same (within a specified degree of rounding tolerance) as the output specified mathematically in the ISO/IEC standard document (ISO/IEC 11172-3).
The MP3 file has a standard format, which is a frame that consists of 384, 576, or 1152 samples (depends on MPEG version and layer), and all the frames have associated header information (32 bits) and side information (9, 17, or 32 bytes, depending on MPEG version and stereo/mono).
The header and side information help the decoder to decode the associated Huffman encoded data correctly.
166  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process).
3.4 Audio Quality When performing lossy audio encoding, such as creating an MP3 file, there is a trade-off between the amount of space used and the sound quality of the result.
Typically, the creator is allowed to set a bit rate, which specifies how many kilobits the file may use per second of audio, as in when ripping a compact disc to MP3 format.
Using a lower bit rate provides a relatively lower audio quality and produces a smaller file size.
Likewise, using a higher bit rate outputs a higher quality audio, and therefore results in a larger file.
Files encoded with a lower bit rate will generally play back at a lower quality.
With too low a bit rate, "compression artifacts" (i.e., sounds that were not present in the original recording) may be audible in the reproduction.
Some audio is hard to compress because of its randomness and sharp attacks.
When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard.
A sample of applause compressed with a relatively low bit rate provides a good example of compression artifacts.
Besides the bit rate of an encoded piece of audio, the quality of MP3 files also depends on the quality of the encoder itself, and the difficulty of the signal being encoded.
As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders may feature quite different quality, even when targeting similar bit rates.
As an example, in a public listening test featuring two different MP3 encoders at about 128 kbit/s, one scored 3.66 on a 1 5 scale, while the other scored only 2.22.
Quality is heavily dependent on the choice of encoder and encoding parameters.
While quality around 128 kbit/s was somewhere between annoying and acceptable with older encoders, modern MP3 encoders can provide adequate quality at those bit rates (January 2006).
However, in 1998, MP3 at 128 kbit/s was only providing quality equivalent to AAC-LC at 96 kbit/s and MP2 at 192 kbit/s.
The transparency threshold of MP3 can be estimated to be at about 128 kbit/s with good encoders on typical music as evidenced by its strong performance in the above test, however some particularly difficult material, or music encoded for the use of people with more sensitive hearing can require 192 kbit/s or higher.
As with all lossy formats, some samples cannot be encoded to be transparent for all users.
167  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY The simplest type of MP3 file uses one bit rate for the entire file this is known as Constant Bit Rate (CBR) encoding.
Using a constant bit rate makes encoding simpler and faster.
However, it is al so possible to create files where the bit rate changes throughout the file.
These are known as Variable Bit Rate (VBR) files.
The idea behind this is that, in any piece of audio, some parts will be much easier to compress, such as silence or music containing only a few instruments, while others will be more difficult to compress.
So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts.
With some encoders, it is possible to specify a given quality, and the encoder will vary the bit rate accordingly.
Users who know a particular "quality setting" that is transparent to their ears can use this value when encoding all of their music, and not need to worry about performing personal listening tests on each piece of music to determine the correct settings.
In a listening test, MP3 encoders at low bit rates performed significantly worse than those using more modern compression methods (such as AAC).
In a 2004 public listening test at 32 kbit/s, the LAME MP3 encoder scored only 1.79/5 - behind all modern encoders - with Nero Digital HE AAC scoring 3.30/5.
Perceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones).
3.5 Design Limitations There are several limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder.
Newer audio compression formats such as Vorbis, WMA Pro and AAC no longer have these limitations.
In technical terms, MP3 is limited in the following ways: i.
Time resolution can be too low for highly transient signals, may cause some smearing of percussive sounds.
ii.
Due to the tree structure of the filter bank, pre-echo issues are made worse, as the combined impulse response of the two filter banks does not, and can not, provide an optimum solution in time/frequency resolution.
iii.
The combination of the two filter banks creates aliasing issues that must be handled partially by the "aliasing compensation" stage, but that create excess energy to be coded in the frequency domain, thereby decreasing coding efficiency.
iv.
Frequenc y resolution is limited by the small long block window size, decreasing coding efficiency.
168  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY v. No scale factor band for frequencies above 15.5/15.8 kHz.
vi.
Joint stereo is done only on a frame-to-frame basis.
vii.
Internal handling of the bit reservoir increases encoding delay.
viii.
Encoder/decoder overall delay is not defined, which means lack of official provision for gapless playback.
However, some encoders such as LAME can attach additional metadata that will allow players that are aware of it to deliver seamless playback.
3.6 Volume Normalisation Since volume levels of different audio sources can vary greatly, it is sometimes desirable to adjust the playback volume of audio files such that a consistent average volume is perceived.
The idea is to control the average volume across multiple files, not the volume peaks in a single file.
This gain normalisation, while similar in purpose, is distinct from dynamic range compression (DRC), which is a form of normalisation used in audio mastering.
Gain normalisation may defeat the intent of recording artists and audio engineers who deliberately set the volume levels of the audio they recorded.
A few standards for storing the average volume of an MP3 file in its metadata tags, enabling a specially designed player to automatically adjust the overall playback volume for each file, have been proposed.
A popular and widely implemented of such proposal is "Replay Gain", which is not MP3-specific.
When used in MP3s, it is stored differently by different encoders, and as of 2008, Replay Gain-aware players do not yet support all formats.
4.0 CONCLUSION MP3s have added variety to disks technology especially in the music world.
Its beauty is that it can store a large volume of audio and video.
The attending challenges as an emerging technology are being looked into.
Definitely we will see better forms in terms of durability.
5.0 SUMMARY • MPEG-1 Audio Layer 3, more commonly referred to as MP3 , is a digital audio encoding format using a form of lossy data compression.
• The MP3 audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking.
In 1894, Mayer reported that a tone could be rendered inaudible by another tone of lower frequency.
• The MPEG-1 standard does not include a precise specification for an MP3 encoder.
Implementers of the standard were supposed to 169  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY devise their own algorithms suitable for removing parts of the information in the raw audio.
• Decoding, on the other hand, is carefully defined in the standard.
Most decoders are "bitstream compliant" , which means that the decompressed output - that they produce from a given MP3 file - will be the same.
• When performing loss y audio encoding, such as creating an MP3 file, there is a trade-off between the amount of space used and the sound quality of the result.
• There are several limitations inherent in the MP3 format that cannot be overcome by any MP3 encoder.
Newer audio compression formats such as Vorbis, WMA Pro and AAC no longer have these limitations.
• Since volume levels of different audio sources can vary greatly, it is sometimes desirable to adjust the playback volume of audio files such that a consistent average volume is perceived.
6.0 TUTOR-MARK ED ASSIGNMENT Technically speaking, discuss briefly the limitations of MP3.
7.0 REF ERENCES/FURTHER READING Jayant, Nikil; Johnston, James; Safranek, Robert (October 1993).
"Signal Compression Based on Models of Human Perception".
Proceedings of the IEEE 81 (10): 1385 1422. doi:10.1109/5.241504.
Retrieved on 2008-06-30.
Mayer, Alfred Marshall (1894).
"Researches in Acoustics".
London, Edinburgh and Dublin Philosophical Magazine 37: 259 288.
Ehmer, Richard H. (1959).
"Masking by Tones Vs Noise Bands".
The Journal of the Acoustical Society of America 31: 1253. doi:10.1121/1.1907853.
Retrieved on 2008-06-30.
Terhardt, E.; Stoll, G.; Seewann, M. (March 1982).
"Algorithm for Extraction of Pitch and Pitch Salience from Complex Tonal Signals".
The Journal of the Acoustical Society of America 71 : 679. doi:10.1121/1.387544.
Retrieved on 2008-06-30.
La historia del Audicom Schroeder, M.R.
; Atal, B.S.
; Hall, J.L.
(December 1979).
"Optimizing Digital Speech Coders by Exploiting Masking Properties of the Human Ear".
The Journal of the Acoustical Society of America 170  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 66 : 1647. doi:10.1121/1.383662.
Retrieved on 2008-06-30.
Received 8 June 1979; accepted for publication 13 August 1979.
Krasner, M. A.
"Digital Encoding of Speech and Audio Signals Based on the Perceptual Requirements of the Auditory S ystem"; Massachusetts Institute of Technology Lincoln Laboratory Technical Report 535; 18 June 1979.
Zwicker, E. F. "On the Psycho-acoustical Equivalent of Tuning Curves"; Proceedings of the Symposium on Psychophysical Models and Physiological Facts in Hearing; held at Tuzing, Oberbayern, April 22 26, 1974.
The Ear as a Communication Receiver.
English translation of Das Ohr als Nachrichtenempfänger by Eberhard Zwicker and Richard Feldtkeller.
Translated from German by Hannes Müsch, Søren Buus, and Mary Florentine.
Originally published in 1967; Translation published in 1999.
"The ASA Edition of Speech and Hearing in Communication" , edited by J.B. Allen, Acoustical Society of America, reprinted in 1995.
IEEE Journal.
Selected Areas in Communications, vol.
6, no.
2, Feb. 1988.
Johnston, James D. (1988).
"Transform Coding of Audio Signals Using Perceptual Noise Criteria".
Selected Areas in Communications, IEEE Journal on 6 (2): 314 323. doi:10.1109/49.608.
Retrieved on 2008-06-30.
Jack Ewing (March 5, 2007).
"How MP3 Was Born".
BusinessWeek.com.
Retrieved on 2007-07-24.
Brandenburg, Karlheinz; Bosi, Marina (February 1997).
"Overview of MPEG Audio: Current and Future Standards for Low-Bit-Rate Audio Coding".
J.
Audio Eng.
Soc 45 (1/2): 4 21.
Retrieved on 2008-06-30.
Mares, Sebastian (2006 01), Results of Public, Multiformat Listening Test @ 128 kbit/s, <http://www.listening-tests.info/mf-128- 1/results.htm.
Retrieved on 17 March 2007 David Meares, Kaoru Watanabe & Eric Scheirer (1998 02).
"Report on the MPEG-2 AAC Stereo Verification Tests" (PDF).
International Organisation for Standardisation.
Retrieved on 2007-03-17.
171  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Amorim, Roberto (2004-07-11), Results of Dial-up bit rate public Listening Test, <http://www.rjamorim.com/test/32kbps/results.html>.
Retrieved on 17 March 2007.
Bouvigne, Gabriel (2006-11-28).
freeformat at 640 kbit/s and foobar2000, possibilities?, tunequest (2007-02-26).
"Big List of MP3 Patents (and supposed expiration dates)".
"Acoustic Data Compression -- MP3 Base Patent".
Foundation for a Free Information Infrastructure (January 15, 2005).
Retrieved on 2007-07-24.
Muzinée Kistenfeger (May, 2006).
"The Fraunhofer Society (Fraunhofer-Gesellschaft, FhG)".
British Consulate-General Munich.
Retrieved on 2007-07-24.
"Early MP3 Patent Enforcement".
Chilling Effects Clearinghouse (September 1, 1998).
172  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 4 INTERNET RADIO AND TELEVISION CONTENTS 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 Internet Radio Technology 3.2 History 3.3 Internet Television 3.3.1 Implementation 3.3.2 Business Considerations for Internet TV 3.3.3 Terminology 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignments 7.0 References/Further Reading 1.0 INTRODUCTIO N Internet ra dio (also known as web radio, net radio, streaming radio and e-radio) is an audio broadcasting service transmitted via the Internet.
Broadcasting on the Internet is usually referred to as webcasting since it is not transmitted broadly through wireless means.
Internet radio involves a streaming medium that presents listeners with a continuous "stream" of audio over which they have no control, much like traditional broadcast media; in this respect, it is distinct from "on- demand" file serving.
Internet radio is also distinct from podcasting, which involves downloading rather than streaming.
Many Internet radio "stations" are associated with a corresponding traditional (or "terrestrial") radio station or radio network.
Internet-only radio stations are independent of such associations.
Internet radio services are usually accessible from anywhere in the world-for example, one could listen to an Australian station from Europe or America.
Some major networks like Clear Channel in the US and Chrysalis in the UK restrict listening to in country because of music licenses.
Internet radio serves listeners with interests that are often not adequately served by local radio stations (such as progressive rock, ambient music, folk music, classical music, and stand-up comedy).
Internet radio services offer news, sports, talk, and various genres of music-everything that is available on traditional radio stations.
173  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 2.0 OBJECTIVES At the end of this unit, you should be able to: • define what Internet radio and television are • describe the technology behind Internet radio and television • explain the development of Internet radio and television • understand the implementation of Internet television • explain the methods and technology behind Internet television.
3.0 MAIN CONTENT 3.1 Internet Radio Technology Strea ming The most common way to distribute Internet radio is via streaming technology using a loss y audio codec.
Popular streaming audio formats include MP3, Ogg Vorbis, Windows Media Audio, RealAudio and HE- AAC (som etimes called aacPlus).
The bits are "streamed" (transported) over the network in TCP or UDP packets, then reassembled and played within seconds.
(The delay is referred to as lag time.)
3.2 History Internet radio was pioneered by Carl Malamud.
In 1993, Malamud launched "Internet Talk Radio" which was the "first computer-radio talk show, each week interviewing a computer expert."
However, as late as 1995, this service was not available via multicast streaming; it was distributed "as audio files that computer users fetch one by one."
A November, 1994 Rolling Stones concert was the "first cyberspace multicast concert."
Mick Jagger opened the concert by saying, "I wanna say a special welcome to everyone that's, uh, climbed into the Internet tonight and, uh, has got into the M-bone.
And I hope it doesn't all collapse."
On November 7, 1994, WXYC (89.3 FM Chapel Hill, NC USA) became the first traditional radio station to announce broadcasting on the Internet.
WXYC used an FM radio connected to a system at SunSite, later known as Ibiblio, running Cornell's CU-SeeMe software.
WX YC had begun test broadcasts and bandwidth testing as early as August, 1994.
WREK (91.1 FM, Atlanta, GA USA) started streaming on the same day using their own custom software called CyberRadio1.
174  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY However, unlike WXYC, this was WREK's beta launch and the stream was not advertised until a later date.
Some of the first Internet-only commercial radio stations emerged in 1995.
NetRadio "was one of the Internet's original Webcasters," eventually "streaming more than 100 channels including both music and spoken material."
Nonetheless, NetRadio Corporation ceased operations in 2001.
In 2002, the Copyright Arbitration Royalty Panel (CARP) system was initiated by the United States Congress in order to oversee decisions regarding royalty rates and terms, particularly in regard to digital distribution of audio.
Many webcasters believed the 2002 proposed royalty structure to be overly burdensome and intended to disadvantage independent Internet-only stations.
CARP was later phased out in favour of the Distribution Reform Act of 2004.
On May 1, 2007, the United States Copyright Royalty Board approved a rate increase in the royalties payable to performers of recorded works broadcast on the internet.
This was the result of a two year proceeding, with dozens of witnesses and hundreds of documents from over twenty different parties, including large and small webcasters, NPR, college stations, and SoundExchange.
The CRB was privy to private financial records and business models of the webcasters, and after reviewing the evidence and testimony, issued their decision on May 1, 2007 (which is currently under appeal).
If enforced, this decision will undermine the business models of many Internet radio stations, which had previously relied on the rate of $0.000768 per song that had been unchanged from 1998-2005.
These rules were scheduled to go into effect on May 1, 2007, with the first due date being July 15, 2007, and apply retroactively to January 1, 2006.
Due to these rate increases, it has been suggested that some U.S.-based Internet broadcasts should be moved to foreign jurisdictions where US royalties do not apply.
For example, Mercora, a service that allows individuals to launch their own webcasts, has established a Canadian site that they believe falls outside U.S. regulatory and royalty rules.
On 26 April 2007, the Internet Radio Equality Act (HR 2060) was proposed to reverse the CRB's decision.
This bill was introduced in the U.S. House of Representatives by Congressmen Jay Inslee (D-WA) and Donald Manzullo (R-IL).
Its Senate counterpart was introduced on 10 May, 2007 by Senators Ron Wyden (D-Ore.) and Sam Brownback (R- Kansas).
As of June 25,2007, the legislation has over 100 Congressional co-sponsors.
175  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Day o f Silence US Internet broadcasters organised a nationwide coalition to oppose the rate hike and in support of the Internet Radio Equality Act.
On June 26,2007, many of them participated in a "Day of Silence" - either shutting off their audio streams entirely, or replacing their streams with static, ocean sounds or other ambience, interspersed with brief public service announcements - to focus attention on the consequences of the impending rate hike.
Rhapsody, Live365, MTV, Pandora, and SHOUTcast were among the participants in the Day of Silence.
Last.FM and Slacker did not participate, saying that they did not want to punish their listeners for the station's problems.
SoundExchange, representing supporters of the increase in royalty rates, pointed out the fact that the rates were flat from 1998 through 2005, without even being increased to reflect cost-of- living increases.
They also point to the fact that CBS recently purchased Last.FM for 280 million dollars, and if internet radio is to build businesses off of the product of recordings, the performers and owners of those recordings should receive fair compensation.
Opponents argued that the purchase price paid for Last.FM reflected that it was primarily a [Social network] that included a radio service.
Recent SoundExchange Developments SoundExchange recently came to an agreement with certain large webcasters regarding the minimum fees that were modified by the recent determination of the Copyright Royalty Board on May 1, 2007.
While the CRB decision imposed a $500 per station or channel minimum fee for all webcasters, certain webcasters represented through DiMA negotiated a $50,000 "cap" on those fees with SoundExchange.
However, DiMA and SoundExchange continue to negotiate over the per song, per listener fees.
SoundExchange also recently offered alternative rates and terms to certain eligible small webcasters, that allows them to calculate their royalties as a percentage of their revenue or expenses, instead of at a per performance rate.
To be eligible, a webcaster had to have revenues of less than $1.25 million dollars a year and stream less than 5 million "listener hours" a month (or an average of 6830 concurrent listeners).
These restrictions would disqualify independent webcasters like AccuRadio, DI.FM, Club977 and others from participating in the offer, and therefore many small comm ercial webcasters continue to negotiate a settlement with SoundExchange.
176  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY An April, 2008 survey showed that, in the US, more than one in seven persons aged 25-54 years old listen to online radio each week.
In 2008, 13 percent of the American population listened to the radio online, compared with 11 percent in 2007.
An August 16, 2008 Washington Post article reported that although Pandora was "one of the nation's most popular Web radio services, with about 1 million listeners daily...the burgeoning company may be on the verge of collapse" due to the structuring of performance royalty payment for webcasters.
"Traditional radio, by contrast, pays no such fee.
Satellite radio pays a fee but at a less onerous rate, at least by some measures."
The article indicated that "other Web radio outfits" may be "doom[ed]" for the same reasons.
3.3 Internet Television Internet television (Internet TV or iTV) is television distributed through the Internet.
Internet television allows viewers to choose the show they want to watch from a library of shows.
The primary models for Internet television are streaming Internet TV or selectable video on an Internet location, typically a website.
The video can also be broadcast with a peer-to-peer network (P2PTV), which doesn't rely on single website's streaming.
It differs from IPTV in that IPTV offerings are typically offered on discrete service provider networks, requiring a special IPTV set-top-box.
Internet TV is a quick-to-market and relatively low investment service.
Internet TV rides on existing infrastructure including broadband, ADSL, Wi-Fi, cable and satellite which makes it a valuable tool for a wide variety of service providers and content owners looking for new revenue streams.
3.3.1 Implementation Many programmers are streaming their content live on the internet today to increase viewership (which in turn increases ad revenue) and protect market share.
This model is efficient due to the relatively inexpensive multicasting protocol.
Viewers may simply request access to the live feed and join into the live stream.
This free model has been used in over- the-air broadcasting for years and still works because of the low cost of reaching viewers via multicast.
Any viewer with a broadband connection and the correct free media player can watch live television from around the world.
Many internet television "portals" are available which include links to live feeds as well as built-in viewers.
Although the live television 177  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY streams are free, most portals are supported by advertising revenue as well.
Those that create valued and interesting video products now have the opportunity to distribute them directly to a large audience - something impossible with the previous television distributing models (closed software, closed hardware, closed network).
The free model has been used around the globe by local and independent television channels aiming for niche target audiences, or to build a collaborative environment for media production, a platform for citizens' media.
It isn't strictly a citizen's format either as the broadcast model used in television for decades will begin to find competition in Internet television supported by advertising.
3.3.2 Business Consider ations for Internet TV The recent rapid growth of fast broadband access, accelerated computer power and larger storage capacity has turned Internet TV into a real opportunity for service providers who want to open new revenue streams and increase ARPU.
A major advantage of Internet TV is that it allows content delivery to a huge population with virtually no geographical limitations.
But while Internet TV is a much easier and cheaper way of publishing content, operators who are pondering whether to launch an Internet TV service nevertheless have to carefully assess the factors affecting their business cases.
High-quality Internet TV services require subscribers to have continuous access to high bandwidth, so pricing, bandwidth, and network neutrality (at least in the US) are all interdependent factors affecting the business case for Internet TV.
For example, while subscribers are generally required to pay more for higher internet bandwidth, it doesn't automatically guarantee good enough bandwidth quality for receiving Internet TV services.
So to receive Internet TV, a subscriber will be required to subscribe to an even higher premium service which may present a barrier to scaling up subscribers quickly.
3.3.3 Ter minology There are many ways to deliver video over an IP network and many buzzwords have been applied to these various ways and are sometimes used interchangeably.
IPTV is commonly referred to those services operated and controlled by the same company that operates and controls the "Last Mile" to the 178  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY consumers' premises.
An IPTV service is usually delivered over a complex and investment heavy walled garden network, which is carefully engineered to ensure bandwidth efficient delivery of vast amounts of multicast video traffic.
The higher network quality also enables easy delivery of high quality SD or HD TV content to subscribers homes.
Internet TV, by definition, is created, managed and distributed via the open Internet.
It rides on existing infrastructure and normally refers to those services sourced over the Internet by service providers that cannot control the final delivery.
Again, transport streams in IP packets are used with one or more services per transport stream.
Other TV-like services are available on the Internet but these send the video and the audio in separate streams over the IP network and do not use transport streams.
Whilst the differences may seem irrelevant to the consumer, the underlying technology employed is quite different and directly affect the range and quality of service that can be achieved.
IPTV users are limited to a relatively small range of programmes but at high quality, whereas an Internet TV user may have access to many thousands of channels from literally all over the world but without any guarantee of being able to watch them.
Streaming services such as YouTube generally offer User Generated Content UGC as individual short clips rather than professionally produced programmes or films grouped as a channel.
Other Names for Internet Television Television on the desktop (TOD) TV over IP - Television over Internet Protocol Vlog For video web logging.
Vodcast For video on demand.
Methods used for Internet Television Broadcatching for a P2PTV paradigm in use today.This model can save the cost of Internet TV service provider.
Streaming from a single website.
Technolo gies used for Internet televisio n BitTorrent Dirac HTTP 179  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Nullsoft Streaming Video, a technology used by AOL to deliver Internet based video content.
RSS RSS enclosure RTSP SMIL Theora WTVML 4.0 CO NCLUSIO N Television and radio broadcasting through the Internet has added a new dimension to the world of broadcasting.
Variety has indeed come to the broadcasting industry.
This technology is novel and its full potential is yet to be realised.
Though not totally competitor to traditional broadcasting, that is, the television aspect, it just brings along its uniqueness like listening to radio station of choice while browsing on the Internet.
5.0 SUMMARY • Internet radio (also known as web radio, net radio, streaming radio and e-radio) is an audio broadcasting service transmitted via the Internet.
• SoundExchange recently came to an agreement with certain large webcasters regarding the minimum fees that were modified by the recent determination of the Copyright.
• The most common way to distribute Internet radio is via streaming technology using a lossy audio codec.
• Internet radio was pioneered by Carl Malamud.
In 1993, Malamud launched "Internet Talk Radio" which was the "first computer-radio talk show, each week interviewing a computer expert."
• On 26 April, 2007, the Internet Radio Equality Act (HR 2060) was proposed to reverse the CRB's decision.
• Interne t television (Internet TV or iTV) is television distributed through the Internet.
Internet television allows viewers to choose the show they want to watch from a library of shows.
• Many programmers are streaming their content live on the internet today to increase viewership (which in turn increases ad revenue) and protect market share.
• The recent rapid growth of fast broadband access, accelerated computer power and larger storage capacity has turned Internet TV into a real opportunity for service providers who want to open new revenue streams and increase ARPU.
180  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • There are many ways to deliver video over an IP network and many buzzwords have been applied to these various ways and are sometimes used interchangeably.
6.0 TUTOR-MARKED ASSIGNMENT 1.
Mention 10 technologies used for Internet television.
2.
Briefly discuss streaming in Internet Radio technology.
7.0 REF ERENCES/FURTHER READING "Cable company is set to plug into Internet".
The Wall Street Journal (August 24, 1993).
"Peering Out a 'Real Time' Window".
New York Times (February 8, 1995).
WXYC Simulcast wrek-net first | wrek atlanta, 91.1 fm "Netradio Goes Offline".
New York Times (October 19, 2001).
Denver - News - Digital Dilemma U.S.
Copyright Office - Licensing and CARP Information Stagnant royalty rates may bring end to Internet radio, The Daily Collegian, April 26, 2007 Web radio may stream north to Canada, The Toronto Star, 9 April 2007 Legality under Canadian Copyright Law.
"Broache", CNet News (2007-04-26.)
Day of Silence: Last.FM Tells Broadcasters to Grow Up | Epicenter from Wired.com CBS Acquires Europe s Last.FM for $280 million "Webcasters and SoundExchange Shake Hands".
BusinessWeek.com (2007-08-23).
"SoundExchange Offers Discounted Music Rates To Small Webcasters".
DigitalMediaWire.com (2007-08-22).
181  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY SoundExchange extends (not very good) offer to small webcasters "Weekly online radio audience increases from 11 percent to 13 percent of Americans in last year, according to the latest Arbitron/Edison media research study," The Earth Times, April 9, 2008.
Giant of Internet Nears Its 'Last Stand' Ahrens Frank, ABC encouraged by Internet TV Trial.
June 2006.
Washington.
pgd2 McLuhan,Marshall, Understanding Media; The Extensions of Man.
New York;Mcgraw Hill, 1964 182  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 5 BLEND ED LEARNING CONTENTS 1.0 Introduction 2.0 Objectives 3.0 3.1 The Benefits of Blended Learning 3.2 Approach to Blended Learning 3.3 How Do You Build A Blend?
3.4 What It Takes To Blend?
3.5 Case Studies 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N There are several definitions and understanding of what a blend is.
A blend is an integrated strategy for delivering on promises about learning and performance.
Blending involves a planned combination of approaches, such as coaching by a supervisor; participation in an online class; breakfast with colleagues; competency descriptions; reading on the beach; reference to a manual; collegial relationships; and participation in seminars, workshops, and online communities.
Also, blended learning is a custom approach that applies a mix of training delivery options to teach, support, and sustain the skills needed for top job performance.
With blended learning, the tried-and-true traditional learning methods are combined with new technology to create a synergistic, dynamic learning structure that can propel learning to new heights.
A study by Peter Dean and his colleagues found that providing several linked options for learners, in addition to classroom training, increased what they learned.
In 2002, Harvard Business School Faculty DeLacey and Leonard reported that students not only learned more when online sessions were added to traditional courses, but student interaction and satisfaction improved as well.
Thomson and NETg released a 2003 white paper that reported speedier performance on real world tasks by people who learned through a blended strategy-faster than those studying through e-learning alone.
Blended learning-a combination of face-to-face and online media, with "seat time" significantly reduced-is an increasing proportion of instruction in US higher education.
Supplementing wholly face-to-face 183  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY courses and wholly online asynchronous courses with technology is nearly ubiquitous.
Blending grows as people recognise the value of asynchronous learning.
An August, 2001 report from Eduventures notes that "roughly 1.3 million postsecondary students [are] taking online courses" (Evans 2001).
The National Center for Education Statistics (2002) reports that in the 1999-2000 academic year, eight percent of undergraduate and ten percent of graduate students participated in distance education.
A National Governors Association report on "The State of E-Learning in the States" noted that "58% of all two- and four-year colleges offered distance learning courses in 1998; 84 percent of all colleges expect to do so by 2002" (NGA 2002).
Primary Research surveyed seventy five distance learning programmes; the mean annual enrollment growth rate for 2002 was 41% (Primary Research 2002).
The Campus Computing Project (2001) reported the number of institutions that have selected a single platform for course management increased in 2001 to 73 percent (from just over 50 percent the year before).
Already in 1998, according to the NCES (2002b), forty percent of full-time faculty made use of "course specific web sites."
The literature about blended courses is full of examples from all disciplines, at all levels across the spectrum of education, and with wide variation in technologies used and in face-to-face meeting time.
Blended learning courses can replace synchronous classroom seat time with asynchronous online learning activities so that instruction occurs both in the classroom and online.
Given the fluidity of the technologies and the near infinite number of ways that technology is applied and courses are organised in higher education, the presence of both conditions distinguish blended from wholly online and wholly classroom programmes and courses.
2.0 OBJECTIVES At the end of this unit, you should be able to: • define blending and what it stands for in education • identify the benefits of adopting blended method of learning • differentiate e-learning solutions from workshop solution • explain the approach in blended learning • know how to build a blend.
184  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.0 MAIN CONTENT 3.1 The B enefits of Blende d Learning A blended learning approach is flexible, using the most effective training delivery option (or combination of options) for each stage of learning.
It is more effective than any single form of learning at creating the results you want: sustained behavioural change that increases the return on your training investment.
For example: The business goal for Bank ABC is to improve its share of the small business market.
The strategy is to distinguish itself in the market by providing superior service through its small business lenders.
A gap has been identified in their employees ability to conduct productive sales conversations about credit some have the skills and some do not.
There are ambitious business targets to meet in a six- month period, a dispersed and diverse population to train, and limited managerial expertise to apply.
Let s compare two common approaches: an e-learning solution and a workshop solution in terms of cost and time.
Then, we ll look at a blended solution and see how it compares.
E-Learnin g Solution Work shop Solution § Online testing to determine § Prereading on cases 2 hours individual skill levels 4 hours § Two-day workshop 16 hours § Online tutorials delivered accordin g to skil l gaps found in testi ng, followed by online si mulations t hat must be passed to confirm knowledge 3-4 hours Total: 7-8 hours of training Total: 18 hours of training Advantages: Advantages: Assessment helps target the train in g Participants have time to review the to the skill gaps.
Trai ni ng time ancdas e s and become familiar with them costs are both mi nimised by using b efore class.
The class offers an online t utorials and simul ations.
opportunity t o t est new skills, and to learn from group discussion, Disadvantages: exerci ses, and leader experience and feedback.
The performance feedback provi d e d in an e-learning environment is lDesiss a dvantages: speci fic t o individual performance than in classroom/workshop setting.
Time and money, and to some extent, control over the amount learned.
Time is spent away from the job and money is spent on facilitati on and travel.
Content is dependent on the leader and will have some variances.
185  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Now, let s see what a blended learning solution would look like.
Training method Time Res ults/Benefits 1.
Online testing to determin e 4 hours Training is determined based on individual skill levels.
need.
Learners participat e only i n the programmes they need and are ready for, maximising ROI.
2.
Online tutorials delivered 3-4 hours Levels t he playing fi eld so all according to skill gaps found in trainees have a common testing are followed by online knowl edge and skill base.
simulations that must be passed prior to coming to workshop or coaching session with manager.
3a.
One-day workshop focused on 8 hours Classroom enables people to learn customer scenarios wit h through their peers experiences.
videotaped feedback, But training alone will not elevate individualised coachi ng on - OR your organisation to higher levels performance.
1 hour of excellence.
Ongoing skills - OR rei nforcement wi ll take you there.
3b.
Manager coaching session on test results.
4.
Access on the job to refresh e2r0 - 30 Tangible confidence-builders sessions and aids such as minutes boost success.
reference guides or flash cardws e ekl y prior to customer calls.
5.
Follow-up coaching session i n3 0 Observation and results coaching four weeks time eit her basmedin u tes further builds on skills and on joint call or simulated monthly proficiency.
customer experience.
6.
Remedial tutorials and testing o n C ontinue to fine tune skills.
any skill gaps observed.
7.
Three month mini-retesti ng o f Reinforcing the relentless pursuit skills.
of perfecti on continuous improvement.
The obvious advantage of the blended learning solution is that learning becomes a process, rather than an event.
Blended learning puts training into the job environment, provides a forum for every learning style, includes reinforcement and coaching, and uses minimum effort and resources to gain maximum results.
It enables people to apply skills continuousl y-skills that become habitual with practice.
3.2 Approach to Blended Learning Options for blended learning go beyond the classroom.
They are formal and informal, technology- and people-based, independent and convivial, and directive- and discovery-oriented.
If you want to help employees with retirement planning, for example, a blend makes sense because the need extends over time.
People can seek the information that they need, 186  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY when they need it.
Assist employees through workshops, coaching sessions, support groups, and online classes and performance support tools, such as the Social Security Administration s Benefit Calculator or the Living to 100 Life Expectancy Calculator.
The table below presents the possibilities of what can constitute a blended learning approach: Live fa ce-to -face (forma l) Live fa ce-to -face (informa l) Instructor-led classroom Collegial connections Workshops Work teams Coaching/mentoring Role modeling On-the-job (OTJ) training Virtua l Virtua l colla bo ratio n/synchronous colla bo ratio n/asynchrono us Live e-learning classes Email E-mentoring Online bulletin boards Listservs Online communities Self -pa ced learning Perfo rmance support Web learning modules Help systems Online resource links Print job aids Simulations Knowledge databases Scenarios Documentation Video and audio CD/DVDs Performance/decision support Online self-assessments tools Workbooks 3.3 H ow Do You Build A Blend?
There s no cookbook for blends.
The topic calls out for empirical research, stymied to date by murky definitions for blends and their ingredients, as well as the normal challenges associated with workplace studies.
In the meantime, here are some guidelines for thinking about and constructing successful combinations.
Derived from experience, observations of best practices, and the instructional design literature, these approaches highlight real constraints.
Stability and urgency: Will this content last for one or two years?
Will there be changes within da ys or weeks?
A good distinction to remember is that product information tends to be fickle, while such concepts as a perspective on leadership or customer service possess more staying power.
187  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Another consideration is the amount of time developers have to create the blend s ingredients.
Does the programme need to be up and running within five days or will there be several months to design and develop assets for the blend?
Figure 1 shows a breakdown of content types based on these issues:Fig not sharp or good for use pls replace.
Now let s apply this breakdown to a real situation.
Imagine that you work for a large health maintenance organisation.
Nearly every newspaper in the United States has raised fears about hormone replacement therapy (HRT).
New opinions and studies appear daily.
Patients are concerned, even frantic.
Phones are ringing off the hook with questions about HRT, and doctors do not have all the answers.
Physicians are eager for help now.
When considering solutions to this problem, our attention is immediately drawn to the left column of Figure 1.
The need is urgent, and there is no time for a generous development cycle.
More important, content is emergent and unstable.
The top goal, therefore, is to provide doctors with access to emergent expertise to boost their knowledge and confidence with patients.
The lower left quadrant recommends several options for distributing information to the doctors.
• An online knowledge database to serve as a central repository for HRT information and a directory containing relatively stable information, including individuals and units within the HMO with special knowledge or responsibility regarding this issue.
188  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Email could blast FAQs and events to doctors, alerting them to local meetings to discuss the issue.
• Coaching over the phone that would allow doctors to process emergent concerns with experts.
• A print job aid, produced by experts on the topic that summarises the benefits and risks of each treatment.
This could be combined with a reading list and links to online articles.
• A devoted listserv that pushes the very latest information to doctors on a daily or weekly basis.
• Live online briefings that provide doctors with updates about the latest findings, as well as allowing them to participate in discussions.
Events with stable content could be archived for later reference, providing value to new doctors and others unable to participate in online events.
To uches and Cost: Next, answer the question; Is human interaction essential or will technology suffice?
If the programme is controversial, abstract, or complex, it makes sense to invest in human interactions provided through instructors in classrooms, coaching, mentoring, synchronous electronic presentations, and informal face-to-face meetings.
Through interactions with other people, you can solidify attitudes and murky concepts.
Cost influences decisions about blending, too.
How much is the organisation or individual willing to invest in achieving specified outcomes?
If the outcomes are not achieved, is the situation dire?
If the blended programme is successful, what kinds of benefits will accrue?
Development of realistic assets and human involvement add cost to the venture.
Figure 2 shows how these considerations influence your choices.Fig not good.
189  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY To understand this better, imagine that you have been asked to introduce global virtual teaming to an organisation that many characterise as individualistic in its culture.
A performance analysis found that employees are unclear about the definition and impact of global virtual teaming and that the majority of people are unconvinced that this approach will be effective.
The executives, however, are keen on this shift and are ready to invest resources.
Because this is a top priority initiative with participants that span the globe, you can assume that the organisation is ready to support higher cost solutions, such as those found in the right column of Figure 2.
But you still need to manage employees concerns about virtual teaming.
What assets might best earn their support?
Without question, human interaction holds the most value in this instance.
Face-to-face workshops would provide opportunities for employees to learn about, question, appreciate, and get comfortable with virtual teaming.
Likewise, a live e- learning event could present a model of virtual teaming and expose employees to the people and processes involved in virtual collaboration.
Once the concept and benefits are clear and conversations have transpired, the lower-right quadrant might have something to offer, including: • e-learning modules that instruct global team leaders on how to set and communicate goals 190  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • a performance support tool that can nudge good decisions associated with the tasks global teams are asked to tackle • an online community that can build exposure and consensus about new approaches that executives are eager to establish across the organisation.
Learning Reso urces and Experience: At this point, it is time to focus on actual resources and assets, and consider how people will use them.
Will learning assets be delivered and quickly vanish or will they be available for the long haul and future reference?
The value of most job aids, documentation, performance support tools, and online knowledge bases is that they are available over time and provide assistance on an as-needed basis.
Extended access to such resources is desirable when people are overloaded with information, content changes frequently, topics are complicated, or material is infrequently used.
Because both independent and communal activities are possible, another issue to consider is the learners experience.
Will they work alone on the job or at a home office, train, or plane?
Will they engage with others in their attempt to learn and improve performance?
Figure 3 gives you an idea about how these factors influence decisions.Fig.
not good.
For this example, imagine that you need to introduce a clerical staff to ACCESS, a database program in the Microsoft O ffice Suite.
The learning goal is to help clerks become familiar and comfortable with using ACCESS in their daily tasks.
191  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY A needs assessment revealed substantial fears about the new program.
Only a small percentage of staff envisions ways that ACCESS could be used for their jobs.
While most said they now use and like computers, the majority were not curious about ACCESS and seemed satisfied with existing programs.
Meanwhile, executives are resolute about the ways employees should use the program.
Given that situation, what should the trainer do?
Ideally, instructor-led classes and workshops would introduce employees to the features and benefits of the software.
Instruction to build confidence and fluency, and work on tasks with peers, is indicated in the lower-left quadrant of Figure 3.
It also makes sense to use live e- learning to provide examples of model applications for their business.
These programs can then be archived and made available for new employees or for refresher training.
Once staff has grown more comfortable with the program, an online help system provided by Microsoft can support employees on the job as needed.
If resistance lingers and more interaction is required, you can use an online community or coaching by managers to help staff work through their concerns.
3.4 What It Takes To Blend?
Too often training professionals focus on which e-learning library to buy or how to explain new roles to instructors.
Blending should rivet attention on how to combine resources to achieve a strategic purpose, such as decreasing time to market or increasing repeat business.
Therefore, staying focused on strategy is crucial to blending success.
Other key factors include: Deliver Assets and Guidance: Because learners do not always know when they need additional instruction or when they are ready to test their skills, effective blends must include guidance.
Direction can appear as sample paths, breadcrumbs marking progress, diagnostics, and recommendations targeted to roles, tasks, priorities and progress.
Wo rk Cro ss-Functio nally: A blend is the opposite of a silver bullet.
Blended learning involves working with and convincing people and units across the organisation.
Make certain that all those involved understand their roles and the reasons for multifaceted approaches.
Enco urage Independence and Co nviviality: Before e-learning, trainers typically made choice between delivering information through self-study or teaching in groups.
Emergent technology makes it possible for employees to enjoy both.
Participants can study on their own in print 192  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY or online while they simultaneously participate in online communities, phone conferences, or live meetings.
Focus on Flexible Optio ns fo r Emplo yees and Customers: Blending enables people to get an answer, regardless of the location, time, and learning preferences.
This has pos itive ramifications for knowledge workers who need access to information immediately, rather than after a scheduled event or an expert available for consultation.
Put People in the Middle o f the Blend: Here are some examples: • a manager helps her employees decide whether they want to participate in an online community • peers advise others on taking the class online or face-to-face • an instructor acts as a catalyst in the online communities, making certain that the right people get to the right experiences and assets • a supervisor reminds staff why the topic matters.
Comm unicate, communica te, com municate: Be clear about links between learning options.
The ingredients associated with a programme on virtual teaming, for example, grow in significance when assembled and associated with daily challenges, performance reviews, and career development options .
Embrace Redunda ncy; Redundancy is part of any good blend because it allows participants to receive the same and elaborated messages from several sources in various formats over time.
For instance, a topic is discussed in a traditional classroom, it is elaborated on in the online community, and actual examples are housed in the online knowledge database.
In addition, supervisors may host lunch chats to practice key concepts while email messages reiterate content.
Finally, self- assessments present directions for development.
Ta ke o n Key Initiatives a nd Measure Results: The best way to counter the preference for quick fixes is to realise results in the organisation.
Blended initiatives-on behalf of sales, IT, or HR-speak volumes to those who would rather not.
Recog nise this as an Opening Salvo in the Quest fo r Effective B lends: In our research, we found few definitions and little research on blends.
To that end, after trying these approaches, measure your efforts and share your results with fellow practitioners.
193  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.5 Case Studies El Agave, a premier Mexican restaurant in San Diego, California, is known for its blends.
Their efforts on behalf of mole and tequila concoctions are instructive for those interested in blending.
They use the highest quality ingredients; they are clear about expectations; and they offer options.
They taste and test and improve; and they use elbow grease to smooth the sauce and beverage as needed.
Of course, El Agave has things going for it.
They do not do everything; they focus on doing one thing really well.
You will not find a blintz, frappe, or pasta on the menu.
The buyer, chef, bartender, and waiter are coordinated in their devotion.
Finally, they have never thought about their signature dishes as blends.
It is just what they do-day in, day out.
Workplace learning professionals should think similarly.
Let us move beyond thinking about blends as something unique or special.
They are simply another method for moving towards the concerted systems that is essential to learning and performance.
Blends are characterised by customisation, integration, purpose, flexibility, and redundancy.
The alternative-one-size-fits-all-is no way to serve a global workforce.
4.0 CO NCLUSIO N Blended learning as a new way of imparting knowledge has made the art of teaching and training to be easier and flexible.
Its flexibility makes a choice for busy people who will not have all the time to go through the conventional models of learning.
The business world is fast incorporating blended learning in the trainings.
It is all out to save time and money and at the same time achieve set goals.
5.0 SUMMARY • A blend is an integrated strategy for delivering on promises about learning and performance.
• Blended learning courses can replace synchronous classroom seat time with asynchronous online learning activities so that instruction occurs both in the classroom and online.
• Blending grows as people recognise the value of asynchronous learning.
An August, 2001 report from Adventures notes that "roughly 1.3 million postsecondary students [are] taking online courses" (Evans 2001).
• Options for blended learning go beyond the classroom.
They are formal and informal, technology- and people-based, independent and convivial, and directive- and discovery-oriented.
194  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • There is no cookbook for blends.
• Too often training professionals focus on which e-learning library to buy or how to explain new roles to instructors.
Blending should rivet attention on how to combine resources to achieve a strategic purpose, such as decreasing time to market or increasing repeat business.
• El Agave, a premier Mexican restaurant in San Diego, California, is known for its blends.
Their efforts on behalf of mole and tequila concoctions are instructive for those interested in blending.
6.0 TUTOR-MARKED ASSIGNMENT 1.
Mention the class of people involved in a blend.
2.
Mention the components of a self-paced blended learning.
7.0 REF ERENCES/FURTHER READING Graham, C. R. (2005).
Blended Learning Systems: Definition, Current Trends, and Future Definitions in Bonk, C. J.; Graham, C. R., Handbook of Blended Learning: Global Perspectives, Local Designs.
San Francisco, CA: Pfeiffer.
Pp.
3-21.
De Praetere, T. (2008).
E-Learning.
Garrison, D.R.
; H. Kanuka (2004).
Blended Learning: Uncovering its Transformative Potential in Higher Education.
The Internet and Higher Education 7(2): 95-105.
Tom, Worthington.
Blended Learning: Using a Learning Management System Live in the Classroom.
The Australian National University, 24 October 2008.
Heinze, A.C. Procter (2004).
Reflection on the Use of Blended Learning Education in a Changing Environment.
University of Salford, Salford, Education Development Unit.
195  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY UNIT 6 VO ICE OVER INTERNET PROTOCOL CONTENT S 1.0 Introduction 2.0 Objectives 3.0 Main Content 3.1 History 3.2 Functionality 3.3 Implementation 3.4 VoIP Challenges 3.5 The Use of VoIP 3.6 Voip Phone Accessibility and Portability 3.7 Mobile Phones and Hand-Held Devices 3.8 Mass-Market Telephony 4.0 Conclusion 5.0 Summary 6.0 Tutor-Marked Assignment 7.0 References/Further Reading 1.0 INTRODUCTIO N Voice-over-Internet protocol (VoIP) is a protocol optimised for the transmission of voice through the Internet or other packet-switched networks.
VoIP is often used abstractly to refer to the actual transmission of voice (rather than the protocol implementing it).
This latter concept is also referred to as IP telephony, Internet telephony, voice over broadband, broadband telephony, and broadband phone.
VoIP providers may be viewed as commercial realisations of the experimental Network Voice Protocol (1973) invented for the ARPANET providers.
Some cost savings are due to utilising a single network to carry voice and data, especially where users have under used network capacity that can carry VoIP at no additional cost.
VoIP-to- VoIP phone calls are sometimes free, while VoIP calls connecting to public switched telephone networks (VoIP-to-PSTN) may have a cost that is borne by the VoIP user.
Voice-over-IP systems carry telephony signals as digital audio, typically reduced in data rate using speech data compression techniques, encapsulated in a data-packet stream over IP.
There are two types of PSTN-to-VoIP services: Direct inward dialing (DID) and access numbers.
DID will connect a caller directly to the VoIP user, while access numbers require the caller to provide an extension number for the called VoIP user.
196  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 2.0 OBJECTIVES At the end of this unit, you should be able to: • define VoIP and trace its history and development • identify the numerous functions of VoIP • answer the questions of challenges facing application of VoIP • understand how to implement VoIP • identify the various uses of VoIP in business and other facets of life.
3.0 MAIN CONTENT 3.1 H istory Voice-over-Internet Protocol has been a subject of interest almost since the first computer network.
By 1973, voice was being transmitted over the early Internet.
The technology for transmitting voice conversations over the Internet has been available to end-users since at least the early 1980s.
In 1996, a shrink-wrapped software product called VocalTec Internet Phone (release 4) provided VoIP along with extra features such as voice mail and caller ID.
However, it did not offer a gateway to the PSTN, so it was only possible to speak to other Vocaltec Internet Phone users.
In 1997, Level 3 began development of its first softswitch (a term they invented in 1998); softswitches were designed to replace traditional hardware telephone switches by serving as gateways between telephone networks.
Revenue in the total VoIP industry in the US is set to grow by 24.3% in 2008 to $3.19 billion.
Subscriber growth will drive revenue in the VoIP sector, with numbers expected to rise by 21.2% in 2008 to 16.6 million.
The US's largest VoIP provider is Vonage.
3.2 F unctionality VoIP can facilitate tasks and provide services that may be more difficult to implement or more expensive using the PSTN.
Examples include: • The ability to transmit more than one telephone call over the same broadband connection.
This can make VoIP a simple way to add an extra telephone line to a home or office.
• Conference calling, call forwarding, automatic redial, and caller ID; zero or near-zero-cost features that traditional telecommunication companies (telcos) normally charge extra for.
197  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • Secure calls using standardised protocols (such as Secure Real- time Transport Protocol.)
Most of the difficulties of creating a secure phone connection over traditional phone lines, like digitising and digital transmission, are already in place with VoIP.
It is only necessary to encrypt and authenticate the existing data stream.
• Location independence.
Only an Internet connection is needed to get a connection to a VoIP provider.
For instance, call center agents using VoIP phones can work from anywhere with a sufficiently fast and stable Internet connection.
• Integration with other services available over the Internet, including video conversation, message or data file exchange in parallel with the conversation, audio conferencing, managing address books, and passing information about whether others (e.g.
friends or colleagues) are available to interested parties.
• Advanced Telephony features such as call routing, screen pops, and IVR implementations are easier and cheaper to implement and integrate.
The fact that the phone call is on the same data network as a user's PC opens a new door to possibilities.
3.3 Implem entation Because UDP does not provide a mechanism to ensure that data packets are delivered in sequential order, or provide Quality of Service (QoS) guarantees, VoIP implementations face problems dealing with latency and jitter.
This is especially true when satellite circuits are involved, due to long round-trip propagation delay (400 600 milliseconds for links through geostationary satellites).
The receiving node must restructure IP packets that may be out of order, delayed or missing, while ensuring that the audio stream maintains a proper time consistency.
This function is usually accomplished by means of a jitter buffer in the voice engine.
Another challenge is routing VoIP traffic through firewalls and address translators.
Private Session Border Controllers are used along with firewalls to enable VoIP calls to and from protected networks.
Skype uses a proprietary protocol to route calls through other Skype peers on the network, allowing it to traverse symmetric NATs and firewalls.
Other methods to traverse firewalls involve using protocols such as STUN or ICE.
3.4 VoIP Challenges • Available bandwidth • Network Latency • Packet loss 198  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY • Jitter • Echo • Security • Reliability • In rare cases, decoding of pulse dialing Many VoIP providers do not decode pulse dialing from older phones.
The VoIP user may use a pulse-to-tone converter, if needed.
Fixed delays cannot be controlled but some delays can be minimised by marking voice packets as being delay-sensitive.
The principal cause of packet loss is congestion, which can sometimes be managed or avoided.
Carrier VoIP networks avoid congestion by means of teletraffic engineering.
Variation in delay is called jitter.
The effects of jitter can be mitigated by storing voice packets in a jitter buffer upon arrival and before producing audio, although this increases delay.
This avoids a condition known as buffer underrun, in which the voice engine is missing audio since the next voice packet has not yet arrived.
Common causes of echo include impedance mismatches in analog circuitry and acoustic coupling of the transmit and receive signal at the receiving end.
Reliability Conventional phones are connected directly to telephone company phone lines, which in the event of a power failure are kept functioning by backup generators or batteries located at the telephone exchange.
However, IP Phones and the IP infrastructure they connect to (routers and servers) typically depend on the availability of mains electricity or another locally generated power source.
Voice travels over the internet in almost the same manner as data does in packets.
So when you talk over an IP network your conversation is broken up into small packets.
The voice and data packets travel over the same network with a fixed bandwidth.
This system is more prone to congestion and DoS attacks than traditional circuit switched systems.
To increase the reliability of VoIP phones the VoIP provider needs to increase dedicated and redundant connectivity via T-1 access and backup DSL, with automatic failover at each location.
The company can create a reliable network by reducing the number of single points of failure.
199  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Qua lity of Service Some broadband connections may have less than desirable quality.
Where IP packets are lost or delayed at any point in the network between VoIP users, there will be a momentary drop-out of voice.
This is more noticeable in highly congested networks and/or where there are long distances and/or interworking between end points.
Technology has improved the reliability and voice quality over time and will continue to improve VoIP performance as time goes on.
It has been suggested to rely on the packetised nature of media in VoIP communications and transmit the stream of packets from the source phone to the destination phone simultaneously across different routes (multi-path routing).
In such a way, temporary failures have less impact on the communication quality.
In capillary routing it has been suggested to use at the packet level Fountain codes or particularly raptor codes for transmitting extra redundant packets making the communication more reliable.
Security Many consumer VoIP solutions do not support encryption yet, although having a secure phone is much easier to implement with VoIP than traditional phone lines.
As a result, it is relatively easy to eavesdrop on VoIP calls and even change their content.
An attacker with a packet sniffer could intercept your VoIP calls if you are not on a secure VL AN.
This security vulnerability could lead to Denial of Service (DoS) attacks to you and anyone on your network.
The DoS would devastate your phone network by creating a continuing busy signal and forced disconnects.
Viper Lab predicts VoIP attacks against service providers will escalate since unlicensed mobile access technology becomes more widely deployed to allow calls to switch from cell networks to VoIP networks.
Viper Labs also warns that service providers are, for the first time, allowing subscribers to have direct access to mobile core networks over IP, making it easier to spoof identities and use illegal accounts to launch a variety of attacks.
There is no such thing as a 100% secure solution to network security.
The implementation of voice over internet protocol just adds to that complexity, by giving hackers another means to access your system.
Customers can secure their network by limiting access to the virtual local area network, thus hiding their voice data network from the users.
If the customer maintains a secure and properly configured gateway, you can keep most of the hackers out.There are several open source solutions that facilitate sniffing of VoIP conversations.
A modicum of security is afforded due to patented audio codecs that are not easily available for open source applications, however such security through obscurity has not proven effective in the long run in other fields.
Some vendors also use compression to make 200  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY eavesdropping more difficult.
However, real security requires encryption and cryptographic authentication which are not widely available at a consumer level.
The existing secure standard SRTP and the new ZRTP protocol is available on Analog Telephone Adapters(ATAs) as well as various softphones.
It is possible to use IPsec to secure P2P VoIP by using opportunistic encryption.
Skype does not use SRTP, but uses encryption which is transparent to the Skype provider.
The Voice VPN solution provides secure voice for enterprise VoIP networks by applying IPSec encryption to the digitised voice stream.
3.5 The Use of VoIP Corpo rate and Telco use Although few office environm ents and even fewer homes use a pure VoIP infrastructure, telecommunications providers routinely use IP telephony, often over a dedicated IP network, to connect switching stations, converting voice signals to IP packets and back.
The result is a data-abstracted digital network which the provider can easily upgrade and use for multiple purposes.
Because of the bandwidth efficiency and lowcosts that VoIP technology provides, businesses are slowly beginning to migrate from traditional copper-wire telephone systems to VoIP systems to reduce their monthly phone costs.
Corporate customer telephone support often use IP telephony exclusively to take advantage of the data abstraction.
The benefit of using this technology is the need for only one class of circuit connection and better bandwidth use.
Companies can acquire their own gateways to eliminate third-party costs, which is worthwhile in some situations.
VoIP is widely employed by carriers, especially for international telephone calls.
It is commonly used to route traffic starting and ending at conventional PSTN telephones.
Many telecommunications companies are looking at the IP Multimedia Subs ystem (IMS) which will merge Internet technologies with the mobile world, using a pure VoIP infrastructure.
It will enable them to upgrade their existing systems while embracing Internet technologies such as the Web, email, instant messaging, presence, and video conferencing.
It will also allow existing VoIP s ystems to interface with the conventional PSTN and mobile phones.
201  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY Electronic Numbering (ENUM) uses standard phone numbers (E.164), but allows connections entirely over the Internet.
If the other party uses ENUM, the only expense is the Internet connection.
Virtual PBX (or IP PBX) allows companies to control their internal phone network over an existing LAN and server without needing to wire a separate telephone network.
Users within this environment can then use standard telephones coupled with an FXS, IP Phones connected to a data port or a Softphone on their PC.
Internal VoIP phone networks allow outbound and inbound calling on standard PSTN lines through the use of FXO adapters.
Use in Ama teur Radio Sometimes called Radio over Internet Protocol or RoIP, Amateur radio has adopted VoIP by linking repeaters and users with Echolink, IRLP, D-STAR, Dingotel and EQSO.
In fact, Echolink allows users to connect to repeaters via their computer (over the Internet) rather than by using a radio.
By using VoIP Amateur Radio operators are able to create large repeater networks with repeaters all over the world where operators can access the s ystem with actual ham radios.
Ham Radio operators using radios are able to tune to repeaters with VoIP capabilities and use DTMF signals to command the repeater to connect to various other repeaters, thus allowing them to talk to people all around the world, even with "line of sight" VHF radios.
Click to Call Click-to-call is a service which lets users click a button on a company website and immediately speak with a customer service representative.
The call can either be carried over VoIP, or the customer may request an immediate call back by entering their phone number.
One significant benefit to click-to-call providers is that it allows companies to monitor when online visitors change from the website to a phone sales channel.
Mo bile Number Porta bility (MNP) in the Internet Telepho ny Environment Mobile number portability (MNP) also impacts the internet telephony, or VOIP (Voice over IP) business.
A voice call originated in the VOIP environment which is routed to a mobile phone number of a traditional mobile carrier also face challenges to reach its destination in case the mobile phone number is ported.
Mobile number portability is a service that makes it possible for subscribers to keep their existing mobile phone number when changing the service provider (or mobile operator).
202  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY VoIP is clearly identified as a Least Cost Routing (LCR) voice routing system, which is based on checking the destination of each telephone call as it is made, and then sending the call via t he network that will cost the customer the least.
With GSM number portability now in place, LCR providers can no longer rely on using the network root prefix to determine how to route a call.
Instead, they now need to know the actual current network of every number before routing the call.
Therefore, VoIP solutions also need to handle MNP when routing a voice call.
In countries without a central database like UK it might be necessary to query the GSM network about the home network a mobile phone number belongs to.
As VoIP starts to take off in the enterprise markets because of least cost routing options, it needs to provide a certain level of reliability when handling calls.
MNP checks are important to assure that this quality of service is met; by handling MNP lookups before routing a call and assuring that the voice call will actually work, VoIP companies give businesses the necessary reliability they look for in an internet telephony provider.
UK- based messaging operator Tyntec provides a Voice Network Query service, which helps not only traditional voice carriers but also VoIP providers to query the GSM network to find out the home network of a ported number.
In countries such as Singapore, the most recent Mobile number portability solution is expected to open the doors to new business opportunities for non-traditional telecommunication service providers like wireless broadband providers and voice over IP (VoIP) providers.
In November, 2007, the Federal Communications Commission in the United States released an o rder extending number portability obligations to interconnected VoIP providers and carriers that support VoIP providers.
3.7 Integration into G lobal Te lephone Number System While the wired public switched telephone network (PSTN) and mobile phone networks share a common global standard (E.164) which allocates and identifies any specific telephone line, there is no widely adopted similar standard for VoIP networks.
Some allocate an E.164 number which can be used for VoIP as well as incoming and external calls.
However, there are often different, incompatible schemes when calling between VoIP providers which use provider-specific short codes.
203  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 3.8 VoIP Phone Accessibility and Portability If using a software based soft-phone, calls can only be placed from the computer on which the soft-phone software resides.
Thus with a soft- phone the caller is typically limited to a single point of calling.
When using a hardware based VoIP phone-device/phone-adapter it is possible to connect traditional analog phones directly to a VoIP phone-adapter without the need to operate a computer.
The converted analog phone signal can then be connected to multiple house phones or extensions, just as any traditional phone company signal can be connected.
A second VoIP hardware configuration option involves the use of a specially designed VoIP telephone which incorporates a VoIP phone adapter directly into the phone itself, and which also does not require the use of a computer.
A third VoIP hardware configuration option involves the use of a WiFi router and a WiFi SIP phone which can extend a service range throughout a home or office.
WiFi SIP phones can also be used at any location where an "unauthenticated" open hotspot Wi-Fi signal is available.
However, note that many hotspots require browser- based authentication, which most SIP phones do not support.
3.9 Mobile Phones and Hand-Held Devices Telco and consumers have invested billions of dollars in mobile phone equipment.
In developed countries, mobile phones have achieved nearly complete market penetration, and many people are giving up landlines and using mobiles exclusively.
Given this situation, it is not entirely clear whether there would be a significant higher demand for VoIP among consumers until either public or community wireless networks have similar geographical coverage to cellular networks (thereby enabling mobile VoIP phones, so called WiFi phones or VoWLAN) or VoIP is implemented over 3G networks.
However, "dual mode" telephone sets, which allow for the seamless handover between a cellular network and a WiFi network, are expected to help VoIP become more popular.
Phones like the NEC N900iL, and later many of the Nokia Eseries and several WiFi enabled mob ile phones have SIP clients hardcoded into the firmware.
Such clients operate independently of the mobile phone network unless a network operator decides to remove the client in the firmware of a heavily branded handset.
Some operators such as Vodafone actively try to block VoIP traffic from their network and therefore most VoIP calls from such devices are done over WiFi.
Several WiFi only IP hardphones exist, most of them supporting either Skype or the SIP protocol.
These phones are intended as a replacement 204  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY for PSTN based cordless phones but can be used anywhere where WiFi internet access is available.
Another addition to hand held devices are ruggedised bar code type devices that are used in warehouses and retail environments.
These types of devices rely on "inside the 4 walls" type of VoIP services that do not connect to the outside world and are solely to be used from employee to employee communications.
3.10 Mass-Market Telephony A major development starting in 2004 has been the introduction of mass-market VoIP services over broadband Internet access services, in which subscribers make and receive calls as they would over the PSTN.
Full phone service VoIP phone companies provide inbound and outbound calling.
With direct inbound dialing, many offer unlimited calling to the U.S., and some to Canada or selected countries in Europe or Asia as well, for a flat monthly fee.
These services take a wide variety of forms which can be more or less similar to traditional POTS.
At one extreme, an analog telephone adapter (ATA) may be connected to the broadband Internet connection and an existing telephone jack in order to provide service nearly indistinguishable from POTS on all the other jacks in the residence.
This type of service, which is fixed to one location, is generally offered by broadband Internet providers such as cable companies and telephone companies as a cheaper flat-rate traditional phone service.
Often the phrase "VoIP" is not used in selling these services, but instead the industry has marketed the phrases "Internet Phone", "Digital Phone" or "Softphone" which is aimed at typical phone users who are not necessarily tech-savvy.
Typically, the provider touts the advantage of being able to keep one's existing phone number.
4.0 CONCLUSION The availability of voice in Internet marketing and online shopping brought about by the VoIP has brought about improvement and convenience in doing business over the Internet.
The technology is being worked upon to improve beyond its level of deployment in e-commerce.
It is a multi billion dollar business that will ever be on the increase because of the better conviction voice will continue to bring to business.
205  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 5.0 SUMMARY • Voice-over-Internet pro tocol (VoIP) is a protocol optimised for the transmission of voice through the Internet or other packet- switched networks.
VoIP is often used abstractly to refer to the actual transmission of voice (rather than the protocol implementing it).
• Voice-over-Internet Protocol has been a subject of interest almost since the first computer network.
By 1973, voice was being transmitted over the early Internet.
The technology for transmitting voice conversations over the Internet has been available to end-users since at least the early 1980s.
• VoIP can facilitate tasks and provide services that may be more difficult to implement or more expensive using the PSTN.
• Because UDP does not provide a mechanism to ensure that data packets are delivered in sequential order, or provide Quality of Service (QoS) guarantees, VoIP implementations face problems dealing with latency and jitter.
• Although few office environments and even fewer homes use a pure VoIP infrastructure, telecommunications providers routinely use IP telephony, often over a dedicated IP network, to connect switching stations, converting voice signals to IP packets and back.
• If using a software based soft-phone, calls can only be placed from the computer on which the soft-phone software resides.
Thus with a soft-phone the caller is typically limited to a single point of calling.
• Telco and consumers have invested billions of dollars in mobile phone equipment.
In developed countries, mobile phones have achieved nearly complete market penetration, and many people are giving up landlines and using mobiles exclusively.
• A major development starting in 2004 has been the introduction of mass-market VoIP services over broadband Internet access services, in which subscribers make and receive calls as they would over the PSTN.
6.0 TUTOR-MARK ED ASSIGNMENT 1.
Identify 5 challenges facing VoIP.
2.
Discuss briefly the quality of VoIP services.
206  MBF 841 EMERGING TECHNOLOGIES IN INFORMATION TECHNOLOGY 7.0 REF ERENCES/FURTHER READING Jackson, Barry.
History of VoIP.
University of Texas at Dallas.
Keating, Tom.
Internet Phone Release 4 Computer Telephony Interaction Magazine The 10 that Established VoIP (Part 2: Level 3).
iLocus (July 13, 2007).
VoIP Leads the Way in IBISWorld Top 10 List for 2008 - VoIP Monitor VoIP - Vulnerability over Internet Protocol Anderson, N. (09-22-2006).
Some VoIP services surpass traditional phones.
Faxing over IP networks Higdon, J.
(2008-01-24).
The Top 5 VoIP Security Threats of 2008 Letter from the City of New York to the Federal Communications Commission Internet Phones Call on Wi-Fi.
PCWorld WiFi phones for Skype and SIP: available now, but be careful what you buy.
Voipally.
Dual-mode cellular/WiFi handset adoption.
TMCnet.
Vodafone Terms and Conditions & Mobile Phones from Phones 4u Examining Two Well-Known Attacks on VoIP.
CircleID.
VoIP Review.
VoIP Review, LLC.
Global VoIP Policy Status Matrix.
Global IP Alliance.
Proenza, Francisco J.
The Road to Broadband Development in Developing Countries is through Competition Driven by Wireless and VoIP Stars and Stripes: USFK deal keeps VoIP access for troops.
207
