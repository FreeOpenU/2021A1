 NATIONAL OPEN UNIVERSITY OF NIGERIA SCHOOL OF SCIENCE AND TECHNOLOGY COURSE CODE: STT 211 COURSE TITLE: PROBABILITY DISTRIBUTION I 1 Course Code STT 211 Course Title Probability Distribution I Writer Arowolo.
OLATUNJI School of Science and Technology Lagos State Polytechnic.
Ikorodu..
Course Editing Team 2PROBABILITY DISTRIBUTION I MODULE 1 UNIT 1 PREREQUISITES The main prerequisites for understanding the content of this book is a knowledge of elementary algebra, set theory, mathematical induction, differentiation and integration.
Set A set is any well-defined list or collection of objectives.
The objectives comprising the set are called its elements or members.
A set will be denoted by capital letters or symbols such as X,Y,A,B,….. and its elements will be denoted by lower case letters x, y, a,b,…..
Example 1.1 Toss a cubical die once.
There are six possible numbers that can appear.
We can write Ω = {1, 2, 3, 4,5, 6} Where Ω is a set consisting of six elements 1, 2, 3, 4, 5, 6 called the element of the set Ω.
There are essentially two ways of specify a particular set.
One way if possible is by listing its elements as in example1.1 above.
The other way is by stating properties which characterize the elements in the set.
The above set Ω can be written as: Ω – {x: is an integer, 1 < x < 6} If x is an element Ω, the notation x € Ω means that x belongs to Ω.
The negation of this assertion i.e.
the statement that x does not belong to Ω will be denoted by x € Ω thus, for the above example 2 € Ω but 8 € Ω.
Definition 1.2 Subset: A set A is a subset of a set if each element in A also belongs to Ω.
In example 1.1 above, the set A ={1,3,5}= (x: x € Ω and x is odd} Is a subset of Ω, that is each element of A is in Ω.
Two sets A and B are called equal if and only if they contain exactly the same elements.
3Throughout this book, whenever the word set is used, itwill be interpreted to mean a subset of a given set denoted by Ω.
The set which contains no elements is called the Null set.
Definition Let A be a set.
The elements which are not included in A also constitute a subset.
This is known as the complement of A and is denoted by Ac.
In example 1,1 if A {1,3,5} Ac = {2,4,6} = {x: x Ω, x even}.
Definition: Two sets A, B define two related sets.
One of these is the set of all elements which belongs to both sets A and B. this is called the intersection of A and B and is denoted by A∩B.
the other is the set of all the elements which occur in either A or B or both.
This is called the union of A and B denoted by A B.
Example 1.2 Let Ω = {1,2,3,4,5,6}, A = {1,3,5} and B = {2,3,5} Then A∩B = {3,5} A ∩B = {1,2,3,5} A and B contain 3 element each while A ∩ B contains 2 elements and A ∩B contains 4 laments.
Note that the number of elements in A ∩ B is not the sum of the number of elements in A and B.
Definition: Difference of Two sets The different of A and B is the set of elements which belong to A but not to B and is denoted by A/B In example 1.2 above A/B = {x: x A€ , x € B} Ω A/B = {1} B/A = {2} The union of two sets A B can be divided into three disjoint sets 4 A/B, A ∩B, B/A.
That is A ∩ B = (A/B) ∩ (A ∩B) ∩ (B/A) The number of element in a set will be denoted by nA.
Thus, n(A ∩ B) = n (A/B) + n(A∩B) ∩ (B/A) Since (A/B), (A ∩ B), (B/A) are disjoint sets nA = n(A/B) + n(A∩B) nB = n(B/A) + n(A∩B) n(A/B_ = nA –n (A ∩B) n (B/A) = nB-n(A∩B) hence, n(A ∩B) = nA-n (A ∩B) + n(A ∩B) + nB-n (A ∩B) = nA + nB – n(A ∩ B) note: A/B = A ∩ Bc, B/A = B ∩ Ac for any set A, A ∩ Ac = Φ, A ∩ Ac = Ω.
For any two sets A and B, we have the following decomposition: B = B ∩ Ω = B ∩ ( A ∩ Ac) = (B ∩ A) ∩ ( B ∩ Ac), Since A ∩ B and Ac ∩ B are disjoints, we have nB = n(A ∩ B) +n(Ac ∩ B) de Morgan’s Law (i) For any two sets A and B (A ∩ B)c = Ac ∩ Bc 5Proof: Ac = ( A ∩ B)c ∩ (B/A), Bc = ( A ∩ B)c ∩ (A/B).
Thus, Ac ∩ Bc = (A ∩ B)c Since B/A and A/B are disjoint, Alternatively, (A B)c = { x: x A and X B}=Ac∩ Bc In general, if A , A ,…..A are any n sets 1 2 n (A ∩ A ∩…∩A )c = A c∩A c…A c. 1 2 n 1 2 n Thus Series A sequence is a set of numbers occurring in order, and there is a simple rule by which the terms are obtained.
For example, 1, x, x2… is a sequence.
If the terms of a sequence are considered as a sum, for instance, 1 + x + x2 + ….
The expression is called a series.
A series with a finite number of terms is called a finite series otherwise it is called a finite series otherwise it is called an infinite series.
The summation is shown by the symbol.
When the sum is taken from the first term (r = 1) to thee nth term (r = n).
Example The series 1 + x + x2 + … + x10 can be written as 6The above series is called a geometric series with common ratio x. the sum Xr-1 = x =1 1'() 1 – x N, x =1 The commonratio is x. if – 1 < x < 1, xn -> 0 as n- > =, then the series converges to 1 1 – x The infinite series Example Find the sum of the series Solution (1 – p)r-1 = 1 + (1 –p) + 1 –p)2 +…….
The common ratio is 1 – p, from (1) let x = 1 –p, we have (1 – p)r-1 = 1 = 1 1 –(1 –p) p And (1 – p)r-1 = 1 – (1 – p)n = 1 –(1 –p)n 1 – (1 – p) P Exponential series The function y = ex is called an exponential function.
This function is one of the special functions of analysis and can be defined as the sum of an infinite series.
It is defined by Ex =1 + x + x2 +…+ xn+ … = xr 2!
n!
r!
The notation r!
is called factorial r and is defined as R!
= r(r – 1) (r-2) ….. 2.1 For example 7 5!
= 5 x 4 x 3 x 2 x 1 Where 0!
= 1 and 1!
= 1.
R!
defined above has no meaning unless r itself is a positive integer.
The factorial of any negative integer is infinite.
Gamma Function It is easily shown by direct integration that when m is an integer M!
= Provided that m > - 1.
It can be proved that (2) will also have a meaning for fraction m. for instance, it is known that Since m!
= m(m-1)!.
By setting m = 1 we have 2 The Binomial Theorem If n is a positive integer, (a+b)2 = an+ nC a n-1 B + nC an-2b2+….+bn 1 2 Where nC = n!
.
Using the summation notation, we have r (n – r)!r!
(a +b)n using the summation notation, we have Example Find the sum of the series S = Pn + nC pn-1 (1 – P) + nC pn-2(1-P)2 +…+ (1-p)n Cr n-r (1-P)r n 1 2 n P Substituting p and 1-p for a, b in the binomial theorem above, we have Thus, Generalization of the Bionomial theorem is the multinomial theorem.
If n is a positive integer (a +a + … a )n = ∑…….∑ n!
a n1a n2…a nk 1 2 k 1 2 k N !
n !....n !
1 2 k Sum is over all n , n ….. n where n + n + … + n = n. 1 2 k 1 2 k 8Product Notation The product of the terms of a sequence x , x …., x can be written as x x x ….x .
This 1 2 n 1 2 3 n product is shown by the symbol.
Where the product is taken from x , (the first term) to the nth term, x .
for example, 1 n N !
N !...n !
and a …a 1 2 k 12 k Can be written as Thus the multinomial theorem can be written as Example (a + a + a )2 = ∑∑ 2!
1 2 3 3 Where n=2, k = 3.
Possible values of n are 1 n =2, n =0, n =0 1 2 3 n = 0, n = 2, n = 0 1 2 3 n = 0, n = 0, n =2 1 2 3 n = 1 2=1 n = 0 1 , n , 3 n =1, n = =, n =1 1 2 3 n = 0,n =1, n = 1 2 3 hence, we have (a + a + a )2 = a 2 + a2 + a2 + 2a a + 2a 3 2 3 a 2 3 1 2 3 1 2 1a +2a a Exponential Functions These are function in which the variable occur in the index, for example ex32x are called exponential functions, Let y = e h(x) then For example, if y = e3x2 then dy =6xe3x2 dx Derivative of log h(x) e If y = log 2 x3 then e 9 dy = 6x.
1 = 3 dx 2x3 x2 10  UNIT TWO MATHEMATICS OF COUNTING Probability had its origin in games of chance such as disc and card games.
The number of definitions you can find for it is limited only by the number of books you may wish to consult.
Probability can be defined as a measure put on occurrence of a random phenomenon.
probability theory is developed as study of the outcomes of trail of an experiment.
Definition An experiment is a phenomenon to be observed according to a clearly defined procedure.
Probabilities are numbers between 0 and 1, inclusive that reflect the chances of a particular physical events occurring.
If a die is tossed once, the possible outcomes are 1,2,3,4,5,6, let Ω = {1,2,3,4,5,6}.
Then Ω is a set consisting of all possible outcome of tossing a die once.
This set is given a name, it is called a sample space.
Example 1.1 Write down the staple space for each of the following experiments (i) Toss a coin 3 times and observe the total number of heads (ii) A box contains 6 items of which 2 are defectives.
One item is chosen one after the other without replacement until the last defective items is chosen.
We observe the total number of items removed from the box.
Solution (i) Possible outcomes are: number of heads is 0 when we have TTT, number of heads is 1 when we have HTT or THT or TTH, number of heads is 2 when we have HHT, THH, HTH and number of heads are 0,1,2,3.
Hence the sample space Ω = {0,1,2,3) (ii) Total number of items removed is 2 if we have “the first is defective (D) and the second is defective D (Note that the total number of items removed can not be 0 or 1) denoted by DD.
The total number of items removed is 3 if we have 11 GDD or DGD (where G denoted good item), and so on.
Thus, the sample space Ω = {2,3,4,5,6} Definition 1.2 Event Any subset A of a sample space Ω is called an event, where A < Ω.
Example 1.2 The following are examples of events (i) An odd number occurs when a die is rolled once Ω = {1,2,3,4,5,6}, A = {1,3,5} (ii) Toss a coin 3 times, the total number of heads observed is even Ω = {0,1,2,3} A = {0,2} Definition 1.3 Trial of an Experiment A trial is a single performance of an experiment There are basically two methods for assigning probabilities to events.
These are (i) Relative frequency approach (ii) Classical approach (i) Relative Frequency Approach If we are interested in the occurrence of an event A, we could perform a large number of trials and define the relative frequency of A as RF(a) = Number of trials in which A occurs Total number of trials The ratio RF(A) is called the empirical probability.
If the number of trials is very large RF(A) will (in most cases0 tend to a particular value called the probability that the event A will occur, that is RF (A) is an estimate of P(A).
the probability that A will occur.
Example 1.3 Suppose we are interested in the occurrence of a “6” when we roll a well balanced die (fair die) the die is then rolled 100 times and “6” appeared 15 times.
The relative frequency of “6” is thus 15 which is the empirical probability of getting 6 100 (ii) Classical Approach: 12 In the above example, there are six ways the die can fall when it is rolled once.
So we can define a theoretical probability by P(6) = number of ways of getting a “6” when rolled once = 1 Total number of possible outcomes when rolled once 6 The classical approach to probability gives P(A) = nA n Ω This definition is valid only when outcomes are equally likely.
Example 1.4 A coin is rolled thrice.
The possible outcomes assumed to be equally likely are Ω= {HHH, HHT, HTH, HTT, TTT, TTH, THT, THH} Let A be the event that 2 heads occur.
Then A = {HHT, HTH, THH} n Ω = 8, nA = 3 hence P(A) = 3 8 This classifiableapproach when applicable (the possible outcomes are equally likely) has the advantage of being exact.
Thus to compute a probability by using the above definition one must be able to count.
(i) n Ω the total number of possible outcomes in the sample space and (ii) nA, the number of ways in which event A can occur equally like outcomes are also called equally probable outcomes.
If an event cannot occur its probability is 0, if it must occur its probability is .
the computation of nA and n Ω is easy if Ω has only a few possible outcome, as the number of possible outcomes becomes large, this method of counting all possible outcomes are cumbersome and time consuming.
Alternative methods of counting must therefore be developed.
For example if one asks for the probability of getting sum of numbers showing to be 30 when 7 dice are rolled 13 one must determine how many different ways are possible to get the sum to be 30.
Such possible ways include (6,6,6,6,2,1,3) (2,5,2,6,6,3), (4,4,4,4,4,4,6) In this chapter weintroduce nontechnical discussion of techniques of mathematics of counting frequently needed in problems of finding nA and n Ω Exercise 1.1 1.
A die is rolled once.
What are the probabilities of getting (i) An even number (ii) A prime number (iii) An off prime number (iv) An odd number (v) An even number Fundamental Principle of Counting First Law of Counting If an event A can occur in n ways and thereafter an event A can occur in n ways, 1 1 2 2 “both A and A can occur in this order in n n ways.
1 2 1 2 Example 1.5 Roll a die first and then a con A can occur in 6 ways (1,2,3,4,5,or 6) and A can occur in 2 ways (H or T).
1 2 Thus, by the above law, there are 6 x 2 =12 Possible ways for the outcomes The outcomes are (1,H) (1,T) (5, H) (5,T) (2,H) (2,T) (6,H) (6, T) (3, H) (3, T) (4, H) (4,T) In general, if an event A can occur in n in different ways and if following this an event 1 1 A can occur in n different ways, and if following this second event, an event A can 2 2 3 14 occur in n different ways and so forth, then the events A and A and A ….
And A can 3 1 2 3 k occur in this order in n n n ways.
1 2….
k Example 1.6 If a die is rolled 10 times.
Let A denote the outcome of the roll, i= 1,2 ,….
10.
A can 1 1 occur in 6 ways.
Thus thenumber of ways A , A ,….
And A can occur is 610possible outcome.
1 2 10 That is N Ω=610 Second Law of Counting If an eventA can occur in n different ways and an event A can occur in n ways then 1 1 2 2 either A orA can occur in n +n different ways.
1 2 1 2 Example 1.7 Let us toss a die or a coin once.
let a ne the event “the die shows an even number and a 1 2 be the event “the coin lands heads”.
a can occur in 3 ways (2, 4or6) and a can occur 1 2 only ocne.
\the number of ways in which an even number or a head be obtain is 3+ 1 =4 A can occur in 3 ways A can occur in 1 way 1 2 Therefore A or A can occur in 4 ways 1 2 In general, if events A , A ….,A can occur in n ….n different ways, then either or A 1 2 k 1 k 1 + n +… + n different ways 2 k Example 1.8 Four people enter a restaurant for lunch in which there are six chars.
In how many ways can they be seated.
Let A , A ,A ,A denote the events “choice of chair by the four people”, the suffix 1 2 3 4 denoting order of seating.
The first person to sit down has six choices.
He can decide tosit on any of the six vacant chairs.
Therefore, there are 6 different ways A can occur, after 1 15 the first person has seated, the second person can sit on any of the remaining 5 chairs.
Thereafter the third person can sit on the remaining 4 chairs.
Thus, using the notation of law of counting N =6, n = 5, n = 4, n = 3 1 2 3 4 6 x 5 x 4 x 3 =360 ways They can be seated .
Example 1.9 How many 4 digits numbers can be formed from the digits 0,1,2,3,4,5 if the first digit must not be 0 and repetition of digits are not allowed.
Let A , A , A , A select the first second, third and fourth digits 1 2 3 4, denote mthe events: respectively.since the first digits can not be 0, the first digit can e either 1,2,3,4, or 5 therefore A can occur in 5 ways, having chosen the first digit the second digit can be 1 selected from the remaining 5 digits.
there the first and second digits have been chosen, there remains 4 digits.
The third digit can be chosen from the remaining 4 digits and the fourth can be chosen from the remaining 3 digits.
Therefore,A , A , A and A can occur 1 2 3 4 in 5,5,4,3 ways respectively Thus there 5 x5x4x3 = 300 numbers Example 1.10 Ten candidates are eligible to fill 4 vacant position.
How many ways are there of filling them?
Let A ,A , A ,A be the events denoting candidates that fill positions 1,2,3,4 respectively.
1 2 3 4 Candidate filling position 1 can be any of the tend candidates, therefore A can occur in 1 10 ways.
Following this, the next position 2 can be filled by any of the remaining 9 candidates since position 1 had been filled by one candidate.
Therefore A can occur in 2 ways.
Similarly, A and A can occur in 8 and 7 ways respectively.
Thus there are 3 4 10x9x8x7 = 5040 ways Exercise 1.2 1.
A student is to answer all the five questions in an examination.
It is believed that the sequence in which the questions are answered may have a considerable effect 16 on the performance of the student.
In how many different order can the question be answered 2.
If a woman has 10 blouses and 6 skirts, in how many ways can she choose a dress assuming any combination of blouse and skirt matches 3.
In a study of plants, five characteristics are to be examined.
If there are six recognizable differences in each of four characteristics and eight, recognizable difference in the remaining characteristics.
How many plants can be distinguished by these five characteristics?
4.
A bus starts with 6 people and stops at 10 different stops.how many different ways can the 6 people depart if (i) Any passenger can depart at any bud stop (ii) No two passengers can leave at the same bus stop 5.
Show that the number of ways of choosing r objects from n objects with replacement is given by n” Permutation and Combination A permutation is an arrangement of objects in a definite order (called ordered sample).
A combination is a selection of objects without regard to order (unordered sample) A group of objects, with regard to permutation and combination has three characteristics: 1.
The way the objects in a group are arranged 2.
The kind of objects in the group 3.
The number of objects of each kind in the group 1.2 permutations: Suppose that a set contain n objects.
We are often interested in arranging the objects in a definite order.
Two groups containing n object are said to form different permutations if they differ in arrangements Consider groups of letters a, b, c, d. (a, b, c, d): (b, c, d, a): (c, a, d, b) 17 They are all different permutations because the arrangement of the letters is different in each group.
Example 1.11 How many permutations of four letters can be formed from the letter a, b, c, d. To answer this question we reason as follows: Since are permuting 4 letters, four events are involved.
Let A be the events denoting the 1 letter to occupy the ith position.
A can occur in 4 ways, that is the first letter can be 1 either a, b, c, or d. after this event has taken place, A can occur in three ways (that is, 2 after having chosen the first letter, the letter occupying the second position can be chosen from the remaining three letters.
After the second event, there remains only two letters from which one is to be chosen tooccupy the third position.
So A is 2 and it remains only 3 one letter.
A can occur only in one way.
Thus, there are 4 x 3 x 2 x 1 = 24 permutation of the four 4 letters.
Permutation of four objects from four objects is called permutation 4 and is denoted by 4P 4 Permutation of n Distinct Objects Consider a set consisting of n district objects.
permutation of this set consist of n events A , A , ….A , where the object occupying the ith position is the outcome of A , I = 1, 1 2 n 1 2,…..n A can occur in n ways, A can occur in n-1 ways and so on.
1 1 2 Thus there are n(n-1) (n-2)… x 3 x 2 x 1 permutation of the n objects.
The number of permutations of n distinct objects is n!.
Example 1.12 If n balls are distributed at random into r boxes, in how many ways can this be done if (i) Each of the balls can go into any of the r boxes (ii) No box has more than one ball.
Solution (i) Let A , A ….A where the object occupying the ith position is the outcome of A I 1 2 n 1 = 1,2,…..n. A can occur in n way, A can occur in n-1 ways and so on.
1 1 2 18 (ii) The first ball can go into any of the r boxes, the second any one of the remaining (r-1) boxes etc, so in all there are r (r-1) (r-2)… (r-n+1) different ways (n < r).
19  Example 1.13 How many permutation of three letters can be formed from the letters a, b, c, d, e. abc, bae, cba, cdb,….. are few of the required permutation.
Since the permutation consist of 3 letters, 3 events are involved, A , A A .
The first letter can be any of the 5 letters, 1 2 3 therefore A can occur in 5 ways.
Similarly A can occur in 3 ways.
Thus, there are 1 3 5 x 4 x 3 =60 Permutation of three letters from 5 letters.
This is denoted by 5P (5 permutation 3).
3 Definition 1.4 The number of permutation of r (r < n) objects from n district objects is called nn permutation r and is denoted by np r It can easily be shown that nP =n(n-1)(n-2)…(n-r+1) r to see this we can argue as follows.
Since we are permuting r objects, there are r events A , A …..A involved.
1 2 3 The first position can be occupied by any of the n objects, following this the second position can be occupied by any of the remaining n-1 objects and so on.
Therefore A , 1 A A can occur in n, n-1, n-2, …n-(r-1) ways respectively.
thus the number 2,… r ofpermutation is n(n-1)(n-20…(n-r+1) But n!= n(n-1) (n-2)…(n-r+1) (n-r)(n-r-1)...1 (3.2.1) (n-r)!
= (n-r)(n-r-1)…2.1 n!
= n(n-1)(n-2)…(n-r+1).
(n-r)!
20  Therefore nP=n(n-1) (n-2)…(n-r+1) r nP= n!
r (n-r)!
In example 1.13 above, n = 5, r = 3 5P = 5!
= 5!
= 5 x 4 x 3 = 60 3 (5-3) 2 Example 1.14 A group of students consist of 5 men and 3 women.
The students are ranked according to their performance in a quiz competition.
Assuming no two students obtain the same score (i) How many different ranking are possible?
(ii) If the men are ranked just among themselves and the women among themselves, how many different rankings are possible?
Solution (i) A possible ranking corresponds to a permutation of the students.
The number of possible permutation of the 8 students gives the number of different ranking possible.
Thus the answer is 8P = 8!
= 8!
= 40320.
8 (8-8)!
(ii) There are 5P = 5!
= 120 possible rankings of the men and 3P =3!
= 6 possible 5 3 rankings of the women.
It follows from the fundamental principle of counting that there are 5P x 3P = 5!
X 3!
= 72- possible rankings 5 3 Example 1.15 Four digits numbers are to be formed using any of the digits 1, 2, 3, 4, 5, 6, (No repetition of digit is allowed).
(i) How many four digit number can be formed (ii) How many 4 digit numbers greater than 3000 can be formed?
(iii) How many 4-digit even numbers can be formed?
21 (iv) How many of these even 4-digit numbers are greater than 3000?
Solution (i) This is permutation of 4 digits from 6 digits.
Therefore the different permutation is 6P = 6!
=360.
4 2!
Alternatively, we can reason as follow: The first digit to be selected can be any of the six given digits, so n = 6.
The second 1 digits to be select can be any of the remaining 5 digits (since no reparation is allowed) so n = 5.
Similarly, n = 4 and n = 3.
Thus, the answer is 2 3 4 N x n x n x n = 6 x 5 x 4 x 3 = 360 1 2 3 4 (ii) Since the number must be greater than 3000, the first digit must be chosen room 3, 4, 5 or 6 so n = 4.
The second digit can be any of the remaining 5 digits, so n 1 2 =5 similarly, n = 4, n = 3.
Thus, there are 3 4 4 x 5 x 4 x 3= 240 4 digit numbers greater than 3000 that can be formed using the digits 1, 2, 3, 4,5, 6, (iii) A number can be defined to be an even number of the last digit of the number is even.
Going by this definition, we see that the required number is even if the last digit is 2,4, or 6.
Since there is a restriction on the last digit.
We have to select the last digit first.
The last digit can be selected from 2,4 or 6, so we have 3 choices.
Having chosen the last digit, we can now select the first, second and third digit.
After chosen the last digits there remains 5 digits, so n 1 = 5 similarly, n = 4,n =3.
Thus, the answer is 5x 4 x 3x 3 = 180 2 3 (iv) The last digit must be 2, 4 or 6 and the first digit must be 3, 4, 5or 6.
If the last digits is 2, then the number of ways of selecting the first digit is 4.
However, if the last digit is selected is 2 or 6 the number of ways of selecting the first digit is 3 (that is 3, 5 or 6, 3, 4 or 5) Case 1: n = 1 n = 4 n =4, n =3 4 1 2 3 The number of 4 digits numbers that can be formed in this case is 4 x 4 x 3 x 1 = 48 Case II: n =2, n = 3n = 4, n = 3 4 1 2 3 22 The number of 4 digit numbers that can be formed in this case is 3 x 4 x 3 x 2 = 72 Hence, the total numbers of even 4 digit numbers greater than 3000 that can be formed is 48 +72= 120.
Example 1.16 The letters A,B,C, D and E are placed at random to form a five letters word (without repetition).
How many ways can a word be formed such that (i) D directly follows A.
(ii) A and D follow each other (iii) A, D and E follow each other Solution (i) For D to directly follow A, we must always have AD appearing in the word so formed AD can be regarded as a letter so that we now have the letters B,C E and AD.
The number of ways of rearranging tehse new four letters is 4!
Thus there are 4!
= 24 ways of forming a word such that D directly follows A (ii) A and D follow each other; either we have AD or Da each having 4!
Ways.
Thus there are 2x 4!
Ways of forming a word such that A and follow each other (iii) ADE can be regarded as a letter so that we now have the letters B.C ADE.
There are 3!
Permutation of the new 3 letters.
ADE can also be permuted in 3!
Ways, that is ADE, AED, DAE, DEA, EAD, EDA.
Thus there are 3!
X 3!
= 36 ways Permutation of Indistinguishable Objects Consider n objects where n are of type 1, of type 2,….
N of type k. ,in shown many 1 k ways can the n object be arranged.
For example in how many ways can the letters of the word book be arranged.
here n= 4, k = 3, n = 1, n = 2, n = 1.
First give the two O’s suffixes bo o k. then treating the O’s as 1 2 3 1 2 different, the 4 letters may be arranged in 4!
Ways.
In every distinct arrangement, the 23 2O’s may be rearranged amongst themselves in 2!
Ways without altering the permutation for instance o bo k are the same when the suffixes are removed.
1 2 Therefore, the number of permutations of the letters of the word book is 4!
= 6 2!
Book, ookb, oobk, obok, okob, koob.
In general, the number of ways in which n objects where n are of type 1, n of type 2…, 1 2 n of type k can be arranged is given by k n!
: n 2 k + n +…+ n n !n !...n !
1 2 k Example 1.17 How many distinct permutations are there of the letters of the word Television?
The ten letters to be permuted consist of 2e’s, 2i’s,IT, I, v, s, o, n. thus the number of distinct permutation is 10!
= 10!
2!
2!
1!
1!
1!
1!
1!
1!
2!
2!
1.3 Combinations Definition 1.5 Two groups are said to form different “combination” if they differ in the number of any kind of object in the groups.
Consider a group of 4 letters a, b, c, d. the combination abcd, bcda, cadb are identical combinations each of them contains the same number of a, b, c, d: one b, one c and one d. The combinations of the 4 letters taken 3 at a time are: Abc, acd, abd, cbd Therefore, there are 4 distinct combination for three letters from the four letters.
Each of tehse combinations has 3!
= 6 permutation.
For instance.
abc = abc, acb cab cba bac bca acd = acd adc cad cda dac dca abd = abd adb bad bda dab dba 24  cdb = cbd cdb bcd bdc dbc dcb The number of distinct permutation is 4P =24 3 The number of distinct combinations is 4.
Therefore.
Number of combinations = number of permutations 3!
The number of distinct combination of 4 objects takn 3 at a time is denoted by 4C .
3 Thus, 4C =4P 3!
3 3/ In general, nC =nP/r!= n!
r r (n-r)!r Theorem 1.1 The number of distinct combinations of n objects taken r at a time (that is the number of ways of choosing r objects out of n, disregarding order and without replacement) is given by Proof: There are nP = n!
r (n-r)!
permutations of n objects taken r at a time.
If we disregard order among the r objects, there are r!
permutation that will give the same combination.
Therefore the number of combinations is the number of permutation divided by r!
thus.
nC = nP = n!
r r r!
(n-r)!r!
Example 1.18 A club consist of 15 members.
In how many ways can a committee of 3 to be chosen?
Solution This can be done in 15C ways 3 15C = 15!
= 15 x 7 x 13 = 455 ways 3 25  3!
12!
3 Example 1.19 A club consist of 10 men and 5 women, in how many ways can a committee of 6 consisting of 4 men and 2 women be chosen.
The 4 men can be chosen from the 10 men in10C ways, the 2 women can be chosen from the 5 women in 5C ways.
Hence 4 2 the committee can be chosen in (by the fundamental principle f counting) 10C x 5C = 10!
X 15!
= 2,100 ways 4 2 6!4!
2!3!
Exmaple1.20 Suppose we have a box containing n balls of which r are black and the remaining white.
A random sample of size k is selected without replacement.
In how many ways can the sample be selected such that it contains x black balls The x black balls can be selected in rC ways and k-x white balls n be selected from n- x r white balls.
Thus, there are rC Xn-rC x k-r ways of selecting a sample such that it contains x black balls and k-x white balls Example 1.21 A committee of 4 men and 2 women is selected from 10 men and 5women.
If two of the men are feuding and will not serve on the committee together, in how many ways can the committee be selected Solution The number of ways of selecting 4 men and 2 women is 10C x 5C =2100 4 2 The number of ways of selecting the committee such that the two men are in the committee is 8C x 5C =280 2 2 26 Hence, the number of different committees that can be formed such that the two men are not in the committee together is 2100 – 280 = 1820 Another method is to consider 3 cases Case I: The two men say, A and B are not in the committee 8C x 5C = 700 4 2 Case II: A is the committee but not B: 8C x 5C = 560 4 2 Case III: B is the committee but not A 8C x 5C = 560 4 2 Thus, the total number of ways the committee can be selected is 700+ 560+ 560 = 1,820 Example 1.22 How man subsets can be formed, containing at least one member from a set of n element Solution There are nC subsets of size k that can be formed, the total number of subsets k containing is least one member is Example 1.23 A student is to answer 5 out of 8 question in an examination.
How many if he must answer at least 2 of the first for 4 questions?
Solution (i) By the combination law the answer of 8C = 56 5 (ii) Possible choices are (2,3) (3,2) (4,1) where (2,3) means answer 2 questions from the first 4 questions and 3 from the remaining 4 questions.
The number of ways of doing this is 4C x 4C 2 3.
Thus the answer is 27  (4C x4C ) + ( 4C x 4C ) + (4C x 4C )= 52 2 3 3 2 4 1 1.4 Partitioning Dividing a population or a sample of n objects into k ordered parts of which the first contain r object, the second r objects and so on is called ordered partition.
If the 2 division is into k unordered parts, then the partition is said to be unordered.
For example suppose a class contains 15 students and we want to divide the class into 3 tutorial groups of 5 each.
Three lecturers are available and each is to take each group.
In otherwise, we want to divide the 15 students into 3 ordered groups (A,B,C) this is an ordered partition since there are 3!
= 6 ways The lecturers can be assigned to take any partition, for instance groups (A, B, C) can be taken by (L , L , L ) or (L L , L ) or (L L , L ) 1 2 3 2, 1 3 1, 3 2 Or (L , L , L ) or (L L , L ) or (L L , L ) 2 3 1 3, 2 1 3, 1 2 Where L means lecturer I ad (L , L , L ) means L takes groups A, L takes group B, 1 1 2 3 1 2 L takes group C and so on 3 There are 15C ways of selecting those students those students to be in group A, 5 following this, there are 10 students left and so there are 20C selecting those to be in 5 the second group, therefore by the fundamental principle of counting, there are 15C x 10C x 5C = 15!
X 10!
X 5!= 15!
5 5 5 10!
5!
5!
5!
5!
5!
5!
5!
Ordered partitions.
The number of ways in which n object can be divided into ordered parts of which the fist contains r objects the second r objects and so on is 1 2 nC .
n-r C n-r1-r2C n-r1-r2-…-rk-1C r1 1 r2.
r3… rk = n!
.
(n-r )!
(n-r -r )!
(n-r -r …r n!
2 1 2 1 2 k-1!
= (n-r )!r !
r !
(n-r -r )!
(r !
)n r -r -r )!
R !
(n-r -r …r -r )!
r !r !r !...r !
1 1 2 1 2 3 1 2 3 k 1 2 k-1 k 1 2 3 k Where n = r +r + … + r .
1 2 k 28  Example 1.24 In how many ways can three committees of five, three and two persons be formed from 10 persons.
We seek the number of ordered partitions of the 10persons This is given by 10!
=2520 5!
3!
2!
Example 1.25 In how many ways can 9 toys be divided among three children if each gets 3 toys If the toys are numbered 1 through 9, the partition {(1, 2, 3), (4, 5, 6), (7, 8, 9}) means child A gets toys 1, 2, 3 child B gets toys (4, 5,6) while child C gets toys 8, 8, 9.
We distinguish between {(1, 2, 3), (4, 5,6) (7, 8, 9)} and {(4, 5, 6), (1, 2, 3), (7,8, 9) so these are ordered partitions.
Thus there are 9!
= 1680 ways 3!
3!
3!
When r = r for I = j, we can distinguish between ordered and unordered partition for 1 j example partition a set consisting of 8 objects numbered 1 to 8 into 3 parts of which the first contains 2 objects, the second 4 objects and the third 2 objects.
A partition is {(1, 2), (3, 4, 5,6), (7, 8)}.
In an ordered partition we distinguish between the partition {(1,2), (3,4, 5, 6), (7, 8)} and {(7, 8), (3, 4, 5, 6), (1, 2)} But they are the same for unordered partition.
Therefore, the number of unordered partition is 8!
.
1 = 210 2!
4!
2!
2!
Example 1.26 29 In how many ways can a family of 9 divide itself into 3 groups so that each group contains 3 persons?
Solution We are seeking for unordered partitions r = 3, r =3 r =3 1 2 3 The number of unordered partitions is 9!
1 = 280 3!
3!3!
3!
Since the three parts contain the same number of objects The same relationship that exists between permutation and combination exist between ordered and unordered partition of a set.
Example 1.27 In how many ways can a family of 10 be divided into three groups, one containing 4 and the others 3?
Solution R =4,r =, r =3 1 2 3 The number of ordered partitions is 10!
=4,200 4!
3!
3!
And the number of unordered partition is 10!
1 = 2,100 4!
3!
3!
2!
Since only two of the three parts contain the same number of objects 1.5 Selection of Non distinct Objects The number of ways r districts balls can be distributed into n cells in r. the number of ways if a specified cell contains exactly k balls (k = 0, 1,2…r) is rC (n-1)r-k k That is the balls can be chosen in rC ways and the remaining r-k balls can be paced k into the remaining n-1 cells in (n-1)r-k ways 30 Now, suppose the balls are non-district (indistinguis-shable) we can only talk about number of balls in the ith cells Let X , x …, x denote the number of balls in the ith cell, then 1 2 n x + x +…+x = r 1 2 n The number of distinct distribution in which no cell remains empty is r-1C n-1 to see this let us assume that the r non-distinct objects are lined by and bars used to divide them into groups.
The r balls r-1 space of which n-1 are to be occupied by bars.
For example if r =9 and n = 5, we have 0∩0∩0∩0∩0∩0∩0∩0∩0 Thus 0/00/0/000/00 corresponds to X =1,x = 2, x = 1, x = 3, x =2.
1 2 3 4 5 And there are 8C possible distribution of the bars.
another possible distribution is 4 000/0/00/00 If x > 0, that is cells can remain empty, the number of non-negative solutions of 1 X + X +…+ x =r 1 2 n Is the same as the number of positive solutions of Y + y +…+ y =r + n 1 2 n Where y = x +1.
Thus, there are 1 1 r + n-1C (1.2) n-1 Distinct solution satisfying x + x +… + x = r 1 2 n The number of distinct distribution is the number of ways n – 1 spaces can be selected out of the n + r-1 spaces Application to Runs Definition A run is any ordered sequence of elements of two kinds For example, by a run of wins we mean a consecutive sequence of wins.
The sequence WWWLWWLLWLWW gives 4 runs of wins.
The first run is length 3, the second run of length 2, the third of length 1 and the fourth of length 2.
31 Suppose now that we have two letters (W and L) n non-distinct letters (L) and m non- district letter (W).
the total number of distinct orderings of W and L is (n+m)C r r runs n- of W is equivalent to arranging the letters W into r cells none of which is empty.
If there are r runs on W , The number of L run is necessarily r+ 1, r-1 or.
Thus from (1.1) we have (m-1)C r- distinct was of having r runs of W. hence there are.
1 (m-1)C (n-1)C ways of having r runs of W and (r + 1) runs of 1… r-1 r A sequence representing r runs of W is LL… WW…W LL… WW…W … WW… L..L Y X Y X X Y 1 1 2 2 r r+1 Where y + y +… y = n, y> x + x+2+ +… + x = m, x> 0.
1 2 r + 1 i 1 r i Let y + 1, y = y, I = 2, …, r, y = y + 1 1 i i r+1 r+1 The number of non-negative solution to Y y + … + y = n, (y> 0) 1 2 1+1 i Is the same as number of positive solution to Y + y +…+ y = n +2 1 2 r+1 Thus, the number of outcomes that result n r runs is n+1 C r Hence, n-1Cm-1C is the total number of ways of having r runs of W r r-1 Exercise 1.3 1.
How many 4 digits numbers can be formed from the digits 0,1, 2, 3, 4,5, 6, 7,8, 9 if the first digit must not be 0 and repetitions is allowed.
how many of these number are (i) even (ii) less than 5,000?
2.
A code consist of five symbols.
The first three symbols are letters and the last two are digits.
How many codes can be made if no letter nor digit is repeated in any code word?
3.
Suppose n objects are permuted at random among themselves.
In how ways can this be done such that k specified objects occupy k specified positions 4.
How many distinctarrangements are there of the letters of word (1) University (ii) biology (iii) Mississippi 32 5.
The number 1,2,3… n are arranged in random order.
in how many ways can this be done such that (i) Q and 2 follow each other (ii) 2, 3 and 4 follow each other in that order 6.
Show that the number of distinct ordered sample of size r that can be drawn from a population with n objects is (i) Nr if sampling is with replacement and (ii) n1 if sampling is without replacement n –r!
7.
A total of n balls are randomly placed into n cells.
In how many ways can this be done.
In how many ways if each cell is occupied 8.
A warehouse has 6 different containers to be distributed among 10 retailers.
In how many ways can this be done?
How many way if no retailer receives more than one container?
9.
Prove the following identities a. nC = nC (b) m+1C = nC +nC x n-x x x x-1 10.
(a) In how many ways can 4 boys and 2 girls be arranged to sit in a row?
(b) In how many ways if only the boys must sit together?
(c) In how many ways if the boy and no girl must sit together?
(d) In how many ways if no boy and no girl must sit together?
11.
In howmany ways can a football team be selected from 15 players.
In how many ways if 6 particular players must be included in the team?
12.
From a box containing 5 red, 4 white and 3 black marbles, three marbles are drawn one after the other without replacement, how many ways can this be done if (i) all are white (ii) 2 re white and 1 is red (iii) at least one is black 13.
A pair of dice is rolled once.
In how many ways can (i) The sum of the two numbers appearing exceeds 8 (ii) The maximum of the two numbers is greater than 4 (iii) The minimum of the two numbers is greater than 4 33 14.
A disciplinary committee of four is to be chosen from six men and five women.
One particular man and one particular woman refuses to serve if the other person is on the committee.
How many committees may be formed 15.
Eleven people are to travel in two cars- salon and station wagon.
The saloon has 4 sets and the station going 7 seats.
In how many ways can party be split up?
16.
In how many ways can a committee of 6 composing of 3 full professors, 2 associate professors and 1 senior lecturer be selected from 5 full professor, 10 associate professors and 20 seniors lecturers 17.
A box contain 12 balls labeled 1, 2, 3… 12, suppose a random sample of size 4 is selected.
In how many ways can the sample be selected if balls labeled 2, 3, are among the four selected.
18.
Suppose a random sample of size r is drawn from a population of n objects.
In how many ways can this be done if k given object must be included in the sample and (a) sampling is without replacement (b) Sampling is with replacement 19.
I bought 2 tickets to a lottery for which n tickets were sold and 4 prizes to be given in how many ways can the tickets be drawn such that 1 win at least a prize?
20.
A committee of 8 is to be formed from 10 couples (10men and 10 women).
In how many ways can the committee be formed if no husband serves on it with his wife.
21.
Lines are drawn to pass through six points.
In how many ways can this be done if each line passes a through only two points?
22.
Interchanges may occur between any two of the n chromosomes of a cell (a) In how many ways can exactly one interchange occur?
(b) In how many ways can exactly k interchanges occur?
(c) If n = 5, in how many ways can at most three inter changes occur?
23.
Prove that (n1-n2)C = n1Cn2C 1 2 k r k-r where k < n , n HINT: Select k objects from n + n obejcts of n of type 1 and n of typeII 1 2 1 2 24.
A company is considering building additional warehouse at new locations.
There are ten satisfactory location and the company must decide how many and which ones to select.
How many choices are there?
34 25.
Are there more samples obtainable in five draws from 10 objects with replacement than 12 objects without replacement?
Each of fifty items is tested and found to be defective or non-defective.
How many possible outcomes are there?
MODULE TWO UNIT ONE ELEMENTARY PRINCIPLE OF THE THEORY OF PROBABILITY In chapter 1, we considered some elementary methods of mathematics of counting essential for determining probabilities of events.
In this chapter, we continue our study by discussing how to apply the knowledge gained in chapter 1 to determine probabilities of events and general properties of probabilities.
We present some theorems and definitions that are basic to the understanding of commonly encountered problems.
Example 2.1 Suppose a fair die is rolled once.
There are six possible outcomes.
The sample space is { } W = 1,2,3,4,5,6. let A , A and A , represent the following events 1 2 3 A = “an even number occurs” 1 35  A = “an odd number occurs 2 A = a prime number occurs” 3 The six possible outcomes in Ω are equally likely.
The probability that A occurs is denoted by P(A) and the probability that A does not occur by P(AC).
if we let n be the A number of outcomes that have attribute A, then A ={2,4,6} 1 A ={1,3,5} 2 A ={2,3,5} 3 Thus, nA 3 3 P(A ) = 1 = 3/6 =1/2, p(A )= , P(A ) = 1 nW 2 6 3 6 Example 2.2 Suppose that a box contains 10 items of which are defective.
Two items are selected at random without replacement.
Find the probabilities that: (i) both items are non-defective (ii) only one item is defective (iii) both items are defective: (iv) at least one item is defective Let A , A , A , A , denote the events 1 2 3 4 “both items are non-defective “only one item is defective” “both items are defective” “at least one item is defective” respectively.
The number of ways of selecting 2 items from 10 is 10C =45ways.
2 36 So there are 45 elements in the sample space.
(i) The number of ways of selecting 2 items from the non-defective items is 6C = 15.
2 That is A can occur in 15 ways.
Thus.
1 P(A ) = * 1 +, 01- 34 1 -.+, / 02 / 54 / 73 (ii) The number of ways of selecting 1 item from the 4 defective items and 1 item from the 6 non-defective items is 4C × 6C = 24 so, A can occur in 2 ways.
1 1 2 Thus, P(A ) = 8 * 2 +- 9 +- :5 ; -.+, / 54 / 34 Similarly (iii) P(A ) = 8 3 +, < : -.
/ / +, 54 34 (iv) At least one defective means 1 or 2 defective items.
So, P(A ) = P(1 defective item) + P(2 defective items) = P(A ) + P(A ) = .
4 2 3 ; : 2 = / 7 34 34 3 Example 2.3 Suppose a fair die is rolled twice.
Find the probability that the sum of the numbers on the two faces is (i) even, (ii) less than 5.
The sample space Ω, consists of 36 elements.
(i) Let A be the event “the sum of the two faces is even”.
Possible outcome are: (1,1) (1,3) (1,5) (2,2) (2,4) (2,6),… (3,1) (5,1) (4,2) (6,2),… A can occur in 18 ways.
Thus, 37  P(A) = 3; / 1⁄2 >< (ii) Let B be the event “the sum is less than 5”.
B occurs if the sum is 2, 3, or 4.
The sum is 2 if the outcome is (1,1) the sum is 3 if the outcome is (1,2) or (2,1) and the sum is four if the outcome is (3,1), (1,3) or (2,2).
Therefore B can occur in 1 + 2 + 3 = 6 ways Thus P(B) = .
< 1 / 7 >< 6 Example 2.4 3 balls are drawn at random with replacement from a box containing 8 red and 3 white balls.
Find the probability that (i) all 3 are red; (ii) 1 is red and 2 are white.
The sample space consists of 113 possible outcomes.
Let A be “the evet all 3 are red” and B the event “1 is red and 2 white” P(A) = A 01 ; / A 02 33 (The first red can be chosen in 8 ways the second in 8 ways and the third Red in 8 ways).
P(B) = , nB = 8,3.3 + 3.8.3 + 3.3.8 0B 02 (8 × 3 × 3) = number of ways of picking the first to be red, second white and third white (RWW), 3 × 8 × 3 is for WRW and 3 × 3 × 8 is for WWR).
Thus P(B) = >.
;.>.> A .
33 2.1 Properties of Probability Definition 2.1: Mutually Exclusive events Two events A and A are mutually exclusive if an only if 1 2 38  A A = for every i ≠ j.
1 j That is, two or more events are mutually exclusive if no two of them have points in common.
In example 2.1 A = {2, 4, 6} 1 A = {1, 3, 5} 2 A A = , 1 2 Therefore A and A are mutually exclusive events.
1 2 Definition 2.1 If A and A are any two events, A A is the event that occurs if and only if both A 1 2 1 2 1 and A occur.
In general, if A A …, A occur.
2 1, 2, n In example 2.1.
A A A is the event that an even prime occurs.
1 2 3 A A = {2} 1 3 Thus The event A A is the event that an odd prime occurs A A = {3,5}.
2 3 2 3 Thus P(A A ) = 1 2 Complementary Event 39 The event “A occurs” and “A does not occur” are mutually exclusive.
The event “A does nt occur” is called the complement of A and is denoted by AC.
Theorem 2.1 If AC is the complement of an event A, then P(Ac) = 1 – P(A).
This theorem states that the probability that an event will not occur is equal to 1 minus the probability that it will occur.
In example 2.2 (iv); The event “no defective item” is the same as the event both items are non-defective”.
From Theorem 2.1, we have P(A ) = 1 – P( 4 A c = A since the event “no defective item” is the same as the event” both items are non- 4 1 defective.
The following events are complementary events (i) at most 2 and greater than 2 (ii) at least 4 and less than 4.
Definition 2.2 If A and A are any two events, A A is the event that occurs if t least one of A and 1 2 1 2 1 A occurs.
2 In general, if A , A ,…, A are any events, A A is the event that occurs if at least one 1 2 n 1 2 of A i = 1, 2,…, n, occurs.
1 In example 2.1, the event A A is the event that an even or a prime number occurs.
1 3 A A = {2, 3, 4, 5, 6}.
1 3 40 Thus P(A A ) = = 1 2 The event A A is the event that an event or an odd number occurs.
1 2 A A = {1, 2, 3, 4, 5, 6} = Ω, 1 2 thus P(A A ) = P(Ω) = = 1.
1 2 Theorem 2.2 If A and A are any events, then 1 2 P(A A ) = P(A ) + P(A ) – P(A A ).
1 2 1 2 1 2 The proof of this theorem follows directly from the definition dividing through by nΩ the result follows.
Corollary (i) If A and A are mutually exclusive event then 1 2 P(A A ) = P(A ) + P(A ) 1 2 1 2 since A and A are mutually exclusive, 1 2 A A = 1 2 ϕ hence 41  P(A A ) = 0 1 2 (ii) If A and A are any two events 1 2 P(A A ) ≤ P(A ) + P(A ) 1 2 1 2 In Example 2.1 above, P(A A ) = P(A ) + P(A ) - P(A A ) = = .
1 3 1 3 1 3 (iii) If A and B are any two events defined on the sample space then P(A) = P(A B) + P(A Bc) P(B) = P(A B) + P(Ac B) In general, if A , A ,…A are any n mutually exclusive events, then 1 2 n P(A A … A )= P(A ) +…+ P(A ).
1 2 n 1 n Theorem 2.2 can be generalized for n > 2 events.
It can easily be shown that for any 3 events A , A , A 1 2 3 P(A A A ) = P(A ) + P(A ) + P(A ) - P(A A ) 1 2 3 1 2 3 1 2 - P(A A ) – P(A A ) +…+ P(A ).
1 3 2 3 3 To see this, let B = A A , then 1 2 P(A A A ) = P(B A ) = P(B) + P(A ) – P(B A ) 1 2 3 3 3 3 P(B) = P(A ) + P(A ) + P(A ) – P(A A ) 1 1 2 1 2 P(B A ) = P(A B) = P{(A (A A )} = P{(A A ) (A A )} 3 3 3 1 2 3 1 3 2 = P(A A ) + P(A A ) + P(A A ) – P(A A A ) 3 1 3 1 3 2 1 2 3 42 (by theorem 2.2).
thus P(A A A ) = P(A ) + P(A ) – P(A A ) + P(A ) 1 2 3 1 2 1 2 3 - P(A A ) – P(A A ) + P(A A A ) 1 3 2 3 1 2 3 Note: A A A is the event that at least one of them will occur.
1 2 3 Definition 2.3 Two or more events are said to be exhaustive if their union equals the whole sample space.
In other words, A , A ,…, A are exhaustive events if 1 2 n P(A A … A ) = 1 1 2 n In other words it is certain that at least one of them will occur.
Note: 1.
0 ≤ P(A) ≤ 1 2.
P(Ω) = 1.
3.
For an sequence of mutually excusive events A , A ,… 1 2 Examples: 2.5 A box contains 6 balls numbered 1 to 6.
A ball was drawn from the box at random.
Find the probability that the number on the ball drawn was either 1, 2, or 6.
Solution 43 Let A , A , A denote the events that the ball drawn was 1, 2 and 6 respectively.
1 2 3 A A A denote the event the number on the ball drawn was either 1, 2 or 6.
1 2 3 A , A and A are mutually exclusive events.
Thus 1 2 3 P(A A A ) = P(A ) + P(A ) + P(A ) 1 2 3 1 2 3 =1/6 + 1/6 + 1/6 = 12.
2.6 Four fair dice are tossed once, what is the probability that the sum of the numbers on the four dice is 23?
Solution The possible outcomes to get 23 are (6,6,6,5), (5,6,6,6), (6,5,6,6), (6,6,5,6).
The sample space Ω consists of 64 elements.
Therefore, the probability that the sum of the numbers on the four dice is 23 is 4/64.
2.7 If P(A ) = 2/3, P(A A ) = ¼ and P(A A ) = 5/6, find P(A ).
1 1 2 1 2 2 Solution From the addition law of probability P(A A ) = P(A ) + P(A ) – P(A n A ) 1 2 1 2 1 2 We have P(A ) = P(A A ) P(A n A ) – P(A ) = 5/6 + ¼ - 2/3 = 5/12.
2 1 2 1 2 1 2.8 Suppose a fair on is tossed three times, what is the probability that at least one head occurs?
Solution 44 Let A be the event that the first toss lands heads, A the event that the second toss 1 2 lands, and A the event that the third toss lands heads.
3 A A A is the event that at least one head occurs.
1 2 3 We are required to compute P(A A A ) The complement of A A A is 1 2 3 1 2 3 A c n A c n A c. That is, the first does not land heads, the second does not and the 1 2 3 third does not.
A c n A c n A c = {T, T, T}.
1 2 3 Thus P(A c n A c n A c) = 1/8.
1 2 3 Hence P(A A A ) = 1 – 1/8 = 7/8.
1 2 3 2.9 If A A , then P(A ) ≤ P(A ) 1 2 1 2 A = A (A /A ).
2 1 2 1 Since A and A /A are mutually exclusive, we have 1 2 1 P(A ) = P(A ) + P(A /A )≥ P(A ) 2 1 2 1 1 Since P(A /A ) ≥ 0.
2 1 2.9 If A , A ,…, A are n events, then 1 2 n = From de Morgan’s law, we have = hence = 2.2 Conditional Probability The conditional probability of an event A given that an event B has occurred is denoted as P(A|B).
the word “given” is represented by the upright stroke.
We wish to determine 45 the probability that an event A will occur “conditional on” the knowledge that another event B has occurred.
Suppose a fair die is rolled and it is known that an even number appeared uppermost.
Let A be the event that the number was greater than 3 and B the event that the number that appeared was even.
The problem is to find the conditional probability that the event A occurred given that the event B has occurred, P(A|B).
since we know that the number was even, the number must be either 2,4 or 6.
Therefore, the conditional sample space contains 3 elements.
The event A occurs if the number showing is 4 or 6, thus P(A|B) = 2/3.
We can therefore define conditional probability of A given Bas the number of ways AnB can occur divided by number of elements in the conditional sample space.
That is, , Where ≠ 0, where Ω is the condtional sample space given that B has occurred.
B Divide the numerator and denominator by nΩ = In the above example, n(A B) = 2 n(B) = 3 Definition 2.4 Let A and B be two events such that P(B) > 0.
Then the conditional probability of A given B, denoted by P(A|B) is defined to be P(A|B) = 2.2 Examples 2.11 Suppose a box contains 4 red balls and 3 black balls.
Compute the probability that (i) the second ball drawn is red if the first ball drawn was red; without replacement, (ii) the second ball drawn is red if the first ball drawn was black 46  Solution If the first ball drawn is red, there remains 6 balls, 3 red balls and 3 black balls.
The probability of the second ball being red is 3=1 .
But if the first ball is black, 6 2 the box is left with 4 red and 2 black so the probability of the second ball being red is then 4/6 = 2/3.
Thus P(2nd is re/first was red) = 3/6/ = ½ P(2nd is red/ first was black) = 4/6 = 2/3.
This shows that the probability of the event “the second ball drawn is red” depends on the colour of the first ball drawn.
2.12 Suppose two fair dice are rolled.
If the sum of the numbers appearing is 6, what is the probability that one of the number is 2?
Solution Let A be the event “one of the numbers is and B the sum is 6. there are five ways for the event B to occur: (3,3), (2,4), (4,2), (5,1) and (1,5) and there are two ways for the event AnB to occur: (2,4) and (4,2).
Thus, P(A˙ B) =2 36,P(B)=5 36 Hence, P(A˙ B) P(AI B) = =2 5.
P(B) 47 2.13 There are two children in a family.
If there is at least a girl in this family, what is the conditional probability that both are girls.
Solution The sample space is W ={BB,GB,BG,GG} Let A be the event “both children are girls and B ”a least a girl in the family.
B = {GB, BG, GG}, A = {GG}, A∩B = {GG}.
P(AnB) 1 4 1 P(AI B) = = = .
P(B) 3 4 3 2.14 There are three children in a family.
If there is at least one boy and at most two boys in this family.
What is the conditional probability that there are exactly two boys in this family.
The sample space is Ω = {BBB, BBG, BGG, BGB, GBB, GBG, GGB, GGG} Let B be the event “at least one boy and at most 2 boys in the family” and let A be the event “exactly two boys in the family”.
Then B = {BBG, BGG, BGB, GBB, GBG.
GGB} A∩B = {BBG, BGB, GBB}.
Therefore P(AnB) 3 8 3 1 P(AI B) = = = = .
P(B) 6 8 6 2 Exercises 2.1 1.
A fair die is thrown twice.
48 (i) If it is known that the sum of the numbers appearing was 8, what is the probability that the difference between the two numbers was 2.
(ii) If it is known that the difference the two numbers was 3, what is the probability that the sum of the two numbers was 7?
2.
Two unbiased dice are thrown once.
What is the probability that (i) at least one 5 is thrown.
(ii) the sum is 10 (iii) the sum is 10 given that no 5 is thrown?
3.
Suppose events A and B are sun that P(A) = 1/5, P(AnB) = 1/6.
Find (i) P(BIA); (ii) P(Ac¨ Bc).
4.
If A and B are two events defined on the same probability space, show that: (i) P(A) = P(A∩B) + P(A ∩ Bc) = P(B) + P(A ∩ Bc) – P(Ac ∩ B).
5.
Prove that P(Ac ∩ Bc) = 1 – P(A) – P(B) + P(A ∩ B).
6.
Suppose a well balanced coin is tossed twice.
Find the conditional probability that (i) both coins show a tail given that the first shows a head; (ii) both are heads given that at least one of them is a head.
7.
A red die and a green die are rolled once.
Find the conditional probability that: (i) the number on red die is odd, given that the sum of the two numbers showing is 9; (ii) the sum of the two numbers is 9 given that one of the numbers is odd and the other even?
49 2.3 Bayes Theorem suppose a box contains r red balls and b black balls.
Two balls are drawn at random without replacement.
Assume that the probability of drawing any particular ball is 1 .
r+b Let A be the event “the first ball drawn is red and let A be the event” the second ball 1 2 drawn is red.
Then r r- 1 P(A )= ,P(A I A )= 1 r+b 2 1 r+b- 1 ( ) r P A I Ac = 2 1 r+b- 1 The probability of the event A depends on A and A c. 2 1 1 That is A is equivalent to A n A ∩ A c 2 2 1 1 A = A ∩ A or A c ∩ A .
2 1 2 1 2 Therefore, P(A ) = P(A ∩ A ) + P(A ∩ A c) = P(A ) P(A ) P(A I A ) + P(A c) P(A I A c).
2 2 1 2 1 1 1 2 1 1 2 1 Theorem 2.3 If the probability of an event B, depends on k mutually exclusive and exhaustive events A , A +…+ A , then 1 2 k P(B) = P(B ∩ A ) + P(B ∩ A ) +…+ P(B ∩ A ) 1 2 k k k = ∑P(B˙ A )=∑P(A )P(BI A ).
1 1 1 i=1 i=1 Example 2.15 Suppose a box contains 3 red balls, 2 black balls and 5 green balls.
Two balls are drawn at random without replacement.
Find the probability that the second ball drawn is red.
Let A be the event “the first ball drawn is red A the event the first ball drawn is black and 1 2 50 A , the event “the first ball drawn is green.
Let B be the event “the second ball drawn is 3 red.
The event B occurs if (i) the first ball is red and the second is red or (ii) the first ball is black and the second is red or (iii) the first ball is green and the second is red.
Thus B = B ∩ A or B ∩ A or B ∩ A , P(B I A ) = 2/9, P(BI A ) = 3/9, 1 2 3 1 2 P(BI A ) = 3/9.
3 P(B) depends on A , A , A which are mutually and exhaustive events.
Therefore, 1 2 3 P(B) = P(B ∩ A ) + P(B ∩ A ) + P(B ∩ A ) 1 2 3 = P(A ) P(BI A ) + P(A ) P(BI A ) + P(A ) P(BI A ) = 3/10.2/9 + 2/10.3/9 + 1 1 2 2 3 3 5/10.3/9 since P(A ) = 3/10, P(A ) = 2/10, P(A ) = 5/10.
1 2 3 Thus P(B) = 1/90 (6 + 6 + 15) = 27/90 = 3/10.
Example 2.16 Suppose a factory has three machines M , M , M which produce 60%, 30% and 10% of 1 2 3 the total production respectively.
Of their output, machine M produces 2% defective 1 items, machine M produce 3% defective items while machine M produces 4% defective 2 3 items.
Find the probability that a part selected at random is defective.
51 Solution Let B be the event “a part selected at random is defective”.
A defective item could have been produced by either machine M , M or M .
Thus 1 2 3 B = (B ∩ M ) U (B ∩ M ) U (B ∩ M ) 1 2 3 Since (B ∩ M ), (B ∩ M ), (B ∩ M ) are mutually exclusive events.
The following 1 2 3 information is contained din the equation.
P(M ) = 60% = 0.6, P(M ) = 0.3, P(M ) = 0.1 1 2 3 P(B|M ) = 0.02, P(B|M ) = 0.03, P(B|M ) = 0.04 1 2 3 Hence P(B) = P(M ) P(B|M ) + P(M ) P(B|M ) + P(B|M ) P(M ) 1 1 2 2 3 3 =(0.6 × 0.02) + (0.3 × 0.03) + (0.1 × 0.04) = 0.002 + 0.009 + 0.004 = 0.025. suppose you are now asked, what is the probability that a given defective part was produced by machine M .
that is, you are to find P(M |B) =P (a part was produced by 1 1 machine M given that the part was defective).
1 NOTE: 1.
P(M | B) ≠ P(B|M ), but 1 1 2.
P(M n B) = P(BnM ) 1 1 3 ∑ P(B) = P(M nB) 1 i=1 hence, P(M nB) P(M )P(B|M ) P(M |B) = i = i i 2.4 1 3 3 ∑ ∑ P(M nB) P(M )P(B|M ) i i i i=1 i=1 52 Thus, 0.6· 0.02 0.012 P(M |B) = = = 0.48 1 0.025 0.025 0.3· 0.03 0.009 P(M |B) = = = 0.36 2 0.025 0.025 0.1· 0.04 0.004 P(M |B) = = = 0.16 3 0.025 0.025 Equation (2.4) is an example of Bayes theorem which may be stated as follows.
Bayes Theorem If A , A ,…A are set of mutual exclusive and exhaustive events in a sample space Ω and 1 2 k B is any other event in Ω such that P(B) > 0, then P(A )P(B|A ) P(A|B) = i i ;i=1,2,...,k 2.5 i k ∑ P(A )P(B|A ) i i i=1 Example 2.17 Suppose a college is composed of 70% male and 30% female students.
It is known that 40% of the male students and 20% of the female students smoke cigarette.
Find the probability that a student observed smoking a cigarette is male?
Let M,F denote male and female respectively and S denotes smoker.
The above problem contains the following information.
P(M) = P(A student selected at random is male) = 0.7 P(F) = P(A student selected at random is female) = 0.3 P(S|M) = P(a student selected at random smokes given that the selected student is male) = 0.4 53 P(S|F) = P(a student selected at random smokes given that the selected student is female) = 0.2 By Bayes theorem, we have P(M|S) = P(a student observed smoking is male) = P(A student selected at random is male given that the selected student is a smoker) P(M nS) = P(S) Where P(S) = P(a student selected at random is a smoker) = P(S ∩ M) + P(S ∩ F) = P(M)P(S|F) = (0.7 × 0.4) + (0.3 × 0.2) = 0.28 + 0.06 = 0.34 Thus P(M)P(S |M) 0.7· 0.4 0.28 14 P(M|S) = = = = .
P(S) 0.34 0.34 17 Example 2.18 A table has drawers.
Drawer 1 contains two red and five black biros, drawer II contains four red and three black biros and drawer III contains one red and six black biros.
A drawer is chosen at random and a biro is chosen from the drawer.
Find the probability that (i) the biro chosen is black (ii) the biro chosen is from drawer I if the chosen biro is black.
Solution 54 Let P(i) denote the probability that drawer I is selected (I – 1, 2, 3) and R and B representing red and black biros respectively.
Then P(1) = 1/3, P(2) = 1/3, P(3), = 1/3 P(B|1) = 5/7, P(B|2) = 3/7, p(B|3) = 6/7 From equation (2.3) (i) P(B) = P(Bn1) + P(Bn2) + P(Bn3) = P(1)P(B|1) + P(2)P(B|2) + P(3)P(B|3) = 1/3.5/7 + 1/3.3/7 +1/3.6/7 = 1/3 (5/7 + 3/7 + 6/7) = 2/3.
(ii) P(1|B) = P(the biro chosen came from drawer 1 given that the chosen biro is black).
P(1˙ B) P(B˙ 1) P(1)P(BI1) 1.5 5 = = = = 3 7 = P(B) P(B) P(B) 2 14 3 Definition 2.5 Let A, B and C be three events such that P(C) > 0. then the conditional probability of A ¨ B given C is defined by P(A¨ B|C) = P(A|C) + P(B|C) – P(A ∩B|C).
Exercise 2.2 1.
Suppose that a box contains 5 balls labeled 1 to 5. two balls are drawn at random (one after the other without replacement).
What is the probability that (i) the sum of the numbers on the two balls selected is even?
55 (ii) The number on the first ball drawn is even if it is know that the sum of the two numbers is even.
2.
In a large population, it is observed that 30 percent of the people that are black have caner and 25 percent of the people are not black have cancer.
Assume that 10 percent of the population is black.
What is the probability that a person selected at random and found to have cancer is not black.
3.
A vaccine produces immunity against smallpox in 98 percent of cases.
Suppose that, in a large population, 20 percent have been vaccinated.
Find the probability that a person who contracts smallpox has been vaccinated, assuming that a vaccinated person without immunity has the same probability of contracting smallpox as an unvaccinated person.
4.
In a factory machines A,B,C produce respectively 20, 30 and 50 percent of the total production.
Of their output 3, 4, 5 percent respectively are defective items.
An item is drawn at random from the total production and is found defective.
What are the probability that it was produced by machine A, B,C?
5.
In a faculty of a certain college, 60% of the students are female: 20% of the females and 50% of the male are studying mathematics.
If a student data card is selected at random and the student is found to be studying mathematics, what is the probability that the selected student is a male?
6.
In JAMB examination each question has 5 possible answers, exactly one of which is correct.
If a student knows the answer he selects the correct answer.
Otherwise he selects one answer at random from the 5 possible answers.
Suppose that the student knows the answer to 70% of the questions.
(i) What is the probability that on a given question the student gets the correct answer?
(ii) If the student gets the correct answer to a question, what is the probability that he knows the answer?
56 (iii) what is the expected score of the student in the examination?
7.
A television set retailer finds out that 80% of this customers buy coloured T.V., and that 4 out of every 20 customers who buy coloured T.V set also buy antenna.
Calculate the probability that: (i) a randomly selected customer buys antenna (ii) a randomly selected customers who buys antenna has bought a colored T.V (iii) a randomly selected customer who has not bought an antenna has bought a colored T.V set.
8.
In a factory, machines A, B and C produce 20, 30 and 40 percent of the total output, respectively.
Of their output 5,4 and 3 percent respectively are defective bolts.
A bolt is chosen at random and found to be defective.
What is the probability that the bolt came from machine (i) A (ii) B (iii) C?
9.
Four percent of an article manufactured by a company are defective.
All the articles produced are regularly inspected and those found defective are rejected.
It is found that one out of every eight of defective article produced is missed by the inspector, while every good article passes inspection.
What is the probability that a customer buys a defective article produced by the company?
2.4 Independence The notion of independence is a basic tool of probability theory.
Consider tossing a die twice, and let A be the event that the first toss gives an even number and A 1 2 the event that the second toss gives an even number.
The event A ∩ A is the 1 2 event that both tosses give even numbers.
The occurrence of A does affect the 1 probability of A occurring.
2 57 Therefore P(A |A ) = P(A ) and P(A ∩ A ) = P(A )P(A |A ) = P(A )P(A ).
2 1 2 1 2 1 2 1 1 2 Hence, we say that two events A and A are independent if 1 2 P(A ∩ A ) = P(A )P(A ).
1 2 1 2 Example 1.19 Toss a fair die twice and let A be the event “the first toss shows 3” and B be the event the sum of the two numbers showing is 7.
P(A) = B ≡{(1,6), (6, 1), (2, 5), (3, 4), (4, 3), (5, 2)} P(B) = Therefore, P(A ∩ B) = Thus P(A ∩ B) = P(A)P(B) hence, A and B are independent events.
In general case of n events, we have the following definition.
Definition 2.6 The events A , A ,…A are independent if 1 2 n (i) P(A ∩ A) = P(A)P(A) for all I ≠ j i j i j (ii) P(A∩ A ∩ A ) = P(A)P(A)P(A ) for all I, j, k such that i ≠ j ≠ k i j k i j k (2.7) 58 (n-1) P(A ∩ A ∩ A … A ) = P(A P(A )P(A )…P(A ) 1 2 3 n 1 2 3 n That is, the events A , A ,…A are said to be mutually independent if 1 2 n P(A A …A ) = P(A )P(A )…P(A ) n1 n2 nk n1 n2 nk For every subsequence of two or more events.
A , A …A is a subsequence of A , A …A if the subscript integers satisfy n1 n2 nk 1 2 n 1 ≤ n < n …n ≤ n. 1 2 k If n = 3, we have, A , A , A are independent if 1 2 3 (i) P(A ∩ A ) = P(A )P(A ) 1 2 1 2 P(A ∩ A ) = P(A )P(A ) 1 3 1 3 P(A ∩ A ) = P(A )P(A ) 2 3 2 3 (ii) P(A ∩ A ∩ A ) = P(A )P(A )P(A ) 1 2 3 1 2 3 Condition (i) is called pairwise independent.
We might think that pairwise independence always implies independence.
But this is not necessarily so, as illustrated by the following example.
Example 2.20 Let a pair of fair dice be rolled once.
Consider the events.
A number appearing on the 1 first die is even, A = the number appearing on the second die is odd = {1,3, 5} and A = 2 3 the difference of the two numbers is even A ≡ {(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (4, 1), (4, 2),…, (6, 1), (6, 2),…} 1 A ≡ {(1, 1), (1, 3), (1, 5), (2, 1), (2, 3), (2, 5), (3, 1),…} 2 A ≡ {(1, 1), (1, 3), (1, 5), (2, 2), (2, 4), (2, 6), (3, 1), (3, 3), (3, 5), (4, 2), (4,4), 3 (4, 6), (5, 1), (5, 3), (5, 5), (6, 2), (6, 4), (5, 6)} A ∩ A ≡ {(2, 1), (2, 3), (2, 5), (4, 1), (4, 3), (4, 5), (6, 1), (6, 3), (6, 5)} 1 2 A ∩ A ≡ {(1, 1), (1, 3), (1, 5), (3, 1), (3, 3), (3, 5), (5, 1), (5, 3), (5, 5) 2 3 A ∩ A ∩ A = Φ.
1 2 3 Thus 59  P(A ) = ½, P(A ) = ½, (P(A ) = ½ 1 2 3 P(A ∩ A ) = , P(A ∩ A ) = , P(A ∩ A ) = 1 2 1 2 2 3 P(A ∩ A ) = P(A )P(A ; P(A ∩ A ) =P(A )P(A ); P(A ∩ A ) = P(A )P(A ) 1 2 1 2 1 3 1 3 2 3 2 3 That is, A , A , A are pairwise independent but 1 2 3 P(A ∩ A ∩ A ) = 0 ≠ P(A )P(A )P(A ) = 1 2 3 1 2 3 Since condition (ii) is not satisfied, we conclude that A , A and A are not independent.
1 2 3 Definition 2.7 If the events A , A …A are independent then 1 2, n P(A ∩ A … A ) = P(A )P(A )…P(A ) (2.8) 1 2 n 1 2 n Example 2.21 A man fires 10 shots independently at a target.
What is the probability that he hits the target (i) 10times; (ii) at least once If he has probability 1/3 of hitting the target on any given shot.
(i) Let A be the event “he hits the target at the ith shot” (I = 1, 2, 3,…10) i A ∩ A ∩ …∩ A is the event he hits the target 10 times A , A ,… A are 1 2 10 1 2 10 independent events.
Therefore, the probability of hitting the target 10 times is P(A ∩ A ∩…, ∩ A ) = P(A )P(A )…p(A ) = 1 2 10 1 2 10 (ii) P(hitting the target at least once) = 1 – P(not hitting the target at all).
P(not hitting the target at all) = P(A ∩ A ∩ … ∩ A ) 1 2 10 Where, A ≡ not hitting the target at the ith shot.
A , A , …, A are independent i 1 2 10 events and P(A) = 1 - = .
i Hence P(Ā ∩ Ā ∩ … ∩ Ā ) = P(Ā )P(Ā )…P(Ā ) = 1 2 10 1 2 10 Examples 60 2.22 Suppose a box contains 5 red and 3 black balls.
A ball is chosen at random from the box and then a second ball is drawn at random from the remaining balls in the box.
Find the probability that (i) both balls are black (ii) both balls are red (iii) the first ball is black and the second is red (iv) the second is red (v) the second is black.
Solution Let A be the event ”the first ball is red 1 Ā be the event “the first ball is black 1 Ā be the event ”the second ball is red 2 Ā be the event “the second ball is black 2 (i) P(both balls are black) = P(Ā ∩ Ā ) 1 2 P(Ā ∩ Ā ) = P(Ā )P(Ā ) Ā ) 1 2 1 2 1 D P(Ā ) = P(Ā Ā ) = 1 2 1 D Thus P(Ā ∩ Ā ) = 1 2 (ii) P(both balls are red) = P(A ∩ A ) 1 2 P(A ∩ A ) = P(A )P(A ) A ) 1 2 1 2 1 D P(A ) = P(A A ) = 1 2 1 D Thus P(A ∩ A ) = 1 2 (iii) P(A ∩ A ) = P(first ball is black and the second ball is red) = P(Ā )P(Ā ) Ā ) 1 2 1 2 1 P(Ā ) = P(A Ā ) = 1 2 1 D Thus P(Ā ∩ A ) = 1 2 61  (iv) P(A ) = P(A ∩ A ) + P(A ∩ Ā ) 2 2 1 2 1 From (ii) and (iii) we have (v) P(Ā ) = P(A ∩ Ā ) + P(Ā ∩ Ā ) = 2 1 2 1 2 2.23 Two fair dice are rolled.
Given that the dice show different numbers, what is the probability that at least one die shows a 6?
Solution Let A be the event: the dice show different numbers.
A ≡ {(1,2), (2,1), (1, 3), (3, 1), (1, 4), (4, 1), (1,5), (5, 1), (1, 6), (6, 1), (2, 3),…(5, 6), (6, 5) B ≡ At least one die show a 6.
≡ {(1,6), (6,1), (2, 6), (6, 2), (3, 6), (6, 3), (4,6), (6, 4), (5, 6), (6, 5)} A ∩ B = {(1,6), (6,1), (2, 6), (6, 2), (3, 6), (6, 3), (4,6), (6, 4), (5, 6), (6, 5)} P(B A) = D 2.24 Let A and B be any two events defined on the same sample space.
Suppose P(A) = 0.3 and P(A B) = 0.6.
Find P(B) such that (i) A and B are independent (ii) A and B are mutually exclusive.
Solution (i) If A and B are independent, then P(A ∩B) = P(A)P(B) Thus P(A B) = P(A) + P(B) – P(A)P(B) We have 0.6 = 0.3 + P(B) – 0.3 P(B) = 0.3 + 0.7 P(B) 62  P(B) = = 0.43 (ii) If A and B are mutually exclusive, then P(A ∩ B) = 0 Thus P(A B) = P(A) + P(B) 0.6 = 0.3 + P(B) P(B) = 0.3 2.25 Two women A, and B share an office with a single telephone.
The probability that any call will be for A is .
Suppose that A is out of her office during the office hours half of the time and B one third.
Find the probability that forany call during the working hours (i) no one is in to answer the call (ii) A call can be answered by the person being called (iii) Two successive calls are for the same woman (iv) A caller who wants A has to try more than two times to get her.
Solution (i) P(A and B are not in the office) = (ii) P(A call can be answered by the person being called) = P(call for A and A is in the office) + P(call for B and B is in the office) = (iii) P(for AA) + P(for BB) = (iv) P(X > 2) = 1 – P(X = 1) – P(X = 2).
Where X number of time for a caller who wants A has to try to get her.
Thus, P(X > 2) = 1 - Exercise 2.3 63 1.
(a) Show that if A and B are independent events, then (i) A and Bc, (ii) Ac and Bc are also independent.
2.
Let A and B denote two independent events such that A is a subset of B. prove that either P(A) = 0 or P(B) = 1.
3.
A man fires 10 shots independently at a target.
The probability of hitting the target at any shot is 1/3.
Calculate the probability that (i) noneof the shots hits the target (ii) At least one shot hits the target (iii) The target is hit at least twice if it is know that it is hit at least once.
4.
A box contains 6 red balls and 4 white balls.
Three balls are drawn from the box one after the other without replacement.
Find the probability that (i) the first two are white and the third red (ii) the first two are white and the third white.
(iii) two are red and one is white (iv) the second ball drawn is red (v) the third ball drawn is white 5.
A die is rolled 8 times.
What is the probability that (i) exactly 2 sixes appear.
(ii) at least 2 sixes appear.
(iii) at most 2 sixes appear.
6.
Prove that if A ,…,A are independent events then 1 n (i) P(A A … A ) = 1 – [1 – P(A )}{1 – P(A )]…[1 – P(A )] 1 2 n 1 2 n (ii) P(A A … A ) ≤ 1- e –[P(A ) + P(A ) +…+ P(A )] 1 2 n 1 2 n Hint: (1 – x )(1 – x )…(1 – x ) ≥ e - (x + x +…+ x )x ≤ 1 1 2 n 1 2 n i 7.
Show that P(A∩B∩C) = P(A B∩C)P(B C)P(C).
D D 8.
A die is tossed n times.
What is the probability that a 6 appears at least two times in the n tosses.
9.
Suppose that A or B occurs is 0.7 while P(A) = 0.2, find P(B).
10.
A boy decides to continue tossing a fair coin until he has thrown total of three heads.
Find P , the probability that exactly n tosses will needed.
n 64 11.
Six blood samples are selected from 40 blood samples, of which four are cancerous.
What is the probability that exactly two of the blood samples selected are cancerous?
12.
Prove that if A , A ,…A are any n events, then 1 2 n P(A ∩ A … ∩ A ) > 1 – {P(A c) + P(A c) + …+ P(A c)}.
1 2 n 2 2 n 13.
Prove that (i) P(A ∩ Bc) = P(A) – P(A ∩ B) (ii) P(A ∩ Bc) = 1 - P(A) – P(B) P(A ∩ B) (iii) P(A) = P(A ∩ B) + P(A ∩ Bc) (iv) P(A ∩ B) ≥ P(A) + P(B) – 1.
14. three coins have probability 0.5, 0.6 and 0.8 for heads respectively.
One of then is selected at random, that is, with equal chance for each, and tossed.
If the outcome is head, what is the probability that the coin with probability 0.8 for heads was selected?
15.
On a certain weekend there are 4 movies.
Calculate the probability that at least one of A and B will be selected by one or more of the 3 students.
16.
A box contains n white balls numbered 1 to n, n black balls numbered 1 to n, and n red balls numbered 1 to n. if two balls are drawn at random without replacement, what is the probability that both balls will be of the same colour or bear the same numbers.
17.
On the first round, three fair coins are flipped at random.
The coins resulting in heads are flipped at random on the second round.
If the second round results in exactly one head, what is the conditional probability that the first round ended inexactly two head?
65  UNIT THREE DISCRETE RANDOM VARIABLES 3.1 Introduction This chapter introduces idea of a random variable and its probability density function.
A random variable is a variable whose actual numerical value is determined by chance.
There are two easily identifiable types of random variables, discrete and continuous.
A discrete variable is one that takes only a limited number of possible values, otherwise the variable is called continuous.
This chapter is devoted to discrete random variables.
Section 3.2-3.4 are devoted to some special discrete random variables.
Bernoulli, Binomial Poisson, uniform,Geometric and negative bionmial.
Example 3.1 Consider variable X, the number of heads in three tosses of a coin.
There are four possible values (0, 1,2, 3,) of X.
The actual value assumed is due to chance therefore X is a random variable.
The sample space for this experiment is Ω= {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT} X = 0 if the outcome is TTT X = 1 if the outcomes is HTT, or THT, or TTH X = 2 if the outcome is HHT or HTH or THH X= 3 if the outcome is HHH Let p be the probability of the con landing tail.
Since landing tail and landing head are exhaustive events the probability of the coin landing head is 1-p. P(T ∩ T ∩ T) = P(T)P(T)P(T) Since the outcome at each trials are independent.
P(TTT) = p.p.p=P3 66  P(HTH) = P(T)P(H)P(H) = p (1-P) (1-P) = (1-P)2 P(HTH)= P(H)P(H)P(H)=( 1-P) (1-P) = P(1-P)2 P(HHT)= P(H)P(H)P(T) = (1-P)(1-P)P==(1-P)2 The probability of getting two heads = P (THH) + P(HTH) + P(HHT) = 3P (1-p)2 Similarly we have P (o head) = P(TTT) = p3 P (1 head) = P(HTT) + P(TTH) + P(THT) = 3p2(1-p) P (3 heads) = P(HHH) = (1-p)3 Thus we have the following table Heads 0 1 2 3 Probability P3 3p2 (1- p) 3p (1 – p)2 (1 –p)3 P (0 head) = P (X is 0) = p3, p(X=1) 3p2 (1-p) and so on The random variable X defined above is an example of what is called a discrete random variable.
A random variable is denoted by a capital letter such as X. Y, Z… and the values that the random variable takes on is denoted by a lower case letter x, y, z… The notation (P(X=x) means the probability that the random variable X takes on the value x.
Definition 3.1 A random variable X on a sample space Ω is a function assigns to each element Ω one and only one real number X ( ) = x, the space ofX is the set of real number Φ = {x:x= x ( ), we Ω).
Definition 3.2 A random variable x is discrete if t can assume at most a finite or a countable infinite number of possible values.
In the above example, Ω = Where w = HHH, w = HHT…, w = TTT 1 2 8 67 = {0, 1, 2, 3} and X (w ) = 3, X (w ) = 2.X(w ) = 2, X (w ) = 2 1 2 3 4 X (w = 1, X (w ) = 1, X (w = 1, X (w ) = 0 5 6 7 8 That is {w:X(w) = x } id an event 1 Definition 3.3 The real valued function f defined on R by f(x) = p(X = x) is called the discrete probability density function of X.
Let X be a discrete random variable and suppose that the values it can assume are x , 1 x …,x 2 n The probability can be written as P(X = x ) = f(x ), = x ) = f(x )…, P(X = x ) = f(x ) 1 1 =P(X 2 2 n n Such that F(x) is calledprobability density function of X For an illustration, let us consider the following examples.
Example 3.2 Suppose a pair of fair dice is tossed onc, Let X,Y,Z represent the sum, maximum and minimum respectively of the two numbers appearing find the probability density function of (i) X, (ii) Y, (iii) Z.
Solution The sample space Ω = {( 1, 1), (1, 2) ,…, (6, 6)} consist of 36 elements (i) P (X =2) = P {( 1, 1)} = 1 36 P ( X = 3) = P{( 1, 2), (2, 1) = 2 36 P (X = 4) = P{(1,3), (3, 1) , (2, 2)} = 3 36 .
.
P (X = 12) = P {(6, 6)} = 1 36 Thus, F(2)= 1, f(3) = 2, f(4) = 3 --- 68  36 36 36 In tabular form we have X 2 3 4 5 6 7 8 9 10 11 12 F(x) 1 2 3 4 5 6 5 4 3 2 1 36 36 36 36 36 36 36 36 36 36 36 in function form (ii) Y =Maximum of the two number Possible values of Y are 1, 2, 3, 4, 5, 6 g (1) = P(Y=1)= P {(1,1)}= 1 36 g(2) = P(Y = 2) = P{( 1, 2), (2, 1) (2, 2)= 3 36 g(3) = P (Y = 3)= P (1, 3), (3, 1), (2, 3) (3, 2) (3, 3)} = 5 36 g(4) = P (Y=$) = P{( 1,4)(4,1) 2, 4) (4, 2) (3, 4) (4, 3) (4, 4) = 7 36 g(5) = P (Y = 5) = P {(1,5), (5,1), (2, 5) (5,2) (3, 5) (5, 3) (4, 5)(5, 4) (5, 5) = 9 36 g(6) = P (Y = 6) = P {( 1,6), (6,1) (2,6) (6,2) (3, 6) (6, 3)(4, 6) (6, 4) (5, 6) (6,5) (6,6) = 11 36 Putting in form a table we have Y 1 2 3 4 5 6 g(y) 1 3 5 7 9 11 36 36 36 36 36 36 This can be written in a functional form as 69  (iii) Z = Minimum of the two numbers The possible values of Z are 1, 2, 3, 4,5,6 P(Z-1) = P{(1,6), (6,1)(1,5)(5,1)(1,4)(4,1) (1,3)(3,1)(1,2)(2,1)(1,1)!
= 11 36 P(Z = 2) = P (2,6)(6,2)(2,5)(5,2)(2,4(4,2) (2,3)(3,2) (2,2)= 9/36 P(Z= 4)P{(4,6) (6,4) (4, 5) (5,4) (4,4) 5.36 P(Z =5) = P(5,6) (6,4) (5,5) 3/36 P(Z= 6) = P(6,6) 1/36 Putting in form of a table we have Z 1 2 3 4 5 6 h(z) 11 9 7 5 3 1 36 36 36 36 36 36 H(Z) = 13 – 2z z = 1, 2, 3, 4,5, 6 36 = 0 for other values of x.
The probability density function of a discrete random variable X has the following properties (i) O < f(x) < 1, x E R (ii) {X:f (x) = 0} is a finite or countable infinite subset of R (iii) ∑f(x = 1 i) The above densities can be represented in terms of a diagram as illustrated in figure (a), (b) (c) 70  Example 3.3 Let X,Y,Z be the random viable introduced in example 3.1 above The above three properties are satisfied, properties (i) & (ii) are immediate from definition of probabilities.
To check (iii) we have ∑f(x ) = 1/36+ 2/36+ 3/36 + 4/36 + 5/36+ 6/36 +5/36 + 4/36 + 3/36 + 2/36+ 1/36 =1 1 ∑G (y) = 1/36 + 3/36 + 5/36 + 7/36 + 9/36 + 11/36= 1 i ∑h(z) = 11/36 + 9/36 + 7/36 + 5/36 + 5/36 + 3/36 + 1/36 = 1 i Example 3.4 Suppose a box contains 1 balls of which 4 are red and 6 are black.
A random sample of size 3 is selected.
Let X denote the number of red balls selected.
Find the probability density function of x if (i) Sampling is without replacement (ii) Sampling is with replacement (iii) The possible values of X are 0, 1, 2, 3.
P(X = 0) = (P no red ball in the three balls selected) = 6C = 6 5 4 = 1 3 10 9 8 6 c5 P(X = 1) = P(1 red and 2 black balls ) = 4C x 6C =1 1 2 10C 2 3 P(X = 2) = 4C x 6C = 3 2 1 10C 10 3 P(X = 3) = 4C /10C = 1 3 3 71  30 Thus the p.d.f is X 0 1 2 3 F(x) 1 1 3 1 6 2 10 30 As a check, we add up all the probabilities 1/6+ ½ + 3/10 + 1/30=1 (ii) Sampling with replacement P(X = 0) =P (first ball is black, second black and the third black) = P(bbb).
The probability of black at any drawing is 6/10= 3/5 This value is constant since drawing is with replacement.
Therefore P(bbb) = P(X = 1) = P(Rbb) + P(bRb) + P(bbR) = = Similarly , P(X = 2) = P(X = 3) = P(RRR) = Thus, the p.d.f of X is X 0 1 2 3 f(x) 72 This can be written as f(x) = 0 elsewhere Example3.5 A box contains 6 balls labeled 1, 2, 3, 4, 5, 6 two balls are drawn at random one after the other.
Let X denote the larger of the two numbers on the balls selected, obtain the probability density function of X if (i) Sampling is without replacement (ii) Sampling is with replacement Solution (i) The possible values of X are 2, 3, 4, 5, 6.
The larger of the two numbers can not be 1 since sampling is without replacement and we can not get same number twice.
P(X = 2) = P{(1, 2), (2, 1)} = P(X = 3) = P{(1, 3), (3, 1), (2, 3), (3, 2)} = P(X =5) = P{(1, 5), (5, 1), (2, 5), (5, 2), (3, 5), (5, 3), (5, 4), (4, 5)} = Similarly, P(X = 6) = Thus the probability density function of X is X 2 3 4 5 6 73 f(x) 1/15 2/15 1/5 4/15 1/3 In functional, form, this can be written as f(x) = x = 2, 3, 4, 5 0 elsewhere (ii) The possible values of X are 1, 2, 3, 4, 5, 6.
The larger can be 1 in this case since (1, 1) is a possible outcome.
The sample space consists 36 possible outcomes P(X = 1) = P(1, 1) = P(X = 6) = P{(1, 6), (6, 1), (2, 6), (6, 2).,,, (6, 6)} = Hence the probability density function X is X 1 2 3 4 5 6 f(x) 1/36 3/36 5/36 7/36 9/36 11/36 In functional form we have f(x) = x = 1, 2, 3, 4, 5, 6 3.6 A couple decides that they will continue to have children until either they have a boy and a girl in the family or they have four children.
Assuming that boys and girls are equally likely to be born.
Let X denote the number of children in the family.
Find the probability density function of X.
The number of children in the family can either be 2, 3, or 4.
P(X = 2) = P{(B, G), (G, B)} 74 That is, the couple will stop having more children if the first child is a boy and the second of girl or the first is a girl and the second a boy.
Thus, P(X = 2) = P(B, G) + P(G, B) = P(B)P(G) + P(G)P(B) Similarly, P(X = 3) = P{(BBG),(GGB)} = P(BBG) + P(GGB) = Hence, the probability density function of X is X 2 3 4 f(x) ½ ¼ ¼ Exercise 3.1 1.
A fair coin is tossed until a head or five tails occur.
Let X denote the number of tosses of the coin.
Compute the probability density function of X.
2.
A box contains 2 red balls and 3 blue balls.
Balls are successively drawn without replacement until a blue ball is drawn.
Let X denote the number of draws required.
Compute the p.d.f of X 3.
The pdf of a random variable X is given by X 1 3 4 6 8 f(x) K Find (i) K (ii) P(X ≥ 3).
75 4.
Toss a fair coin two times.
Let X be the number of heads obtained.
Find the pdf of X.
5.
A coin with probability p of a head is tossed until a head appears.
Let X denote the number of times the coin is tossed.
Find the pdf of X.
6.
A fair die is tossed twice.
Let X denote the product of the two numbers appearing.
Find the pdf of X 7.
A fair coin is tossed3 times.
Let X represent the difference between the number of heads and the number of tails obtained.
Find the pdf of X.
8.
The pdf of a random variable X is given by f(x) = {k 2x, x = 1, 2, 3, …N, zero elsewhere}.
Find the value ofK.
Definition 3.3 Probability Distribution function.
Let X be a random variable with probability density function f(x).
The probability distribution function of X denoted f(x) is defined by F(x) = p(X ≤ x) for x real.
= Properties of the Probability distribution function 1.
F is a non decreasing function, that is if a < b, then F(a) < F(b).
2. lim F(b) = 1. b-∞ lim F(b) = 0 b-∞ 3.
F is right continuous.
That is F(b + ) = F(b).
Let A be any subset of R and let f(x) be the probability density function of X. ew can compute P(X ε A) by noting that {w : X (w) ε A} is an event and that {w : X(w) ε A} = U { w : X(w) = x}.
i XεA i 76 Thus P(X ε A) = If A is an interval with end points a and b, say A = [a,b].
Then {(X ε A) = P(a ≤ X ≤b) = Example 3.7 Consider the random variable X the sum of the two numbers appearing when a fair die is tossed twice of Example 3.2.
The probability distribution function for X is given by X 2 3 4 5 6 7 8 9 10 11 12 F(x) Suppose we wish to find the probability that X is between 4 and 9 (4 and 9 inclusive) we write it as P(4 ≤ X ≤ 9) = P(X = 4) + P(X = 5) + P(X = 6) + P(X = 7) + P(X = 8) + P(X = 9) = This can also be written as P(X≤ 9) – P(X ≤ 3) = = = F(9) – F(3) 3.2 SPECIAL DISCRETE RANDOM VARIABLES 3.2.1 In Section 3.1 we introduced discrete random variables and the probability density function of some random variables were determined.
Some variables are so common and important that names are given to them.
In this section we shall consider in considerable detail a number of important discrete random variables.
Bernoulli Random variables 77 Definition 3.4 Bernoulli Trail: A random trail or experiment is which the outcome can be classified into one of two mutually exclusive and exhaustive ways usually called success or failure is called a Bernoulli trail.
The random variable associated with Bernoulli trail is called a Bernoulli random variable (X).let X=0 if outcome is a failure and X =1 if the outcome is a success.
That is any variable assuming only two values is called a Bernoulli random variable.
Suppose that we toss a coin once.
Let the probability of it landing head be p. p= , if the coin is fair and let denote the outcome of the toss.
Then there are two possible values for X, Heads or tails.
These two values are mutually, exclusive and exhaustive and we may associate the two possible outcomes of the toss with values 1, 0 of the random variables X.
That X = 1 when a head appears and X = 0 when a tail appears.
P(X = 1) = p, P(X = 0) = 1 –p.
The p.d.f.
of X is X 0 1 f(x) 1 – p P Or in functional form F(x) = pX(1 –p)1-x, x = 0, 1 0 elsewhere 3.1 f(x) as defined above is called the Bernoulli probability density function and any variable X having (3.1) has its probability density function is called a Bernoulli random variable and is said to have the Bernoulli distribution.
3.2.2 The Binomial Random Variable This is one of the most important random variables in statistics and the most important discrete random variable.
Consider n independent repetitions of Bernoulli trails.
Let X, I i = 1, 2,…, n be Bernoulli random variables associated with the trails.
The random 78 variables X , X ,…X are independent Bernoulli random variables.
Let us assume the 1 2 n probability of success is p and failure 1-p and P(x = 1) = p 1 Then, sum S = + x +---+ X is the number of successes in n Bemoulli trials.
That is, S n 2 n n is a counting- variable counting the number of successes in n repeated trials.
This random variable S is called the Binomial random variable.
The possible values of S are 0, 1, 2, n n 3, 4,----, n. P(S = 0) = p(no success) n = p(1sttrail is a failure) P(2nd trail is a failure)…P(nth trail is a failure).
= P(X = 0) P(X = 0) P(X = 0)…P(X = 0) 1 2 3 n = (!
– p) (1 – p)(1 – p)…(1 –p) = (1-p)n. S is 1 if the sequence of outcome is n 1 0 0 0 0 0 0 …0 or 0 1 0 …0 or 0.0 1 0 …0,…0 0 0 0 0 0 1 P(S = 1) = P(1, 0, 0, 0,…0) + P(), 1, 0, 0,…, 0) + …+ P(0, 0,…1) n = P(X = 1)P(X = 0)…P(X = 0) + P(X = 0)P(X = 1)…P(X = 0) +…+ 1 2 n 1 2 n + P(X = 0)…P(X = 1) 1 n = P(1 – p)…(1 – p) + (1- 1)P(1 –p)…(1 – p)…(1 – p)+ (1 – p)(1 – p) = P(1-p)n-1 + P(1-p)n-1 +…+ P(1-p)n-1 = np(1-p)n-1 Similarly, P(S = 2) = nC P2 (1-p)n-2 n 2 Where nC is the number of sequences in which exactly 2 have value 1 and the others 0 2 e.g.
(1, 1, 0, 0, 0, 0,0,…, 0), (1, 0, 1, 0,…, 0)… In general, it can easily be seen that P(S = k) = nC pk (1-p)n-k, k = 0, 1,…, n n 2 Where nC is the number of sequences in which exactly k have value 1 and others 0.
2 For example when n = 4, possible sequences of outcomes are given below.
Sequence S P(S ) n n 79 (0,0,0,0) 0 (1 – p)4 (1,0,0,0) 1 P(1 – p)3 (0,1,0,0) 1 P(1 – p)3 (0,0,1,0) 1 P(1 – p)3 (0,0,0,1) 1 P(1 – p)3 (1,1,0,0) 2 P2(1 – p)2 (1,0,1,0) 2 P2(1 – p)2 (1,0,0,1) 2 P2(1 – p)2 (0,1,1,0) 2 P2(1 – p)2 (0,0,1,1) 2 P2(1 – p)2 (0,1,0,1) 2 P2(1 – p)2 (1,1,1,0) 3 P3(1 – p) (1,1,0,1) 3 P3(1 – p) (1,0,1,1) 3 P3(1 – p) (0,1,1,1) 3 P3(1 – p) (1,1,1,1) 4 P4 Thus, P(S = 0) = (1 – p)4 n P(S = 1) = 4p(1 – p)3 n P(S = 2) = 6p2(1 – p)2 n P(S = 3) = 4p3(1- p) n P(S = 4) = p4 n Theorem 3.1 Let S denote number of successes in n repeated Bernoulli trails, with probability of n success p. the probability density function of S is given by n 80  f(x) = P(S = x) = nC px(1 – p)n-x x = 0, 1, …,n (3.2) n x 0 Definition 3.5 A discrete random variable X denoting total number of successes in n trails is said to have the binomial distribution if P(X = x) = nC px(1-p)n-x ; x = 0, 1, 2,…, n x 0 ≤ p ≤ 1 The conditions under which binomial distribution will arise are (i) The number of trails is fixed (ii) There are only two possible outcome ‘success’ or ‘failure’ at each trial.
(iii) The trails are independent (iv) The probability p of success at each trail is constant (v) The variable is the total number of successes in n trails.
Example 3.8 A soldier fires 10 independently at a target.
Find the probability that he hits the target.
(i) once (ii) at least 9 times (iii) at most two times.
If he has probability 0.8 of hitting the target at any given time?
Let X denote the number of times he hits the target.
Then X is a binomial variable with n =10 and p = 0.8 From equation (3.2), we have P(X = x) = 10C (0.8)x (0.2)10-x x (i) P(X = 1) = 10C 0.8(0.2)9 = 8(0.2)9 x (ii) P(He hits the target at least 9 times) = P(X ≥ 9) = P(X = 9) + P(X = 10) P(X = 9) = 10C (0.8)9 (0.2) = 10 × (0.8)9 × 0.2 = 2(0.8)9 9 P(X = 10) = 10C (0.8)10 = (0.8)10 10 Hence, P(X ≥ 9) = 2(0.8)9 + (0.8)10 = (0.8)9(2 +0.8) = (0.8)9(2.8) = 0.3758 (iii) P(at most twice) = P(X ≤ 2) = P(X = 1) + P(X = 2) P(X = 0)= (0.2)10; P(X = 1) = 8(0.2)9; P(X = 2) = 45 (0.8)2 (0.2)8 Thus, P(X ≤ 2) = (0.2)10 + 8(0.2)9 + 45(0.8)2 (0.2)8 = 0.00008.
Example 3.9 A fair die is rolled four times.
Find the probability of getting 2 sixes.
Let us call a six a success on a toss of a die and let X be number of sixes (successes) in 4 trails.
X is binomial random variable with n = 4 and p = 1/6.
Thus, 81 P(X = 2) = 4C .
.
2 Example 3.10 Suppose that a certain type of electric bulb has a probability of 0.3 of functioning more than 800hours.
Out of 50 bulbs, what is probability that less than 3 will function more than 800 hours.
Let X be the number of bulbs functioning more than 800 hours.
Assuming that X has a Binomial distribution, P(X = x) = 50 C (0.3)x(0.7)50-x x P(X < 3) = P(X = 0) + P(X = 1) + P(X = 2) P(X = 0) = (0.7)50; P(X = 1) = 50.
(0.3) (0.7)49 P(X = 2) = 50C (0.3)2(0.7)48.
2 Thus, P(X < 3) = (0.7)50 + 15(0.7)49 + 110.25(0.7)48 = 0.0000046 Exercise 3.2 1.
A fair coin is tossed 4 times.
Compute the probability that (i) exactly two heads occur (ii) at least 3 heads occur.
2.
An investigation reveals that four out of every five patients are cured of malaria when treated with a new drug.
If a sample of ten patients is treated by the new drug, compute the probability that (i) exactly six patients are cured, (ii) at most four patients are cured 3.
A man fires 12 shots independently at a target.
The probability of hitting his target is ¼.
(i) What is the probability of his hitting the target at least two times?
(ii) How many times must he fire so that the probability of his hitting the target at least once is greater than 7/9?
4.
Six children are born in a hospital in a given day.
Calculate (i) the probability that the number of boys is the same as the number of girls.
(ii) the probability that there are more girls tha boys (iii) the most likely number of boys.
82 5.
An experiment has 90 percent probability of success and 10 percent probability of failure.
The experiment is repeated four times.
Find the probability of obtaining (i) No success (ii) No failure (iii) two successes and two failures.
6.
Let X be a Binomial random variable with paraments (n.p).
show that (i) P(X = k) first increases monotonically and then decreases monotonically.
(ii) The value of k that maximizes P(X = k) is the largest integer less than or equal to (n + 1)P (iii) P(X = k) = 7.
A fair coin is tossed repeatedly until three are obtained.
Find P , the probability n that exactly n tosses are needed.
8.
Show that (i) nC px (1 – p)n-x = 1 (ii) nC px (1 – p)n-x = np x x 0<p<1.
3.3 Geometric Random Variable Consider a Bernoulli trail with probability p of a success on one trail.
The trail is continued until a success occurs.
Let X denote number of trails before the first success.
For example, a student decides to continue taking JAMB examination until he passes.
X in this case denote number of times he takes the examination before the first success.
The probability that the first x – 1 trails are failures and the xth trail is a success is given by (1 – p)x=1 p. To see this, the required probability is P(F F F …F S) = P(F)P(F) …P(F)P(S) = (1 – p)(1 – p)(1 – p) … (1 – p)P = (1 – p)x-1 p. The probability that x trails are needed for the first success is the same as the probability that the first x – 1 trails are failures and the xth is a success.
Thus, P(1 – p)x -1, x = 1,2… P(X = x) = 0 otherwise Where X is the number of trials before the first success and if we define Y as the number of failures preceding the first success, we have 83  P(1 – p)y y = 0,1,2… P(Y= y) = 0 otherwise Definition 3.6 A discrete random variable X is called a geometric random variable if its probability density function is given by (1 – p)x -1 x = ,1,2,3… p F(x) =p(X= x) = 0 otherwise 0 < p< 1 (3.3) Where X is the number of independent Bernouilli trials taken for the first success to occur (the successful trial is included in the count).
To see that (f(x) is a probability density function, all that needed to be checked is that (1-p)x=1= 1 = P[1 + (1-p) + (1-p)2 +…] From geometric series, (1-p)x=1= .
Example 3.11 A fair is tossed until a head appears (a) What is the probability that three tosses are needed?
(b) What is the probability that at most three tosses are needed?
Solution Let X denote the number of tosses until a success (a head) occurs.
Since the coin is fair, .
P(X = 3) = (1 – p)3-1 P(at most three tosses are needed) = P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3) 1 1 1 12 1 1 12 13 7 = + .
+   .
= +   +   = .
2 2 2 2 2 2 2 2 8 84 3.12 A fair die is rolled until a six appears.
What is the probability that (i) at most 4 rolls are needed.
(ii) at least 3 rolls are needed?
Solution (i) P(at most 4 rolls are needed)= P(X ≤ 4) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) 1 X is a geometric random variable with P = 6 5x1 1 P(X = x) =   6 6 1 5 1 52 1 P(X = 1) = , P(X = 2) = , ,P(X =3)=  .
6 6 6 6 6 53 1 P(X = 4) =   .
.
6 6 Thus 1 5 1 52 1 53 1 P(X ≤ 4) = + .
+  .
+  .
6 6 6 6 6 6 6 1 4 5x=1 1 1- (5/6)4 = ∑  = .
=1- (5/6)4 6 6 6 1- 5/6 x=1 (ii) P(at least 3 rolls are needed) = P(X ≥ 3) = P(X = 3) + P(X = 4) + … ¥ ¥ 5x=1 1 1 ¥ 5x=1 1 1- (5/6)2 52 25 P(X ≥ 3) = ∑ f(x) =∑  = ∑  = .
=  = 6 6 6 6 6 1- 5/6 6 36 x=3 x=3 x=3 or 11 25 P(X ≥ 3) = 1 – {P(X = 1) + P(X = 2)} = 1 - = 36 36 Example 3.13 The probability that a certain test yields a “positive” reaction is 0.6. what is the probability that not more than 4 negative reactions occur before the first positive one?
Solution 85 Let X denote the number of negative reactions before the first positive one, then P(X = x) = (0.4)x(0.6); x = 0,1, 2,… Thus, 4 4 4 P(X ≤ 4) = ∑P(X =x)=∑(0.4)X(0.6)=(0.6)∑(0.4)x =(1- (0.4)5)=0.9898 0 x=0 x=0 The geometric random variable has an interesting property which is summarized in the following theorem.
Theorem: 3.2 Suppose that X is a geometric random variable.
Then for any given two positive integers s and t, P(X > s + t|X > s) = P(X > t).
Proof: P(X >s +tand X >s) P(X > s+t P(X > s + t | X > s) = = P(X > s) P(X > s) P(X = x) = p(1- p)x-1, x = 1, 2, 3, … ¥ (1- p)s+t P(X > s + t) = P ∑ (1- p)X- 1=P =(1- p)s+t 1- (1- p) X=S+t+1 ¥ P(1- p)s P(X > s + t) = P ∑(1- p)X- 1= =(1- p)s 1- (1- p) S+1 Thus, (1- p)s+t P(X > s + t| X > s) = =(1- p)t (1- p)s ¥ p(1- p)t P(X > t) = ∑p(1- p)X- 1= =(1- p)t. p t+1 Hence, P(X > s + t |X > s) = P(X > t).
The above theorem states that if a success has not occurred during the first s repetitions of Bernouli trails, then the probability that it will not occur during the next t repetitions is the same as the probability that it will not occur during the first t repetitions of Bernoulli trails.
Therefore the distribution is said to have “no memory”.
86 Exercise 3.3 1.
On a certain road the probability of an accident on any day is 0.05 (assuming not more than one accident can occur on any day).
Assuming independence of accidents from day to day on this road, what is the probability that the first accident of the year occurs in the month of March.
2.
A fair die is rolled until a six appears.
Calculate the probability that he has to throw the die more than three times before he gets a six.
3.
Let X be a geometric random variable with p = 0.2.
Calculate the following probabilities.
(i) P(3 < x ≤ 6) (ii) P(2≤ X≤ 4) (iii) P(X ≤ 2).
¥ 1 4.
Prove that ∑xp(1- p)x=1 = .
p x=1 5.
Suppose a Bernoulli trail with probability p of success is continued until rth success occurs.
Let X be the number of independent trails needed in order to have r successes.
Show that k - 1 P(X =K)= p r(1- p)k- r,k = r,r+1,...   r- 1 3.13 Pascal or Negative Binomial Random variable A probability distribution closely related to the geometric distribution is the Pascal or negative binomial distribution.
Suppose that independent repetition of Bernoulli trail needed to have “success” occurs exactly r times.
X = x if and only if success occurs on the xth trail and success occurs exactly (r – 1) times in the previous x -1 trails.
The probability of this event is determined as follows: The probability of r – 1 successes in x – 1 trails is given by the binomial formula X=1C pr-1(1 – p)x=1 – (r – 1)= x -1 C r – 1 x – r r – 1 r– 1 p (1 – p) Therefore, the probability of this event is P.x-1C pr-1(11 – p)x-r+ r-1 hence, f(x) = P(X= x) = x-1C pr(1 – p)x-r, x = r, r + 1,… (3.4) r-1 Similarly, if we let Y be the number of failures before the rth success we have 87  P(Y = y) = p. (y+r-1C pr-1(1 – p)y y F (y) = y+r-1C r(1 – p)y, y = 0, 1, 2,… Y y Definition 3.7 A random variable having its probability density function given by (3.4) or (3.5) is said to gave a Negative Binomial or Pascal distribution.
Example 3.14 Show that Solution By Binomial theorem, = Example 3.15 A fair die is rolled until two sixes occurs, find the probability that (i) exactly 5 tosses are needed (ii) at most 5 tosses are needed.
Solution Let X be the number of tosses needed to get two sixes.
X is a Pascal random variable with pdf: f(x) = x-1C p2(1-p)x-2, x = 2,3,.. 1 Where p = ½ , r = 1.
(i) (ii) Thus, 88  3.3.2 The Hyper geometric Random Variable Suppose that we have a box containing n items of which n are defective and n – n are 1 1 non-defective.
Suppose that we choose at random k items from the box without replacement.
Let X denote the number of defective items in the k items selected.
Then (3.4) The reader will notice that X = x if and only if we obtain x defective items from n 1 defective items in the box (n C ways of doing this).
1 x Any random variable having its probability density function as given by (3.5) is called hyper geometric random variable and is said to have hyper geometric distribution.
3.4 The Poisson Random Variable Definition: Rare Event An event is said to be rare if the probability p of observing the event is very small.
Consider n repeated Bernoulli trails, where n is very large and P very small, let X be be the number of successes in n trials.
Then P(X = x) = nC px(1 – p)n-x.
x Setting λ = n p, we have Now as n ∞.
1.
2.
3.
Consequently, nC px(1 – p)n-x = x (3.5) gives an approximation to the binomial distribution with λ = np, when n is large and p is small, where e = 2.71828 is the base of natural logarithms.
Definition: A random variable X is called a Poisson random variable if its probability density function is given by 89  (λ > 0) (3.8) 0 otherwise X represents the total number of events which have occurred up to time t. When t= 1 then f(x) corresponds to the probability density function of number of events in a unit interval.
For examples, the number of calls that come into a telephone exchange in a unit time interval, the number of vehicles passing through a designated point in a unit time interval.
In order to motivate our discussion, let us consider the following examples.
Example 3.16 Suppose a rare disease occurs in 2 percent of a large population.
A random sample of 10,000 people are chosen at random from this population and tested for the disease.
Calculate the probability that at least two people have the rate disease.
Solution The probability p of having the disease is 0.02 and n = 10,000.
Let X be the number of people having the disease.
X is a binomial random variable with parameters n= 10,000 and p = 0.02. we shall apply the result (3.5) since n is large and p very small.
Hence, Np = 0.02 × 10,000 = 20,, Thus, P(X = x) = P(X = ) = e-200 P(X = 1) = 200 e-200 Hence P(X ≥ 2) = 1 – P(X < 2) = 1 – e-200 (1 + 200) = 1 – 201 e-200.
Example 3.17 On a given road, an average of five accidents occur every month.
Calculate the probability that over a year period there will be (i) no accident (ii) at most 2 accidents.
Solution In this case, λ = 5, t = 12.
From (3.8) we have f(0) = e-λt = e-60 f(1) = = 60e-60 f(2) = = e-60 thus, P(no accident) = f(0) = e-60 P(at most two accidents) = f(0) + f(1) + f(2) = e-60 + 60 e-60 + e-60 90  = e-60 (1 + 60 + 1800) = 1861 e-60.
Tables for the Poisson distribution are available and brief abulation is given in the Appendix.
Exercises 3.4 1.
If 3% of the items manufactured in a factory are defective.
Compute the probability that in a simple of 100 items.
(i) 2 items will be defective (ii) at least 2 items will be defective.
Use Poisson approximation to the Binomial.
2.
Use the Poisson approximation to calculate the probability that at least two sixes are obtained when six dice are rolled once.
3.
The telephone switchboard of a University has an average of two incoming calls per minutes.
Calculate the probability that, over a three-minute interval, there will be (i) no incoming calls (ii) exactly one incoming call.
(iii) at most two incoming calls.
4.
Let P be the negative binomial pdf with parameters r and P. prove that r where P(k) = P(X= k).
r 5.
Prove that Hint: Use integration by parts.
6.
A fair die is tossed 20 times.
Let a success on the ith toss corresponds to at least a five appears.
What is the probability that (i) exactly 10 failure prior to the first success?
(ii) exactly 10 failures prior to the fifth success?
(iii) exactly 8 failure and 3 successes for the first II throws.
7. if (i) show that (x + 1)f(x +1)= λ f(x).
(ii) f(x) = nC px(1 –p), show that (1 – p)(x + 1)f(x + 1) =p(n – x)f(x).
c In each case, show that f(x) increases monotonically and then decreases monotonically as x increases.
Find x that maximizes f(x).
8.
Suppose that the number of accidents occurring on a highway each day is a Poisson random variable with parameter (a) What is the probability that at least two accidents occur in a day?
(b) what is the probability that at least three accidents occur in a day given that at least one accident occurs in a day?
9.
Describe the Poisson distribution, stating clearly the meanings of the symbols used.
Show that the variance of the distribution is equal to its mean.
10.
Show that f(x) = x-1C Pn(1-p)x-n, x, x = n, n+1,… is a pdf.
n-1 Note P-n= (1 – (1-p))-n = .
11.
Show that f(x) = x = 1,2, …, 0< p<1is a pdf.
Find the probability generating function of X.
12.
Solve the problem of exercise 3.2, No 7.
91 13.
If n ≥ 8 and in s binomial distribution the probability of 7 successes in n trails is equal to the probability of 8 successes in n trail, find the probability of success on any given trail.
14.
Let X be a random variable with moment generating function M (t).
If R(t) = In’’ X M (t) for all t. Find R”(0).
X 15.
If a fair die is rolled repeatedly, find the probability that the first “six” will appear on an odd-numbered roll.
p, x = -1 16. f(x) = (1-p)2px, X = 0,1,2 (i) Show that f(x) is a pdf UNIT FOUR EXPECTATION OF DISCRETE RANDOM VARIABLE In this chapter we introduce the concept of the mean value of a random variable.
It is closely related to the notion of weighted average.
The moment and probability generating functions of a random variable are also introduced.
4.1 Let X be a random variable having possible values x , x , …, x , and let the experiment 1 2 n on X be performed n times.
For example let X be the outcome of rolling a die.
There are six possible values x = 1, x = 2, x = 3, x = 4. x = 5 and x = 6.
Suppose the die is 1 2 3 4 5 6 rolled n times.
The successive rolls constitute independent repetitions of the same experiment.
Let X , X , .
…, X denote the outcomes of the experiment of rolling a die n 1 2 n times (that is X denote the outcome of the ith toss).
Then i Is the average of the numbers that appeared.
Let f denote the number of times x occur.
i i Then we have We know that is called the expectation of the random variable X. thus the expected value of a random variable X is the long-run theoretical average value of X.
92 Definition 1 Mathematical Expectation.
Let X be a random variable with probability density function as follows: Value of X, x x x x ….
x 1 2 3 k Probability f(x ) f(x ) f(x ) f(x ) 1 2 3 k The mathematical expectation of X, denoted by E(X), is defined to be E(X) = x f(x ) + x f(x ) +…+ x f(x ) 1 2 2 2 k k E(X) = E(X) is weighted average of possible values of x, the weight attached to the value xi is its probability f(x).
E(X) is also called the mean of X or the population mean.
i We may express the result of (1) in words.
To compute the expected value or mean of a random variable, multiply each possible value of the variable by its probability and add these products.
Examples 4.1 A fair coin is tossed three times.
Let X denote the number of heads obtained.
Find the mathematical expectation of X.
Solution The probability density function of X is as follows.
X 0 1 2 3 f(x) Hence, E(X) = (1 × 1/8) + (1 × 3/8) + (2 × 3/8) + (3 × 1/8) = 0 + 3/8 + 6/8 +3/8 = 12/8 = 1.5 4.2 What ate the mathematical expectations of the variables X, Y and Z as defined in example 3.1.1 of chapter 3.
Solution The probability density function of X is X 2 3 4 5 6 7 8 9 10 11 12 f(x) 1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36 E(X) = (2 × 1/36) + (3 × 2/36) + (4 × 3/36) + (5 × 4/36) + (6 × 5/36) + (7 × 6/36) + 93  (8 × 5/36) + (9 × 4/36) + (10 × 3/36) + (11 × 2/36) + (12 × 1/36) = 7.0 The p.d.f of Y is Y 1 2 3 4 5 6 g(y) 1/36 3/36 5/36 7/36 9/36 11/36 E(Y) = (1 × 1/36) +(2 × 3/36) + (3 × 5/36) + (4 × 7/36) + (5 × 9/36) + (6 × 11/36) = 4.47 The p.d.f of Z is Z 1 2 3 4 5 6 h(Z) 11/36 9/36 7/36 5/36 3/36 1/36 E(Z) = (1 × 11/36) + (2 × 9/36) + (3 × 7/36) + (4 × 5/36) + (5 × 3/36) + (6 × 1/36) = 2.53.
Exercises 4.1 1.
Suppose a box contains 10 balls of which 4 are red and 6 are black.
A random sample of size 3 is selected.
Let X denote the number of red balls selected.
Find (E(X) if (i) Sampling is with replacement.
(ii) Sampling is without replacement.
2.
A box contains 6 balls labeled 1, 2, 3, 4, 5, 6.
Two balls are drawn at random one after the other.
Let X denote the larger of the two numbers on the balls selected.
Compute E(X) if (i) Sampling is with replacement, (ii) Sampling is without replacement.
3.
A box contains 3 balls and 2 white balls.
A ball is without replacement one after the other until a white ball is drawn.
Find the Expected number of draws required.
Expectation of a Binomial Random Variable The probability density function of a binomial variable X is defined by f(x) = nC px(1 – p)n-x; x = 0, 1, 2, …,n.
x the mean or expected value of X is E(X) = and since the term of x = 0 is zero, E(X) = Using the binomial theorem, we have 94  since p + 1 – p = 1.
Hence, E(X) = np.
Example 4.3 A fair die is rolled 12 times, what is the expected number of six appear?
Solution Let X be the number of sixes that appear.
X is a binomial random variable with n = 12 and P = 1/6 (probability of a six).
Hence E(X) = np = 12x 1/6 =2.
That is, we expected to get 2 sixes when a die is rolled 12 times.
Definition 2 Let X be a discrete random variable having probability density function f(x).
then we say that X has finite expectation otherwise we say that X does not have finite expectation.
Examples 4.4 The Random Variables X has Probability X -2 -1 0 3 5 f(x) ¼ 1/8 1/8 ¼ ¼ Compute the expected value of the following random variables.
(i) X, (ii) 3X, (iii) X + 5, (iv) X2 Solution (ii) = -2/4 – 1/8 +3/4 +5/4 = 11/8 = 1 (iii) The possible values of 3x are -6, -3,0, 9, 15.
The probability density function of 3x is 3x -6 -3 0 9 15 f(x) ¼ 1/8 1/8 ¼ ¼ 95  E(3X) = (-6 × ¼) + (-3 × 1/8) + (0 × 1/8) + (9 ×1/4) + (5 × ¼) = -6/4 – 3/8 + 9/4 + 15/4 = 33/8 Note that P(3X = -6) = P(X = -2_, And so on and E(3X) = 33/8 = 3 × 11/8 = 3E(X) (iii) x + 5 3 4 5 8 10 f(x) ¼ 1/8 1/8 ¼ ¼ Note that P(X +5) = 3 =P(X = 3 – 5) = P(X = -2) E(X + 5) = (3 × ¼) + (4 × 1/8) + (5 × 1/8) + (8 × ¼) + (10 ×1/4) = ¾ + 4/8 + 5/8 + 8/4 + 10/4 + 51/8 = 6 = 5 + 1 = 5 + E(X).
(iv) X2 4 1 0 9 25 f(x) ¼ 1/8 1/8 ¼ ¼ E(X2) = (4 × ¼) + (1 × 1/8) + (0 × 1/8) + (9 × ¼) + (25 × ¼) = 1 + 1/8 + 9/4 + 25/4 = 77/8 = 9 Note that E(X2) ≠ [E(X)]2.
Definition 3 Let X be a random variable whose probability density function is given by x x x ………….
x 1 2 k f(x) f(x ) f(x ) f(x ) 1 2 k Let ψ(X) be a function of X. then the mean or expected value of the new random ψ(X) is given by E[ψ(X)] = ψ(x ) f(x ) + ψ(x ) f(x ) +…+ψ(x ) f(x ) 1 1 2 2 k k That is E[ψ(X)] = ψ(x) f(x) i i Theorem 1 Let X be a random variable and let ψ(X) = a X + b where a, b are constants, then 96  E(ψ(X)) = aE(X)+ b Proof: Suppose the probability density function of X is {x, f(X), I = 1, 2, …,k) i i From the above definition we have E(aX + b) = (ax + b) f(x ) + (ax + b) f(x ) + … + (ax + b) f(x ) + bf(x ) +…+ bf(x ) 1 1 2 2 k k 1 k = a[x f(x ) + x f(x ) +… x f(x )] + b[f(x ) + f(x ) +…+ 1 1 2 2 k k 1 2 f(x )]= k since {E(X) = Properties of Excitations 1.
If c is a constant and P(X = c) = 1, then E(X) = c. 2.
If a and b are constants and X has a finite expectation, then aX has finite expectation and (i) E(aX) = aE(X) + b.
(ii) E(aX + b) = aE(X) + b.
3.
If X and Y are two random variables having finite expectations, then (i) X + Y has random finite expectation and E(X + Y) = E(X) + E(Y) (ii) E(X) ≥ E(Y) if P(X ≥ Y) = 1.
(iii) EX ≤ E X .
4.
A bounDded DrandoDm vDariable has a finite expectation.
That is if P(X ≤M) = 1, then X has a finite expectation and EX ≤ M. D D Definition 4 Variable.
Let X be a random variable with mean E(X) = µ.
The variance of X, denoted by Var(X) is defined by Ver(X) = E{X - µ)2} = A general formula that is usually simple for computing the variable is given below Var(X) = E{(X - µ)2} = E{X2 – 2 Xµ + µ2} = E(X2) – E(2µX) + E(µ2) = E(X2) - 2µE(X) + µ2 (property 2) = E(X2) - 2µ2 + µ2 = E(X2) - µ2.
Thus, the computing definition of variance is Var(X) = E(X2) – [E(X)]2 (7) The variance of X is interpreted as a numerical measure of spread or dispersion about its mean.
Example 4.5 1.
Find the variance of a random variable having the following probability density function.
x 1 2 3 4 5 6 97 f(x) 1/6 1/6 1/6 1/6 1/6 1/6 E(X) = (1 × 1/6) + (2 × 1/6) + (3 × 1/6) + (4 × 1/6) + (5 × 1/6) + (6 × 1/6).
= 1/6 + 2/6 +3/6 + 4/6 + 5/6 + 6/6 = 21/6 E(X2) = (12 × 1/6) + (22 × 1/6) + (32 × 1/6) + (42 × 1/6) + (52 × 1/6) + (62 × 1/6) = 1/6 + 4/6 + 9/16 + 16/6 + 25/6 + 36/6 = 91/6.
Hence from (7) we have Var(X) = 91/6 – (21/6)2 = 105/36 = 2 (11/12).
Example 4.6 Find the variance of a Bernoulli random variable with parameter p. that is X has the following p.d.f.
X 0 1 f(x) 1 – p P E(X) – 0 × (1-p) + (1 × P) = p E(X2) = 02 × (1-p) + (12 × p) = p. Hence, Var(X) = p - p2 = p(1 – p) Properties of Variance 1.
If c is a constant and P(X) = c) = 1, then Var (X) = 0.
2.
If a and b are contents, then (i) Var(aX) = a2 Var(X) (ii) Var(aX + b) = a2 Var(X) (iii) Var(X) ≥ 0 The proof of property (1) is very trivial and this is left as an exercise to the reader, we shall now give a proof of (ii).
From (7), Var(aX) = E[aX2] – [E(aX)]2 = E a2 X2 – [aE(X)]2 = a2 E(X2) – a2[E(X)]2 = a2{(E(X2)]2} = a2 Var(X).
Definition 5 Standard deviation.
Let X be a random variable with mean µ.
The standard deviation is the positive square root of the variance, and is given by The variance of the binomial random variable.
98 To calculate the variable of X, we need E(X) and E(X2).
Var(X) = (E(X))2.
From (3) we have E(X) =np.
E[X(X – 1)] = n( px(1 – p)n-x = px(1 – p)n –x x (w2hen x = 1 or 1 the expression is zero) = px(1 –p)n-x = n(n -1) p2 px-2(1 – p)n-x Let y = x – 2 = n(n – 1) p2 ( py (1 – p)n-2-y = n(n – 1) p2 [ P + (1 – p)]n-1 y Hence E(X(X – 1) = n(n – 1) p2 E(X(X – 1)) = E(X2) – E(X) = n(n – 1)p2 E(X2) = n(n – 1) p2 + E(X) = n(n – 1) p2 + np.
Thus, Var(X) = N(n – 1) p2 + np – n2p2 = n2p2 – np2 + np – n2p2 = np(1 – p).
The Variance of a Geometric Random Variable The probability density function of X is given by f(x) = p(1 – p)x-1, x = 1, 2, … E(X) = E(X) = P + 2p (1- p) + 3p(1 – p)2 + … = P{1 + 2(1 + p) + 3(1-p)2 + …}.
From the Binomial theorem (1 – a)-2 = 1 + 2a + 3a2 + … a < .
Putting a = (1 – p), we obtain1 D D D E(X) = P. (1 – (1 – p))-2 = E[X(X – 1)] = = p[2.1(1-p) + 3.2 (1 – p)2 +…+ r(r -1)(1 – p)r-1+…].
Multiply both sides by 1-p, (1 – p) E[X(X- 1)] = p(2.1(1 –p)2 + (3.2(1 – p)3 +…+ r(r-1)(1-p)r +…+] E(X(X-1)] – (1-p)E[X(X-1)] = p[2(1-p) + 4(1-p)2 + 6(1 – p)3 +…+ 2r(1 – p)r +…] = 2p(1 – p) [ + 2(1-p) + 3(1 – p)2 +…] = 2p(1- p).
Hence 99  E[X(X)] – (1-p) E[X(X – 1)] = E[X(X – 1)] {1 – (1-p)} = which implies that E[X(X – 1)] = E(X2) = + E(X) = Thus Var(X) = The Variance of a Poisson Random Variable Let X be a poisson random variable with parament λ. E(X) = = λe-λ e-λ eλ =λ E[X(X – 1)] = = e- λ λ2 E(X2) – E(X) = λ2 E(X2) = λ2 + E(X) =λ2 + λ Hence, Var(X) = λ2 + λ – λ2 = λ.
This shows that the mean and variance of a passion random variable are both equal.
Exercise 4.2 1.
Compute the variance and standard deviation of the random variable X defined in exercises 4.1.1 2.
A fair coin is tossed twice.
Let X denote the number of heads that appear.
Compute (i) the expected value of X (ii) the variance of X 100  (iii) the expected value of 3.
Calculate the mean, variance and standard deviation of the random variable having the following probability density function.
X -2 1 0 1 2 f(x) 3/10 1/5 1/10 1/5 1/5 4.
Let X be any random variance, show that E(AX + B) = aE(X) + B Where a, b are contants.
5.
Let f(x) = p X , x = 1, 2,…, 0 < p < 1 D D ± ± Show that f(x) is a pdf.
Find E(X).
6.
Let X be any random variable having finite expectation.
Prove that EX ≤ E X .
D D D D 7.
Let X be a random variable such that for some constant K, P( X ≤K).
Prove that D D X has finite expectation and EX ≤ k. D D 8.
Show that if X and Y are any two dependent random variables with finite variances, then Var(X + Y) ≠ Var(X) + Var(Y).
4.3 Probability Generating Function In this section we shall introduce important mathematical concepts in probability theory.
The rth moment of a random variable X is defined by E(Xr), where E(Xr) and E(X - µ)r are called the rth moment of X about 0 and µ respectively.
From (2) E(Xr) = Note that the second moment of X about µ is the variance of X. an indirect way of calculating expectation is the use of probability generating function or moment generating function.
it is a mathematical device to simplify the calculations of moments of X. its more general usefulness will not be apparent until we get to chapter 6.
101 Definition 6 The probability generating function of a non-negative, integer-valued random variable X is defined by E(Sx) = P(S) = (8) From (6), we see that P(S) = E(Sx) By differentiating with respect to s we have P’ (s) = E(XSX-1) P’’(s) = E[X(X – 1) SX-2] P(r) (s) = E(X(X – 1)(X – 2)…(X – r + 1)sX-1) Putting s =1 in the above derivatives, we have P’(1) = E(X) P’’(1) = E[X(X – 1)] = E(X2) – E(X) P(r) (1) = E[X(X – 1)(X – 2)…(X – + 1r)].
Thus the mean and variance of X can be obtained from P(s) by the following formulas.
E(X) = P’(1) E(X2) – E(X) = P’’(!)
E(X2) = P’’(1) + E(X) = P’’(1) + P’(1) And Var(X) = E(X2) – [E(X)]2 = P’’(1) + P’(1) – [P’(1)]2 We now illustrate the use of these formulas with the following examples.
Examples 4.7 Let X be a poisson random variable wit parameter λ.
Find the mean variance of X f(x) = The probability generating function is defined by P(S) = E(SX)= ( pX(1 – p)n-x = n(x(sp)x(1 – p)n-x.
x By the Binomial theorem = [sp + (1 –p)]n 102 Thus, P(S) = [sp + (1 – p)]n (12) Differentiating, we have P’(s) =np [sp + (1 – p)]n-1 P’’(s) = n(n – 1)p2 [sp + 1 – p]n-2 Var(X) = p’’ (1) + - [p’(1) – [p’(1)]2 = n(n – 1) p2 + np – (np)2 = np(1 – p).
The table below summarizes some of the results of this chapter Random Probability Mean Variance Probability Moment variable density generating function generating function function Bernoulli Px(1-p)1-x P P(1-p) (sp+1 – p) pet + (1-p) - ∞< t <∞ Binomial nC px(1-p)n- Np Np(1-p) [sp +(1-p)]n [pet+(1-p)]n x x x = 0, 1, 2, ….n Geometric p(-p)x-1 P[1-s(1-p)]-1 x = 1,2,… Poisson Λ Λ e λ(s-1) e λ(et – 1) Uniform X= a, a + 1,…b The Moment Generating Function Definition 7 Let X be a discrete random variable with pdf f(x).
the moment generating function denoted by M (t) is defined by x M (t) = E(etX) X Where t is any real constant for which the expectation exists.
(13) 103  M (t) = X RtX = 1 + tX + + … E(etX) = E(1 + tX + + …+ Under fairly general conditions we shall assume that expectation of infinite sum equals the sum of the expected value but this is true in general for finite sum 1 + tE(X) + + + … That is M (t) = 1 + tE(X) + X Differentiating with respect to t, we have M’ = E(X2) +…+ X(t) M’’ (0) = E(X2+) x hence, Var(X) = M ’’(0) – [M ’(0)2] x X And M (t) = E(etX), M (r)(t) = E[Xr e tX] (15) x x Putting t = 0, we have M(r)(0) =E(Xr) that is, E(Xr) is the rth derivative of M (t) evaluated at t=0.
(16) x Note: It is assumed that ∑ etX f(x) converges for all values of t. The domain of M (t) is all real number t such that ∑ etX f(x) converges.
If M (t) is defined x x for some t > 0, then it is defined in the interval 0 ≤ t ≤ t .
0 0 Examples 4.9 Let X be a binomial random variable with parameters n and p. then M (t) = E(etX) = nC Px(1 – p)n-x = nC (Pet)x(1 – p)n-x X x x from the binomial theorem (a + b)n = nC axbn-x = [pet + (1 – p)]n, x 104  on putting a = pet, b = 1- p. Example 4.10 Let X be a Poisson random variable with parameter λ.
Then M (t) = e-λeλet = eλ(et -1) X Differentiating, we have M’ (0) = λete-λ(1-et) X M’ (0) = λete-λ(1-et) = λ E(X) x λ=0 D M ’’(t) = λet e-λ(1-et) + λ2 e2te-λ(1-et) x M ’’(0) = λ + λ2 = E(X2) x Hence, Var(x) = λ + λ2 – λ2 = λ.
Example 4.11 Let X be a geometric random variable with parameter p. them M (t) = x = The fourth equality follows from sun to infinity of geometric series Further applications of M (t) will be considered later in Chapter 6. x Exercise 4.3 1.
Find the probability generating function of the geometric distribution with parameter P. hence determine the mean and variance of the distribution.
2.
Let X be uniformly distributed on (0, 1, 2,…n).
Find the mean and variance of X.
Hint: 3.
Let X be a random variable with finite variance.
Prove that for any real number a, Var(X) = E[X – a)2] – [E(X) + a]2.
4.
A fair die is tossed 50 times.
Let X be the number oftimes six appears.
Evaluate 105  (i) E(X) (ii) E(X2) (iii) VAR(X) 5.
Find the expected value and variance of the random variable X os Example 3.1.1.
6.
Show that if c is a constant, Var(x + c) = Var(X).
7.
Let X be uniformly distributed on (0, 1, 2,..N).
Find P(S), mean and variance of X.
8.
Let X be defined by x 1 2 3 f(x) 1 1 1 2 4 4 Find P(S) and hence the mean and variance of X.
9.
Prove that if X has variance 2, and mean µ, then σ every real number a.
10.
Derive the MGF for the following: (a) Bernoulli, (b) Negative Binomial, (c) Binomial and use it to find the mean and variance.
11.
A random variable X has the pdf f(x) defined by c(x + 2), x = 1, 2, 3, 4 f(x) = 0 elsewhere Find (i) c, (ii) moment generating function of X, (iii) use result of (ii) tofind the mean and variance of X.
12.
Prove that for any random variable, X, E(X2) > [E(X)]2.
13.
Show that [E(X – a)2] is minimized when a = E(X).
14.
A fair die is rolled until all the 6 sides appeared at least once.
Find the expected number of rolls needed.
p, x = -1 15.
Let f(x) = Px(1-p)2 x = 0,1, 2,… , (a) Show that f(x) is a pdf.
(b) Find M (t), the moment generating function of x (b) Hence, the mean and variance of X.
106  MODULE THREE (UNIT 1) CONTINUES RANDOM 5.1 INTRODUCTION In the chapter so far, we consider discrete random variables and their probability density functions e.g.
Binomial, Poisson, Geometric.
These are essentially counting variables.
However, there exist random variables whose set of possible uncountable, such random variables are called continuous random variables.
In this chapter, we shall study in considerable detail, a number of important continuous random variables and their characteristics.
We shall consider an idealized range space for a random variable X, in which all possible real number (in some specific intervals or set of intervals) many be considered as possible outcomes.
We shall essentially replace the summation in the previous chapters in the discrete probability function case by integration for the continuous probability density function case.
Example 5.1 Let a point be chosen at random from the interval (0, 2) and let X represent the point chosen.
Then 0 ≤ X≤ 2. the range space of X consists of unaccountable real numbers.
Any real number in the interval (0, 2) is a sample point.
Let A be the event that the point chosen is 0.3.
From the basic definition of probability we have 107  number of point A 1 P(A)= = =0.
number of pointsin W ¥ Hence, In general, if X is a continuous random variable, probability that X takes a specific value is 0. random variables denoting measurements of physical quantities such as time, weight, temperature, length are continuous random variables.
The proability that X lies between two defined values is given by P(a ≤ X ≤ b).
P(X ε A) = ∫ f(x)dx A For any set A of real numbers P(X) is denoted by F(X).
Definition The distribution function F of a random variable X is the function P(X) = P(X ≤ x), -¥ < x <¥ , If we know the probability distribution function of X, then we can calculate the probability that X fails in a specific interval.
For example P(a < X ≤ b) = P(X ≤ b) – P(X ≤ a) = F(b) – F(a).
Properties of Distribution Function 108 A probability distribution function is any function F satisfying the following conditions: (i) 0 ≤ F(x) ≤ 1 for all x (ii) F(x) is a non-decreasing function of x.
(iii) F(-∞) = 0 and F(+∞) = 1.
(iv) F(x +) = F(x) for all values of x where F(x+) = LimF(x + δ).
d- 0 Graphs of a typical distribution function Example 2 Show that ex x < 0 F(x) =  1 x > 0 is a probability distribution function Solution 109  0 ≤ ex ≤ 1 for x ≤ 0 and ex is a non decreasing continuous function, hence F(x) is a probability distribution function.
Probability density function P.df For discrete random variable x F(x) = P(X ≤ x) = ∑ f(y) - ¥ Where f(y) = P(X = y).
Similarly, for continuous random variables, there exists a function f(x) such that F(x) = P (X ≤ x) = ∫x f(y)dy x - ¥ Such a function f is called probability density function of X. to see this consider the following.
The probability that the random variable X takes on a value between x and x + Δx is P(x < X < x + Δx) = P(X ≤ x + Δx) – P(X < x) = F(x + Δx) – F(x).
this is the probability associated with an interval of length Δx F(x+ D x) - F(x) D x is the probability per unit length in the interval.
If we now let Δx tends to zero, we have F(x+x) - F(x) Lim = F'(x) = f(x) D x- 0 D x Hence, F(x) = ∫x f(y)dy where F(-∞) = 0 - ¥ 110 F(x) represents the probability per unit distance along the x acis and d F(x) = F(x) dx This shows that the p.d.f of X is obtained by differentiating the distribution function of X.
Properties of probability density function A probability density function is any function satisfying the following conditions.
(i) f(x) ≥ 0 ¥ (ii) ∫ f(x)dx = 1 - ¥ (iii) P(X < a) = ∫a f(x)dx = 1 - ∫¥ f(x)dx = 1 – P(X > a) - ¥ a (iv) if A is any interval, P(X ε A) = ∫ f(x)dx.
- ¥ Figure (2) represents a graph of the function f(x).
the area enclosed by the curve f(x) between a and b is ∫b f(x)dx = P(a ≤ X ≤ b) = F(b) – F(a).
a Graphical Representation of a Continuous Random Variable.
111  Theorem (i) The function F is non decreasing (ii) Lim F(x) = 0 and Lim F(x) = 1.
-x ¥ -x ¥ Proof: (i) Suppose x ≤ x and let A = { X ≤ x }, B = {X ≤ x ), then 1 2 1 2 A (cid:204) B since x ≤ x 1 2 Thus P(A) ≤ P(B)).
(ii) LimF(X) = Lim ∫x f(y)dy=0 -x ¥ -x ¥ - ¥ LimF(x) = Lim ∫x f(y)dy=1 -x ¥ -x ¥ - ¥ Examples e- lx, x ‡ 0 Exponential Distribution.
Let f(x) =  0, x < 0 The graph of f(x) is shown in Figure 2 below f(x) 112  Figure 3.
Density function of exponential distribution.
The random variable X having its probability density function given by (1) is said to have exponential distribution with parameter λ.
5.4 Normal Distribution Let f(x) = 1 e - 12x2,- ¥ <<x ¥ .
2p The graph of f(x) is shown in Figure 4 below.
Figure 4.
Standard Normal Distribution 113 5.5 Let the random variable X have the probability density function f(x) – 1/2e-1/2x 0 ≤ x ≤ ∞.
Find (i) P(2 ≤ X ≤ 7) (ii) P(X ≥ 10).
Solution 7 P(2 ≤ x ≤ 7) = ∫71/2e- 1/2x dx= - e - 2x I =e- 1- e- 7/2 =0.338 2 2 And ¥ ¥ P(X ≥10) = ∫ 1/2e- 1/2x dx= - e- 1/2xI =e- 5.
10 10 Suppose that X is a continuous random variable whose probability density function is given by A e-2x 0< x < ∞ f(x) = 0 otherwise (i) what is the value of a?
and (ii) find P(X > 1).
Solution (i) From property (ii) of a probability density function, we have ¥ ∫ f(x)dx =1 - ¥ implying that ¥ ¥ ∫ ae- 2x dx =1=a ∫ e- 2x dx =1/2 0 0 = = > a = 2 114 hence 2e -2x 0 < x < ∞ f(x) = 0 otherwise ¥ (ii) P(X > 1) = ∫ 2e- 2x dx=e-2 = 0.14 1 5.2 Expectation and variance of continuous random variables.
Let X be a continuous random variable with probability density function f(x).
Suppose we divide the range of X into small intervals, each of length Δx (by the definition of f(x)).
The expected value of X is therefore approximately ∑xf(x)D x The limiting value of this as Δx 0 ∫xf(x)dx.
This leads us to the following definition.
Definition 3 If X is a continuous random variable with probability density function f(x), then ¥ E(x) = ∫ xf(x)dx - ¥ In general, ¥ E(Xr) = ∫ xr f(x)dx - ¥ 115 ¥ E(Φ(X)) = ∫ F (x) f(x)dx.
- ¥ ¥ The variance σ2 = E[(X – μ)2] = ∫ (x- m )2 f(x) dx = E(x2) – μ2 - ¥ Where μ = E(x).
Examples 5.6 Suppose X is a continuous random variable with p.d.f f(x) = λe-λx, 0 < x < ∞.
Find (i) the mean and variance of X and (ii) the probability distribution function F(x), and the probability that X lies between 2 and 5.
Solution ¥ ¥ E(X) = ∫ x.le- lxdx=∫ xl e- lxdx - ¥ 0 By integrating by parts ¥ ¥ d ¥ 1 = ∫ - x (e- lx=) - xe- lx I +∫ e- lxdx= .
- ¥ dx 0 0 l Similarly, ¥ ¥ ¥ d ¥ E(x2) = ∫ x2.le- lxdx=∫ - x2 (e- lx)= - x2e- lx I +2∫ xe- lxdx 0 0 dx 0 = 2∫0xo- 2xdx = 2∫ xll- 2xdx = 2E(x) = 2 o l l2 116  2 1 1 hence, var(x) = - = l2 l2 l2 ∫xf(y)dy=∫ f(y)dy = ∫xll- lydy= - e- ly[0x= 1- e1x (u) F(x) = - = p(2< x<5) = ∫5le2xdx = F(5)- F(2)1- e- 5 - 1+e- m =e- m - e- m 2 5.7 Let X be a continuous random variable with probability density function 1 6 F(x) = 0<x<6 elesewhere find (1) F(x) (ii), E(x), E(x2) and var (x).
solution (1) F(x) = ∫x1 = p(x x) £ 0 6 E(x) = ∫0x1dx = 1 1 x2[6 = 1 - 1 - 36=3 0 6 6- 2 0 6 2 E(x2)= ∫6x2 - 1dx = 1 - 1x3[6=1 - 1- 63 =12 0 6 6 3 0 6 3 117 hence var(X) = 12-9 =3 5.3 symmetric probability Density Function definition 4 a probability density function f(x) is called a symmetric probability function if f(-x) =f(x) for all x.
1 1x2 for example.
If f(x) = 2xe isasynmericfunction.
2 similarly, 1 f(x) = - 0< x < a 2x is a symmetric probability density function Definition.
A probability density function f(x) is said to be symmetric about a if F(a + x)=f(a – x) for all x.
If x = 0. then f(x) is symmetrical about 0. for example, if 1 - 1 F(x) = 2p e (x- p )2 =< x <= 2 118  Then 1 1 m m 1 - 1 F((m - x) = e ( + x- )2 = e 2x 2 2p 2 2 1 1 m m 1 - 1 F(m - x) = e ( + x- )2 = e 2x 2 2p 2 m m hence, f( +x) = f( -x).
thus f(x) is symmetrical about m .
theorem 1 if f(x) is a symmetric probability density function then f(x) = 1-f(-x) where f(x) is the distribution function of x. proof f(-x)= ∫xf(y)dy = ∫- x f(- y)dy - - since f(y) =f(-y).
Let z = -y, then 119  f(-x) = ∫xf(z)(- 1)dz = ∫= f(z)dz = ∫=f(z)dz- ∫xf(z)dz =1- f(x) - x - - and hence f(x) = 1-f(x) terminology when we speak of the probability distribution of a random variable X we mean its probability density function and when we speak of the distribution function or cumulative distribution function we always mean f(x).
5.2 some important continuous random variables.
5.2.1 The Normal Distribution one of the most important continuous random variables is the Normal random variable, X.
The range of X is the real line.
Definition The Normal Distribution.
The continuous random variable X has the normal distribution with mean m and varianceo3 if m has the probability density function.
( ) 1 - 1 x- m 3 F(x) = e =< x<= 2p 2 02 The graph of f(x) is given in Figure (4) It can be verified that f(x) satisfies 120  f(x)dx =1 1.
∫ 2.
∫ xf(x)dx =m (x- m )2 f(x)dx =02 3.
∫ Properties of the Normal Distribution.
f(x)dx =1 1.
∫ this can be verified.
x- m Thus let z = s ( ) ¥ 1 - 1 x- m 2 ¥ 1 - 1 ∫ e 2 dx = ∫ e 2 dz =1 - ¥ s 2p s 2 - ¥ 2x 1 ¥ - 1y2 1 ¥ ¥ - 1(z2+y2) I2 = ∫ e 2 dy = ∫ ∫ e 2 dzdy.
2p - ¥ 2p - ¥ - ¥ Let us introduce polar coordinates to evaluate this double integral: Z = rcos θ, y = rsin θ I2 =1 ∫2p ∫¥ re- r2drdq = 1 ∫2p - e- r22 I ¥ dq = 1 ∫2p dq =1.
2 0 0 2p 0 0 2p 0 where dzdy = rdrdθ, Hence ¥ 1 - 1 (x- m )2 ∫ e 2 dx=1.
- ¥ s 2p s 2 121  2.
E(X) = μ, Var(X) = σ2.
The p.d.f is completely specified when μ and σ are known.
When μ = 0 and σ2 = 1.
X is said to have a standardized normal distribution.
This is, the pdf of X is f(x) = 1 e- 12x2,- ¥ <<x ¥ 2p The importance of the standardized normal distribution is the fact that it is tabulated.
3. f(x) is symmetrical about the point μ, hence F(-x) = 1 – F(x).
Most available tables are tabulated only for positive values of x since F(-x) can be found from f(x) and f(x) are given at the back of this book.
The graph of f(x) is bell-shaped curve centred at its mean.
X - m 4.
IF X has a normal distribution with mean μ and variance σ2 then Y = has a , o standardized normal distribution.
The proof of this is left as an exercise.
Y is said to have a standard Normal distribution.
5.
The moment generating function of X is defined by M (t) = E(etx) = ∫¥ etx 1 e- 12 (x- m )2 dx.
x - ¥ s 2p s 2 X - m 1 Let y = ,thendy= dx.
s s x = μ + y σ.
Thus, M (t) = ∫¥ et(m+ys ) 1 e- 12y2.s dxdy.
x - ¥ s 2p 122  = etm ∫¥ e- 12(y- ts )2 dy= etm +12s 2t2 1 ∫¥ e- 12(y- ts )2 dy.
2x- ¥ 2p - ¥ Let y – tσ = y, then 1 ∫¥ e- 12(y- ts )2dy= 1 ∫¥ e- 12y2 dy=1.
2p - ¥ 2p - ¥ Hence, tm +1s 2t2 M(t) = e 2 x Examples 5.8 If X has a standard normal distribution i.e.
mean of X, μ=0, and variance = 1.
Find (i) P(X ≤ 0.5) (ii) P(X ≥ 0.5) (iii) P(X ≤ -0.3) (iv) P(0.3 ≤ X ≤ 0.5) (v) P(-0.3 ≤ X ≤ 0.5) Solution ¥ (i) P(X ≤ 0.5) = ∫ f(x)dx = F(0.5).
- ¥ From table (3) F(0.5) is found by locating the first two digits (0.5) in the column headed x, the desired probability of 0.6915 is found in the row labeled 0.5 and the labeled 0.00.
That is P(X ≤ 0.5) = ∫0.5 1 e- 12x2dx = 0.6915.
- ¥ 2p and (ii) P(X ≥0.5) = 1 – P(X < 0.5) = 1 – 0.6915 = 0.3085.
(iii) P(X ≤ -0.3) = F(-0.3) = 1 – F(0.3) From the table, F(0.3) = 0.6179.
123 (iv) P(0.3 ≤ X ≤ 0.5) = F(0.5) – F(0.3) = 0.6915 – 0.6179 = 0.0736 (v) P(-0.3 ≤ X ≤ 0.5) = F(0.5) – F(-0.3) = 0.6915 – 0.3821 = 0.3074.
Example 5.9 If x is a normal variable = 5 and variance 02 = 16, find (i) P(X) < 6) (ii) P(3 < x < 7) (iii) P{X-5\>2}.
Solution From property (4) (i) P (X <(6) = P IJ K <J4 3 H L M / NHO L M / 0.5987 5 5 5 (ii) P(3 < X < 7) = P >J 4 IJ4 VJ4 H U U M / W'0.5 U O U 0.5X 5 5 5 = Ф(0.5) – Ф(-0.5) = 0.6915 – 0.3085 = 0.3830 (iii) P{|X-5|>2} = P(X – 5 > 2 or X – 5 < -2) = P(X > 7 or X < 3} = P(X > 7) + P(X<3) = P VJ4 >J4 HO Y M= HO U M 5 5 = P(Y > 0.5) + P(Y < -0.5) = 0.3085 + 0.3085 = 0.6170.
Notation: When we write X is N (o2), we mean that X has a Normal probability distribution with mean and variance o2 5.3.2 The Normal Approximation to the Binomial Let X be a binomial random variable with parameters n and p, then E(X) = np, var (X) = np (1-p) An approximation to the distribution of X is given by De Moivre Laplace limit theorem.
124 Theorem (De Moivre laplace Limit Thoerem) If X denote a binomial random variable with mean up and variance np(1-p), then for any a < b, ^ ')_ lim N\] L L ab / ФWaX' ФW]X.
0Z[ `)_W1'_X The accuracy of this approximation is quite good for values of n greater than 10 P(1-p).
This theorem says that if X is B(n,p) then large n, X is N(np, np (1-p).
Example 5.10 Let X be a binomial random variable with parameters n = 40, p = 0.20. find the probability that X = 5.
Since X is discrete and normal distribution is for continuous random variable, we write P (X = 5) = {(4.5 < 5.5 = P 5.4J5 IJ5 4.4J5 H U Y M 3.de 3.de 3.de = p (0.26 < y < 0.79) = Ф (0.79) – Ф(0.26) = 0.18226.
Notation: Whenwe write X is N (µ o), we mean that X has a normal probability 0 distribution with mean µ variance 02 5.2.4 The Exponential Distribution A continuous random variable, X is said to have an exponential distribution with parameter > 0 if its pdf is given by, Jjk hi lf ( m 0n fW(X / g 0 ( U 0 The pdf has the graph shown in Figure 3.
It can easily be shown that 125 [ Jjk o hi / 1. e The exponential distribution plays an important role in point processes and in describing random phenomena.
A number of real life situations can be modeled using an exponential distribution.
For example, time to failure of an equipment.
Properties of the Exponential Distribution (1) E(X) and standard deviation of X are the same.
The expected value of X is given by E(X) = [ Jjk p (hi q( e On integrating by parts we have [ [ [ Jjk Jjk Jjk ∞ Jjk 1 o (hi q( / o '(q ri s / '(i | = o i q( / e e 0 e h (ii) The variance of X is given by Var(X) = E(X2) – (E(x))2 E(X2) = [ : Jjk [ : Jjk : Jjk ∞ [ Jjk : pe ( hi q( / pe '( qri s / '( i |0 = 2pe (i q( / j, Hence, Var(X) = : 3 3 j, ' j, / j, .
Thus, the mean and standard deviation of an exponential distribution are the same.
(ii) Lack of Memory of the Exponential Distribution A random variable X is said to be without memory =, or memorable, if P(X> s + t\X> t) = P (X > s) for all s, t > 0 P(X > s + t) X > t = P (X > s + t and X > t = P(X> s + t P(X> t) P(X > t) 126 By the definition of memoryless, we have, P (x > s) = 1 – F(x + t) 1 – F (t) That is, 1 – F(s) = F(s+t) 1-F(t) (1-F(s)) (1-F(t) = 1 –F (s + t) F(X) = P(X< x) = dy = 1-ee 1 – F (x) = ex P (X > s) = 1-F (s) = ex P(X > s + t ) = 1 – F(s + t) = e (p(x>t) = 1 – F (t) = e P(X > s) = e e-µ = e thus, an exponential random variable is memoryless.
The only continuous random variable X assuming non negative values satisfying P(X < s + t/X > t ) = p (X > s) for all s, t > 0 (iii) The moment generating functions of X is given by (definition 7 of chapter 4).
5,2,5 The Gamma Distribution Definition 6 The gamma function, denoted by I’, is defined as follows: I (x) = By integrating by parts, we have F(X) = p['vkJ3 qWiJwXqv / 'vkJ3iJwx∞ = p[W( '1XvkJ:iJwqvn e 0 e 127  = (x – 1) [ kJ: Jw p v i qv / W( '1XyW( '1X e Γ(x) = (x – 1)Γ(x-1) = (x-1).
(x-2) Γ(X-2) = (X-1)!Γ(1)\ (1) = [ Jw p i qv / 1.
Hence e Γ(x) = (x-1)!
It is also easy to verity that (i) Γ - H3M / p[vJ,iJw qv / √{ : e (ii) |}- }(cid:127) [w ~ €w pe (cid:129)W|X / 1.
Definition 7 Let X be a continuous random variable assuming only non-negative values.
Then X is said to have a gamma distribution if its pdf is given by |}- jWjkX Jjk fW(X / \ (cid:129)W0X i ( Y 0 n 0 i‚ƒi„…i†i λ and n are called the paramenters of the distribution.
f(x) is denoted by Γ(n, λ).
Properties of the Gamma distribution 1.
E(X) = 0 0 ,ˆ]†W^X / , j j To see this, we have E(X) = |}- }‰Š | | }‰Š | [ jWjkX ~ [j I ~ j [ 0 Jjk p (.
q( /p q( / p ^ i q(.
e (cid:129)W0X e (cid:129)W0X (cid:129)W0X e Let y = λx, dy = λdx [ [ 0 [ v 1 1 0 Jw 0 Jw o ( i 'h( q( / o H M i qv / o v i qv.
0‹3 e e h h h e Hence, 128  E(X) = | .
j (cid:129)W0‹3X 0 .
|Œ- / (cid:129)W0X j j Similarly, E(X2) = |}- }‰Š | [ : jWjkX ~ j [ 0‹3 Jjk p ( .
q( / p ( i q(.
e W0X (cid:129)W0X e [ yW) =2X 0‹3 Jjk o ( i q( / .
0‹: e h Hence, E(X2) = | j (cid:129)W0‹:X 0W0‹3X (cid:129)W0X.
j|Œ, / j, .
Thus Var(X) = , 0W0 ‹3X 0 0 j, ' j, / j,.
The moment generating function of x is given by M (t) = | |}- }‰Š , x [ (cid:141)kj I ~ j [ 0J3 JkWjJ(cid:141)X p i q( / p ( i q( e (cid:129)W0X (cid:129)W0X e = | j 3 [ 0J3 JkWjJ(cid:141)X (cid:129)W0X .WjJ(cid:141)X|}-pe Ž(Wh '(cid:143)X(cid:144) i q(.
Let y = x(λ – t), dy = (λ – t) dx.
Therefore M (t) = | | 0 (8) x j [ 0J3 Jw j j (cid:129)W0XWjJ(cid:141)X|pe v i qv / (cid:129)W0XWjJ(cid:141)X|.yW)X / xjJ(cid:141)‘ 2.
If X is a random variable having Γ(n, λ) distribution and Y is a poisson random variable with parameter λx, then P(X ≤ x) = P(Y ≥ n) That is | |}- }‰(cid:127) “ }‰Š [j w ~ [WjkX ~ pe (cid:129)W0X qv / ∑0 ”!
.
3.
If n = 1, f(x) becomes 129  f(x) = λe-λx.
Hence, the exponential distribution is a special case of the Gamma distribution.
5.2.6 The Chi-square Distribution A special case of the gamma distribution in which λ = and n replaced by n/2, where n 3 is a positive integer is called the chi-square distributio:n. The pdf is given by | | | J3 3 , J k (, H:M i , 1 | J 3 JŠ fW(X / / | (, i ,,( Y 0 0 0 yH M 2,yH M : : A continuous random variable having pdf f(x) given by (7) is said to have a chi-square distribution with n degrees of freedom (denoted by χ ).
Because of its importance, the n chi-square distribution is tabulated for various value of the parameter n) see table 4).
If X has a chi-square distribution with n degrees of freedom, we have (i) E(X) = n; (ii) Var(X) = 2n; (iii) M (t) = (1 – 2t)-n/2; t > .
x 3 : The graph of f(x) is shown in figure below.
130  5.2.7 The uniform distribution A continuous random variable X assuming all values in the interval (a, b) where both a and b are finite is said to be uniformly distributed over the interval (a, b) if its pdf is given by 3 ] L ( L a fW(X / –—J˜ n 0 i‚ƒi„…i†i The graph of f(x) is shown in figure below: Properties of the Uniform Distribution 131 1.
E(X) š› œ› ˜‹ — ~ J ~ , ™kW(cid:143)X / : (cid:141)W— J ˜X , (cid:141)˜ W(cid:141)˜X i / 1=(cid:143)] = =(cid:157) :!
, (cid:141)˜ W(cid:141)—X i =1=(cid:143)a = =(cid:157) :!
š› œ› , , A A , ~ J ~ 3 r— J ˜ s(cid:141) W— J ˜ X(cid:141) / xWa ']X= = =(cid:157)‘ (cid:141)W— J ˜X — J ˜ :!
>!
, , A A , 3 r— J ˜ s(cid:141) W— J ˜ X(cid:141) ™IW(cid:143)X / x = =(cid:157)‘ — J ˜ :!
>!
M ’(0) = , , x W— J ˜ X W—J˜XW— ‹ ˜X — ‹ ˜ / / :W—J˜X :W— J ˜X : Similarly, M ’(0) = A A x W— J ˜ X >W—J˜X Hence, Var(X) = A A , , W— J ˜ X ˜ ‹ — — ‹ ˜— ‹ ˜ X ˜ ‹ — 3 : : ' / – / Ž2Wa = ]a =] X' 3W] =aX(cid:144) >W—J˜X : > : < F(x) = P(X ≤ x) = k J ˜ ] L ( L a \— J ˜ n 1 ( Y a 5.2.8 The Beta Distribution A continuous random variable X is said to have a beta distribution if its pdf is given by α, β > -1 f(X) = Ÿ ‹ ‹ 3X!
Ÿ ( W1'(X , 0 U ( U 1 \ Ÿ!
!
n 0 i‚ƒi„…i†i f(x) becomes the pdf of a uniform distribution over (0, 1) when α = β = 0.
Properties of the Beta distribution 132 1.
E(xr) = 3 ¡WŸ ‹ ‹ 3X!
Ÿ WŸ ‹ ‹ 3X!
¡ ‹ Ÿ p ( ( W1'(X q( / p ( W1 –(X q(.
e Ÿ!
!
!
Ÿ!
e From definition (Beta function) Ÿ!
!
3 Ÿ / p ( W1 '(X q( WŸ ‹ ‹3X!
e and 3 W† = ¢X!
£!
¡ ‹ Ÿ o ( W1 –¢X q( / e W† = ¢ = £ =1X!
We have E(xr) = WŸ ‹ ‹ 3X!
W¡ ‹ ŸX!
!
WŸ‹ ‹3X!W¡‹ ŸX!
9 / .
!
Ÿ!
W¡ ‹ ‹3X!
Ÿ!W¡‹ ‹3X!
The moment generating function of X does not have a simple form.
5.2.9 The Weibull Distribution A random variable is said to have a Cauchy distribution with parameter θ, -∞ < θ < ∞, if its pdf is given by f(x) = 3 3 ¤.Ž3 ‹ WkJ ¥X,(cid:144),'∞ U ( U ∞ For an illustration of a Cauchy distribution random variable.
See example 6.12 Exercises 1.
If X is N(0,1), determine the following probabilities (i) P(X > 0.9) (ii) P(X< -0.7) (iii) P(0.7 < X < 0.9) (iv) Find a such that P(X < -a) 0.10 (v) P(|X| < 1.5) 2.
A random value X has the pdf.
133  fW(X / ¦§( 0 U ( U 1n 0 i‚ƒi„…i†i Determine the value of the constant c. find F(x) and the mean and variance of X.
3.
A random variable X has the pdf.
( 0 L ( L 1 n fW(X / ¨2'( 1 ( L 2 0 i‚ƒi„…i†i (i) Show that f(x) is actually a pdf (ii) Find the means and variance of X.
4.
Let X bed a continuous random variable having probability density function f(x) = e|x|, - < x < 3 ∞ ∞ Find (i) P(-2<X<2) (ii:) EW(X) (iii) Var (X) 5.
Let X be a Normal random variable with mean 8 and variance 2.
Find P(-2<X<6).
Determine the values of a and b such that P(X< a) = 0.25 and P(X>b) = 0.25 6.
Suppose that X is N(5, 9).
Find a constant c such that P(X > c) = 2P(X< c).
7.
Let a point be chosen uniformly in the interval (a, b).
).
Let let X denote the distance of the point chosen from a. find the probability distribution function F(x) of X.
8.
(a) Show that [ J-k, p i , q( / √2{ J[ (b) Show that f(x) = 1-|α|, -1 < x < 1 is a pdf 9.
Show that “}- Š }‰ [w Jw ”J3j ~ pj (cid:129)W”X i qv / ∑k©e k!
.
10.
The lifetime in hours of an electric light bulb is a random variable with probability density function f(x) = 3 ªexpH' (M,( Y 0 :4e Determine the value of θ 11.
A probability density function f of random variable X is defined by 134 0, ( U 0 ¯ (, 0 U ( U 1 fW(X / 3 n , 1 U ( U § ® : 0, ( Y § Where c is a constant (i) Calculated the value of c and hence the mean and variance of X.
(ii) Sketch the graph of f. 12.
A random variable X has pdf.
F(x) = k exp {-(x-2)2/9} < x < (i) Find the value of k, (ii) What is the probability distribution of X?
13.
Suppose (x) = f(x) , where F(x) = f(x) x > 0 1 – F(x) Show that F(x) = 1 – exp – { j p hW(Xq(° e 14.
The pdf of a random variable X is defined by f(x) = ½ e|x|, - < x < ∞ ∞ Find P(1<) /x/ < 2 15.
(a) If X is N (o2) find k (as a function of u and 02 such that P(X > k) = 2P (X <k) 16.
Let X be a random variable with pdf (a) f(x) = λe-λ(x-a) x ≥ a; (b) f(x) = ½ -λ(x-a) - < x < ∞ ∞ Find the M.G.F of X and hence the mean and variance of x in each case (a) F(x) = e x > a; (b) F(x) = 1/2 e < x < Find the M.G.F of X and hence the mean and variance of X in each case 17.
A random variable X has mean 5 and variance 3.
If the pdf of X is given by f(x) = , if a < x < b and zero otherwise.
Find a and b 3 — J ˜ 135 18.
If f(x) = in 1/x, 0 < x < 1 and 0, otherwise show that f(x) is a pdf 19.
Let X be a random variable with zero expectation and probability density function given by f(x) = k ¢ i '∞ U ( U £ n ¦ 0 i‚ƒi„…i†i Find α.
20.
Let f(x) = ½ (k) = ½ (k-1)/ (1+/x/)k, k ≥ 1-∞ < x< ∞show that f is pdf 136 UNIT 2 JOINTLY DISTRIBUTED RANDOM VARIABLES 6.1 Bivariate Distribution In our study so far, we have considered only univariate distribution, Univariate distribution depends on only one random variable but we are frequently interested in specifying two or more random variables for a member of a population and interested in probability statements.
For example, the age A and the systolic blood pressure, P may be of interest and we would consider (A,P) as a single experimental outcome.
We might study the height H and the weight W of a chosen person given rise to the outcome (h, w).
A probability distribution which depends on two random variables.
Table (1) gives the joint variables.
Table (1) gives the joint probability density function of two discrete random variables X, number of white marbles, and Y, number of blue marbles in a sample of 3 chosen at random from a box containing 4 red, 3 white and 5 blue marbles.
Table 1 X/Y 0 1 2 3 0 1/55 3/22 2/11 1/22 21/55 1 9/110 3/11 3/22 0 2755 2 3/55 3/44 0 0 27/220 3 1/220 0 0 0 1/220 35/220 21/44 7/22 1/22 1 We denote the probability that X takes the value of x and Y the value y by f(x, y) where f(x, y) = P(X = x, Y = y).
For example, the probability that the sample contains 1 white and 2 blue marbles is denoted by f(1, 2).
From the table, 137  f(1, 2) = 3/22 This is calculated thus: P(X = 1, Y = 2) = The reader should verify the entries in table (1).
Note: (i) ∑∑ f(x, y) = 1.
(ii) 0 ≤ f(x, y) ≤ 1 for all (x, y).
(iii) the row totals form what is called the marginal distribution of X denoted by f (x) x and the column totals is called the marginal distribution of Y denoted by f (y).
y From table (1) the probability of obtaining a particular value of X irrespective of the value of Y is given below.
Table 2 x 0 1 2 3 f (x) 21/55 27/55 27/220 1/220 x Similarly for Y, we have y 0 1 2 3 35/220 21/44 7/22 1/22 f (x) Y If we know the joint probability den sity functio n of two ra ndom variables X and Y, then we can compute the marginal probability density function f (x) by summing over y and x the marginal probability density function f (y) by summing over x i.e.
y f (x) = x f (y) = (1) y Note: It may happen that X is discrete while Y is continuous, but in most applications it is either both are discrete or both are continuous.
138 Definition 1 Two discrete random variables X and Y are said to be independent if their joint density function is give by f(x, y) = f (x) f (y) x Y i.e.
P(X = x, Y = y) = P(X = x) P(Y = y) for all pairs of x, y.
In the above example, f (1) = x f (2) = y f(1, 2) = hence X and Y are dependent (not independent) random variables.
6.2 Expectation of Sum and Product of Jointly Distributed Discrete Random Variables Let f(x, y) be the joint probability density function of X and Y and f (x), f (y), the x y marginal probability densities of X and Y respectively.
Then E(X) = ∑ xf (x) x and E(Y) = ∑ yf (y).
y y For example, consider the variables X and Y having the joint pdf given by the table (1).
The marginal pdf is given by table (2).
Then and E(X + Y) = E(X) +E(Y) = 2.99.
Expectation of Product The expectation of product of two discrete random variables is defined as 139  E(XY) = For the above example we have ( (0 ×0) × + ( (0 ×1) × ( (0 ×2) × + ( (0 ×3) × + + + + + × = + Variance of Sum of Two Random Variables From the definition variance E(X2) – [E(X)]2 replacing X by X + Y we have Var(X + Y) = E(X + Y)]2 = E(X2 + Y2 + 2XY) – [E(X)]2 – [E(Y)]2 – 2E(X)E(Y) = E(X2) + E(Y2) + 2E(XY) – [E(X)]2- [E(Y)]2 – 2E(X)E(Y) = E(X2) – [E(X)]2 + E(Y2) – [E(Y)]2 + 2[E(XY) – E(X)E(Y)] = Var(X) + Var(Y) + 2[E(XY) – E(X)E(Y)] = Var(X) + Var(Y) + 2Cov(X, Y)] Where Cov(X<Y)= E(XY) – E(X)E(Y) Definition 2: Covariance The covariance of two random variables X and YU denoted by Cov(X< Y) is denoted by Cov(X, y) = E{(X –E(X))((Y – E(Y)} = E[XY – XE(Y) – YE(X) + E(X)E(Y) = E(XY) – E(X) E(Y) Example 1 Given the following joint probability density function of X and Y.
Find (i) The marginal probability density function of X and Y (ii) The expectation of X and Y (iii) The covariance of X and Y 140 (iv) The variance of X + Y. X/Y 1 2 3 3 .1 .2 0 4 0.2 0 .1 6 .15 .15 .1 (i) Summing along the rows we have x 3 4 6 f (x) .3 .3 .4 X Summing along the column, we have y 1 2 3 f (y) 0.45 0.35 0.2 Y (ii) E(X) = (3 × 0.3) + (4 × 0.3) + (6 × 0.4) = 4.5 Similarly E(Y) = (1 × 0.45) + (2 × 0.35) + (3 × 0.2) = 1.75 (iii) Cov(X, y) = E(XY) – E(X)E(Y) E(XY) = ∑∑xyf(x, y) = ((3 × 1) × 0.1) + ((3 × 2) × 0.2) + ((4 × 1) × 0.2) + ((4 × 3) × 0.1) + ((6 × 1) × 0.15) + (12 × 1.5) + ((6 × 3) × 0.1) = 8.0 Hence, Cov(X, Y) = 8.0 – (4.5)(1.75) = 0.125 (iv) E(X2) = 21.9 Var(X) = 21.9 – 20.25 = 1.65 E(Y2) = 3.65 Hence, 141  Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y) = 1.65 + 0.25 = 2.49.
6.3 Independent Random Variables X and Y are said to be independent if P(X = x, Y = y) = P(X = x) P(Y = y) For all pairs (x. y).
That is, f(x, y) = f (x)f (y) for all pairs (x, y).
x y thus for any subset A and Bof R P(X ε A, Y ε B) = P(X ε A) P(Y ε B).
Example 2 Two random variables X and Y have the joint probability distribution given by X/Y 1 2 1 0.2 0.4 2 0.3 0.1 Show that X and Y are not independent.
Solution The marginal pdf of X is x 1 2 f (x) 0.6 0.4 X And that of Y is Y 1 2 f (y) 0.5 0.5 y 142 P(X = 1, Y = 1) = 0.2 P(X = 1) = 0.6, P(Y = 1) = 0.5 P(X = 1) P(Y =1) = 0.30 ≠ P(X = 1, Y =1) = 0.2 Hence X and Y are not independent.
Consider two independent random variables X and Y. then E(XY) = E(X)E(Y).
To see this, note that E(XY) = = Thus if two random variables X and Y are independent, then E(XY) = E(X)E(Y) (3) That is Cov(X, Y) = 0.
Note that independent and zero covariance are not the same.
That is if E(XY) = E(X)E(Y) it does not imply that X and Y are independent.
Definition 3: Correlation Coefficient Let X and Y be two random variable having finite non-zero variance.
One measure of degree of dependence between X and Y is the correlation coefficient denoted by p(X, Y) and defined by ρ = ρ (X, Y) = it can be shown that -1 ≤ p ≤ 1.
If X and Y are independent random variables then (i) ρ = 0 (ii) Var(X + Y) = Var(X) + Var(Y) (iii) Var(aX + bY) = a2Var(X) + b2Var(Y) where a and b are constants.
(iv) Var(X – Y) = Var(X) + Var(Y).
6.4 Conditional Probability Density Function In chapter Two, we defined the conditional probability of an event A given event B by 143  P(A B) = D Similarly, this idea can be extended to random variables.
Thus P(X =x Y =y) = D Definition 4 If X and Y are two jointly distributed discrete random variables, trhe conditional probability density function of X given Y = y is denoted and defined by f (x y) = P(X = x Y = y) = X y D D D It follows that f (x y) = X y D D And F(x, y) = f (x y)f (y).
X y y D D Example 3 Let the joint probability density function of X and Y be given by the table below.
X/Y 1 2 3 3 .1 .2 0 4 .2 0 .1 6 .15 .15 .1 Compute the conditional probability density function of X given that Y = 2.
Solution From the joint p.d.f.
table we have using the formula f (x y) = X y D D f (3 2) = X y=2 D D f (4 2) = X y=2 D D 144  f (6 2) = X y=2 D D Hence, the conditional pdf of X given Y = 2 is X 3 4 6 f (x 2) 0 X y=2 D D It is left as an exercise to show that the conditional pdf of X given that Y= 1,3 are 1. f (x ) X 1 D D1 x 3 4 6 fX 1(x 1) 2 4 3 D D 9 9 9 2. f (x ) X D3 D3 X 3 4 6 f (x 3) 0 X 3 D D Definition 5: Conditional Expectation The conditional expectation of X given Y = y is defined by E(X y) = ∑xf (x y) x y D D D Where f (x y) is the conditional probability density function of X given Y = y. X y D D Definition 6: Conditional Variance The conditional variance of X given y is defined by Var(X y) = E(X2 y) – [E(X y)]2 D D D Example 4 Calculate the conditional expectation and conditional variance of X given that Y = 2 where the joint pdf of X and Y is given in Example 3.
From example 3, we have 145  E(X Y = 2) = D E(X2 Y = 2) = D Hence condition variance of X given that Y = 2 is Var(X Y = 2) = E(X2 Y = 2) – [E(X Y = 2)]2 = D D D Exercise 6.1 1.
Two discrete variables X and Y have the joint probability distribution given below.
X/Y -1 5 7 0 0.1 0.2 K 1 0.05 0 0.15 2 0.3 0.1 0 (a) Find the value of k, (b) Find the marginal distribution of X and Y and (c) Are X and Y independent?
2.
Given the following joint probability density function of random variables X and Y. X/Y 1 2 3 1 1/8 1/8 ¼ 2 3/8 0 0 3 0 1/8 0 Find 146  1.
The marginal probability density function of X and Y and hence E(X) and E(Y).
2.
Are X, Y independent?
3.
Find the conditional pdf of Y given that X = x for which E(Y x) is defined.
D 3.
The joint probability distribution of two random variables X and Y is given by X/Y -1 0 3 2 .1 0 .1 3 .2 .1 0 4 0 .3 .2 (a) Determine whether or not X and Y are independent.
(b) Calculate the correlation coefficient, P(X, Y).
(c) Find the variance of X + Y.
4.
Suppose a box has 3 balls labeled 1, 2, 3.
Two balls are drawn at random one after the other without replacement.
Let X and Y denote the number on the first and second balls drawn respectively (a) Find the joint probability distribution of X and Y (b) Determine whether or not X and Y are independent.
5.
Suppose the situation is as an exercise 4, except now that the two balls are selected with replacement.
(a) Find the joint probability distribution of X and Y (b) Determine whether or not X and Y are independent.
(c) Find the Var(X + Y).
147 6.
Let X and Y be two random variables having the joint probability density function given by the following table.
X/Y -3 -1 0 2 5 -1 1/20 1/10 0 0 3/20 3 1/40 1/40 1/20 1/20 0 4 3/40 1/40 1/40 1/20 1/20 5 1/20 1/40 1/10 1/20 1/10 Compute the following probabilities (i) X is even (ii) Y is odd (ii) X + Y is even (iv) XY is odd (v) X > 0 and Y > 0 (vi) X is even given that Y is even (vii) X > 0 given that Y is ≤ 0.
6.5 Multinomial Distribution Multinomial distribution is a generalization of the binomial distribution in which each of n independent identical experiments can result in any of k possible outcomes with probabilities p p ,…p , ∑p = 1.
1, 2 k k For k= 2 we have the binomial distribution.
Consider an experiment, such as rolling a die, that can result in only a finite number of k distinct outcomes at let X be the number of the n experiments that result in outcome i number i.
Then P(X = x , X = x = … X = x ) = x , x … x ) 1 1 2 2 k k 1 2, k 148 Where c x , x … x ) = number of possible sequences of outcomes for the n experiments 1 2, k to yield X = x , X = x = … X = x .
1 1 2 2 n n From our knowledge of mathematics of counting, c x , x … x ) = 1 2, k can be regarded as the number of ways n objects can be partitioned into k classes such that class contains x objects (I = 1, 2, …k).
Thus, i P(X = x , X = x ,… X = x ) = 1 1 2 2 k k Example 5 Suppose that a fair die is rolled 12 times.
Find the probability that 1 and 6 appear 2 times each.
3 thrice each, 3 times, and 4 and 5 once each.
Solution Let X be the number of times ith outcome appears.
i n=12, x = 2, x = 3, x = 3, x = 1, x = 1, x = 2, p = 1 2 3 4 5 6 i Hence, P(X = 2, X = 3, x = 3, x = 1,x = 1, x = 2) 1 2 3 4 5 6 = Example 6 In a certain large population, 70% are right-handed, 20% left handed and 10% are ambidextrous.
If 10 persons are chosen at random from the population, what is the probability that (i) all are right-handed?
(ii) 7 are right-handed, 2 are left handed and 1 is ambidextrous.
Solution 149 P = 0.7,P = 0.2,P = 0.1 1 2 3 Let X , X , X denote number that are right-handed and ambidextrous respectively, then 1 2 3 P(X = x , X = x = X = x ) = 1 1 2 2 3 3 (i) P(X = 10, X = 0 = X = 0) = (0.7)10(0.2)0(0.1)0 = (0.7)10 = 0.028 1 2 3 (ii) P(X = 7, X = 2 = X = 1) = (0.7)7(0.2)2(0.1)1 = 0.0119.
1 2 3 6.6 Continuous Random Variables In this section we will consider a pair of continuous random variables X and Y and some of their properties.
The results in the earlier section, which have been proved for discrete random variables are also applicable to continuous random variables.
The joint probability density function of X and Y is denoted by f(x, y) such that and f(x,y) ≥ for all (x, y) ε RRR (that is, summation in discrete case is replaced by integrals) and the marginal probability density functions are given by f (x) = X f (y) = Y The joint probability distribution function F is defined by F(x, y) = P(X ≤ x, Y ≤ y) Where X, Y are defined on the same probability space.
F(x, y) = 150 If B is in the range space of (X, Y) P(X, ε < = B) = P(B) = and The marginal probability distribution function are defined yb F (x) = P(X ≤ x) and F (y) = P(Y ≤ y).
x y From the joint probability distribution function we can get the marginal distribution functions by the following relationship F (x) = F(x, ∞) = X F (y) = F(∞, y) = Y Let X and Y be any continuous random variables.
Then X and Y are independent if and only if f(x, y) = f (x)f (y), -∞< x <∞, -∞< y <∞ X Y The conditional probability density function of X is defined by Similarly, the conditional probability function of Y given X is The conditional expectation is defined by E(Y X=x) = D 151 Example 7 Let X and Y have the joint pdf 2 0 < x < y < 1 f(x, y) = 0 elsewhere Find (i) The marginal probability density functions.
(ii) Determine whether or not X and Y are independent (iii) Find the conditional expectation of X given Y = y.
Solution The marginal probability density function is given by (i) f (x) = X f (x) = X and f (y) = Y = Hence 2(1 – x) 0 < x < 1 f (x) = x 0 elsewhere and 2y 0 < y < 1 f (y) = y 0 elsewhere (ii) f (x)f (y) = 2(1 - x) 2y = 4y(1-x) ≠ f(x, y) x Y So X and Y are not independent.
(iii) E(X y) = D = = Example 8 Let the random variables X and Y have the joint pdf.
x+y 0 < x < 1, 0 < y < 1 152  f(x, y) = 0 elsewhere Find (i) the marginal probability density functions, (ii) are X, Y independent?
And (iii) find the Cov(x, Y).
Solution (i) f (x) = X = f (y) = Y = Hence, +x 0 < x < 1 f (x) = X 0 elsewhere +y 0 <y< 1 f (y) = Y 0 elsewhere (ii) f(x, y) ≠ f (x)f (x)).
Thus, X and Y are not independent.
X Y (iii) E(X) = E(Y) = E(Y) = Hence E(XY) = 1/3 153 Thus, Cov(X, Y) = 1/3 – 7/12.7/12.
= -1/144.
Example 9 Let the random variables X and Y have the joint pdf 0 <y< x < 1 f(x, y) = 0 elsewhere Find the marginal p.d.f of X and of Y and the conditional pdf of Y given X = x.
Solution Therefore 1, 0 < x < 1 f (x) = X 0 elsewhere F (y) = Y Therefore -log y 0 <y< 1 f (y) = y 0 elsewhere 0 <y< x < 1 f (y x) = = Y x D D 0 Hence X and Y/x are uniformly distributed over (0, 1), (0, x) respectively.
Example 10 Let the random variables X and Y have the joint pdf x2+ 0 ≤x ≤ 1, 0 ≤ y ≤ 2 f(x, y) = 0 elsewhere Find the P(X + Y ≤ 2).
154 Solution Let A ≡{x, y: x + y ≤ 2}.
P(A) = = Exercise 6.2 1.
Suppose that X and Y have the joint p.d.f K e-λ(x+y)x≥ 0, y ≥ 0 f(x, y) = 0 elsewhere Find (i) K and (ii) the marginal pdf of X and of Y.
2.
Suppose that X and Y have the joint pdf C(x-y) 0 <x <2 f(x, y) = -x< y < x.
0 elsewhere (a) Find (i) c and (ii) the marginal pdf of Xand of Y.
(b) Compute the following (i) P(x< 1) and (ii) P(0 < Y < 2).
3.
Suppose that X and Y have the joint pdf 0 ≤x ≤2 f(x, y) = 0 elsewhere (i) Find K using the fact that ∫ ∫ f(x, y) dxdy = 1.
(ii) Are X and Y independent random variables.
4.
Two random variables X and Y have the joint pdf given by c(x – 2xy + y)0 ≤x ≤ 1, 0 ≤ y ≤ 1 f(x, y) = 0 elsewhere (a) Show that X and Y are uniformly distributing over [0, 1].
(b) Find the joint probability distribution function of X and Y.
Hence, find the marginal distribution functions of X and Y.
5.
Suppose that X and Y have the joint pdf λ2e-λy 0 ≤x ≤y f(x, y) = 0 elsewhere (i) Find the marginal probability density functions of X and of Y.
6.
Suppose that X and Y have the joint pdf f(x, y) = -∞< x <∞ 0 -∞< x <∞ (i) Determine the value of c. 155  (ii) Are X and Y independent random variables?
7.
Let X and Y have the probability density function defined by 2 if (x,y) is inside the triangle having variances at f(x, y) = (0, 0), (0,1), (1, 0) 0 elsewhere Find the marginal probability density function of X and Y 8.
Let X /X + X ) (ii) E(X IX + X = m).
1 1 2 1 1 2 9.
A fair die is successively rolled.
Let X and Y denote, respectively, the number of rolls necessary to obtain a 6 and a 5.
Find (a) E(X) (b) E(XIY = 1), and (c) E(XIY = 5).
10.
Let the random variables X and Y have the joint pdf F(x, y) = 0 < x <∞; 0< y <∞.
Find E(XIY = y).
11.
If it is assumed that, in single – car accidents in a certain area, the probabilities of minor severe, and fatal injuries to the driver are 0.5, 0.4 and 0.1 respectively, find the probability that in 10 accidents there are 4 minor, 4 severe and 2 fatal injuries to the 10 drivers.
156  Example 2 Let X have the poisson pdf with parameter λ.
Find the pdf of Y = X2 f(x) = }‰ ± ~ j ;^ / 0,1,2,… k!
Solution Possible values of Y are: 0, 1, 4, 9, 16… P(Y = y) = P(X2 = y) = P(X = + ) (single-valued inverse function) `v =f( ), x = `v `v Jj `(cid:127) = ~ j , n ¨ √w v / 0,1,4,9,16… Since X is a positive random variable)0, y = 0 ,i 1‚ƒ, i4„, 9…,i 1†6i, … Example 3 Suppose X is a random variable having probability density function f(x) given by X -2 -1 0 1 2 3 f(x) .15 .25 0.15 .2 .15 .1 Find the p.d.f.
of Y = X2 Solution Possible values of Y are 0, 1, 4, 9.
This is not a one-to-one function P(Y = 0) = P(X = 0) = 0.15 P(Y = 1) = P(X = -1 or 1) = P(X = -1) + P(X = 1) = 0.25 + 0.2 = 0.45 P(Y = 4) = P(X = -2 or 2) = P(X = -2) + P(X = 2) = 0.15 + 0.15 = 0.3 P(Y = 9) = P(X = 3) = 0.1 Hence, the pdf of Y is given by y 0 1 4 9 h(y) 0.15 0.45 0.3 0.1 Example 4 Let X has the pdf given by k 3 fW(X / H M ,( / 1,2 0: elsewhere Find the pdf of Y defined by 1 if X is even Y = 1 if X is odd Solution 157  P(Y = 1) = P(X = 2, 4, 6, 8,… = P(X =2) + P(X = 4) + P(X = 6) +… : 5 < 3 3 3 3 / H M = H M = H M = … / Similarly, : : : > P(Y = -1) = : .
Hence, the pdf of Y is given by > Y 1 1 g(y) 1 2 3 3 Continuous Case Let X be a continuous random variable having probability density function f(x).
we wish to find the probability density of a variable Y = g(X).
We shall discuss methods for finding the pdf of Y.
Let F (y) be the probability distribution function of Y.
Then, expressing F (y) in terms of Y Y X, we obtain F (y) = P(Y ≤ y) = P(g(X)) ≤ y) Y = P (X ≤ g-1(y)) if Y is a strictly increasing function of X P (X ≥ g-1(y)) if Y is a strictly increasing function of X Where X = g-1(y) is the inverse of the transformation Y = g(X) and g(X) is a one-to-one function.
Hence, F (y) = F (g-1(y)).
Y X Thus, the pdf of Y is given by F (y) = F (y) = F (g -1(y)).
Y Y X Example 5 Let X be uniformly distributed over (0, 1).
Find the pdf of Y = J3 log~W1'(X, ( Y 0. j Solution F (y) = P(Y ≤ y) = P Y 3 = P(log (1 – X)j ≥lo -gλ~yW)1 =' P((1X –L X v ≥X e/-λyN) W=' Pl(oXg W≤1 1' - (e-XλyL) hvX The pdf of X is given by 1 0 < x < 1 f(x) = 0 elsewhere and 158  F (u) = X ´ ´ p fW(Xq( / p 1 q( / µ Hence e e F (y) = (1 - e-λy) Y Thus, the pdf of Y is given by λe-λy y ≥ 0 f(x) = 0 elsewhere Example 6 Let X has the pdf, f(x) = e-x, x > 0.
Find the pdf of Y = 2X + 1.
Solution P(Y ≤ y) = P(2X + 1 ≤ y) = P wJ3 H^ L M P(X ≤ x) = : k Jk Jk p i q( / 1'i e F X wJ3 wJ3 JWwJ3X/: H M / PH^ L M / 1'i .
On differentiating, we obtain: : F (y) = F’ (y) = - Y y 3 J WwJ3X i , .
Hence, : - 3 J WwJ3X i , ,v m 1 f (y) = : Y 0 elsewhere Example 7 let X has the density function given by jJ3 f (x) = λ( x 0 ≤ x ≤ 1 0 elsewhere Show that Y = - log X is an exponential random variable.
e Solution P(Y ≤ y) = P(-log X ≤ y) = P(X ≥ e-y) = 1 – P(X ≤ e-y) e P(X ≤ u) = ´ jJ3 j µ j thus, pe h( q( / ( | / µ 0 P(X < e-y) = e-λy P(Y ≤ y) = 1 – e-λy; f (y) = λe-λy, y ≥ 0 Y Hence, F (y) = P(Y ≤ y) = 1- e-λy, Y λe-λy , y ≥ 0 F (y) = F’ (y) = y Y 0 elsewhere This shows that Y is an exponential random variable.
Example 8 159 Let X be a random variable having pdf 2x 0 ≤ x ≤ 1 f (x) = x 0 elsewhere Find the pdf of Y = 4x2.
Solution FY(y) = P(Y ≤ y) = P(4X2 ≤ y) = PH^: ≤ wM 5 = PH^: ≤ wM / PHJ√w ≤ ^ ≤ ‹√wM= FXH‹√wM - FXHJ√wM 5 : : : : FX(u) = p´fW(Xq( / p´2( q( / (:|µ / µ: e e 0 Thus, : F (y) = F √w F J√w Y XH M ; XH M / 0 : : since X cannot be less than zero.
Hence, : F (y) = √w w Y H M / .
: 5 Thus, the pdf of Y is 3 0 < Y < 4 5 F’ (y) = f (y) = Y Y 0 elsewhere that is, Y is uniformly distributed over (0, 4).
Example 9 Let X be a continuous random variable having probability density function f(x).
derive an expression for the probability density function of the random variable Y = X2.
Solution F (y) = P(Y ≤ y) = P(X2 ≤ y) = = F (+ ) - F ).
Y W'`v ≤ ^ ≤ = `vX X `v XW'`v Differentiation gives F’ ( ) = 3 f , F’ ) = J3 f .
X `v XW`vX XW'`v XW'`vX :√w :√w Hence 3 (f + f y > 0 Xr`vs Xr'`vs; :√w F’ (y) = f (y) = Y Y 0 elsewhere Example 10 Lex X be a random variable having the pdf.
3 (x + 1); -1 ≤ x < 1 : f(x) = 160  0 elsewhere Find the pdf of Y = X2.
Solution From example 10above, we have f (y) = (f + f ) Y 3 X X W`vX W'`v :√w f ( ) = X 3 `v W`v hence, : = 1X - 3 3 3 3 3 , 0 fIWvX / :√w¦:r`v = 1s= :W'`v = 1X¸ / :√wW1X / \0:v U v U 1n i‚ƒi„…i†i Note: The transformation Y = X2 mapped x:-1 ≤ x < 1 onto the set y: 0 < y < 1.
Example 11 Let X be a continuous random variable having pdf f(x).
Derive the formula for the pdf of Y = |X| F (y) = P(Y ≤ y) = P(|X| ≤ y) = P(-y < X < y) = F (y) – F (-y) Y X X By differentiation we see that F (y) = Y 0 WfkWvX= WfkW'vX v Y g 0 0n v ≤ Theorem 1 Let Y = g(X) be a differentiable strictly increasing or strictly decreasing function on an interval D and let D * denote the range of g. let X e a continuous random variable with pdf f(x) such that f(x) = 0 for x ϵ D. Then the pdf of Y is given by f (y) = f (g-1(y)) * Y X }- €º WwX where x = g -1(y) is the inverse functio¹n of g. e¹q;uvi v»ale¼ntly €w f (y) = f(x) Y €k where x is expressed in terms of ¹y, y¹ ϵ D* and x = g -1(y).
€w Proof: Case 1: suppose that g(X) is strictly increasing F (y) = P(Y ≤ y) = P(g(X) ≤ y) = P(X ≤ g -1(y)) = F (g-1(y)) Y X Thus by differentiation, we have F’ (y) = f (g-1(y)) (g-1(y)).
Y X € Since g(X) is strictly increasing so also is g-1(y).
€w Hence € J3 € J3 Since €wŽ ½ThuWsv XX(cid:144) / ¹€wŽ½ WvX¹.
€ J3 0 ¾½ WvXs Y .
€w 161  F (y) = f (g -1(y)) Y X € J3 €k J3 ¹ Ž½ WvX¹ / fIW(X¹ ¹;( / ½ WvX €w €w .
Case II: When g(X) is a strictly decreasing function of X F (y) = P(Y ≤ y) = P(g(X) ≤ y) = P(X ≥ g-1(y)) Y = 1 – P(X ≤ g-1(y)) = 1 – F (g-1)y)).
X Thus, F’ (y) = f (y) = -f (g-1)y)).
Y Y X Since g(X) is strictly decreasing so also is g-1(y).
Therefore, g-1(y) is negative and g-1(y) = .
€ € € Thus, ¹ g'1WyX¹ €w €w €w f (y) = f (g-1(y)) = f (x) ; x = g -1(y) y X X € €k ¹ g'1WyX¹ ¹ ¹ €w €w Example 12 Let X be a random variable having the pdf 1 0 < x < 1 f(x) = 0 elsewhere Find the pdf of Y = -2 log x. e Solution Y is a strictly decreasing differentiable function of X. hence by Theorem 1 f (y) = f (x) ; x = e-y/2 Y X €k Y = -2¹log¹ x, €we then e-y/2, e-y/2 €k 3 €k 3 Hence / ' ¹ ¹ / €w : €w : f (y) = 1, e-y/2, y ≥ 0.
Y 3 Thus : e-y/2 y > 0 3 f (y) = Y : 0 elsewhere Example 13 Let X be a random variable with pdf 2x 0 ≤ x ≤ 1 f (x) = X 0 elsewhere Find the pdf of (i) Y = 2X – 5 (ii) Y = X2.
Solution (i) Y is a strictly increasing function of X. thus, 162  f¿WvX / €k w‹4 fIW( X€w,( / : €k 3 Thus, / €w : 3 w‹4 Hence f¿WvX / 2(, / ( / : : w‹4 fY(y) = : '5 ≤ v ≤ '3 0 elsewhere (ii) Y is strictly increasing on (0, 1) and 0 ≤ Y ≤ 1.
- q( 1 J , thus, ^ / = `v ])q / v qv 2 €k f¿WvX / fIW(X €w - - - 3 J J 3 J , , , f¿WvX / 2( ¹:v ¹ / 2.v .
:v ,( / `v / 1.
Hence, 1 0 ≤ y ≤ 1 f (y) = Y 0 elsewhere Example 14:Chi-square Distribution Let X be a random variable having the N(µ, σ2).
Find the pdf of (i) (ii) Z = Y2.
IJ À O / Á Solution (i) , X = µ + Yσ, IJ À €k OY /is a strictly increasing fu/nc tÂion of X, hence Á €w f (y) = f (x) ; x µ + yσ Y X €k ¹ ¹ €w - , 3 J Wk J ÀX , Substituting for x, we see thfaIt W(X / i , .
Á√:¤ Á 3 J-w, , Thus fIWμ=vÂX / i Á√:¤ 3 J-w, 3 J-w, , , That is fY¿W ivs Xa /st andardi norm.Âal /ra ndomi varia,b'le∞ (NU(0v, 1U)).∞ .
Á√:¤ Á√:¤ (ii) Z = Y2 163  f (z) = f (y) Z Y €w 0 0 ¹ ¹ = €w €Ä '∞ f¿WvX¹ ¹'∞ - €Ä €w - v / Å,, / 3 , Hence €Ä Å : - - - - 3 J Ä 3 3 J Ä 3 , , , , fÄWÅX / i .
Æ = i .
Æ √:¤ : √:¤ : = Ä}-, J-Ä H-,M-,È-,~}-,É 0 U √:¤i , / Ç √¤ Å U ∞ n 0 i‚ƒi„…i†i Definition 3 A continuous random variable X that has the pdf | fW(X / ÇH-,M|, (|,J 3iJŠ,n, 0 U ( U ∞ (cid:129)H,M 0 elsewhere is said to have a chi square distribution with n degrees of freedom and is written : ^ W0X.
Let X be a random variable having the N(µ, σ2) distribution.
µ Then the pdf of : IJ : O / H Á M lƒ^ W3X.
Proof: See Example 14.
The pdf of Y = g(X) when g is not one to one.
Theorem 2 Let X be a continuous random variable having pdf f(x) such that f(x) > 0, x D and f(x) = 0, x D. Suppose D can be partitioned into two sets D and D such that the transformation Y = g(x) 1 2 » ∉ which is a monotone differentiable function in D and is a strictly increasing function on D and 1 strictly decreasing differentiable function of X on D , then the pdf of Y is given by 2 €k €k f¿WvX / fIW(X¹€w¹ = fIW(Xn€w¹ Ë0Ì- Ë0Ì, Where x = g-1(y), D = D D 1 2 ∪ 164 Consider the following examples.
Example 15 Let X be a continuous random variable having pdf f(x) given by ∞ ∞ - , fW(X / 3 J,k √:¤i ' U ( U Find the pdf of (i) Y = (ii) Y = X2 .
|^| 165 (i) The graph of Y is given below.
y x Figure 1 Y is strictly increasing on (0, ∞) = D and strictly decreasing on (-∞, 0) = D 1 2 On D , y = x, 1 €k €w / 1.
- , €k 3 J,w fIWvX¹€w¹ / √:¤i On D , y = x, 2 €k €w / 1.
- , €k 3 J,w fIW'vX¹€w¹ / √:¤i .
Thus, - , - , €k €k 3 J,w 3 J,w f¿WvX / fIWvX¹€w¹Ì- = fIW'vX¹€w¹Ì, / √:¤i = √:¤i , Hence (ii) The function Y = X2 is a strictly decreasing differentiable function of X on (-∞, 0) and is a strictly increasing function of X on (0, ∞).
Thus D = (-∞, ∞) can be partitioned in to D 1 = (-∞, 0) and D = (0, ∞) so that D = D D .
2 1 2 ∪ Thus, by Theorem 2, we have €k €k f¿WvX / fIW(X¹€w¹ = fIW(X¹€w¹ Ë0Ì- Ë0Ì, 166  - , fW(X / 1 J,k i √2{ ( / E`v on D , x = 1 - €k 3 J, '`v])q¹€w¹ / :v - - €k 3 J,w 3 J, fIr'`vs¹€w¹ / √:¤i .
:v On D , x = + 2 `v])q - - €k 3 J,w 3 J, fIr`vs¹€w¹ / √:¤i .
:v Hence - - } (cid:127) w,~ , 0 - - - - f¿WvX / √:3¤iJ,w.3:v, =√:3¤iJ,w.3:v, / Î √:¤ v(cid:3)m n 0 i‚ƒi„ i†i Example 16 Let X be a uniform random variable over the interval Find the probability density J¤ ¤ function of Y = tan X. H : ,:M. Solution 1'{ { fkW^X / ¨{ 2 U ( U 2n 0 elsewhere Y = tan X is a strictly increasing function of X in the interval –¤ ¤ H: ,:MÏi)§i, €k J3 f¿WvX / fW(X¹€w¹;( / (cid:143)]) v 3 3 ¤.3 ‹ w, Thus, the pdf of Y is given by 167  -∞< y < ∞ f¿WvX / 3 ¤W3 ‹ w,X This pdf is called the Cauch pdf.
Exercise 7.1 1.
Let X be a uniform random variable over the interval (-1, 1).
(i) Let Y = 2X – 1.
Find the pdf of Y.
(ii) Find the pdf of (a) ¤ Æ / |^|WaXÐ / sinH:M^ 2.
Let X has the pdf given by Š : fW(X / \:ÑŒ-J :,( / 1,2,…ÒWiÓi)Xn 0 elsewhere Find the pdf of Y defined below.
2 if X is even Y = 5 is X is odd Hence, find the mean and variance of Y.
3.
Let X has pdf ∞ , Jk , 0 U ( U n 2(i fW(X / g 0 elsewhere Find the pdf of (i) Y = X2 (ii) Y = |^| .
4.
Suppose that X is a continuous random variable defined over (0, 1) such that P(X ≤ 0.45) = 0.
Let Y = 1 – X.
Find the value of K such that P(Y ≤ k) = 0.40.
5.
Suppose that X has the pdf f(x) = e-x, x > 0.
Find the pdf of the following random variables, (a) Y = X3 (b) 3 Å / WI‹3X,.
6.
Suppose that X is a geometric random variable with parameter p. Find the pdf of 168 (a) Y = X2 (b) Z = X + r, where r is a constant.
7.
Find the pdf of Y = ex if X has (i) Normal distribution with mean µ and variance σ2 (ii) Exponential distribution with mean λ.
8.
Let X be uniformly distributed over (0, 1).
Find the pdf of Y = Xn.
9.
Let X be a continuous random variable having probability distribution function F. show that Y = F(X) is a uniform random variable over (0, 1).
10.
Let X be uniformly distributed over (0, 1).
Find the pdf of (a) Y = eX (b) Z = X2 + 1 (c) (d) Y = X1/β, where β ≠ 0.
Ð / 3 I ‹ 3 11.
Let X be exponentially distributed with parameter λ.
Find the pdf of Y = log X. e 12.
Let X be have gamma density with parameters (α, λ).
Find the pdf of (i) (ii) Z = cX, where C > 0.
O / √^ 13.
An angle θ is chosen at random from Let X = sin θ.
Find the pdf of X.
–¤ ¤ H: ,:M. 14.
Let X be exponentially distributed with parameter λ.
Let Y = n if n ≤ X < n + 1 where n is a nonnegative integer.
Find the pdf of Y.
15.
Let X and Y be independent random variable each having uniform pdf on (0, 1,…N).
Find the pdf of (i) min (x, Y) (ii) Max (X, Y).
7.2 Functions of Two Random Variables The method discussed in section 7.1 of finding the pdf of a function of one random variable of the continuous type will now be extended to functions of two random variables Let X and Y be continuous random variables and Z = X + Y.
The problem of finding the pdf of Z is somewhat more involved.
The following steps are useful.
1.
Introduce a second random variable W = K(X, Y).
169 2.
Obtain the joint pdf of Z and W, say d(z, w).
3.
Obtain the desired pdf of Z by simply integrating d(z, w) with respect to w. that is ∫ d(z, w) dw.
The following will be assumed in obtaining the jointing pdf of Z and w. (i) The functions h(X, Y) and k(X, Y) satisfy the following conditions.
(a) The equations Z = h(x, y) and w = k(x, y) may be uniquely solved for x and y in termsof z and w, ssay x = g (z, w), 1 y = g (x, w) 2 (b) The partial derivatives exist and are continuous Ôk Ôk Ôw Ôw , , , ÔÄ ÔÕ ÔÄ ÔÕ From the result of section 7.1 it can be proved using results of advanced calculus that under the assumptions (i) and (ii), the joint pdf of Z and W is given by d(z, w) = f(x, y) where x = g (z, 1 |Ö| w), y = g (z, w) and 2 Ôk Ôk Ö ÔÄ ÔÕ / ×Ôw Ôw×.
ÔÄ ÔÕ The determinant J, is called the Jacobian of the transformation (x, y)-(cid:1) (z, w).
some of the important random variables we shall be interested in are X +Y, XY, X/Y, min.
(X, Y), max.
(X, Y), etc.
Distribution of Sum Let Z = X + Y and W = X, the Jacobian, 0 1 Ö / ¹ ¹ / '1 1 '1 Thus d(z, w) = f(x, y) = f(w, (z – w)), |Ö| ∞ ∞ F (z) = Z ∞ ∞ pJ W„,Å'„Xq„ / pJ fW(,Å'(Xq( If X and Y are nonnegative independent random variables, then F (z) = Z Ä pe fIW(Xf¿WÅ'(Xq( 170 Example 17 1 0 < x < 1 0 < y < 1 Let X and Y have the joint pdf f(x, y) = 0 elsewhere Find the pdf of Z = X + Y.
Solution Let W = X; Z = X + Y, then x = w; y = z – w and 0 Ö 1 / ¹ ¹ / '1 1 '1 Hence, d(z, w) = 1.1 = 1.
Let A be the set A = {(x, y), 0 ≤ x ≤ 1, 0 ≤ 1}.
A is shown in figure below.
f(x) 1 A x Figure 2 Under the transformation, Z = X + Y, W = X, the boundaries of A are x = 0, x = 1, y = 0 and y = 1.
The set B in the z, w –plane which is the mapping of A under the one-to-one transformation is determined as follows: x = 0 implies that w = 0; y = 1 implies that w = 1 y = 0 implies that z = w; y = 1 implies that z = w + 1.
171 Thus, the boundaries of B are the lines w = 0, w = 1, z = w, z = w + 1.
This is show in the figure below.
z z =w+1 w=1 B z =w w Figure 3 Hence d(z, w) = 1 thus, (z, w) B ∈ 0 < z < 1 qWÅX / Ä pe 1 q„ / Å 1 < z < 2 pÄJ31 q„ / 1'Å Z 0 < z ≤ 1 d(z)= 2-z 1 < z < 2 0 elsewhere Example 18 Suppose a point (x, y)is selected at random in a circle centre (0, 0) and radius 1 and that this point is uniformly distributed over the circle.
Find the pdf of the distance from the origin.
: : Ø / `W^ = O X, The joint pdf of X and Y is if (x, y) lies inside or on the circle 3 ¤ d(z)= 0 elsewhere 172 Let W = tan-1(Y/X) then tan W = Y/X, x = r cos w, y = r sin w. y r r w 1 x B 2 x Figure 4 The Jacobian is Ú( Ú( Ö Ú† Ú„ cos„ '†sin„ : : / Ù Ù¹ ¹ / †W§Ûƒ „ =ƒl) „X / †.
Úv Úv sin„ †cos„ Ú( Ú„ hence r (r, w) B 3 ¤ » d(r, w) = f(x, y) = 0 elsewhere |Ö| Thus, d(r) = ∫ d(r, w) dw = r 0 L † L :¤3 pe ¤q„ / ¦02† 1 n i‚ƒi„…i†i Example 19 3x 0 < y < x If f(x, y) = 0 < x < 1.
Find the pdf of Z = X – Y.
0 elsewhere Let W = X, A = {(x, y), 0< y < x, 0 < x < 1} and B ={(z, w) : z=x – y, w = x} 173  y Z A B x W 1 1 Figure 5 The Jacobian is 0 Ö 1 / ¹ ¹ '1 1 hence 3w 0 < z < w d(z, w) = 0 < w < 1 0 elsewhere 3 qWÅX / / pÄ 3 „q„ 0 < z < 1 > : :W1'Å X d(z) = 0 elsewhere Distribution of Product Suppose X and Y are continuous random variables having the joint pdf (x, y).
show that the pdf of Z = XY is given by 3 Ä p¹k¹fH(,kMq( Solution Let W = X, thus, x = w, y = .
The Jacobian Ä Õ 174  0 Ö J1Ä 3Ü / 3 / ÜÕ, Õ Õ Hence, Ä qWÅ,„X / fH„, ÕM|Ö| Thus, (2) 3 Ä qWÅX / p¹Õ¹fH„, ÕMq„ Example 20 Let X and Y be independent random variables with the following pdfs.
0 L ( L > fW(X / ¦02( 1 n, ½WvX / g4v0/15 1 ≤ v ≤ 2 n i‚ƒi„…i†i i‚ƒi„…i†i Find the pdf of Z = XY.
Solution The joint pdf of X and Y is f(x. y) = f(x) g(y).
Thus, from 2. , where W = X 3 Ä qWÅX / p¹Õ¹½W„X½HÕMq„ > 3 5 Ä ; > 3 qWÅX / p¹Õ¹.2„34HÕM q„ / 34Å pÕAq„.
Let A = {(x. y): 0< x < 1, 1 ≤ y ≤ 2}.
The transformation, Z = XY, W = X gives the set B determined as follows: When x = 0, w = 0; when x = 1, w = 1, When y = 1, Z = w, when y = 2, z = 2w.
175 The lines w = 0; w = 1; x = w, z = 2w are the boundaries of B as shown in the figure below.
y 2 B 1 x Figure 6 Thus A qWÅX / ;Ä Ä 3 5 0 U Å U 34 pÄ/:ÕAq„ / 4Å, 1 A ;Ä 3 3 5Ä : / 34 pÄ/:ÕAq„ / 34 W4'Å X, 1 U Å U 2 5Ä 0 U Å U 4 1 qWÅX / Ç5Ä : n 34W4'Å X 1 U Å U 2 Distribution of Quotients Suppose X and Y are continuous random variables having the joint pdf f(x, y).
show that the pdf of Z = Y/X is given by ∞ ∞ ∞ ∞ fÄWÅX / o |(|fW(,(ÅXq(' U Å U J Let W = X, then x = w, y = zw, the Jacobian is 0 1 ¹ / Ö / ¹Å „ „.
Hence qWÅ,„X / fW„,Å„X|Ö| / fW„,Å„X|„| Thus (3) fÄWÅX / p|„|fW„,Å„Xq„.
176 Example 21 Let X and Y be independent random variables having the respective gamma densities Γ(α , λ) 1 and Γ(α , λ).
Find the probability density function of Z = Y/X.
2 x > 0 fIW(X / jÝ-kÝ-}-~}‰Š (cid:129)WŸ-X y > 0 jÝ,kÝ,}-~}‰Š f¿WvX / (cid:129)WŸ,X The joint pdf of X and Y is given by jÝ-Œ Ý,kÝ-}-~}‰WŠŒ(cid:127)X fW(, vX / fIW(Xf¿WvX / (cid:129)WŸ-X(cid:129)WŸ,X .
From (3) we have jÝ-ŒÝ,ÕÝ-}-WÄÕXÝ,}- jWÕ‹ÕÄX fÈWÅX / p„fW(,Å„Xq„ / p„ (cid:129)WŸ-X (cid:129)WŸ,X i q„ = jÝ-ŒÝ,ÈÝ,}- [ Ÿ-Œ Ý,J3 JjkW3 ‹ ÄX (cid:129)WŸ-X (cid:129)WŸ,X pe ( i q„ But [ Ÿ-‹Ÿ,J3 JjÕW3‹ ÄX (cid:129)WŸ-‹ Ÿ,X pe „ i q„ / WjW3‹ÄXXÝ-Œ Ý, Hence f (z) = z > 0 Z (cid:129)WŸ-‹ Ÿ,X ÈÝ,}- (cid:129)WŸ-X (cid:129)WŸ,XW3‹ÄXÝ-Œ Ý,, Example 22 Let X and Y be independent exponential random variables with parameter λ.
Find the joint pdf of Z = X + Y and W = .
I Solution I ‹ ¿ The joint pdf of X and Y is given by f (x, y) = λ2 e-λ(x + y) x > 0, y > 0 X,Y The transformation gives x = wz, y = z(1 – w) The Jacobian of the transformation is = z The joint pdf of Z and W is given by |Ö| : JjÄ 0,0 U „ U fÈ,ÕWÅ,„X / gÅ0h i , Å Y 1n 1 7 7 , i‚ƒi„…i†i Example 23 Let X and Y have the joint pdf f (x,y) = x ≥ 1, y ≥ 1, find the joint pdf of Z = XY X,Y 3 , , k w and .
Ð I / Solution ¿ From the transformation z = wy, w = x/y, we have v ( J:k 3 Jk Ö / Ü Ü / , w w w Thus |Ö| / 2„.
J3 3 3 f È,ÕWÅ, „X / =fÈ ,Õ W(,vX|Ö| / È,.
:Õ 3 0 U = 0, oth,e Årw mise1.
, „ U Å :ÕÄ Example 24 Let X and X be random variable with joint pdf (x , x ).
Let Y = X + X , 1 2 1 2 1 1 2 Y2 = X1 - X2.Find the joint pdf of Y1 and Y2 in termsf ko-f, Š,, (x1, x2).
fk-, Š,, Solution (y , y ) = (x , x ) 1 2 1 2 f¿-, Þ,, fk-, Š,, |Ö| 3 3 (3 / :Wv3 = v:X,(: / :Wv3 ' v:X Ôk- Ôk- 3 3 Ôw- Ôw, : : 3 Ö / ×Ôw, Ôk,× / ×3 3× / ':.
Hence Ôw- Ôw, : ' : (y , y ) 1 2 3 v1= v2 v1' v2 Example 25 f¿-, Þ,, / :f(1, (2 H 2 , 2 M Let X and Y be independent exponential random variables with parameters λ and λ 1 2 respectively.
(i) find the joint pdf of U = X + Y and V = I (ii) show that U and V are independent if λ = λ .
1I ‹ ¿2 Solution The joint pdf of X and Y is given by f (x, y) = λ λ X,Y 1 2 x = uv and y = u –J Wuj-vk.
‹ j,wX i The joint pdf of u and v is given by f (u,v) = f (x,y) .
u,v X,Y Substituting for x and y in terms of u and v |Ö| f (u,v) = λ λ u,v 1 2 (ii) Putting λ = λ =J λWj.
-W´ße‹ ojb,´taW3inJ ßXX J´ßWj-J j,XJ ´j, 1 2i |Ö| / µh3h:i f (u,v) = uλ2e-λu, u ≥ 0, 0 < v < 1 u,v 178  3 f´WµX / pe µh:iJj´qÓ / µh:ij´,µ m 0 Example 26 fßWÓX / [ : Jj´ 0 U Ó U pe µh i qµ / 1 1.
The joint pdf of X and Y is given by JWk‹wX 0, v Y 0 find (i) P(X > f2W, (Y, v>X 3/), g(iii)0 P(Y < X ^) Yand (iii) then pdf of Z = X/Y.
Û(cid:143)…i†„lƒi Solution Let A = {x, y)!
x > 2, y > 3), we have P(X > 2, Y > 3) = [ [ = à1 fW(,vXq( qv / p> p: fW(,vXq( qv [ [ [ [ [ JWk‹wX Jw Jk J: Jw J4 (ii) Let A {(x, y); y < x} p> ¾p: i q(áqv / p> i ¾p: i q(áqv / i p> i qv / i P(Y < X) = [ [ JWk‹wX [ J:w 3 Example27 à1 fW(,vXq(qv / pe xpw i q(‘qv / pe i qv / :.
Let f(x, y) = 3 : : : ,( = v ≤ † ¤¡ n \ find (i)f (x) an0d (ii) f (z) where Z = X Z i‚ƒi„…i†i :Solutio:n √^ = O .
(i) [ 3 w : : : (ii) ffk(Wz()X =/ P {pXJ[2 +f WY(2, ≤v Xzq2)v =/ p¤¡,qv / ¤¡,â√†: =(: / ¤k,√†: '(: n Z √† '( 3 àk,‹w,ãÄ,fW(,vXq(qavre/a oàf ka, ‹ciwr,cãlÄe, r¤a¡dqiu(sq zv.)
, , ¤Ä Ä (iii) F = P(/z ≤ ¤ z¡), = / ¡,Wƒl)§i àk,‹w,ãÄ,q(qv / z I ± = NH¿ ≤ (M / àÞãÈfW(,vXq ( qv [ wÄ JWk‹wX 3 Differentiating wpee obtpaein i q(qv / 1' Ä‹3 3 0 U Å U Example 28 fÄWÅX / ,, ∞.
WÄ‹3X Distribution of sum.
Let X and Y have the joint pdf f(x, y) and let Z = X + Y then F (z) = P(X + Y ≤ z) = Z [ ÄJw Changing variable, x = v – y àk‹wäÄfW(,vXq( qv / pJ[ pJ[ fW(,vXq( qv.
F (z) = z [ ÄJw Ä Ä Thus pJ[ pJ[ fWÓ 'v,vXqÓ qv / pJ[ ¾pJ[fWÓ 'v,vXqváqÓ.
F (z) = Z [ Example 29: Student’s t-Distribution pJ[fWÅ 'v,vXqv.
Let X be a standard normal distribution and Y a chi-square random variable with v degrees of freedom.
Find the pdf of where X and Y are independent.
I√ß Æ / 179 √¿Solution Let and W = Y then y = w and x = Æ / I√ß √Õ Then Jaco bian of the transformation is Å .
√¿ √ß 0 1 3 √Õ √Õå / √w Thus Ö / å':Åæ}A, √ß √Õ.
d (z,w) = f (x)f (y) Z,w x Y √Õ 3 J-k, 3 èJ3 J(cid:127)√é , , , substituting for x and ¹y,æ w¹e/ h a√v:e¤ i (cid:129)HçM:è/,v i æ , dZ,w(z,w) = 3 J-É,ê 3 èJ3 Jê √é Õè},- J-É,êJ ê , è | , , è , è , The margina√l: ¤pdif of z is.
g(cid:129)iHvç,eMn: ,b„y i .
√æ / √:¤(cid:129)Hç,M:,i .
è}- , -É,ê ê [ è}- ê É, „ J, è J , √Ð 1 , J,ë3‹ èì√Ð qÄWÅX / o æ |i .
q„ / æ èo „ i q„ Let u = √2{yH:M2, √ˆ √ 2{yH:M2, e √ˆ , , Õ Ä €´ 3 Ä :´ H1= M; / H1= M,„ / í, : ß €Õ : ß 3‹ ç è}- , [ 2 2µ J´ qµ qÈWÅX / æ è J3o î Ä,ïi 3 Ä, , e √2{ÓyH:M2 1= æ :H1= æM è}- J3 , 1 J´ è}-oµ i qµ.
æ è Ä, , , Note √2{yH:M2 H1= æM è [ ,J3 J´ ß‹3 Hence pe µ i qµ / yH : M. èŒ- (cid:129)H , M 3 qÈWÅX / ç .
èŒ-,'∞ U Å U ∞.
√:¤ß(cid:129)H,M É, , The above distribution, dZ(z) is caël3le‹d ç aì t-distribution.
Note (i) That a t-distribution is completely determined by the parameter v, the number of degrees of freedom of the random variable that has the chi-square distribution.
(ii) The t-distribution is a bell-shaped distribution, symmetric about zero.
(iii) The t-distribution is flatter than the Normal distribution.
As the sample size becomes large, t-distribution approaches the standard normal distribution (n > 30 is sufficient).
(iv) The values of the t-distribution for various values of degrees of freedom have been tabulated.
180  Example 30 Exercise 7.2 1.
Let X and X be independent normal random variables N(µ , σ2 ), N(µ , σ2 ) 1 2 1 1 2 2 respectively.
Find the pdf of Y = X + X .
1 2 2.
Let X and Y be independent random variables having the respective pdfs.
f(x) = e-x, x ≥ 0, g(y) = 2e-y y ≥ 0.
Find the pdf of Æ / I 3.
Let X and Y have the joint pdf f(x, y) = λ2 e-λ(x+y), x > 0, y > 0.
Find the pdf of ¿ Z = X + Y.
4.
Let X and Y be independent random variables each having the normal probability density function N(0, σ2).
Find the probability density function of (i) (ii) (iii) (iv) , ¿ ¿ ¿ |¿| 5.
Suppose X and Y ar,e independent normal random variables with mean 0 and I I |I| |I| variance σ2.
Find the pdf of (i) X + Y (ii) X2 + Y2.
6.
If X and Y are independent exponential random variables with parameter λ.
Find the joint pdf of Z = X + Y and .
Hence find the pdf of Z and of W. I 7.
Let X and Y have uniform disÐtrib/u tion over the interval (a, b).
Find the pdf of I‹ ¿ 8.
Let X be uniformly distributed over (0, 1) and let Y be uniformly distributed over Æ / |O '^|.
(0, x).
Find (i) The conditional pdf of Y given X = x.
(ii) The joint pdf of X and Y (iii) The marginal pdf of Y.
9.
Let f(x, y) = c(y – x)2, 0 ≤ x < y ≤ 1 = 0, elsewhere Find the pdf of Z = X + Y.
7.3 Sum of Independent Identically Distributed RandomVariables The Moment Generating Function Technique Let X , X ,….X be independent identically distributed random variables.
In this section 1 2 n we will find the pdf of sum of the X’s.
The moment generating function technique will be used to establish the pdf of sum Bernoulli, Poisson, exponential, Normal, Gamma random 181 variables.
All the probability distributions above except Bernoulli and exponential has the following remarkable and very useful property called Reproductive properties.
Definition: Reproductive Property A probability distribution F is said to have reproductive property if when two or more independent random variables having the probability distribution F are added, the resulting random variable has the distribution F. we shall establish this result using moment generating function defined in Chapter 4, section 4.3.
Moment Generating Function The moment generating function M (t) of a random variable X is defined by X M (t) = E(etX) X The domain of M (t) is all numbers t such that etX has finite expectation.
x Theorem 3 Suppose that a random variable X has mgf M (t).
Let Y = aX + b.
Then, the mgf of the x random variable Y is given by M (t) = ebt M (at).
Y x Proof: M (t) = E(ety) = E(e(aX + b)t) = E(eatX + bt) = E(eatX ebt) = ebt E(eatX) = ebt M (at).
Y X Theorem 4 Let X and Y be two random variables with mgf’s M (t) and M (t), respectively.
If M (t) x Y x = M (t) for all values of t, then X and Y have the probability distribution.
That is, the Y mgf uniquely determines the probability distribution of the random variable.
The proof of this theorem is outside the scope of this work, the reader is referred to volume II of this book.
This theorem will now be used to show that a linear function of a random variable also has a normal distribution.
Example 31 Suppose that X has distribution N(µ, σ2).
Let Y = aX + b. show that Y also has a Normal distribution with mean aµ + b and variance a2 σ2.
The mgf of Y is M (t) = E(ety) = ebt(M (at)) Y X from theorem I. M (t) = E(etx) X [ (cid:141)k -/,WŠ}ðX, [ -/,WŠ}ðX, o i / 1 J ñ, 1 (cid:141)k J ñ, Let Jth[en M ( t) becomies q( / o i i q( X Â√2{ Â√2{ J[ kJ À ƒ / , Á [ (cid:141)À [ 1 (cid:141)WÁò‹ ÀX J3/:ò, i J3/:ó,‹(cid:141)Áò ™IW(cid:143)X / o i i Âqƒ / o i qƒ Â√2{ J[ √2{ J[ 182  ôõð ÷õ÷ ‹ ö ÷ [ J3/:ó,‹(cid:141)Áò Making the change of v√a÷røiablep Ju[ =i s – σt, we s qeeƒ .that Mx(t) = (cid:141)À‹ -,Á,(cid:141), 3 [ J´, (cid:141)´‹ ñ,,›, i √:¤pJ[i qµ / (cid:143)μ=i Substituting at for t, we have M (at) = X , , , Hence, ˜(cid:141)À‹ Á ˜ (cid:141) /: i : : : ñ,œ,›, —(cid:141) ˜(cid:141)À Â ] (cid:143) (cid:141)W—‹˜ÀX‹ , (cid:141)ú‹ Á,(cid:141),˜,/: Where γ = b™ +¿ Wµ(cid:143)aX =/ Ei(Y)–, Viara == E(Y), Vùar(/Y)i = a2σ2.
/ i 2 This is the mgf of a normal variable with mean γ = aµ + b and variance a2σ2.
Thus, by theorem 2, Y is a normal random variable with mean aµ + b and variance a2σ2.
Example 32 Let X be N(µ, σ2).
Show that is N(0, 1).
IJ À O / Á Solution = IJ À I À Let a = and bO =/ Á, thenÁ Y' = Á a.X + b.
3 À From Example 31'2, Y is a normal random variable with mean aµ + b and variance a2σ2.
Á Á Substituting for a and b, we see that (i) aµ + b = À À 0 (ii) a2σ2 = ,Á ' Á / Á , / Thus Y is stanÁdard 1normal (N(0, 1)) random variable.
Theorem 5 Let X and Y be independent random variables and Z = X + Y.
Let M (t), M (t) and M (t) be the mgfs of the random variables X, Y and Z respectively.
X Y Z Then M (t) = M (t) M (t) (4) Z X Y Proof: By definition, M (t) = E(eZt) = E(e(X + Y)t) = E(eXt+eYt) = E(e+Xt) E(eYt) = M (t) M (t).
Z X Y In general, if X , X ,…, X are independent random variables.
1 2 n The mfg of Z = X + X + …+ X is 1 2 n ™ÈW(cid:143)X / ™I-W(cid:143)X™I,W(cid:143)X…™I|W(cid:143)X 183 If X , X ,..…, X are independent identically distributed with mgf M (t), then 1 2 n X M (t) = [M (t)]n. Z X Example 33: Sum of Binomial Random Variables Let X and Y be independent binomial random variables with parameters (n , p), (n , p) 1 2 respectively.
Find the pdf of Z = X + Y. M (t) = E(etX) = X 0- (cid:141)I )3 I ∑k©ei H M_ W 0-Jk (cid:141) 0, The mgf of Z is given by (from The(orem 51).
'_X / Ž_i =W1'_X(cid:144) .
M (t) = M (t) M (t) = Z X Y but this is the mgf of a binomially distr(cid:141)ibuted rando0m-‹ 0v,ariable with parameters (n + n , Ž_i =W1'_X(cid:144) 1 2 P).
thus, by theorem 2, Z is a binomial random variable and its pdf is given y 0-‹0, È 0-‹0,JÄ 0, In general,if XfÈ, WXÅX,…/ Xg are indûeÄp_0enWd1en't _biXnomial r a nÅd/om v1a,r2ia,b…le)s 3w=ith) :pnarameters 1 2 K i‚ƒi„…i†i (n , p), (n , p),…(n , p) respectively, then the moment generating function of Z = X , 1 2 k 1 X ,..…, X is given by 2 k M (t) = Z Thus Z is a binomial varia(cid:141)ble with para0m-‹e0t,e‹r(cid:157)s ‹(n 0,“ p) where n = n + n … n .
Ž_i =W1'_X(cid:144) .
1 2 k Example 34: Sum of normal random variables Suppose that X and Y are independent random variables with distribution N(µ , σ2 ) and 1 1 N(µ , σ2 ) respectively.
Show that Z = X + Y is a normal random variable with mean µ + 2 2 1 µ and variance σ2 + σ2 2 1 2 M (t) = X , , À-(cid:141) ‹ 3/:Á -(cid:141) M (t) = Y i , , Hence À,(cid:141) ‹ 3/:Á ,(cid:141) i M (t) = M (t)M (t) = Z X X , , - , , = À-(cid:141) ‹ 3/:Á -(cid:141) À,(cid:141)‹,Á ,(cid:141) i i , , , , , where µ = µ + µ , σ2 = σ(cid:141)2WÀ -+ ‹ σ À2,X.‹ S 3in/:c(cid:141)e rÁ -‹ Á ,s is(cid:141) tÀh‹e3 /m:(cid:141)gfÁ of N(µ, σ2), hence by 1 2 i 1 2 / i , , Theorem 4, z is a normal random variable(cid:141)wÀ‹it3h/ :m(cid:141) eÁan µ = µ + µ and variance i 1 2 σ2 = σ2 + σ2 .
1 2 In general, if X , X ,..…, X are independent N(µ , σ2 ) then 1 2 n 1 1 Z = X + X +.…+ X 1 2 k is N(µ, σ2) where µ = µ + µ +…+ µ , σ2 = σ2 + σ2 +…+ σ2 1 2 n 1 2 k Example 35: Sum of Poisson Random Variables.
Suppose that X and Y are independent poisson random variable with mean λ and λ 1 2 respectively.
Show that Z = X + Y is a Poisson random with mean λ + λ 1 2 (cid:141)I [ (cid:141)I~}‰-jŠ- Jj-r3J~›s Similarly, ™IW(cid:143)X / üWi X / ∑k©ei / i .
k!
M (t) = .
Y › Jj-r3J~ s i 184 Hence, M (t) = Z Jj-r3J~›s Jj,r3J~›s JW3J~›XWj-‹ › i i / i j,X JW3J~ Xj The mgf of Poisson distribution with parameter λ is ,/ thius Z is a ,phoi/ss ohn3 r=an dho:m › variable with mean (parameter) λ = λ + λ .
In generalJ, jifW3 XJ~, XX ,…,X are independent 1 2 i 1 2 n P(λ ), then 1 Z = X + X +… + X 1 2 n is a Poisson random variable with parameter λ = λ + λ +…+ λ .
1 2 n Example 36 Let X and Y be independent gamma random variable with parameters (n , λ) and (n , λ) 1 2 respectively.
Show that Z = X + Y is a gamma random variable withparameter (n + n , 1 2 λ).
Solution [ 0- 0J3 Jjk [ 0-J3 JkWjJ(cid:141)X (cid:141)I (cid:141)kh (3 i 0- W(X i ™IW(cid:143)X / üWi X / o i q( / o h q( J[ yW)3X e yW)3X 0- [ 0- [ h 0J3 JkWjJ(cid:141)X h 0-J3 J´ / o ( i q( / 0-J3 o µ i qµ But yW)3X e Wh'(cid:143)X yW)3X e [ 0-J3 J´ Hencpee, µ i qµ / yW)X. M (t) = X 0- j x ‘ .
jJ(cid:141) Similarly M (t) = Y 0, j Thus, x ‘ .
jJ(cid:141) M (t) = Z 0- 0, 0-‹0, 0 j j j j This show that xZ is ‘a g.axmm‘a ra/nd oxm v‘ariable/ w ixth p‘ara.m)e/ter)s3 (n= )+: n. , λ).
jJ(cid:141) jJ(cid:141) jJ(cid:141) jJ(cid:141) 1 2 In general, if X , X ,… + X are independent gamma random variables with parameters 1 2 k (n , λ), I = 1,2,…k.
then Z = X + X +… + X has a gamma distribution with parameters 1 1 2 k (n, λ) where n = n + n +…+ n .
1 2 k Example 37 Suppose that X has χ2 (n ) (chi-square distribution with n degrees of freedom) I = 1, i 1 i 2,…k where the X’s are independent random variables.
i Let Z = X + X +… + X .
Show that Z has distribution χ2 where n = n + n +…+ n .
1 2 k n 1 2 k Recall (5.9) that chi-square distribution is a special case of the gamma distribution in which λ = 1/2 and n replaced by n/2.
Since the mgf of gamma (n, λ) is given by 0 j substituting for λ = 1/2 and replacing n byx n/2‘ w;e have jJ(cid:141) M (t) = .
X 0/: 0/: 3/: 3 The mgf of X is therefxore gi‘ven /by x ‘ i 3/:J(cid:141) 3J:(cid:141) 185  M (t) I = 1, 2,… X 0/: 3 Hence, the mgf of Z / is x ‘ 3J:(cid:141) M (t) = |-Œ|,Œ(cid:157)Œ|“ | Z 3 , 3 , where n = n + ™nI +-W…(cid:143)X+™ nI, Ww(cid:143)Xhi…ch™ isI “thW(cid:143)eX m/gfx of χ‘2 distributio/n.
x ‘ .
1 2 k 3J:(cid:141) n 3J:(cid:141) Example 38 Let X , X ,… X be independent identically normal variables with mean µ and variance 1 2 n σ2.
Show that (i) E( ) = µ, where = (X + X +…+ X ) (ii) Var( ) = and 1 2 n , 3 Á (iii) ^ý has a χ2(n).
^ ý 0 ^ý 0 | , ∑-WI-J ÀX O / , Á (i) E( ) = 3 0 3 0 3 (ii) Va^ýr( ) ü= HV0a∑r 3^þM / 0∑3üW^þX / 0.
)μ / μ , 3 0 3 3 : Á ^ý H ∑3^þM / ,∑ˆ]†W^þX / ,)Â / (iii) 0 0 0 0 | , , , , ∑-WI-JÀX WI-JÀX WI,JÀX WI|JÀX O / , / , = , = (cid:157)= , Á Á Á Y 2 + Y 2 +…Á+ Y 2. : : : 1 2 n I-JÀ I,JÀ I|JÀ From Example 14, we know that Y 2 has χ2(1) and from Example 30, since Y 2 are / H Á M = H Á M = (cid:157)= 1H Á M / 1 independent χ2(1) we have that Y is χ2(n).
Example 39 Let X , X ,…X be independent identical normal random variables with mean µ and 1 2 n variance σ2 Show that (cid:2)(cid:3) has a χ2(n-1).
| , : ∑ -WI-JIýX We shall pr(cid:1)oce/ed as fo,llows: Á X - µ = X - + - µ 1 i (X - µ)2 = (X - )2 + ( - µ)2 + 2(X - )( - µ) 1 1 ^ý ^ý i Summing, we have ^ý ^ý ^ý ^ý X X(cid:5) X X : 0 : 0 0 : 0 W þ ' ^ýX = ∑þ©3W ' μX =2∑þ©3W þ ' ^ýXW^ý ' μX ∑ þ©3W þ ' μX / ∑þ©3 \ X X since 0 0 ∑þ©3W þ ' ^ýXW^ý ' μX / W^ý ' μX∑þ©3W þ ' ^ýX 0,„i ∑W^þ '^ýX / ƒii (cid:143)…](cid:143) X X(cid:5) X : 0 : 0 : 0 W þ ' ^ýX = ∑þ©3W ' μX ∑þ©3W þ ' μX / ∑þ©3 dividing through by σ2, we have (cid:2)(cid:3) (cid:6)(cid:2) (cid:2)(cid:3) (cid:6)(cid:5) | , | , ∑ -W J ÀX ∑WI-JIýX ∑ -W J ÀX , / , = , Á Á Á 186 From Example 37, we have (cid:2) is χ2(n) and (it is left as an exercise to show that W , ∑0 (cid:6) J ÀX (cid:6)(cid:2) is χ2(1).
þ©3 Á, , ∑W J ÀX LetÁ Z, = (cid:6)(cid:2) and Y = (cid:6)(cid:2) , , ∑W J ÀX ∑W J ÀX Á, S2 = (cid:6)(cid:2)Á,(cid:6)(cid:5) , ∑W J X Since X(cid:5) and S2 are stochasticÁal,ly independent, we have E(etZ) = E(et(Y + S2) = E(etY)E(ets2) but M (t) = E(etZ) , since Z is χ(n), Z 0/: 3 M ( t/) = x E3J(e:(cid:141)tY‘) (Y is Z2 ) Y (cid:141)/: (1) 3 hence, / x ‘ 3J:(cid:141) E(etS)2 |}- 3 , and this is the mgf of a chi-square distribu /tio nx with‘ n – 1 degrees of freedom.
This shows 3J:(cid:141) that S2 has a chi-square distribution with n – 1 degrees of freedom.
Exercise 7.3 1.
Let X , X ,…X be independent identically distributed random variables, each of 1 2 n which is N(µ, σ2).
Show that is N , I-‹ I,‹(cid:157)‹ I| Á 2.
Let X , X ,…X be indepe^nýd/en t identically disHtrμi,butMe.d exponential random 1 2 n 0 0 variables with parameters λ.
Show that Z = X + X +…+ X has a gamma 1 2 n distribution with parameters n and λ.
3.
Let X , X ,…X be independent N(µ , σ2 ).
Show that 1 2 n 1 1 Y = a X + a X +…+ a X is a normal random variable with mean µ = a µ + 1 1 2 2 n n 1 1 a µ +…+ a µ and variance σ2 = a2 σ2 + a σ2 + …+ a2 σ 2.
2 2 n n 1 1 2 2 n n 4.
Let S2 = (cid:2) where X , X ,… X are independent identically distributed , 1 2 n ∑WI JIýX normal rando0mJ 3variable with mean µ and variance σ2.
Show that has a chi- , W0J3Xó square distribution with n – 1 degrees of freedom.
, Á 5.
If X has χ2(n), show that E(X) = n and Var(X) = 2n.
6.
If X –χ2 and Y - χ2 where X and Y are independent random varaible.
Prove that k1 k1 X + Y - χ2 k1+k2.
7.
(a) If X – N(0, 1) prove that (i) Y = X2 has χ2(1) (b) If X – N(µ, σ2) prove that IýJ À : χ: 8.
Let S2 = where X aHrÁe/ √i0nMdep'e ndeWn1tX identically distributed random ∑WI(cid:2)JIýX, i variables N(µ,0 σJ23), prove that (i) E(S2) = σ2 (ii) Var(S2) = .
8 :Á (iii) Show that and S2 are stochastically independent.
0J3 ^ý 187 9.
Suppose X is uniformly distributed over (0, 1).
Find the moment generating function of (i) Y = -2 In X and (ii) Z = where the X’s are i independent uniformly distributed over (0, 1) ra'ndom0 variables.
2∑þ©3ln^þ, 10.
Let X , X ,.. be independent identically distributed geometric random variable 1 2 with parameter p. Find the moment generating function of Y = X + X +…+ X .
1 2 n 11.
Let X and Y be independent random variables with the respective pdf N(0, 1) and χ2 .
Find the pdf of .
n I Æ / Þ 12.
Let X and Y be indepen(cid:8)de|nt chi-square random varaibles with parameters k1 and k .
Find the pdf if 2 I/”- 13.
Let X , X be indeÆpe/nd ent .random variables each having an exponential pdf with 1 2 ¿/”, parameter λ. Y = X +, …+ X; 1 ≤ i ≤ n. Find the joint pdf of Y , Y ,…Y .
1 1 i 1 2 n 14.
Let X , X be independent Poisson variables with mean λ.
Show that the 1 2 conditional pdf of X given t has the binomial pdf , where t = X + X 1 1 2 3 aH ,(cid:143)M : 7.4 Order Statistics 7.4.1 Independent Identically Distributed Random Variables Let X , X ,…, X be independent continuous random variables, each having distribution 1 2 n function F(x) and probability density function f(x).
define X = smallest of X , X ,… X (1) 1 2 n X = second smallest of X , X ,… X (2) 1 2 n .
.
.
X = largest of X , X ,… X (n) 1 2 n That is, X , X ,… X are random variable obtained by arranging X , X ,… X in an (1) (2) (n) 1 2 n increasing order.
X = min (X , X ,… X ) (1) 1 2 n X = max (X , X ,… X ).
(n) 1 2 n The random variables X is called the ith order statistics and the ordered values x ≤ x (i) (1) (2) ≤ … ≤ x are known as the order statistics corresponding to the random variables X , (n) 1 X ,…X .
2 n For Example, let x , x , x be a random sample of size 3 from a random variables X 1 2 3 having probability density function f(x) -∞ < x < ∞.
The possible values of X , X and X are as follows: (1) (2) (3) x = x , x = x , x = x if x < x < x (1) 1 (2) 2 (3) 3 1 2 3 x = x , x = x , x = x if x < x < x (1) 2 (2) 1 (3) 3 2 1 3 x = x , x = x , x = x if x < x < x (1) 3 (2) 2 (3) 1 3 2 1 .
.
x = x , x = x , x = x if x < x < x (1) 3 (2) 1 (3) 2 3 1 2 188 There are 3!
= 6 permutations of (x , x , x ) and hence 6 possible ordered statistics.
We 1 2 3 shall now find the joint probability density function of X , X , X .
Since for any (1) (2) (3) permutation, say (x < x < x ) 2 1 3 Në(: ' » » » » » U ^W3X U (: = ,(3 = U ^W:X U (3 = , (W>X U (> = ì / » fW(:X 2 2 2 2 2 as illustrated in the Figure (1) below > » fW(3X » fW(>X / » fW(3XfW(:XfW(>X, (+) (+) (+) x x x 2 1 3 Figure 1 Since there are 3!
Possible permutations, the region D is partitioned into six dejoint sets D , D D .
We have X X X .
1 2=,… 6 (1) (2) (3) Thus, for x < x < x 1 2 3 » » » » » Në(3 ' U ^W3X U (3 = U (W:X U (: ' ,(> ' U ^W>X U (: ' ì 2 2 2 2 2 Dividing by ϵ3 and letting ε ->> 0, we have / 3!
» fW(3XWfW(:XXfW(>X f(x , x , x ) = n!
f(x f(x ) f(x ); x < x < x (1) (2) (3) 1 2 3 1 2 3 in general, we can extend this idea to a sample of size n and we have for x x … x 1 2 n f x , … x (x , x ,… x ) = n!
f(x )f(x )…f(x ); x(1), (2) (n) 1 2 n 1 2 n x < x <… < x .
1 2 n the marginal density function of X the ith order statistic can be obtained by integrating (i) the probability density function (5) fk-W(X / p…pfkW-X,kW,X…kW|XW(3(:,…(0X q(: q(>…q(0 Examplef k3W3-X W(þX / o…ofkW-X…,kW|XW(3,(:,…(0Xq(3 q(:… q(þJ3q(þ‹3…q(0.
Let x , x , x be a random sample of a random varaible X having the p.d.f.
1 2 3 0 U ( U Find fW(X / ¦01 1n i‚ƒi„…i†i (i) the joint p.d.f.
of X , X and X 1 2 3 (ii) the joint p.d.f of X , X and X the ordered statistics (1) (2) (3) (iii) the marginal p.d.f of X .
(1) Solution < 1 0 U (3,(:,(> fk-,k,,kAW(3,(:,(>X / fk-W(3Xfk,W(:XfkAW(>X'1; < 1 fkW-X,kW,X,kWAXr(W3X,(W:X,(W>Xs / 3!fk-W(3 Xfk,W(:XfkAW(>X; 0 U (3,(:,(> 189 3 3 fk-W(3X / o (cid:9)o 3!q(W>X(cid:10)q(W:X k- k, 3 (cid:11)1 1 :(cid:12) 3!
: / 3!
o W1'(:Xq(: / 3!
'(3 ' (3 / Ž1'2(3 ' (3 (cid:144) k- 2 2 2 : / 3W1'(3X Thus, : 0 U ( U 3W1'(X 1n Another methfokdW- XWo(f Xf/ind ging0 the joint marginal p..d.f of X , X ,… X is by direct (1) (2) (n) i‚ƒi„…i†i reasoning as follows: (i) For X to equal x, I – 1 of (x , x ….x ) should be less than x and n-1 should be (i) 1 2 n greater than x and 1 of them equals x. i – 1 observations n-I observations x (cid:13) (cid:13) P(I – 1 of the Xi’s are less ^th'an :x) = [F(x)]i^-1 = : P(n – 1 of the X’s are greater than x) = [1 – F(x)]n-i i Thus for any given set of partition of (X , X …X ) into 3 as stated above, the required 1 2 n probability is from Chapter 1 section 14 number of ways of partitioning a set of n object into the 3 group above is 0!
WþJ3X!3!W0JþX!.½lÓi) av (6) 0!
(cid:14) þ©3 (cid:14) 0©3 (ii) The joifnkt- pW(.dX.f/.
o fW þXJ3(iX)!
3a!nW0dJ XþX!
( j)ŽI <W (j X(cid:144) Ž1' W(X(cid:144) fW(X.
(i – 1) values (j – i – 1)values (n – j) values x x i j Figure 3 The probability that exactly i of the X’s lie in (-∞, x) and (n-1) lie in (x, ∞) is i nC [F(x)]i[1 – F(x)]n-1 i i since F(x) = P(X ≤ x) = P(any of the x , …, X falls in (-∞, x).
1 n the event {X ≤ x} occurs if and only i or more of the X’s lie in (-∞, x).
Thus (i) i nC [F(x)]j [1 – F(x)]n-j –∞ < x < ∞ (8) j (cid:14) 0(cid:15) IkfW -iX =W( 1X, /weN hra^vWeþX ≤ (s / ∑ ©3 nC [F(x)]j [1 – F(x)]n-j (9) j By Bin(cid:14)okmW-iXaWl( tXh/eo r∑em0(cid:15)© 3 nCj [F(x)]j [1 – F(x)]n-j=1 Thus 0(cid:15) ∑ ©3 nC [F(x)]0 [1 – F(x)]n-0 = 1-[1 – F(x)]n –∞ < x < ∞ (10) j (cid:14) kW-XW(X / 1' 190 Similarly, when we put i = n in (8) [F(x)]n –∞ < x < ∞ (11) By differentiating, w(cid:14)ek hW-aXvW(e X / [1 – F(x)]n-1 f(x) –∞ < x < ∞ fkW-XW(X / (cid:14)k-W(X / =) n[F(x)]n-1f(x) –∞ < x < ∞ (cid:14)kW|XW(X / (cid:14)kW|XW(X 2.
The Joint Distribution Function of X , X .
(1) (n) To find the joint distribution function of X and X , we proceed as follows.
i n {(x , x ) : X ≤ x , x ≤ x } = {(x , x ) : X ≤ x }\{x, y)}: 1 n (1) 1 (n) (n) 1 n (n) (n) X > x , x ≤ x }.
(1) 1 (n) n See figure 5 below.
X (n) X (n) Figure 5 We know that P(X ≤ x ) = [F(x )]n, therefore (n) n n P(X > x , X ) = P(x < X ≤ x , x < x ≤ x … x < X ≤ x ) (1) 1 (n) 1 1 n 1 2 n 1 n n = (F(x ) – F(x ))n n 1 Thus P(X ≤ x , X ≤ x ) = [F(x )]n – {F(x ) - F(x )}n (12) (1) 1 (n) n n n 1 ffrkoW-mXk Ww|XhW(ic3h, (th0eX d/ensity function can be derived by differentiation: : fkW-X,kW,XW(3,(0X / Ú = n(n-1) f(x )f(x ) [F( x ) – F(x(cid:14)k)]W-nX-2k,W |xXW (≤3 x,( 0X (13) 1 n Ún(3Ú(0 1 1 n Distribution Function of the Range of a Random Sample The random variable R, defined by R = X – X is called the range of the observed (n) (1) random variables.
P(R ≤ r) = P(X – X ≤ r) = ∫ (x , x ) dx dx (n) (1) 1 n 1 2 = à [kF|(Jxk-)ã –¡ ffk(xW-X),k]Wn|-2X f(x ) f(x ) dx dx .
n 1 1 n n 1 [ k-‹¡ pJ[ pk- )W)'1X Making the change of variable y = F(X ) – F(x ), dy = f(x ) dx n 1 n n k-‹¡ (cid:16)Wk-‹¡XJ(cid:16)Wk-X 0J: 0J: 1 o Ž(cid:14)W(0X'(cid:14)W(3X(cid:144) fW(0Xq(0 / o v qv / Ž(cid:14)W(3 =†X k- ) '1 0J3 '(cid:14)W(3X(cid:144) 191 Thus F (r) = P(R ≤ r) = n (14) R [ 0J3 F (r) = n(n - 1) pJ[Ž(cid:14)W(3 =†X'(cid:14)W(3X(cid:144) fW(3Xq(3 R p[ W(3XfW(3 =†XŽ W(3 =†X' 0J: J[(cid:14) (cid:14) (cid:14)W(3X(cid:144) q(3 Example 40 Let X , X ,… X be a random sample from uniform distribution on (0, 1).
Find 1 2 n (i) The distribution function of R = X – X (n) (1) (ii) The probability density function of R. Solution (i) P(R ≤ r) = k 0J3 Where F(x)p e=Ž (cid:14)W(3 =†X' (cid:14)W(3X(cid:144) fW(3Xq(3 k F(x + r) = 1 if x + r > 1 pe 11 q( / (; 1 = x + r if x + r < 1.
1 1 Thus P(R ≤ r) = n pe3FJR¡(r†)0 =J 3nq(1( 3–= r))rn-p13 3+J ¡rWn1 '(3X0J3q(3 (ii) f (r) = F’ (r) = .
R R 0J: 0 )W) '1XW1'†X† ≤ † ≤ 1n Another method of fingdi0ng the probability density function of R is by using the formula Û(cid:143)…i†„lƒi (1) for sum of random variables.
[ f(cid:17)W†X / pJ[fkW-X,kW|XW(3,† =(3Xq(3 0J: 0 [ Example 41 / g)W) '1XpJ[0fW(XfW† =(XŽ(cid:14)W† =(X'(cid:14)W(X(cid:144) q( † Y 0 n † U Let X , X , … X be independent random variables having the uniform distribution on (a, 1 2 n b).
Find the pdf of (i) X = min (X , X ,… X ) (ii) X = max (X , X ,… X ).
(1) 1 2 n (n) 1 2 n Solution The pdf of X is 3 The pdf of X(i) is given by fkW(X / —J˜,] U ( U a.
0!
þJ3 0J3 where fkW-XW(X / WþJ3X!W0J3X!
Ž(cid:14)W(X(cid:144) Ž1'(cid:14)W(X(cid:144) fW(XfW(X F(X) = k 3 kJ˜ p˜ —J˜ q( / —J˜ 0J3 0J3 ( '] 1 )Wa '(X fkW-XW(X / )x1' ‘ / 0 ,] U ( U a a '] a '] Wa ']X (ii) |}- 0J3 0W—JkX kJ˜ 3 fkW-XW(X / )x—J˜‘ —J˜ / \0 W—J˜X| ,] U ( U a n i‚ƒi„…i†i 192 Exercise 7.4 1.
Let X , X ,…X be independent and each is uniformly distributed over (a, b).
find 1 2 n the joint p.d.f of X , X ,… X .
(1) (2) (n) 2.
Let X , X ,… X be independent random variables each having an exponential 1 2 n density with parameter λ.
Find the pdf of (i) X = min (X , X ,… X ) (1) 1 2 n (ii) X = max (X , X ,… X ).
n 1 2 n 3.
Let X , X ,… X (n odd) be independent random varaibles each uniformly 1 2 n distributed over (0, 1).
Find the p.d.f of the median of the sample.
Note:When n is odd, the median is ^ |Œ- H M 4.
Show that if n people are distributed at, ra.ndom along road Y miles long, then the probability that no 2 people are less than a distance k miles apart is 0 W0J3X” ¿ 5.
Let X , X ,… X bxe1 a' ra ndom ‘sa,m(cid:18)pUle from uniform distribution on (0, 1).
Show 1 2 2n-1 w 0J3 that the median of the sample has a beta distribution with parameter (n + 1, n + 1).
193 MODULE FOUR UNIT 1 LIMIT THEOREMS The Law of Large Numbers and the Central Limit Theorem The most important theoretical results in probability are limit theorems.
In this chapter we prove often useful tools in probability, the Chebychev’s inequality.
This inequality is then used to deduce the law of large numbers for independent and identically distributed random variables.
In section 8.5 the central limit theorem which is a fundamental result in the theory and applications f probability theory is given.
8.1 Chebychev’s Inequality Chebychev;s inequality gives an upper bound in terms of variance of a random variable X for probability that X deviates from its mean by more than k units.
Theorem 8.1:Chebychev’s Inequality Let X be a random variable (discrete or continuous) with mean µ and variance 2 σ .
Then for any positive number k we have , Á Or equi valentl y P(cid:20)DX-μD Y k} L ”, 3 P(cid:20)DX-μD m k σ}} L ”,.
Proof: Let X be a non-negative random variable such that E(X) = µ <∞.
Define another random variable Y as follows: 0 if X < k Y = k if X > k This new variable Y is a discrete variable having two values 0,k.
The probability density function of Y can be written thus: y 0 k P(Y = y) P(X < k) P(X > k) Hence, E(Y) =OP(X < k) + kP(X > k) That is E(Y) = kP(X ≥ k).
194 Since the variable X ≥ Y for all possible values, we have E(X) ≥ E(Y) = kP(X ≥ k), Thus, P(X ≥ k) ≤ .
(2) WIX (cid:22) ” Equation (2) is called the Markov inequality and can be generalized thus: For any j ≥ 0, k > 0 P{ W ≥ k} ≤ (3) é (cid:24) D D (cid:22)(cid:23) (cid:23) The proof of Chebychev’s inequality is an immedia”t(cid:24)e consequences of (3) by putting j = 2 and W = X - µ, we obtain P{ X - µ ≥ k} ≤ WIJ , , D D (cid:22) KX Á Since (X - µ)2 ≥ k2<==> X-µ > k, we have , / , ” ” P{ DX-µ D> k } = P {(X - µ)2 ≥ k2} ≤ .
, Á Thus D D , ” P{ X-µ ≤ k } ≤ .
, Á D D , ” Example 8.1 Let X be a random variable having Poisson distribution with mean λ and variance 2.
Use Chebychev’s inequality to show that (i) P{P X-λ ≥ 1} ≤ λ (ii) P σ >j 5 D D H^ Y M ≤ .
: j Solution .
Using Chebychev’s inequality with µ = λ, and 2 = λ we have (i) P{ X-µ ≥ k } ≤ .
σ j PuDtting kD = 1 we obt,ain ” P{ X-µ ≥ k } ≤ λ.
(ii) D D 2 2 σ 4 2 4(cid:18) O(cid:25)in(cid:143) sku2b/st iλtu /ti/ng/ YforÂ 2/ w eh have σλ2 = 4k2, k2 = .
(cid:26) : (cid:26) (cid:18) H M , / : : Thus P{ X-µ ≥ } ≤ .
j 5 Since X-λ ≥ if and only if X-λ <D or DX-λ>: λ/2j, we see that j Jj D D : P{ X-λ ≥: } = P{X-λ <- j j j n D =D P( : :M= NH^ ' h Y :M j >j 5 Since P H^ U :M= NH^ Y :M ≤ j j H^ U :M m,„ i …]ÓPi >j 5 H^ Y M ≤ .195 : j 8.2 The Law of Large Numbers of Bernoulli Trails Let X , X ,…, X be n independent and identically distributed Bernoulli random variables 1 2 n and let X = X + X +…+ X be Binomial random variables (number of successes), with 1 2 n parameters n and p. The mean and variable of X are np and npq respectively.
The mean µ grows as n increases but the standard deviation grows only as , Using Chebychev’s inequality √) P{ X - np > ε} ≤ 0 D D (cid:27),(cid:28) 0 (cid:13) ≤ (cid:13),.
Theorem 8.2 Let X be the number of successes in n independent Bernoulli trails with probability p of success.
For any ε > 0.
(cid:27)(cid:28) (cid:29)I (cid:29) (cid:30) 3 and N¦ ' N Y ¸ ≤ (cid:31), ≤ (cid:31), 0 0 5 0 (cid:29)I (cid:29) (cid:30) 0 Proof: lim0Z[N¦ ' N Y ¸ / 0 Applying Chebychev’s inequality I I (cid:27)(cid:28) üH M / N,ˆ]†H M / 3 0 0 ,.
)_ / 0 0 ± (cid:27)(cid:28) æ˜¡H M | 0 », / 0»,'Y ]ƒ )'Y ∞ 3 0 Hence _ / W1'_X_ ≤ fÛ† ]‚‚ _, ≤ _ U 1 5 I 0 This means that for Nla¦rg¹0e 'nu m_¹bmer »w¸e' cYan b ]eƒ a)lm'oYst ∞certain that will be very close to I probability p of success.
This shows that the relative frequency of success in independent 0 Bernoulli trails converges (in a probabilistic sense) to the theoretical probability p of success at each trail.
This important result can be stated more precisely as the law of large numbers.
The Law of Large Numbers 8.3 The Weak Law of Large Numbers: Let X , X ,…, X be independent and identically distribution random variables with all 1 2 n E(X2) < ∞ and let S – X + X +…+ X .
i n 1 2 h We know that E(X ) = E(X ) = … E(X ), E(S ) = nE(X ) and Var(S ) = nVarX 1 2 n n 1 n 1 ó| 3 (cid:1) 3 üVaHr0M / 0 üW 0X / 0.)
üW^3X / üW^3X ó| 3 (cid:1) 0æ˜¡I- 3 This shows that Hex0pMec/ta t0i,on ˆ ]o†f W 0iXs /eq ua0l ,to /th e0 e ˆx]p†ecWt^a3tiXon of X and the standard i ó deviation of is 0 ó| 0 196  æ˜¡I ¦ ¸ / ó(cid:141)€I- Which tends to zero as n tends to(cid:8) infin0ity.
T hu√s0, the distribution of Sn/n becomes more and more concentrated near E(X ).
1 We state the more general result in the following theorem.
Theorem 8.3: A (WLLN) Weak Law of Large Numbers for Independent Random Variables.
Let X , X …X be independent and identically distributed random variables 1 2 n with finite mean µ and σ2.
Let S = X – X + … + X .
then for δ > 0. n 1 2 n (4) ó| 0 Proof: lim0Z[NH¹ ' !¹ Y "M / 0 Applying Chebychev’s inequality to , we have ó| 0 ó| / üH M ó| 3 !
0 .ˆ]†H M / , ˆ]†W(cid:1)0X 0 0 , 3 :Á ± N¦¹ (cid:1)0 – üW^X¹ Y "¸ ≤ , ” Á 0 For at least one k satisfying n ≤ k ≤ m}.
Corollary 1: Let f(x) be a continuous real function on [0, 1].
Then as n -> ∞ uniformly with respect to p ε [0, 1] ó| Corollary II: Weierstress Aüp¦pfrHoxiMm¸aZtiofnW _TXh.eorem.
0 Let f(X) be a continuous real function on [0, 1] defined on the real interval 0 ≤ p ≤1 and let P (p) = nC pk (1 – p)n – k. n k 0 ” Then P (p) -> ∞ f(p) as n -> ∞ and∑ ”th©ee fcoHnvMergence is uniform in p. P (x) is called the n 0 n Bernstein Polynomials.
Theorem 8.5 A WLLN for independent but not necessarily identical random variables.
Let X , X ,…X be independent random variables with all 1 2 n E((X2) < ∞ and let S = X + X +… + X and µ = E then n 1 2 n ó| H0M, ó| 3 0 When VNa¦r¹X ≤' M!
0fo¹rm a l»l ¸I, ≤tha t i,s w,(cid:20)hˆe]n† tWh^e3 vXa=ria nˆc]e†sW a^r:eX a=ll b(cid:157)ou=ndˆe]d† Wb^ya0 Xc°o.n(cid:30)sYtant.
which 0i » 0 does not depend on n, then we have # ó| 0 N¦¹ '!0¹ m »¸ ≤ , 'Y ]ƒ )'Y ∞.
0 » 0 8.5 The Central Limit Theorem The central limit theorem is a fundamental result in the theory and application of probability.
The law of large numbers asserts that in a series of n independent trails ó| 197 0with constant probability p of success in one trail tends (in a certain probabilistic sense) to p as n increases.
But this assertion does not tell us anything about the distribution of ó| 0 as n becomes large.
The answer to this question is given by the so called central limit theorem.
The theorem asserts that under quite general conditions the sum of independent variables has the Normal distribution in the limit.
Theorem 8.6: Central limit theorem Let X , X , …, X be a sequence of independent random variables with E(X) = µ and 1 2 n i Var(X) = σ2 I = 1, 2,…Let S = X + X +…+ X .
Then under general conditions, i i n 1 2 n 0 Æ0 / (cid:1)0 ' üW(cid:1)0X (cid:1)0 ' ∑þ©3!þ / 0 : Has approximately the standard `Noˆrm]a†l d(cid:1)i0stribution.
`∑þ©3Âþ The Identically Distribution Case Theorem 8.7 Let X , X , …, X be independent identically distributed random variables.
Then 1 2 n $ (i) (ii) | ó|J 0K | J K Æ0 / , Æ0 / , √0Á `Á /0 tend to the standard normal distribution as n -> ∞.
What is remarkable about this theorem is that all we need to know about X is its mean and variance and there is no need to specify the nature of its distribution.
Also X can be discrete, continuous or both.
The outline of the proof is given below, 198  Proof: Let M ,(t) = E(etX), S (t) = [E(etX)]n X n Let Z = n ò0 J 0K ò0 K The m.g.f √o0fÁ Z, is/ √0Á ' √).Á.
n 0 W`0K/ÁX(cid:141) (cid:141) ™È|W(cid:143)X / i x™IH M‘ Á√0 Taking log we have K 3 Using Maclaurin seri(cid:29)e)s ™È|W(cid:143)X / '√)Á(cid:143) ') (cid:29)) ™IHÁ√0M.
M(t) = 1 + M’(0)t + M’’ , WeX(cid:141) =(cid:157) From above we have :!
(7) , , , K(cid:141) K(cid:141) rK J Á s(cid:141) where A is th(cid:29)e) r™emÈ|aWi(cid:143)nXd/er 'te√rm) fÁro(cid:143)m= M) (cid:29)a)clxa1u ri=n sÁe√ri0e s=.
S im:il0aÁr,ly, w=he n% ‘the Maclaurin series In (1 + x) = x - , A I I = =(cid:157) : > is used to expand In , , , , , K(cid:141) WK ‹ Á X K(cid:141) rK ‹ Á s(cid:141) H1 = = , (cid:143) = %M.ƒi(cid:143)(cid:143)l)½ ( / = , = %, we find that √0Á 0 Á √0Á :0Á In , , K(cid:141) WK ‹ Á X H1 = = , (cid:143) = %M √0Á 0 Á = - , , , , : K(cid:141) WK ‹ Á X 3 K(cid:141) WK ‹ Á X = , (cid:143) = % x = , (cid:143) = %‘ =(cid:157) It √is0 Áleft as a0n Á exercise to sho:w√ 0thÁat for l0a rÁge n. , , K(cid:141) WK ‹ Á X ¹ = , = %¹ U 1.
On substituting in (7) √w0Áe obtain: 0 Á 199 ) !
(cid:143) (cid:29) ™È|W(cid:143)X / '√) Â : : : : : : !
(cid:143) W! '
!
X 1 !
(cid:143) W!
= Â X(cid:143) & =) ¨ = : = %' (cid:9) = : = %(cid:10) = … It can easily be showÂn√ th)at as n -2>) ∞Â 2 Â√) 2)Â , (cid:29) (cid:141) ) ™È|W(cid:143)X / .
Hence, : , (cid:141) /: lim ™È|W(cid:143)X / i , the moment generating function 0Zof[ a random variable having the standard normal distribution.
Theorem Let X - n > 1, and X be random variables such that n ' lim0Z[™I W(cid:143)X / ™IW(cid:143)X,'∞ U (cid:143) U .
Then Lim is continuous.
(cid:14) (cid:14) (cid:14) ∞ ^)(/ ^(](cid:143) ]‚‚ _Ûl)(cid:143)ƒ „…i†i ^( Example 8.2 A boy throws a fair die 100 times.
What is the probability that his mean score will exceed 3.
Let X represent the outcome of each toss.
Possible values of X are 1, 2, 3, 4, 5, 6.
E(X) = 3.5.
Var(X) = 2.92.
That is µ = 3.5. σ2 = 2.92 Let X be the mean score.
By the central limit theorem X is normally distributed with mean 3.5 and Varσ2/n.
, Á :.d: / .
Hence, 0 3ee IýJ >.4 Æ / has a standard normal distribution√.e .e:d Je.4 0 NW^ý Y 3X / NHÆ Y M / NWÆ Y '2.94X / .9984. e.3Ve 200 Example 8.3 Marks in an I.Q examination are normally distributed with mean 55 and standard deviation 10.
What is the probability that (i) the mean mark of a group of 10 students will be above 50 (ii) the mean mark of a group of 20 students will be between 40 and 50.
(iii) the sum of the marks of the 10 students will be less than 500.
Solution Let X , X ,… X be the mark scored by each student respectively.
1 2 10 E(X ) = E(X ) =…= E(X ) = 55 1 2 10 ÂI- / 10.
ÂI, / ÂI| / (i) = mean, be the central limit theory (assuming n = 10 is large enough for this ^ý distribution) Iý J 44 has a standard Normal distribution.
Á/√0 IýJ 44 Iý J 44 / 3e/√3e √3e Iý J 44 4e J 44 NW^ý Y 50X / NH Y M / 0.9306.
√3e √3e (ii) The mean mark of a group of 20 students between will be between 40 and 50.
In this case, n = 20.
NW40 U ^ý U 50X / NW^ý U 50X' NW^ý L 40X 5e J 44 NW^ý L 40X / NHÅ L M / NWÆ L '3 √5X / 0.00 √4 4e J 44 NW^ý U 50X / NHÅ U M / NWÆ U ' √5X / 0.01 √4 Hence P(40 < < 50) = 0.01 – 0.00 = 0.01 ^ý (iii) S = X + X +…+ X 10 1 2 10 has a standard Normal ó-.
J 0K √0Á, ó-.
J 44e 4ee J 44e J4e / / / '1.53 √3eee 3e√3e 3e√3e Hence, P(S < 500) = 0.057. n 201 8.6 Normal Approximately to the Binomial Distribution Let X , X , …, X be n independent identically distributed Bernoulli trails where 1 2 n 1 with probability P X = i 0 with probability 1 - P E(X) = p and Var(X) = p(1 – p).
From theorem 2, the distribution i i --> standard Normal.
ó| J (cid:22)Wó|X , where S = X +√ X0Á + … + X n 1 2 n E(S ) = np n Var(S ) = np(1 – p), σ2 = p(1 – p).
n Note that S has binomial distribution with mean np and variance np(1-P).
n P(S ≤ y) = nC Pk (1 – p)n-k n k w For large n ∑”©e P(S ≤ y) = n w J 0(cid:27) ( NgÆ0 ≤ `0(cid:27)W3 J (cid:27)X (cid:27) ) w J 0 / H (cid:27) (cid:27) M `0 W3 J X where Z is standard Normal random variable.
Thus, from above we have n nC pk (1 – p)n-k = k (cid:27) (cid:14) (cid:1) w ) w J 0 ó|WvX / NW 0 ≤ vX / ∑”©e H0(cid:27)W3 J (cid:27)XM Example 8.3 It is claimed that 60% of the voters I a given ward are going to vote for party A. assuming that all voters will vote, and that there are 100 voters.
What is the probability that Party A receives at least 50 votes.
Let p = 0.6. prob.
Of voting for A. n = 100.
Assuming independent voting C pk (1 –p)100-k = C (0.6)k(0.4)100-k 3ee k 3ee k (cid:1) 50X / ”©4e100 ∑”©4e100 NUWsin0gY normal a p∑proximation (6) above, we have P(S > 50) = n 4e – 3ee 9e.< 4e J <e NHÆ m M / NHÆ m M / 0.9793 `3eeWe.<XWe.5X √:5 8.7 The Normal Approximation to the Poisson Distribution 202 If a random variable X has a poisson distribution whose mean is λ, then for large λ the standardized random variable.
Æ / I J j has a standard Normal Distr√ijbution.
you will recall that E(X) = λ , Var(X) = λ, Continuity correction may be introduced.
It has been shown that the approximation is good for λ > 5.
Example 8.4 A system suffers random breakdown at a constant rate of 10 per month.
Find the probability that there will be at least 8 breakdowns in any month.
Let X be the number of time the system breaks down in any month.
Then X has a poisson distribution with λ = 10.
Thus -.
“ [ ~ W3eX NW^ Y 8X / ∑”©; Using Normal approximation, we have ”!
has N(0.1) I J 3e √3e ; J 3e J: 0 NW^ m 8X / NHÆ Y M / NHÆ Y M / .7357 Exercise 8 √3e √3e 1.
It is claimed that 30% of the voters in a given local government area are going to vote for party A in a local government election.
Assuming that all voters will vote, there are 4000 voters and the claim is based on a proper (unbiased) sampling method.
What is the probability that Party A will receive more than 1,500 votes.
2.
(a) Suppose that a system consists of components each of which has a probability of 0.05 of failing during a specific time.
The system functions properly when at least 150 components function.
Assuming these components function independently of one another, what is the probability that the system functions properly during a specific time.
(b) Suppose that the above system is made of n components each having probability of 0.90 of not failing during a given time.
The system will 203 function if at least 80 percent of the components function properly.
Determine n so that the probability that the system functions properly during a specific time is 0.96.
3.
Let X be a non-negative interger-valued random variable whose probability generating function P (s) = E(sX) is finite all s. X Use Chebychev’s inequality to verify the following inequalities: (a) any positive inequalities: NW^ *±WòX 0 ≤ (cid:18)X ≤ “ .
≤ ƒ ≤ 1,(cid:18) ó (b) *±WòX NW^ Y (cid:18)X ≤ “ .ƒ Y 1 4.
Prove the Chebychev’só inequality.
Let X be a random variable (discrete or continuous) with E(X) = µ and Var(X) = σ2.
Then for any positive constant number k we have P{|X - µ| ≥ k σ} ≤ 3 5.
Prove that (i) 1 – x ≤ e-x for real x, (ii) lo”,g.
x ≤ x -1 for x > 0, and (iii) , Ñ(cid:2)(cid:3) (cid:2) J∑ -I ∏þ©3^þ ≤ i lf ^þ ≤ 1,Ò / 1,2,… 204
